
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An online course on introduction to data science with Python.  Extensive use of AI tools and MicroSims to help you learn.">
      
      
        <meta name="author" content="Dan McCreary">
      
      
        <link rel="canonical" href="https://dmccreary.github.io/data-science-course/chapters/11-nonlinear-models-regularization/">
      
      
        <link rel="prev" href="../10-numpy-computing/">
      
      
        <link rel="next" href="../12-intro-to-machine-learning/">
      
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>Non-linear Models - AI Based Data Science with Python</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-RTBCWGJKKR"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-RTBCWGJKKR",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-RTBCWGJKKR",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Non-linear Models - AI Based Data Science with Python" >
      
        <meta  property="og:description"  content="An online course on introduction to data science with Python.  Extensive use of AI tools and MicroSims to help you learn." >
      
        <meta  property="og:image"  content="https://dmccreary.github.io/data-science-course/assets/images/social/chapters/11-nonlinear-models-regularization/index.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://dmccreary.github.io/data-science-course/chapters/11-nonlinear-models-regularization/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Non-linear Models - AI Based Data Science with Python" >
      
        <meta  name="twitter:description"  content="An online course on introduction to data science with Python.  Extensive use of AI tools and MicroSims to help you learn." >
      
        <meta  name="twitter:image"  content="https://dmccreary.github.io/data-science-course/assets/images/social/chapters/11-nonlinear-models-regularization/index.png" >
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#non-linear-models-and-regularization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AI Based Data Science with Python" class="md-header__button md-logo" aria-label="AI Based Data Science with Python" data-md-component="logo">
      
  <img src="../../img/logo-192.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Based Data Science with Python
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Non-linear Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dmccreary/data-science-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AI Based Data Science with Python" class="md-nav__button md-logo" aria-label="AI Based Data Science with Python" data-md-component="logo">
      
  <img src="../../img/logo-192.png" alt="logo">

    </a>
    AI Based Data Science with Python
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dmccreary/data-science-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Chapters
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Chapters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-intro-to-data-science/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Data Science
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-python-environment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Environment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-python-data-structures/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Data Structures
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-data-cleaning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Cleaning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-data-visualization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Visualization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-statistical-foundations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistical Foundations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-simple-linear-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simple Linear Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-model-evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Evaluation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-multiple-linear-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multiple Linear Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10-numpy-computing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NumPy Computing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Non-linear Models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Non-linear Models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-covered" class="md-nav__link">
    <span class="md-ellipsis">
      Concepts Covered
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-beyond-the-straight-line" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction: Beyond the Straight Line
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-regression-when-lines-arent-enough" class="md-nav__link">
    <span class="md-ellipsis">
      Non-linear Regression: When Lines Aren't Enough
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#polynomial-regression-curves-through-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Polynomial Regression: Curves Through Linear Regression
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#degree-of-polynomial-how-much-flexibility" class="md-nav__link">
    <span class="md-ellipsis">
      Degree of Polynomial: How Much Flexibility?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Degree of Polynomial: How Much Flexibility?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-polynomial-degree-explorer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Polynomial Degree Explorer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#curve-fitting-the-art-and-science" class="md-nav__link">
    <span class="md-ellipsis">
      Curve Fitting: The Art and Science
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformation-changing-the-shape-of-data" class="md-nav__link">
    <span class="md-ellipsis">
      Transformation: Changing the Shape of Data
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#log-transformation-the-exponential-tamer" class="md-nav__link">
    <span class="md-ellipsis">
      Log Transformation: The Exponential Tamer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-transformation-engineering-better-inputs" class="md-nav__link">
    <span class="md-ellipsis">
      Feature Transformation: Engineering Better Inputs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Feature Transformation: Engineering Better Inputs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-transformation-gallery" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Transformation Gallery
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-flexibility-the-complexity-dial" class="md-nav__link">
    <span class="md-ellipsis">
      Model Flexibility: The Complexity Dial
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization-taming-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization: Taming Overfitting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ridge-regression-the-l2-penalty" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge Regression: The L2 Penalty
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lasso-regression-the-l1-penalty" class="md-nav__link">
    <span class="md-ellipsis">
      Lasso Regression: The L1 Penalty
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lasso Regression: The L1 Penalty">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-ridge-vs-lasso-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Ridge vs Lasso Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#elastic-net-the-best-of-both-worlds" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net: The Best of Both Worlds
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lambdaalpha-parameter-finding-the-sweet-spot" class="md-nav__link">
    <span class="md-ellipsis">
      Lambda/Alpha Parameter: Finding the Sweet Spot
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lambda/Alpha Parameter: Finding the Sweet Spot">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-lambda-tuning-playground" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Lambda Tuning Playground
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#shrinkage-what-regularization-actually-does" class="md-nav__link">
    <span class="md-ellipsis">
      Shrinkage: What Regularization Actually Does
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#putting-it-all-together-a-complete-workflow" class="md-nav__link">
    <span class="md-ellipsis">
      Putting It All Together: A Complete Workflow
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Putting It All Together: A Complete Workflow">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-regularization-decision-tree" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Regularization Decision Tree
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-pitfalls-and-best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Common Pitfalls and Best Practices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-your-regularization-toolkit" class="md-nav__link">
    <span class="md-ellipsis">
      Summary: Your Regularization Toolkit
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#looking-ahead" class="md-nav__link">
    <span class="md-ellipsis">
      Looking Ahead
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../12-intro-to-machine-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intro to Machine Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13-neural-networks-pytorch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural Networks and PyTorch
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../chapters-v1/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Old v1 Content
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../labs/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Labs
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../sims/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    MicroSims
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../learning-graph/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Learning Graph
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../prompts/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Sample GenAI Prompts
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frequently Asked Questions
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../how-we-built-this-site/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How We Built This Site
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checklist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customization Checklist
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../license/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    License
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    References
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checklist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customization Checklist
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-covered" class="md-nav__link">
    <span class="md-ellipsis">
      Concepts Covered
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-beyond-the-straight-line" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction: Beyond the Straight Line
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-linear-regression-when-lines-arent-enough" class="md-nav__link">
    <span class="md-ellipsis">
      Non-linear Regression: When Lines Aren't Enough
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#polynomial-regression-curves-through-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Polynomial Regression: Curves Through Linear Regression
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#degree-of-polynomial-how-much-flexibility" class="md-nav__link">
    <span class="md-ellipsis">
      Degree of Polynomial: How Much Flexibility?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Degree of Polynomial: How Much Flexibility?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-polynomial-degree-explorer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Polynomial Degree Explorer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#curve-fitting-the-art-and-science" class="md-nav__link">
    <span class="md-ellipsis">
      Curve Fitting: The Art and Science
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformation-changing-the-shape-of-data" class="md-nav__link">
    <span class="md-ellipsis">
      Transformation: Changing the Shape of Data
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#log-transformation-the-exponential-tamer" class="md-nav__link">
    <span class="md-ellipsis">
      Log Transformation: The Exponential Tamer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feature-transformation-engineering-better-inputs" class="md-nav__link">
    <span class="md-ellipsis">
      Feature Transformation: Engineering Better Inputs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Feature Transformation: Engineering Better Inputs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-transformation-gallery" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Transformation Gallery
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-flexibility-the-complexity-dial" class="md-nav__link">
    <span class="md-ellipsis">
      Model Flexibility: The Complexity Dial
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization-taming-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization: Taming Overfitting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ridge-regression-the-l2-penalty" class="md-nav__link">
    <span class="md-ellipsis">
      Ridge Regression: The L2 Penalty
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lasso-regression-the-l1-penalty" class="md-nav__link">
    <span class="md-ellipsis">
      Lasso Regression: The L1 Penalty
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lasso Regression: The L1 Penalty">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-ridge-vs-lasso-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Ridge vs Lasso Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#elastic-net-the-best-of-both-worlds" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net: The Best of Both Worlds
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lambdaalpha-parameter-finding-the-sweet-spot" class="md-nav__link">
    <span class="md-ellipsis">
      Lambda/Alpha Parameter: Finding the Sweet Spot
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lambda/Alpha Parameter: Finding the Sweet Spot">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-lambda-tuning-playground" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Lambda Tuning Playground
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#shrinkage-what-regularization-actually-does" class="md-nav__link">
    <span class="md-ellipsis">
      Shrinkage: What Regularization Actually Does
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#putting-it-all-together-a-complete-workflow" class="md-nav__link">
    <span class="md-ellipsis">
      Putting It All Together: A Complete Workflow
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Putting It All Together: A Complete Workflow">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-regularization-decision-tree" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Regularization Decision Tree
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#common-pitfalls-and-best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Common Pitfalls and Best Practices
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-your-regularization-toolkit" class="md-nav__link">
    <span class="md-ellipsis">
      Summary: Your Regularization Toolkit
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#looking-ahead" class="md-nav__link">
    <span class="md-ellipsis">
      Looking Ahead
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/dmccreary/data-science-course/blob/master/docs/chapters/11-nonlinear-models-regularization/index.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="non-linear-models-and-regularization">Non-linear Models and Regularization</h1>
<hr />
<p>title: Non-linear Models and Regularization
description: Bend the line and tame the beast - mastering curves and preventing overfitting
generated_by: chapter-content-generator skill
date: 2025-12-15
version: 0.03</p>
<hr />
<h2 id="summary">Summary</h2>
<p>This chapter expands modeling capabilities beyond linear relationships. Students will learn polynomial regression for capturing non-linear patterns, various transformation techniques, and the concept of model flexibility. The chapter introduces regularization as a technique for preventing overfitting, covering Ridge regression, Lasso regression, and Elastic Net. By the end of this chapter, students will understand how to balance model complexity with generalization and apply regularization to improve model performance.</p>
<h2 id="concepts-covered">Concepts Covered</h2>
<p>This chapter covers the following 15 concepts from the learning graph:</p>
<ol>
<li>Non-linear Regression</li>
<li>Polynomial Regression</li>
<li>Degree of Polynomial</li>
<li>Curve Fitting</li>
<li>Transformation</li>
<li>Log Transformation</li>
<li>Feature Transformation</li>
<li>Model Flexibility</li>
<li>Regularization</li>
<li>Ridge Regression</li>
<li>Lasso Regression</li>
<li>Elastic Net</li>
<li>Regularization Parameter</li>
<li>Lambda Parameter</li>
<li>Shrinkage</li>
</ol>
<h2 id="prerequisites">Prerequisites</h2>
<p>This chapter builds on concepts from:</p>
<ul>
<li><a href="../08-model-evaluation/">Chapter 8: Model Evaluation and Validation</a></li>
<li><a href="../09-multiple-linear-regression/">Chapter 9: Multiple Linear Regression</a></li>
</ul>
<hr />
<h2 id="introduction-beyond-the-straight-line">Introduction: Beyond the Straight Line</h2>
<p>You've mastered linear regression—congratulations! But here's the truth: the real world doesn't always follow straight lines. House prices don't increase linearly with size forever. Learning curves flatten out. Population growth accelerates and then stabilizes. To model these patterns, you need curves.</p>
<p>This chapter gives you two new superpowers:</p>
<ol>
<li><strong>Bending the line</strong>: Using polynomial regression and transformations to capture curved relationships</li>
<li><strong>Taming the beast</strong>: Using regularization to prevent models from going wild with overfitting</li>
</ol>
<p>Together, these techniques let you build models that are flexible enough to capture complex patterns yet disciplined enough to generalize to new data. It's a delicate balance—and by the end of this chapter, you'll be a master at finding it.</p>
<h2 id="non-linear-regression-when-lines-arent-enough">Non-linear Regression: When Lines Aren't Enough</h2>
<p><strong>Non-linear regression</strong> refers to any regression approach where the relationship between features and target isn't a simple straight line. Look at this data:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="c1"># Generate curved data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;This Data Needs a Curve, Not a Line!&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>If you fit a straight line to this data, you'll miss the curve entirely. The model will systematically underpredict in some regions and overpredict in others. That's not a random error—it's a sign that your model isn't flexible enough.</p>
<p>Non-linear regression captures these curved patterns by:</p>
<ul>
<li>Adding polynomial terms (x², x³, etc.)</li>
<li>Transforming features (log, square root, etc.)</li>
<li>Using inherently non-linear models (which we'll cover in later chapters)</li>
</ul>
<p>The key insight: even though the relationship is curved, we can still use linear regression techniques! We just need to transform our features first.</p>
<h2 id="polynomial-regression-curves-through-linear-regression">Polynomial Regression: Curves Through Linear Regression</h2>
<p><strong>Polynomial regression</strong> is a clever trick: we create new features by raising the original feature to different powers, then use regular linear regression on these expanded features.</p>
<p>For a single feature <span class="arithmatex">\(x\)</span>, polynomial regression of degree 3 looks like:</p>
<div class="arithmatex">\[y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3\]</div>
<p>This is still "linear" regression in the sense that it's linear in the <em>coefficients</em> (β values). But the resulting curve can bend and twist to fit complex patterns.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Create polynomial regression pipeline</span>
<span class="n">poly_model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly_features&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Reshape x for sklearn</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">poly_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Generate predictions for smooth curve</span>
<span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">poly_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_plot</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Polynomial Fit&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Polynomial Regression: Curves that Fit!&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>The magic happens in <code>PolynomialFeatures</code>—it takes your original feature and creates new columns for each power up to the specified degree.</p>
<h2 id="degree-of-polynomial-how-much-flexibility">Degree of Polynomial: How Much Flexibility?</h2>
<p>The <strong>degree of polynomial</strong> controls how flexible your curve can be:</p>
<ul>
<li><strong>Degree 1</strong>: Straight line (regular linear regression)</li>
<li><strong>Degree 2</strong>: Parabola (one bend)</li>
<li><strong>Degree 3</strong>: S-curve possible (two bends)</li>
<li><strong>Degree 4+</strong>: Increasingly complex curves</li>
</ul>
<p>Here's the critical trade-off:</p>
<table>
<thead>
<tr>
<th>Degree</th>
<th>Flexibility</th>
<th>Risk of Underfitting</th>
<th>Risk of Overfitting</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Low</td>
<td>High</td>
<td>Low</td>
</tr>
<tr>
<td>2-3</td>
<td>Medium</td>
<td>Medium</td>
<td>Medium</td>
</tr>
<tr>
<td>5-7</td>
<td>High</td>
<td>Low</td>
<td>Medium-High</td>
</tr>
<tr>
<td>10+</td>
<td>Very High</td>
<td>Very Low</td>
<td>Very High</td>
</tr>
</tbody>
</table>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># Fit polynomials of different degrees</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">degree</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_plot</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span>
                             <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Degree </span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)))</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Effect of Polynomial Degree on Fit&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>Notice how degree 10 goes wild, trying to pass through every data point? That's overfitting in action. The curve fits the training data perfectly but would fail miserably on new data.</p>
<h4 id="diagram-polynomial-degree-explorer">Diagram: Polynomial Degree Explorer</h4>
<details>
<summary>Polynomial Degree Explorer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Apply, Evaluate</p>
<p>Learning Objective: Interactively explore how polynomial degree affects curve flexibility and the bias-variance tradeoff</p>
<p>Canvas Layout (850x550):
- Main area (850x400): Scatter plot with polynomial curve
- Bottom area (850x150): Controls and metrics</p>
<p>Main Visualization:
- Data points (20-50 points) with some noise
- Polynomial curve that updates in real-time
- Shaded confidence region showing uncertainty
- Residual lines from points to curve (optional toggle)</p>
<p>Interactive Controls:
- Slider: Polynomial Degree (1 to 15)
- Dropdown: Dataset type (linear, quadratic, cubic, sine wave, step function)
- Slider: Noise level (0 to high)
- Button: "Generate New Data"
- Checkbox: "Show Train/Test Split"</p>
<p>Metrics Panel:
- Training R²: updates live
- Test R²: updates live (when split enabled)
- Number of coefficients: degree + 1
- Visual warning when overfitting detected (train &gt;&gt; test)</p>
<p>Educational Overlays:
- At degree 1: "Underfitting: Missing the curve"
- At degree 2-4: "Good fit for this data"
- At degree 10+: "Overfitting: Chasing noise!"
- Arrow pointing to where train/test scores diverge</p>
<p>Animation:
- Smooth curve transition when degree changes
- Coefficients displayed with size proportional to magnitude</p>
<p>Implementation: p5.js with polynomial fitting</p>
</details>
<h2 id="curve-fitting-the-art-and-science">Curve Fitting: The Art and Science</h2>
<p><strong>Curve fitting</strong> is the process of finding the mathematical function that best describes your data. While polynomial regression is one approach, the broader goal is matching the right curve shape to your data's underlying pattern.</p>
<p>Good curve fitting requires:</p>
<ol>
<li><strong>Visual inspection</strong>: Plot your data first! What shape does it suggest?</li>
<li><strong>Domain knowledge</strong>: Does theory predict a certain relationship?</li>
<li><strong>Validation</strong>: Does the curve generalize to new data?</li>
<li><strong>Parsimony</strong>: Prefer simpler curves when they fit adequately</li>
</ol>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Find optimal degree using cross-validation</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">)</span>
    <span class="n">cv_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Plot CV scores vs degree</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">degrees</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">cv_scores</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Cross-Validation Score vs Polynomial Degree&#39;</span><span class="p">,</span>
              <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="s1">&#39;Polynomial Degree&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="s1">&#39;CV R² Score&#39;</span><span class="p">})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Find best degree</span>
<span class="n">best_degree</span> <span class="o">=</span> <span class="n">degrees</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal degree: </span><span class="si">{</span><span class="n">best_degree</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>The CV score typically rises, peaks, then falls as degree increases. The peak is your sweet spot—enough flexibility to capture the pattern, not so much that you're fitting noise.</p>
<h2 id="transformation-changing-the-shape-of-data">Transformation: Changing the Shape of Data</h2>
<p><strong>Transformation</strong> is a broader technique for handling non-linear relationships. Instead of adding polynomial terms, we transform the original variables to make the relationship more linear.</p>
<p>Common transformations include:</p>
<table>
<thead>
<tr>
<th>Transformation</th>
<th>Formula</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Log</td>
<td><span class="arithmatex">\(\log(x)\)</span></td>
<td>Exponential growth, multiplicative effects</td>
</tr>
<tr>
<td>Square root</td>
<td><span class="arithmatex">\(\sqrt{x}\)</span></td>
<td>Count data, variance stabilization</td>
</tr>
<tr>
<td>Reciprocal</td>
<td><span class="arithmatex">\(1/x\)</span></td>
<td>Inverse relationships</td>
</tr>
<tr>
<td>Power</td>
<td><span class="arithmatex">\(x^n\)</span></td>
<td>Accelerating/decelerating patterns</td>
</tr>
<tr>
<td>Box-Cox</td>
<td><span class="arithmatex">\((x^\lambda - 1)/\lambda\)</span></td>
<td>General normalization</td>
</tr>
</tbody>
</table>
<p>The key insight: if your scatter plot curves, the right transformation can straighten it—making linear regression appropriate again.</p>
<h2 id="log-transformation-the-exponential-tamer">Log Transformation: The Exponential Tamer</h2>
<p>The <strong>log transformation</strong> is probably the most useful transformation in data science. It's perfect when:</p>
<ul>
<li>Your data spans several orders of magnitude (1 to 1,000,000)</li>
<li>The relationship looks exponential</li>
<li>You want to interpret coefficients as percentage changes</li>
<li>Residuals show increasing variance (heteroscedasticity)</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">from</span> <span class="nn">plotly.subplots</span> <span class="kn">import</span> <span class="n">make_subplots</span>

<span class="c1"># Generate exponential-ish data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y_exp</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">x_exp</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Create subplots</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">make_subplots</span><span class="p">(</span><span class="n">rows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">subplot_titles</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Original Scale (Curved)&#39;</span><span class="p">,</span> <span class="s1">&#39;Log-Transformed (Linear!)&#39;</span><span class="p">])</span>

<span class="c1"># Original scale</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_exp</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_exp</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Original&#39;</span><span class="p">),</span>
              <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Log-transformed</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_exp</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_exp</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Log(y)&#39;</span><span class="p">),</span>
              <span class="n">row</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;The Magic of Log Transformation&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>Notice how the curved relationship becomes nearly linear after log transformation? Now regular linear regression will work beautifully.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Log-linear regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Transform y</span>
<span class="n">y_log</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_exp</span><span class="p">)</span>

<span class="c1"># Fit model on log scale</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_exp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_log</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficient: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Interpretation: For each unit increase in x, y increases by </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<div class="admonition tip">
<p class="admonition-title">Interpreting Log-Transformed Coefficients</p>
<p>When your target is log-transformed, coefficients represent <em>multiplicative</em> effects. A coefficient of 0.4 means each unit of x multiplies y by e^0.4 ≈ 1.49, or a 49% increase.</p>
</div>
<h2 id="feature-transformation-engineering-better-inputs">Feature Transformation: Engineering Better Inputs</h2>
<p><strong>Feature transformation</strong> is the deliberate modification of input features to improve model performance. This is closely related to the feature engineering we covered earlier, but with a specific focus on mathematical transformations.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">PowerTransformer</span>

<span class="c1"># Sample data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;income&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">30000</span><span class="p">,</span> <span class="mi">45000</span><span class="p">,</span> <span class="mi">55000</span><span class="p">,</span> <span class="mi">80000</span><span class="p">,</span> <span class="mi">150000</span><span class="p">,</span> <span class="mi">500000</span><span class="p">],</span>
    <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">22</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">65</span><span class="p">],</span>
    <span class="s1">&#39;experience_years&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">35</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># Log transform skewed income</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;log_income&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">])</span>

<span class="c1"># Square root of experience (diminishing returns)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;sqrt_experience&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;experience_years&#39;</span><span class="p">])</span>

<span class="c1"># Age polynomial</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;age_squared&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Scikit-learn's <code>PowerTransformer</code> can automatically find good transformations:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PowerTransformer</span>

<span class="c1"># Yeo-Johnson transformation (handles zeros and negatives)</span>
<span class="n">pt</span> <span class="o">=</span> <span class="n">PowerTransformer</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;yeo-johnson&#39;</span><span class="p">)</span>
<span class="n">df_transformed</span> <span class="o">=</span> <span class="n">pt</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;experience_years&#39;</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformation parameters:&quot;</span><span class="p">,</span> <span class="n">pt</span><span class="o">.</span><span class="n">lambdas_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<h4 id="diagram-transformation-gallery">Diagram: Transformation Gallery</h4>
<details>
<summary>Transformation Gallery</summary>
<p>Type: infographic</p>
<p>Bloom Taxonomy: Understand, Apply</p>
<p>Learning Objective: Show common transformations side-by-side with their effects on data distribution and relationships</p>
<p>Layout: 2x3 grid of transformation examples</p>
<p>Each Panel Contains:
- Original data histogram/scatter (left mini-plot)
- Transformed data histogram/scatter (right mini-plot)
- Transformation formula
- When to use it</p>
<p>Panels:
1. Log Transformation
   - Before: Right-skewed histogram
   - After: Symmetric histogram
   - Formula: y' = log(y)
   - Use: Exponential relationships, multiplicative effects</p>
<ol>
<li>Square Root</li>
<li>Before: Count data with variance proportional to mean</li>
<li>After: Stabilized variance</li>
<li>Formula: y' = √y</li>
<li>
<p>Use: Count data, Poisson-like distributions</p>
</li>
<li>
<p>Reciprocal (1/x)</p>
</li>
<li>Before: Hyperbolic scatter</li>
<li>After: Linear scatter</li>
<li>Formula: y' = 1/y</li>
<li>
<p>Use: Inverse relationships</p>
</li>
<li>
<p>Square (x²)</p>
</li>
<li>Before: Decelerating curve</li>
<li>After: Linear relationship</li>
<li>Formula: y' = x²</li>
<li>
<p>Use: Accelerating patterns</p>
</li>
<li>
<p>Box-Cox</p>
</li>
<li>Before: Arbitrary skewed data</li>
<li>After: Approximately normal</li>
<li>Formula: y' = (y^λ - 1)/λ</li>
<li>
<p>Use: General normalization</p>
</li>
<li>
<p>Standardization</p>
</li>
<li>Before: Different scales</li>
<li>After: Mean=0, SD=1</li>
<li>Formula: z = (x - μ)/σ</li>
<li>Use: Comparing features, regularization</li>
</ol>
<p>Interactive Elements:
- Click panel to see full-size comparison
- Slider to adjust transformation parameter
- Button: "Try on your data" - upload CSV option</p>
<p>Implementation: HTML/CSS/JavaScript with D3.js visualizations</p>
</details>
<h2 id="model-flexibility-the-complexity-dial">Model Flexibility: The Complexity Dial</h2>
<p><strong>Model flexibility</strong> refers to how adaptable a model is to different patterns in data. A highly flexible model can capture intricate patterns but risks overfitting. A rigid model may miss important patterns but generalizes better.</p>
<p>Think of flexibility as a dial:</p>
<ul>
<li><strong>Low flexibility</strong> (simple models): Few parameters, strong assumptions, high bias, low variance</li>
<li><strong>High flexibility</strong> (complex models): Many parameters, weak assumptions, low bias, high variance</li>
</ul>
<p>The relationship between flexibility and error follows a U-shaped curve:</p>
<ul>
<li><strong>Training error</strong> always decreases with more flexibility</li>
<li><strong>Test error</strong> decreases initially, then increases (the overfitting zone)</li>
<li>The optimal flexibility minimizes test error</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Track errors across flexibility levels</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="k">for</span> <span class="n">degree</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="n">train_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">test_errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Plot the flexibility curve</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">degrees</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">train_errors</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines+markers&#39;</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Training Error&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">degrees</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">test_errors</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines+markers&#39;</span><span class="p">,</span>
                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Test Error&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;The Bias-Variance Tradeoff in Action&#39;</span><span class="p">,</span>
                  <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;Model Flexibility (Polynomial Degree)&#39;</span><span class="p">,</span>
                  <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;Error (1 - R²)&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>The gap between training and test error is your overfitting indicator. When it's large, your model has learned noise specific to the training data.</p>
<h2 id="regularization-taming-overfitting">Regularization: Taming Overfitting</h2>
<p>Here's the million-dollar question: if more flexibility leads to overfitting, but we need flexibility to capture complex patterns, what do we do?</p>
<p>Enter <strong>regularization</strong>—a technique that adds a penalty for model complexity. Instead of just minimizing prediction error, regularized models minimize:</p>
<div class="arithmatex">\[\text{Loss} = \text{Prediction Error} + \lambda \times \text{Complexity Penalty}\]</div>
<p>The complexity penalty discourages large coefficients, effectively simplifying the model. This creates a controlled trade-off between fitting the data and keeping the model simple.</p>
<p>Regularization gives you the best of both worlds:</p>
<ul>
<li>Use a flexible model (high-degree polynomial)</li>
<li>Let regularization automatically "turn off" unnecessary complexity</li>
<li>Result: captures real patterns, ignores noise</li>
</ul>
<p>The <strong>regularization parameter</strong> (often called <span class="arithmatex">\(\lambda\)</span> or <code>alpha</code> in scikit-learn) controls this trade-off:</p>
<ul>
<li><strong>λ = 0</strong>: No regularization (standard linear regression)</li>
<li><strong>Small λ</strong>: Light penalty, nearly flexible</li>
<li><strong>Large λ</strong>: Heavy penalty, nearly rigid</li>
<li><strong>λ → ∞</strong>: All coefficients shrink to zero</li>
</ul>
<h2 id="ridge-regression-the-l2-penalty">Ridge Regression: The L2 Penalty</h2>
<p><strong>Ridge regression</strong> (also called L2 regularization or Tikhonov regularization) adds a penalty proportional to the <em>squared</em> coefficients:</p>
<div class="arithmatex">\[\text{Loss}_{\text{Ridge}} = \sum(y_i - \hat{y}_i)^2 + \lambda \sum \beta_j^2\]</div>
<p>The squared penalty means:</p>
<ul>
<li>All coefficients are shrunk toward zero</li>
<li>Large coefficients are penalized more heavily</li>
<li>Coefficients never become exactly zero (just very small)</li>
<li>Good for multicollinearity—it stabilizes correlated features</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Create Ridge regression pipeline with polynomial features</span>
<span class="n">ridge_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)),</span>  <span class="c1"># High degree</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>              <span class="c1"># Essential for regularization!</span>
    <span class="p">(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>                <span class="c1"># alpha is λ</span>
<span class="p">])</span>

<span class="n">ridge_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Train R²: </span><span class="si">{</span><span class="n">ridge_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test R²: </span><span class="si">{</span><span class="n">ridge_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compare to unregularized</span>
<span class="n">unreg_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="p">])</span>
<span class="n">unreg_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Unregularized Train R²: </span><span class="si">{</span><span class="n">unreg_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unregularized Test R²: </span><span class="si">{</span><span class="n">unreg_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Notice how Ridge maintains good test performance even with degree 10, while unregularized regression overfits!</p>
<div class="admonition warning">
<p class="admonition-title">Scale Your Features for Regularization</p>
<p>Regularization penalizes large coefficients. If features are on different scales, the penalty affects them unequally. Always standardize features before applying regularization.</p>
</div>
<h2 id="lasso-regression-the-l1-penalty">Lasso Regression: The L1 Penalty</h2>
<p><strong>Lasso regression</strong> (Least Absolute Shrinkage and Selection Operator) uses the <em>absolute value</em> of coefficients as the penalty:</p>
<div class="arithmatex">\[\text{Loss}_{\text{Lasso}} = \sum(y_i - \hat{y}_i)^2 + \lambda \sum |\beta_j|\]</div>
<p>The L1 penalty has a special property: it can shrink coefficients all the way to exactly zero. This means Lasso performs <strong>automatic feature selection</strong>—useless features get eliminated entirely.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="c1"># Create Lasso pipeline</span>
<span class="n">lasso_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">lasso_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Count non-zero coefficients</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">lasso_pipeline</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;lasso&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)</span>
<span class="n">n_nonzero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coefficients</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total features: </span><span class="si">{</span><span class="n">n_features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Non-zero coefficients: </span><span class="si">{</span><span class="n">n_nonzero</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Features eliminated: </span><span class="si">{</span><span class="n">n_features</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">n_nonzero</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Train R²: </span><span class="si">{</span><span class="n">lasso_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="w"> </span><span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test R²: </span><span class="si">{</span><span class="n">lasso_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Ridge (L2)</th>
<th>Lasso (L1)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Penalty</td>
<td>Sum of squared coefficients</td>
<td>Sum of absolute coefficients</td>
</tr>
<tr>
<td>Coefficients</td>
<td>Shrunk toward zero</td>
<td>Can become exactly zero</td>
</tr>
<tr>
<td>Feature selection</td>
<td>No</td>
<td>Yes (automatic)</td>
</tr>
<tr>
<td>Multicollinearity</td>
<td>Handles well</td>
<td>Arbitrary selection</td>
</tr>
<tr>
<td>Best for</td>
<td>Many small effects</td>
<td>Few important features</td>
</tr>
</tbody>
</table>
<h4 id="diagram-ridge-vs-lasso-comparison">Diagram: Ridge vs Lasso Comparison</h4>
<details>
<summary>Ridge vs Lasso Comparison</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Analyze, Evaluate</p>
<p>Learning Objective: Visualize and compare how Ridge and Lasso penalties affect coefficient shrinkage and feature selection</p>
<p>Canvas Layout (900x550):
- Top left (400x250): Ridge coefficient path
- Top right (400x250): Lasso coefficient path
- Bottom (900x250): Side-by-side coefficient comparison and controls</p>
<p>Coefficient Path Plots:
- X-axis: Log(λ) from small to large
- Y-axis: Coefficient values
- Each line represents one coefficient
- Show how coefficients shrink as λ increases
- Ridge: All lines approach zero asymptotically
- Lasso: Lines hit zero and stay there</p>
<p>Comparison Panel:
- Bar chart showing final coefficient values
- Ridge bars (blue): All non-zero, varying heights
- Lasso bars (orange): Some exactly zero
- Highlight eliminated features in gray</p>
<p>Interactive Controls:
- Slider: λ (regularization strength) - both plots update
- Dropdown: Select dataset (housing, synthetic, medical)
- Checkbox: "Show cross-validation optimal λ"
- Toggle: "Show mathematical penalty visualization"</p>
<p>Penalty Visualization (optional):
- 2D contour plot showing loss surface
- Ridge: Circular penalty (L2 ball)
- Lasso: Diamond penalty (L1 ball)
- Optimal point where loss contours meet penalty boundary</p>
<p>Key Insights Displayed:
- "Lasso zeros out X features"
- "Ridge reduces largest coefficient by Y%"
- Optimal λ marked on both paths</p>
<p>Implementation: p5.js with interactive plots</p>
</details>
<h2 id="elastic-net-the-best-of-both-worlds">Elastic Net: The Best of Both Worlds</h2>
<p><strong>Elastic Net</strong> combines Ridge and Lasso penalties:</p>
<div class="arithmatex">\[\text{Loss}_{\text{ElasticNet}} = \sum(y_i - \hat{y}_i)^2 + \lambda_1 \sum |\beta_j| + \lambda_2 \sum \beta_j^2\]</div>
<p>Or equivalently, using a mixing parameter <span class="arithmatex">\(\rho\)</span> (called <code>l1_ratio</code> in scikit-learn):</p>
<div class="arithmatex">\[\text{Loss} = \text{MSE} + \alpha \left( \rho \sum |\beta_j| + (1-\rho) \sum \beta_j^2 \right)\]</div>
<p>When <span class="arithmatex">\(\rho = 1\)</span>: Pure Lasso
When <span class="arithmatex">\(\rho = 0\)</span>: Pure Ridge
When <span class="arithmatex">\(0 &lt; \rho &lt; 1\)</span>: Combination</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>

<span class="c1"># Elastic Net with equal mix of L1 and L2</span>
<span class="n">elastic_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;elastic&#39;</span><span class="p">,</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>  <span class="c1"># 50% L1, 50% L2</span>
<span class="p">])</span>

<span class="n">elastic_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">coefficients</span> <span class="o">=</span> <span class="n">elastic_pipeline</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;elastic&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Non-zero coefficients: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coefficients</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test R²: </span><span class="si">{</span><span class="n">elastic_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Elastic Net is particularly useful when:</p>
<ul>
<li>You have groups of correlated features (Lasso arbitrarily picks one; Elastic Net keeps related features together)</li>
<li>You want some feature selection but not as aggressive as pure Lasso</li>
<li>You're not sure whether Ridge or Lasso is better (try Elastic Net!)</li>
</ul>
<h2 id="lambdaalpha-parameter-finding-the-sweet-spot">Lambda/Alpha Parameter: Finding the Sweet Spot</h2>
<p>The <strong>lambda parameter</strong> (called <code>alpha</code> in scikit-learn) controls regularization strength. Too small and you overfit; too large and you underfit. Finding the optimal λ is crucial.</p>
<p>Use cross-validation to find the best λ:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">,</span> <span class="n">ElasticNetCV</span>

<span class="c1"># RidgeCV automatically finds optimal alpha</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>  <span class="c1"># Range from 0.0001 to 10000</span>

<span class="n">ridge_cv</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">10</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">ridge_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">optimal_alpha</span> <span class="o">=</span> <span class="n">ridge_cv</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;ridge&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">alpha_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal alpha: </span><span class="si">{</span><span class="n">optimal_alpha</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test R²: </span><span class="si">{</span><span class="n">ridge_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Visualize the regularization path:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ridge_regression</span>

<span class="c1"># Calculate coefficients for each alpha</span>
<span class="n">coef_path</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
    <span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">coef_path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;ridge&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">coef_path</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coef_path</span><span class="p">)</span>

<span class="c1"># Plot coefficient paths</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">coef_path</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">alphas</span><span class="p">),</span> <span class="n">y</span><span class="o">=</span><span class="n">coef_path</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Coef </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                             <span class="n">showlegend</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Ridge Coefficient Path: How λ Affects Coefficients&#39;</span><span class="p">,</span>
                  <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;log₁₀(λ)&#39;</span><span class="p">,</span>
                  <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;Coefficient Value&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<h4 id="diagram-lambda-tuning-playground">Diagram: Lambda Tuning Playground</h4>
<details>
<summary>Lambda Tuning Playground</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Apply, Evaluate</p>
<p>Learning Objective: Practice finding optimal regularization strength through interactive experimentation</p>
<p>Canvas Layout (850x600):
- Top area (850x350): Data and model fit visualization
- Bottom left (425x250): CV score vs lambda plot
- Bottom right (425x250): Coefficient magnitudes</p>
<p>Top Panel - Model Fit:
- Scatter plot of data
- Polynomial curve showing current fit
- Toggle between Ridge/Lasso/Elastic Net
- Curve updates as lambda changes</p>
<p>Bottom Left - Cross-Validation:
- X-axis: log(λ) scale
- Y-axis: CV Score (R² or MSE)
- Line showing CV performance across λ values
- Vertical marker at current λ
- Optimal λ highlighted with star</p>
<p>Bottom Right - Coefficients:
- Bar chart of coefficient magnitudes
- Updates in real-time as λ changes
- For Lasso: Gray out zero coefficients
- Show total number of non-zero coefficients</p>
<p>Interactive Controls:
- Slider: Lambda value (log scale)
- Dropdown: Regularization type (Ridge, Lasso, Elastic Net)
- Slider: Polynomial degree (2-15)
- Slider: l1_ratio (for Elastic Net, 0-1)
- Button: "Find Optimal λ" - animates search
- Button: "Generate New Data"</p>
<p>Metrics Display:
- Current λ value
- Train R²
- Test R²
- Cross-Validation R²
- Number of non-zero coefficients</p>
<p>Educational Callouts:
- When λ too small: "Overfitting warning!"
- When λ too large: "Underfitting warning!"
- At optimal: "Sweet spot found!"</p>
<p>Implementation: p5.js with real-time model fitting</p>
</details>
<h2 id="shrinkage-what-regularization-actually-does">Shrinkage: What Regularization Actually Does</h2>
<p><strong>Shrinkage</strong> is the technical term for what regularization does to coefficients—it pulls them toward zero. But why does shrinking coefficients help prevent overfitting?</p>
<p>Consider what happens when a model overfits:</p>
<ol>
<li>It finds patterns in noise</li>
<li>These patterns require extreme coefficients</li>
<li>Small changes in data cause large prediction changes</li>
<li>High variance = poor generalization</li>
</ol>
<p>Shrinkage counters this by:</p>
<ol>
<li>Penalizing extreme coefficients</li>
<li>Forcing the model to find simpler solutions</li>
<li>Reducing sensitivity to noise</li>
<li>Lower variance = better generalization</li>
</ol>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Demonstrate shrinkage effect</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="n">coefficients_by_alpha</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
            <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">8</span><span class="p">)),</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
        <span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
            <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">8</span><span class="p">)),</span>
            <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
            <span class="p">(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
        <span class="p">])</span>

    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">alpha</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">coeffs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">coeffs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;ridge&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>

    <span class="n">coefficients_by_alpha</span><span class="p">[</span><span class="n">alpha</span><span class="p">]</span> <span class="o">=</span> <span class="n">coeffs</span>

    <span class="c1"># Calculate coefficient magnitude</span>
    <span class="n">coef_magnitude</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coeffs</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;α=</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">6</span><span class="si">}</span><span class="s2">: Coefficient L2 norm = </span><span class="si">{</span><span class="n">coef_magnitude</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Test R² = </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>As regularization increases:</p>
<ul>
<li>Coefficient magnitudes shrink</li>
<li>Model becomes more stable</li>
<li>Test performance often improves (up to a point)</li>
</ul>
<h2 id="putting-it-all-together-a-complete-workflow">Putting It All Together: A Complete Workflow</h2>
<p>Here's a complete workflow for building regularized non-linear models:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">ElasticNet</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="c1"># 1. Load and split data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 2. Create pipeline with polynomial features and regularization</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;regressor&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># 3. Define hyperparameter grid</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span>
    <span class="s1">&#39;regressor__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># 4. Grid search with cross-validation</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 5. Results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:&quot;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best CV score: </span><span class="si">{</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test score: </span><span class="si">{</span><span class="n">grid_search</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 6. Visualize the fit</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_estimator_</span>
<span class="n">X_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_plot</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_plot</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Regularized Polynomial Regression&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_plot</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">y_plot</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Best Model&#39;</span><span class="p">,</span>
                <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<h4 id="diagram-regularization-decision-tree">Diagram: Regularization Decision Tree</h4>
<details>
<summary>Regularization Decision Tree</summary>
<p>Type: workflow</p>
<p>Bloom Taxonomy: Evaluate, Apply</p>
<p>Learning Objective: Guide students through choosing the right regularization approach for their problem</p>
<p>Visual Style: Flowchart with decision diamonds and outcome rectangles</p>
<p>Start: "Need to Prevent Overfitting?"</p>
<p>Decision 1: "Linear relationship?"
- Yes → Consider if regularization is needed
- No → Add polynomial features</p>
<p>Decision 2: "How many features vs samples?"
- Many features, few samples → Strong regularization needed
- Balanced → Moderate regularization
- Few features, many samples → Light regularization</p>
<p>Decision 3: "Do you want feature selection?"
- Yes, aggressive → Use Lasso
- Yes, some → Use Elastic Net
- No, keep all features → Use Ridge</p>
<p>Decision 4: "Highly correlated features?"
- Yes → Use Ridge or Elastic Net (Lasso is unstable)
- No → Any method works</p>
<p>Decision 5: "Interpretability important?"
- Yes → Lasso (sparse solution)
- No → Ridge (often better accuracy)</p>
<p>Final Outcomes:
- Ridge: "Many small effects, correlated features"
- Lasso: "Few important features, interpretability"
- Elastic Net: "Best of both, groups of features"</p>
<p>Interactive Elements:
- Click each decision to see explanation
- Hover shows examples of each scenario
- "Take Quiz" mode walks through with your data characteristics</p>
<p>Implementation: HTML/CSS/JavaScript with interactive flowchart</p>
</details>
<h2 id="common-pitfalls-and-best-practices">Common Pitfalls and Best Practices</h2>
<p><strong>Always Scale Before Regularizing</strong>
Regularization penalizes coefficient magnitude. If features aren't scaled, features with larger values will be unfairly penalized.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Good: Scale inside pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
    <span class="p">(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Bad: Features on different scales penalized unequally</span>
<span class="n">ridge_bad</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">ridge_bad</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_unscaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><strong>Don't Regularize the Intercept</strong>
Scikit-learn doesn't regularize the intercept by default (which is correct). Be careful if using other implementations.</p>
<p><strong>Use Cross-Validation for Lambda</strong>
Never set λ by looking at test performance. Use cross-validation to find optimal λ, then evaluate on test data.</p>
<p><strong>Consider the Problem Type</strong>
- Prediction focus → Ridge often wins
- Interpretation focus → Lasso for sparsity
- Groups of related features → Elastic Net</p>
<p><strong>Watch for Warning Signs</strong>
- Very large or very small λ optimal → reconsider model specification
- All coefficients near zero → λ too large
- Test performance much worse than CV → something's wrong</p>
<h2 id="summary-your-regularization-toolkit">Summary: Your Regularization Toolkit</h2>
<p>You now have powerful tools for handling non-linear relationships and overfitting:</p>
<ul>
<li><strong>Polynomial regression</strong> captures curved patterns using powers of features</li>
<li><strong>Degree selection</strong> balances flexibility with overfitting risk</li>
<li><strong>Transformations</strong> (log, sqrt, etc.) can linearize relationships</li>
<li><strong>Model flexibility</strong> is the dial between underfitting and overfitting</li>
<li><strong>Regularization</strong> adds complexity penalties to prevent overfitting</li>
<li><strong>Ridge (L2)</strong> shrinks all coefficients, handles multicollinearity</li>
<li><strong>Lasso (L1)</strong> performs automatic feature selection</li>
<li><strong>Elastic Net</strong> combines L1 and L2 penalties</li>
<li><strong>Lambda tuning</strong> via cross-validation finds the optimal penalty strength</li>
</ul>
<p>With these techniques, you can build models that are flexible enough to capture complex real-world patterns while remaining robust enough to generalize to new data.</p>
<h2 id="looking-ahead">Looking Ahead</h2>
<p>In the next chapter, we'll explore machine learning more broadly—including classification problems where we predict categories instead of numbers. You'll see how the regularization concepts you learned here apply to new types of models.</p>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<ul>
<li>Polynomial regression captures non-linear patterns while still using linear regression techniques</li>
<li>Higher polynomial degrees increase flexibility but also overfitting risk</li>
<li>Log and other transformations can linearize curved relationships</li>
<li>Regularization adds a penalty for complexity, balancing fit with generalization</li>
<li>Ridge (L2) shrinks coefficients; Lasso (L1) can zero them out entirely</li>
<li>Elastic Net combines both penalties for flexible feature selection</li>
<li>Always scale features before regularizing and use CV to find optimal λ</li>
<li>The goal is finding the sweet spot: flexible enough to learn patterns, constrained enough to ignore noise</li>
</ul>







  
  



  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../10-numpy-computing/" class="md-footer__link md-footer__link--prev" aria-label="Previous: NumPy Computing">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                NumPy Computing
              </div>
            </div>
          </a>
        
        
          
          <a href="../12-intro-to-machine-learning/" class="md-footer__link md-footer__link--next" aria-label="Next: Intro to Machine Learning">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Intro to Machine Learning
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 Dan McCreary. Licensed under <a href="license/">CC BY-NC-SA 4.0</a> for non-commercial use.
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.path", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "navigation.footer", "content.action.edit"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../js/extra.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>