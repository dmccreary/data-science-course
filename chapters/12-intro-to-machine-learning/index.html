
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An online course on introduction to data science with Python.  Extensive use of AI tools and MicroSims to help you learn.">
      
      
        <meta name="author" content="Dan McCreary">
      
      
        <link rel="canonical" href="https://dmccreary.github.io/data-science-course/chapters/12-intro-to-machine-learning/">
      
      
        <link rel="prev" href="../11-nonlinear-models-regularization/">
      
      
        <link rel="next" href="../13-neural-networks-pytorch/">
      
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>Intro to Machine Learning - AI Based Data Science with Python</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-RTBCWGJKKR"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-RTBCWGJKKR",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-RTBCWGJKKR",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Intro to Machine Learning - AI Based Data Science with Python" >
      
        <meta  property="og:description"  content="An online course on introduction to data science with Python.  Extensive use of AI tools and MicroSims to help you learn." >
      
        <meta  property="og:image"  content="https://dmccreary.github.io/data-science-course/assets/images/social/chapters/12-intro-to-machine-learning/index.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://dmccreary.github.io/data-science-course/chapters/12-intro-to-machine-learning/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Intro to Machine Learning - AI Based Data Science with Python" >
      
        <meta  name="twitter:description"  content="An online course on introduction to data science with Python.  Extensive use of AI tools and MicroSims to help you learn." >
      
        <meta  name="twitter:image"  content="https://dmccreary.github.io/data-science-course/assets/images/social/chapters/12-intro-to-machine-learning/index.png" >
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#introduction-to-machine-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AI Based Data Science with Python" class="md-header__button md-logo" aria-label="AI Based Data Science with Python" data-md-component="logo">
      
  <img src="../../img/logo-192.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Based Data Science with Python
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Intro to Machine Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dmccreary/data-science-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AI Based Data Science with Python" class="md-nav__button md-logo" aria-label="AI Based Data Science with Python" data-md-component="logo">
      
  <img src="../../img/logo-192.png" alt="logo">

    </a>
    AI Based Data Science with Python
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dmccreary/data-science-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub Repo
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Chapters
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Chapters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-intro-to-data-science/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Data Science
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-python-environment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Environment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-python-data-structures/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Data Structures
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-data-cleaning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Cleaning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-data-visualization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Visualization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-statistical-foundations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistical Foundations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-simple-linear-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simple Linear Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-model-evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Evaluation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-multiple-linear-regression/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multiple Linear Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../10-numpy-computing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NumPy Computing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../11-nonlinear-models-regularization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Non-linear Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Intro to Machine Learning
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Intro to Machine Learning
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-covered" class="md-nav__link">
    <span class="md-ellipsis">
      Concepts Covered
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-welcome-to-the-machine-learning-revolution" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction: Welcome to the Machine Learning Revolution
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      What Is Machine Learning?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supervised-learning-learning-with-a-teacher" class="md-nav__link">
    <span class="md-ellipsis">
      Supervised Learning: Learning with a Teacher
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsupervised-learning-discovering-hidden-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised Learning: Discovering Hidden Structure
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Unsupervised Learning: Discovering Hidden Structure">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-supervised-vs-unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Supervised vs Unsupervised Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classification-predicting-categories" class="md-nav__link">
    <span class="md-ellipsis">
      Classification: Predicting Categories
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#clustering-finding-natural-groups" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering: Finding Natural Groups
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-training-process-how-models-learn" class="md-nav__link">
    <span class="md-ellipsis">
      The Training Process: How Models Learn
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Training Process: How Models Learn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-training-process-animator" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Training Process Animator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-algorithm-and-model-training" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Algorithm and Model Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalization-the-ultimate-goal" class="md-nav__link">
    <span class="md-ellipsis">
      Generalization: The Ultimate Goal
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-error-test-error-and-prediction-error" class="md-nav__link">
    <span class="md-ellipsis">
      Training Error, Test Error, and Prediction Error
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training Error, Test Error, and Prediction Error">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-error-types-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Error Types Visualizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-function-measuring-prediction-quality" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Function: Measuring Prediction Quality
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cost-function-total-training-error" class="md-nav__link">
    <span class="md-ellipsis">
      Cost Function: Total Training Error
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-finding-the-best-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization: Finding the Best Parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-the-universal-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent: The Universal Optimizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Descent: The Universal Optimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-gradient-descent-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Gradient Descent Visualizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-rate-the-step-size" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate: The Step Size
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convergence-knowing-when-to-stop" class="md-nav__link">
    <span class="md-ellipsis">
      Convergence: Knowing When to Stop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#local-minimum-vs-global-minimum" class="md-nav__link">
    <span class="md-ellipsis">
      Local Minimum vs Global Minimum
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Local Minimum vs Global Minimum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-optimization-landscape-explorer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Optimization Landscape Explorer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#putting-it-all-together-the-ml-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Putting It All Together: The ML Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-the-machine-learning-mental-model" class="md-nav__link">
    <span class="md-ellipsis">
      Summary: The Machine Learning Mental Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#looking-ahead" class="md-nav__link">
    <span class="md-ellipsis">
      Looking Ahead
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../13-neural-networks-pytorch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural Networks and PyTorch
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../chapters-v1/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Old v1 Content
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../labs/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Labs
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../sims/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    MicroSims
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../learning-graph/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Learning Graph
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../prompts/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Sample GenAI Prompts
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frequently Asked Questions
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Glossary
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../how-we-built-this-site/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How We Built This Site
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checklist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customization Checklist
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../license/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    License
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    References
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../checklist/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customization Checklist
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-covered" class="md-nav__link">
    <span class="md-ellipsis">
      Concepts Covered
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      Prerequisites
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-welcome-to-the-machine-learning-revolution" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction: Welcome to the Machine Learning Revolution
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      What Is Machine Learning?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supervised-learning-learning-with-a-teacher" class="md-nav__link">
    <span class="md-ellipsis">
      Supervised Learning: Learning with a Teacher
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsupervised-learning-discovering-hidden-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Unsupervised Learning: Discovering Hidden Structure
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Unsupervised Learning: Discovering Hidden Structure">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-supervised-vs-unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Supervised vs Unsupervised Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classification-predicting-categories" class="md-nav__link">
    <span class="md-ellipsis">
      Classification: Predicting Categories
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#clustering-finding-natural-groups" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering: Finding Natural Groups
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-training-process-how-models-learn" class="md-nav__link">
    <span class="md-ellipsis">
      The Training Process: How Models Learn
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Training Process: How Models Learn">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-training-process-animator" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Training Process Animator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-algorithm-and-model-training" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Algorithm and Model Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#generalization-the-ultimate-goal" class="md-nav__link">
    <span class="md-ellipsis">
      Generalization: The Ultimate Goal
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-error-test-error-and-prediction-error" class="md-nav__link">
    <span class="md-ellipsis">
      Training Error, Test Error, and Prediction Error
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training Error, Test Error, and Prediction Error">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-error-types-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Error Types Visualizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#loss-function-measuring-prediction-quality" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Function: Measuring Prediction Quality
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cost-function-total-training-error" class="md-nav__link">
    <span class="md-ellipsis">
      Cost Function: Total Training Error
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-finding-the-best-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization: Finding the Best Parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-the-universal-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent: The Universal Optimizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Descent: The Universal Optimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-gradient-descent-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Gradient Descent Visualizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-rate-the-step-size" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate: The Step Size
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convergence-knowing-when-to-stop" class="md-nav__link">
    <span class="md-ellipsis">
      Convergence: Knowing When to Stop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#local-minimum-vs-global-minimum" class="md-nav__link">
    <span class="md-ellipsis">
      Local Minimum vs Global Minimum
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Local Minimum vs Global Minimum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-optimization-landscape-explorer" class="md-nav__link">
    <span class="md-ellipsis">
      Diagram: Optimization Landscape Explorer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#putting-it-all-together-the-ml-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Putting It All Together: The ML Pipeline
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-the-machine-learning-mental-model" class="md-nav__link">
    <span class="md-ellipsis">
      Summary: The Machine Learning Mental Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#looking-ahead" class="md-nav__link">
    <span class="md-ellipsis">
      Looking Ahead
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/dmccreary/data-science-course/blob/master/docs/chapters/12-intro-to-machine-learning/index.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="introduction-to-machine-learning">Introduction to Machine Learning</h1>
<hr />
<p>title: Introduction to Machine Learning
description: Teaching computers to learn from experience - the ultimate superpower
generated_by: chapter-content-generator skill
date: 2025-12-15
version: 0.03</p>
<hr />
<h2 id="summary">Summary</h2>
<p>This chapter provides a conceptual foundation for machine learning. Students will learn the distinction between supervised and unsupervised learning, understand the training process, and explore key concepts like generalization and error types. The chapter covers loss and cost functions, optimization theory, and gradient descent as the fundamental algorithm for training models. By the end of this chapter, students will understand how machine learning models learn from data and be prepared for neural networks.</p>
<h2 id="concepts-covered">Concepts Covered</h2>
<p>This chapter covers the following 20 concepts from the learning graph:</p>
<ol>
<li>Machine Learning</li>
<li>Supervised Learning</li>
<li>Unsupervised Learning</li>
<li>Classification</li>
<li>Clustering</li>
<li>Training Process</li>
<li>Learning Algorithm</li>
<li>Model Training</li>
<li>Generalization</li>
<li>Training Error</li>
<li>Test Error</li>
<li>Prediction Error</li>
<li>Loss Function</li>
<li>Cost Function</li>
<li>Optimization</li>
<li>Gradient Descent</li>
<li>Learning Rate</li>
<li>Convergence</li>
<li>Local Minimum</li>
<li>Global Minimum</li>
</ol>
<h2 id="prerequisites">Prerequisites</h2>
<p>This chapter builds on concepts from:</p>
<ul>
<li><a href="../07-simple-linear-regression/">Chapter 7: Simple Linear Regression</a></li>
<li><a href="../08-model-evaluation/">Chapter 8: Model Evaluation and Validation</a></li>
<li><a href="../10-numpy-computing/">Chapter 10: NumPy and Numerical Computing</a></li>
</ul>
<hr />
<h2 id="introduction-welcome-to-the-machine-learning-revolution">Introduction: Welcome to the Machine Learning Revolution</h2>
<p>Everything you've learned so far has been building to this moment. Linear regression? That was machine learning. Model evaluation? Essential for machine learning. NumPy? The engine that powers machine learning. You've been doing machine learning all along—you just didn't know it yet.</p>
<p>But now we're going to pull back the curtain and understand the <em>why</em> and <em>how</em> behind it all. How does a computer actually "learn"? What does training a model really mean? And how can a bunch of math magically give computers the ability to recognize faces, translate languages, and predict the future?</p>
<p>This chapter answers these questions and gives you the conceptual foundation you need for the most powerful tools in data science. By the end, you'll understand not just <em>how</em> to use machine learning, but <em>how it works</em>. That's the difference between using a superpower and truly mastering it.</p>
<h2 id="what-is-machine-learning">What Is Machine Learning?</h2>
<p><strong>Machine learning</strong> is a field of computer science where we build systems that learn from data rather than being explicitly programmed. Instead of writing rules like "if email contains 'free money,' mark as spam," we show the computer thousands of spam and non-spam emails and let it figure out the patterns itself.</p>
<p>Here's the key insight: traditional programming is about <em>rules</em>; machine learning is about <em>patterns</em>.</p>
<table>
<thead>
<tr>
<th>Traditional Programming</th>
<th>Machine Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input: Data + Rules</td>
<td>Input: Data + Answers</td>
</tr>
<tr>
<td>Output: Answers</td>
<td>Output: Rules (the model)</td>
</tr>
<tr>
<td>Human writes the logic</td>
<td>Computer discovers the logic</td>
</tr>
<tr>
<td>Brittle to new situations</td>
<td>Adapts to new patterns</td>
</tr>
</tbody>
</table>
<p>A simple definition:</p>
<blockquote>
<p>Machine learning is the study of algorithms that improve their performance at some task through experience.</p>
</blockquote>
<p>That "experience" is data. The "improvement" is measured by some metric. And the "task" could be predicting house prices, recognizing cats in photos, or recommending movies you'll love.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Traditional programming approach</span>
<span class="k">def</span> <span class="nf">is_spam_traditional</span><span class="p">(</span><span class="n">email</span><span class="p">):</span>
    <span class="n">spam_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;free&#39;</span><span class="p">,</span> <span class="s1">&#39;winner&#39;</span><span class="p">,</span> <span class="s1">&#39;click here&#39;</span><span class="p">,</span> <span class="s1">&#39;urgent&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">spam_words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">email</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>

<span class="c1"># Machine learning approach (conceptual)</span>
<span class="c1"># We don&#39;t write rules - we train a model on examples</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_emails</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">)</span>  <span class="c1"># Learn from data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_emails</span><span class="p">)</span>       <span class="c1"># Apply learned patterns</span>
</code></pre></div></td></tr></table></div>
<p>The magic is in <code>model.fit()</code>—that's where the learning happens.</p>
<h2 id="supervised-learning-learning-with-a-teacher">Supervised Learning: Learning with a Teacher</h2>
<p><strong>Supervised learning</strong> is the most common type of machine learning. It's called "supervised" because we provide the correct answers during training—like a teacher grading homework. The model learns to map inputs to outputs by studying examples where we already know the answer.</p>
<p>The setup:</p>
<ul>
<li><strong>Features (X)</strong>: The input information (house size, location, age)</li>
<li><strong>Labels (y)</strong>: The correct answers (house price, spam/not-spam)</li>
<li><strong>Goal</strong>: Learn a function f(X) → y that works for new data</li>
</ul>
<p>All the regression you've learned is supervised learning! You provided house features and prices, and the model learned to predict prices from features.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Supervised learning: we provide both X (features) and y (labels)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;square_feet&#39;</span><span class="p">,</span> <span class="s1">&#39;bedrooms&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">]]</span>  <span class="c1"># Features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span>                               <span class="c1"># Labels (correct answers)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># &quot;Supervised&quot; by the labels</span>

<span class="c1"># Now predict on new data where we don&#39;t know the answer</span>
<span class="n">new_house</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
<span class="n">predicted_price</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_house</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Supervised learning powers:</p>
<ul>
<li>Price prediction (regression)</li>
<li>Email spam detection (classification)</li>
<li>Medical diagnosis (classification)</li>
<li>Weather forecasting (regression)</li>
<li>Credit scoring (classification)</li>
</ul>
<h2 id="unsupervised-learning-discovering-hidden-structure">Unsupervised Learning: Discovering Hidden Structure</h2>
<p><strong>Unsupervised learning</strong> works without labels—no correct answers are provided. Instead, the model discovers patterns and structure in the data on its own. It's like exploring a new city without a map; you find natural groupings and patterns through observation.</p>
<p>The setup:</p>
<ul>
<li><strong>Features (X)</strong>: The input information</li>
<li><strong>No labels (y)</strong>: We don't tell the model what to look for</li>
<li><strong>Goal</strong>: Discover interesting structure in the data</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># Unsupervised learning: only X, no y!</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;spending&#39;</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">,</span> <span class="s1">&#39;recency&#39;</span><span class="p">]]</span>

<span class="c1"># Find natural groupings of customers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># No labels provided</span>

<span class="c1"># Which cluster does each customer belong to?</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Unsupervised learning powers:</p>
<ul>
<li>Customer segmentation</li>
<li>Anomaly detection</li>
<li>Topic discovery in documents</li>
<li>Dimensionality reduction</li>
<li>Recommendation systems (partially)</li>
</ul>
<h4 id="diagram-supervised-vs-unsupervised-learning">Diagram: Supervised vs Unsupervised Learning</h4>
<details>
<summary>Supervised vs Unsupervised Learning</summary>
<p>Type: infographic</p>
<p>Bloom Taxonomy: Understand</p>
<p>Learning Objective: Clearly distinguish between supervised and unsupervised learning paradigms through visual comparison</p>
<p>Layout: Side-by-side comparison with examples</p>
<p>Left Panel - Supervised Learning:
- Visual: Training data with input features AND color-coded labels
- Example: Photos of cats and dogs, each labeled
- Arrow showing: Data + Labels → Model → Predictions
- Use cases listed: Spam detection, Price prediction, Medical diagnosis
- Key insight: "Learning WITH a teacher"</p>
<p>Right Panel - Unsupervised Learning:
- Visual: Training data with input features, NO labels
- Example: Unlabeled customer data points
- Arrow showing: Data → Model → Discovered Groups/Patterns
- Use cases listed: Customer segments, Anomaly detection, Topic modeling
- Key insight: "Learning to find structure"</p>
<p>Center Comparison:
- Table showing key differences
- Input data visualization (labeled vs unlabeled)
- Output type (predictions vs structure)</p>
<p>Interactive Elements:
- Click each panel for expanded examples
- Hover over use cases for brief explanations
- Toggle: "Show math notation" for formal definitions
- Quiz mode: "Which type is this?" with scenarios</p>
<p>Color Scheme:
- Supervised: Green (has guidance)
- Unsupervised: Blue (exploring)
- Labels shown in distinct colors in supervised examples</p>
<p>Implementation: HTML/CSS/JavaScript with click interactions</p>
</details>
<h2 id="classification-predicting-categories">Classification: Predicting Categories</h2>
<p><strong>Classification</strong> is a type of supervised learning where the target variable is categorical (a class or category) rather than numerical. Instead of predicting a number, you're predicting which group something belongs to.</p>
<p>Examples of classification:</p>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Input Features</th>
<th>Output Classes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Email spam</td>
<td>Email text, sender, links</td>
<td>Spam, Not Spam</td>
</tr>
<tr>
<td>Disease diagnosis</td>
<td>Symptoms, test results</td>
<td>Disease A, B, C, Healthy</td>
</tr>
<tr>
<td>Image recognition</td>
<td>Pixel values</td>
<td>Cat, Dog, Bird, ...</td>
</tr>
<tr>
<td>Customer churn</td>
<td>Usage patterns, demographics</td>
<td>Will Leave, Will Stay</td>
</tr>
<tr>
<td>Loan default</td>
<td>Income, history, debt</td>
<td>Default, No Default</td>
</tr>
</tbody>
</table>
<p>Binary classification has two classes; multi-class classification has more than two.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="c1"># Binary classification example</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="s1">&#39;debt_ratio&#39;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;will_default&#39;</span><span class="p">]</span>  <span class="c1"># 0 or 1</span>

<span class="c1"># Split data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Train classifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Evaluate</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
<p>Classification metrics differ from regression:</p>
<ul>
<li><strong>Accuracy</strong>: Fraction of correct predictions</li>
<li><strong>Precision</strong>: Of predicted positives, how many are correct?</li>
<li><strong>Recall</strong>: Of actual positives, how many did we catch?</li>
<li><strong>F1 Score</strong>: Harmonic mean of precision and recall</li>
</ul>
<h2 id="clustering-finding-natural-groups">Clustering: Finding Natural Groups</h2>
<p><strong>Clustering</strong> is a type of unsupervised learning that groups similar data points together. The algorithm discovers natural groupings without being told how many groups exist or what they should look like.</p>
<p>K-Means is the most common clustering algorithm:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="c1"># Customer data (no labels!)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;annual_spending&#39;</span><span class="p">,</span> <span class="s1">&#39;visit_frequency&#39;</span><span class="p">]]</span>

<span class="c1"># Find 4 customer segments</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Visualize clusters</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;annual_spending&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;visit_frequency&#39;</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Customer Segments Discovered by K-Means&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Examine cluster centers</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cluster Centers:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Clustering applications:</p>
<ul>
<li><strong>Customer segmentation</strong>: Group customers by behavior for targeted marketing</li>
<li><strong>Document organization</strong>: Group similar articles or papers</li>
<li><strong>Image compression</strong>: Group similar colors to reduce file size</li>
<li><strong>Anomaly detection</strong>: Points far from any cluster may be anomalies</li>
<li><strong>Biology</strong>: Group genes with similar expression patterns</li>
</ul>
<p>The key challenge: choosing the right number of clusters. Too few, and you miss distinctions. Too many, and you're overfitting to noise.</p>
<h2 id="the-training-process-how-models-learn">The Training Process: How Models Learn</h2>
<p>Now let's understand what actually happens when you call <code>model.fit()</code>. The <strong>training process</strong> is the procedure by which a model adjusts its internal parameters to better match the training data.</p>
<p>Here's the cycle:</p>
<ol>
<li><strong>Initialize</strong>: Start with random (or default) parameter values</li>
<li><strong>Predict</strong>: Use current parameters to make predictions</li>
<li><strong>Measure error</strong>: Compare predictions to actual values</li>
<li><strong>Update parameters</strong>: Adjust to reduce the error</li>
<li><strong>Repeat</strong>: Go back to step 2 until error is small enough</li>
</ol>
<p>This is iterative learning—the model gets a little better with each cycle.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Conceptual training loop (what happens inside model.fit())</span>
<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># Step 1: Initialize parameters randomly</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="c1"># Step 2: Make predictions with current parameters</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>

        <span class="c1"># Step 3: Measure error (mean squared error)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 4: Calculate how to adjust parameters (gradients)</span>
        <span class="n">weight_gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">error</span>
        <span class="n">bias_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

        <span class="c1"># Step 5: Update parameters</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">weight_gradients</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">bias_gradient</span>

        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: MSE = </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span>
</code></pre></div></td></tr></table></div>
<p>This simple loop is the heart of nearly all machine learning!</p>
<h4 id="diagram-training-process-animator">Diagram: Training Process Animator</h4>
<details>
<summary>Training Process Animator</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Understand, Apply</p>
<p>Learning Objective: Visualize the iterative training process showing how parameters adjust over time to fit the data</p>
<p>Canvas Layout (850x550):
- Main area (850x400): Scatter plot with evolving regression line
- Bottom area (850x150): Controls and metrics</p>
<p>Main Visualization:
- Data points (fixed throughout training)
- Regression line that updates with each iteration
- Residual lines from points to current line
- Ghost trails of previous line positions (fading)
- Current parameter values displayed: w = X.XX, b = X.XX</p>
<p>Training Animation:
- Step through iterations one at a time or auto-play
- Line visibly adjusts toward better fit
- Error metric (MSE) decreases over time
- Color intensity of line changes (red = high error, green = low error)</p>
<p>Metrics Panel:
- Current iteration counter: 0 / 1000
- Mean Squared Error: updating value
- Line chart showing MSE over iterations
- "Converged!" message when improvement stops</p>
<p>Interactive Controls:
- Button: "Step" - advance one iteration
- Button: "Play/Pause" - auto-advance
- Speed slider: iterations per second
- Button: "Reset" - restart training
- Slider: Learning rate (0.001 to 1.0)
- Dropdown: Different starting positions</p>
<p>Educational Overlays:
- First iteration: "Starting with random parameters"
- Early iterations: "Big adjustments to reduce error"
- Later iterations: "Fine-tuning approaches optimal"
- Converged: "Training complete!"</p>
<p>Implementation: p5.js with smooth animation</p>
</details>
<h2 id="learning-algorithm-and-model-training">Learning Algorithm and Model Training</h2>
<p>A <strong>learning algorithm</strong> is the specific procedure used to find good parameters. It defines how the model adjusts its weights based on the error. Different algorithms have different strategies:</p>
<ul>
<li><strong>Ordinary Least Squares</strong>: Solve directly using linear algebra (fast, exact for linear regression)</li>
<li><strong>Gradient Descent</strong>: Iteratively follow the slope downhill (general, works for complex models)</li>
<li><strong>Stochastic Gradient Descent</strong>: Use random samples for faster updates (scales to big data)</li>
</ul>
<p><strong>Model training</strong> is the execution of the learning algorithm on your data. It's the process of finding parameter values that minimize prediction error.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Different learning algorithms for the same problem</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">SGDRegressor</span>

<span class="c1"># Method 1: Ordinary Least Squares (closed-form solution)</span>
<span class="n">ols_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">ols_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Method 2: Stochastic Gradient Descent (iterative)</span>
<span class="n">sgd_model</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s1">&#39;optimal&#39;</span><span class="p">)</span>
<span class="n">sgd_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Both should give similar results!</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OLS coefficients: </span><span class="si">{</span><span class="n">ols_model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SGD coefficients: </span><span class="si">{</span><span class="n">sgd_model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>For linear regression, OLS is typically faster and more accurate. But gradient descent becomes essential for complex models like neural networks where closed-form solutions don't exist.</p>
<h2 id="generalization-the-ultimate-goal">Generalization: The Ultimate Goal</h2>
<p><strong>Generalization</strong> is the ability of a trained model to perform well on new, unseen data. This is the whole point of machine learning! A model that only works on training data is useless—we need it to work in the real world.</p>
<p>Think about it:</p>
<ul>
<li>We train on past house sales, but want to predict future prices</li>
<li>We train on known spam, but want to catch new spam</li>
<li>We train on diagnosed patients, but want to diagnose new patients</li>
</ul>
<p>The challenge: training data is limited, but the real world is vast. A model must learn <em>general patterns</em> that transfer to new situations, not <em>specific quirks</em> of the training data.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># The generalization test: does it work on data it hasn&#39;t seen?</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">train_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># How well it memorized</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>      <span class="c1"># How well it generalized</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training R²: </span><span class="si">{</span><span class="n">train_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test R²: </span><span class="si">{</span><span class="n">test_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generalization gap: </span><span class="si">{</span><span class="n">train_score</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>A small gap means good generalization. A large gap means the model memorized training data instead of learning patterns.</p>
<h2 id="training-error-test-error-and-prediction-error">Training Error, Test Error, and Prediction Error</h2>
<p>Understanding different types of error is crucial for diagnosing model problems.</p>
<p><strong>Training error</strong> (also called in-sample error) measures how well the model fits the training data. It's calculated using the same data used to train the model.</p>
<p><strong>Test error</strong> (also called out-of-sample error) measures how well the model performs on new data it hasn't seen. This is the true measure of model quality.</p>
<p><strong>Prediction error</strong> is the error on any specific prediction—the difference between predicted and actual values.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c1"># Training error</span>
<span class="n">train_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>

<span class="c1"># Test error</span>
<span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training MSE: </span><span class="si">{</span><span class="n">train_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test MSE: </span><span class="si">{</span><span class="n">test_mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Training Error</th>
<th>Test Error</th>
<th>Diagnosis</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both high</td>
<td>High</td>
<td>High</td>
<td>Underfitting (model too simple)</td>
</tr>
<tr>
<td>Train low, test high</td>
<td>Low</td>
<td>High</td>
<td>Overfitting (model memorized)</td>
</tr>
<tr>
<td>Both low</td>
<td>Low</td>
<td>Low</td>
<td>Good fit!</td>
</tr>
<tr>
<td>Train high, test low</td>
<td>High</td>
<td>Low</td>
<td>Rare; check for data leakage</td>
</tr>
</tbody>
</table>
<p>The pattern to watch: if training error is much lower than test error, you're overfitting.</p>
<h4 id="diagram-error-types-visualizer">Diagram: Error Types Visualizer</h4>
<details>
<summary>Error Types Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Analyze, Evaluate</p>
<p>Learning Objective: Understand the relationship between training and test error, and diagnose underfitting vs overfitting</p>
<p>Canvas Layout (850x500):
- Left panel (425x350): Training data and model fit
- Right panel (425x350): Test data and model fit
- Bottom area (850x150): Error metrics and diagnosis</p>
<p>Left Panel - Training View:
- Scatter plot of training data
- Fitted model curve/line
- Residual lines shown
- Training MSE displayed
- Color coding: blue for data, green for good fit</p>
<p>Right Panel - Test View:
- Scatter plot of test data (different points)
- Same model from training overlaid
- Residual lines to new points
- Test MSE displayed
- Color coding: orange for data, fit quality color-coded</p>
<p>Bottom Panel - Diagnosis:
- Bar chart comparing Training MSE vs Test MSE
- Gap indicator with color coding
- Diagnosis text: "Underfitting", "Good Fit", or "Overfitting"
- Recommendations based on diagnosis</p>
<p>Interactive Controls:
- Slider: Model complexity (polynomial degree 1-15)
- Button: "Generate New Data"
- Slider: Noise level in data
- Slider: Training set size
- Checkbox: "Show residuals"</p>
<p>Visual Feedback:
- As complexity increases, show training error dropping
- Show test error following U-shaped curve
- Highlight the optimal complexity point
- Animate the gap between train and test growing with overfitting</p>
<p>Key Learning Moments:
- Degree 1-2: "Model too simple - both errors high"
- Degree 3-4: "Sweet spot - errors low and similar"
- Degree 10+: "Model too complex - train low, test high"</p>
<p>Implementation: p5.js with split-panel visualization</p>
</details>
<h2 id="loss-function-measuring-prediction-quality">Loss Function: Measuring Prediction Quality</h2>
<p>A <strong>loss function</strong> (also called error function or objective function) measures how wrong a single prediction is. It takes the predicted value and actual value, and returns a number indicating how bad the prediction was.</p>
<p>Common loss functions for regression:</p>
<table>
<thead>
<tr>
<th>Loss Function</th>
<th>Formula</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr>
<td>Squared Error</td>
<td><span class="arithmatex">\((y - \hat{y})^2\)</span></td>
<td>Penalizes large errors heavily</td>
</tr>
<tr>
<td>Absolute Error</td>
<td>$</td>
<td>y - \hat{y}</td>
</tr>
<tr>
<td>Huber Loss</td>
<td>Squared if small, absolute if large</td>
<td>Best of both</td>
</tr>
</tbody>
</table>
<p>For classification:</p>
<table>
<thead>
<tr>
<th>Loss Function</th>
<th>Use Case</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr>
<td>Binary Cross-Entropy</td>
<td>Two classes</td>
<td>Measures probability error</td>
</tr>
<tr>
<td>Categorical Cross-Entropy</td>
<td>Multiple classes</td>
<td>Extension of binary</td>
</tr>
<tr>
<td>Hinge Loss</td>
<td>SVM classifiers</td>
<td>Margin-based</td>
</tr>
</tbody>
</table>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Loss for a single prediction&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;More robust to outliers&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Example</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="mi">90</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Squared Error: </span><span class="si">{</span><span class="n">squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 100</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Absolute Error: </span><span class="si">{</span><span class="n">absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">y_pred</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 10</span>
</code></pre></div></td></tr></table></div>
<p>The choice of loss function affects what the model optimizes for. Squared error emphasizes getting big predictions right; absolute error treats all errors equally.</p>
<h2 id="cost-function-total-training-error">Cost Function: Total Training Error</h2>
<p>The <strong>cost function</strong> (also called objective function) aggregates the loss across all training examples. While loss measures error for one prediction, cost measures error for the entire training set.</p>
<div class="arithmatex">\[J(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(y_i, \hat{y}_i)\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(J(\theta)\)</span> is the cost as a function of parameters <span class="arithmatex">\(\theta\)</span></li>
<li><span class="arithmatex">\(L\)</span> is the loss function</li>
<li><span class="arithmatex">\(n\)</span> is the number of training examples</li>
</ul>
<p>For Mean Squared Error:</p>
<div class="arithmatex">\[J(\theta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate total cost (MSE) for given parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="n">squared_errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">squared_errors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>

<span class="c1"># Example: cost at different parameter values</span>
<span class="n">weights_bad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="n">weights_good</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">150</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="o">-</span><span class="mi">500</span><span class="p">])</span>

<span class="n">cost_bad</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">weights_bad</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">cost_good</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">weights_good</span><span class="p">,</span> <span class="mi">50000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cost with bad weights: </span><span class="si">{</span><span class="n">cost_bad</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cost with good weights: </span><span class="si">{</span><span class="n">cost_good</span><span class="si">:</span><span class="s2">,.0f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Training is all about minimizing this cost function. The model that minimizes cost is the best fit to the training data.</p>
<h2 id="optimization-finding-the-best-parameters">Optimization: Finding the Best Parameters</h2>
<p><strong>Optimization</strong> is the mathematical process of finding parameter values that minimize (or maximize) some objective. In machine learning, we minimize the cost function.</p>
<p>Imagine the cost function as a landscape:</p>
<ul>
<li>High points = bad parameters (high cost)</li>
<li>Low points = good parameters (low cost)</li>
<li>The goal = find the lowest point (global minimum)</li>
</ul>
<p>For simple linear regression, we can find the optimal parameters directly using calculus (the "normal equations"). But for complex models, we need iterative methods.</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Closed-form solution (only works for linear regression)</span>
<span class="c1"># The normal equations: θ = (X^T X)^(-1) X^T y</span>
<span class="k">def</span> <span class="nf">solve_normal_equations</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X_with_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">X</span><span class="p">])</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_with_bias</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_with_bias</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_with_bias</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># bias, weights</span>

<span class="n">bias</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">solve_normal_equations</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal bias: </span><span class="si">{</span><span class="n">bias</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimal weights: </span><span class="si">{</span><span class="n">weights</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>This direct solution is fast and exact for linear regression. But what about models where no closed-form solution exists? That's where gradient descent comes in.</p>
<h2 id="gradient-descent-the-universal-optimizer">Gradient Descent: The Universal Optimizer</h2>
<p><strong>Gradient descent</strong> is the workhorse algorithm of machine learning. It finds the minimum of a function by repeatedly taking steps in the direction of steepest descent.</p>
<p>The intuition: Imagine you're blindfolded on a hilly landscape and want to find the lowest point. What would you do? Feel the slope under your feet and step downhill. Repeat until you can't go any lower.</p>
<p>That's gradient descent:</p>
<ol>
<li>Calculate the gradient (slope) of the cost function at current position</li>
<li>Take a step in the opposite direction (downhill)</li>
<li>Repeat until you reach a minimum</li>
</ol>
<div class="arithmatex">\[\theta_{new} = \theta_{old} - \alpha \cdot \nabla J(\theta)\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(\theta\)</span> is the parameter vector</li>
<li><span class="arithmatex">\(\alpha\)</span> is the learning rate (step size)</li>
<li><span class="arithmatex">\(\nabla J(\theta)\)</span> is the gradient (direction of steepest ascent)</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find optimal parameters using gradient descent</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="c1"># Predictions with current parameters</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>

        <span class="c1"># Gradients (direction of steepest ascent)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">weight_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">error</span>
        <span class="n">bias_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

        <span class="c1"># Update parameters (step downhill)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">weight_gradient</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">bias_gradient</span>

        <span class="c1"># Track cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">costs</span>

<span class="c1"># Train with gradient descent</span>
<span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Plot convergence</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">costs</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Gradient Descent Convergence&#39;</span><span class="p">,</span>
              <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="s1">&#39;Cost (MSE)&#39;</span><span class="p">})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<h4 id="diagram-gradient-descent-visualizer">Diagram: Gradient Descent Visualizer</h4>
<details>
<summary>Gradient Descent Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Understand, Apply</p>
<p>Learning Objective: Visualize gradient descent as navigating a cost landscape to find the minimum</p>
<p>Canvas Layout (850x600):
- Main area (850x450): 3D surface or 2D contour plot of cost function
- Bottom area (850x150): Controls and current state</p>
<p>Main Visualization Options:
Toggle between:
1. 3D Surface View:
   - Cost function as a bowl-shaped surface
   - Current position marked with a ball
   - Path of descent shown as connected line
   - Axes: weight1, weight2, cost</p>
<ol>
<li>2D Contour View:</li>
<li>Top-down view with contour lines (like a topographic map)</li>
<li>Current position marked with dot</li>
<li>Gradient arrow showing direction of steepest descent</li>
<li>Path traced as line with markers at each step</li>
</ol>
<p>Animation:
- Ball/dot moves along gradient descent path
- Arrow shows current gradient direction
- Leave trail showing history of positions
- Cost value updates in real-time</p>
<p>Interactive Controls:
- Button: "Step" - take one gradient step
- Button: "Run" - animate continuous descent
- Slider: Learning rate (0.001 to 2.0)
- Dropdown: Starting position (corner, middle, near minimum)
- Checkbox: "Show gradient arrows"
- Checkbox: "Show path history"</p>
<p>Learning Rate Effects:
- Too small: Slow progress, many small steps
- Just right: Steady progress to minimum
- Too large: Overshooting, oscillation, or divergence</p>
<p>Visual Feedback:
- Speed indicator showing step sizes
- Warning when oscillating (too high learning rate)
- "Converged!" message when reaching minimum
- Display current parameter values and cost</p>
<p>Different Landscapes:
- Dropdown: Simple bowl, Elongated valley, Multiple minima
- Shows how gradient descent behaves differently</p>
<p>Implementation: p5.js with WEBGL for 3D or 2D canvas</p>
</details>
<h2 id="learning-rate-the-step-size">Learning Rate: The Step Size</h2>
<p>The <strong>learning rate</strong> (often denoted <span class="arithmatex">\(\alpha\)</span> or <span class="arithmatex">\(\eta\)</span>) controls how big each step is during gradient descent. It's one of the most important hyperparameters in machine learning.</p>
<table>
<thead>
<tr>
<th>Learning Rate</th>
<th>Behavior</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr>
<td>Too small</td>
<td>Very slow convergence</td>
<td>May never finish</td>
</tr>
<tr>
<td>Just right</td>
<td>Steady progress to minimum</td>
<td>Goldilocks zone</td>
</tr>
<tr>
<td>Too large</td>
<td>Overshoots minimum</td>
<td>May diverge (explode)</td>
</tr>
</tbody>
</table>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>

<span class="c1"># Compare different learning rates</span>
<span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>

<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">costs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;LR = </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">))</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Effect of Learning Rate on Convergence&#39;</span><span class="p">,</span>
                  <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;Iteration&#39;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>Finding the right learning rate often requires experimentation. Some strategies:</p>
<ul>
<li><strong>Start large, decay</strong>: Begin with a larger rate, reduce over time</li>
<li><strong>Grid search</strong>: Try several values, pick the best</li>
<li><strong>Adaptive methods</strong>: Algorithms like Adam adjust the rate automatically</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Learning Rate Rules of Thumb</p>
<p>Start with 0.01 or 0.001 as a default. If training is too slow, increase it. If cost increases or oscillates wildly, decrease it. For neural networks, use adaptive optimizers like Adam that adjust automatically.</p>
</div>
<h2 id="convergence-knowing-when-to-stop">Convergence: Knowing When to Stop</h2>
<p><strong>Convergence</strong> is when the optimization process has reached a stable solution—the parameters stop changing significantly. At convergence, additional iterations don't improve the model.</p>
<p>Signs of convergence:</p>
<ul>
<li>Cost function value stops decreasing</li>
<li>Parameter changes become very small</li>
<li>Gradient magnitudes approach zero</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">gradient_descent_with_convergence</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient descent that stops when converged</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">prev_cost</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Check for convergence</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">prev_cost</span> <span class="o">-</span> <span class="n">cost</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converged after </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> iterations!&quot;</span><span class="p">)</span>
            <span class="k">break</span>

        <span class="c1"># Gradient update</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">error</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>

        <span class="n">prev_cost</span> <span class="o">=</span> <span class="n">cost</span>

    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">i</span>

<span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="n">gradient_descent_with_convergence</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training completed in </span><span class="si">{</span><span class="n">iterations</span><span class="si">}</span><span class="s2"> iterations&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>Common stopping criteria:</p>
<ul>
<li>Maximum iterations reached</li>
<li>Cost improvement below threshold</li>
<li>Gradient magnitude below threshold</li>
<li>Validation performance stops improving (early stopping)</li>
</ul>
<h2 id="local-minimum-vs-global-minimum">Local Minimum vs Global Minimum</h2>
<p>When optimizing, we want to find the <strong>global minimum</strong>—the lowest point across the entire cost landscape. But gradient descent can get stuck in a <strong>local minimum</strong>—a point that's lower than its neighbors but not the absolute lowest.</p>
<p>Think of it like hiking in the mountains:</p>
<ul>
<li><strong>Global minimum</strong>: The valley with the lowest elevation in the entire range</li>
<li><strong>Local minimum</strong>: A small valley that's lower than nearby areas but not the lowest overall</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Illustration of local vs global minima</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>

<span class="c1"># A function with multiple minima</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span>  <span class="c1"># Has a local and global minimum</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Cost Function&#39;</span><span class="p">))</span>

<span class="c1"># Mark the minima</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_annotation</span><span class="p">(</span><span class="n">x</span><span class="o">=-</span><span class="mf">1.3</span><span class="p">,</span> <span class="n">y</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">&quot;Global Minimum&quot;</span><span class="p">,</span> <span class="n">showarrow</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">arrowhead</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">add_annotation</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">y</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">&quot;Local Minimum&quot;</span><span class="p">,</span> <span class="n">showarrow</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">arrowhead</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Local vs Global Minima&#39;</span><span class="p">,</span>
                  <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;Parameter Value&#39;</span><span class="p">,</span> <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;Cost&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<p>For linear regression, the cost function is convex (bowl-shaped), so any minimum is the global minimum. But for neural networks and other complex models, the landscape can have many local minima.</p>
<p>Strategies to avoid local minima:</p>
<ul>
<li><strong>Random restarts</strong>: Run optimization from different starting points</li>
<li><strong>Momentum</strong>: Add "inertia" to roll through small local minima</li>
<li><strong>Stochastic gradient descent</strong>: Random sampling adds noise that can escape local minima</li>
<li><strong>Learning rate schedules</strong>: Adjusting the rate during training</li>
</ul>
<h4 id="diagram-optimization-landscape-explorer">Diagram: Optimization Landscape Explorer</h4>
<details>
<summary>Optimization Landscape Explorer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy: Analyze, Evaluate</p>
<p>Learning Objective: Understand the difference between local and global minima and how optimization strategies affect which minimum is found</p>
<p>Canvas Layout (850x550):
- Main area (850x400): Interactive cost landscape with optimizer
- Bottom area (850x150): Controls and explanation</p>
<p>Main Visualization:
- 2D function plot with multiple valleys (minima)
- One global minimum (deepest valley)
- Several local minima (shallower valleys)
- Current optimizer position marked with ball
- Gradient direction shown with arrow</p>
<p>Optimization Journey:
- Animate ball rolling down toward minimum
- Show where it gets "stuck" in local minima
- Display "Stuck in local minimum!" vs "Found global minimum!"</p>
<p>Interactive Controls:
- Click on landscape to set starting position
- Button: "Start Optimization"
- Slider: Learning rate (affects whether it escapes local minima)
- Checkbox: "Add Momentum" (helps escape shallow minima)
- Dropdown: Cost landscape type (convex bowl, multi-modal, complex)
- Slider: Noise level (stochastic gradient descent effect)</p>
<p>Landscape Types:
1. Convex (simple bowl): Always finds global minimum
2. Two minima: May get stuck depending on start
3. Many minima: Very sensitive to start and learning rate
4. Saddle points: Shows how gradient can slow at flat regions</p>
<p>Educational Annotations:
- Mark each minimum with its cost value
- Highlight when optimizer escapes a local minimum
- Show gradient magnitude decreasing near minima
- Compare final cost to global minimum cost</p>
<p>Statistics Panel:
- Number of iterations
- Final cost value
- Distance from global minimum
- Success rate across multiple random starts</p>
<p>Implementation: p5.js with physics-based ball animation</p>
</details>
<h2 id="putting-it-all-together-the-ml-pipeline">Putting It All Together: The ML Pipeline</h2>
<p>Here's how all these concepts connect in a typical machine learning workflow:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>

<span class="c1"># 1. Load and prepare data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>

<span class="c1"># 2. Split for generalization testing</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 3. Scale features (important for gradient descent!)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 4. Train using gradient descent</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span>      <span class="c1"># Loss function</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="s1">&#39;optimal&#39;</span><span class="p">,</span>    <span class="c1"># Learning rate strategy</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>              <span class="c1"># Maximum iterations</span>
    <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>                   <span class="c1"># Convergence tolerance</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 5. Evaluate generalization</span>
<span class="n">train_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">test_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=== Training Performance ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">train_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R²: </span><span class="si">{</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="w"> </span><span class="n">train_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Test Performance (Generalization) ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE: </span><span class="si">{</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">test_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;R²: </span><span class="si">{</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">test_pred</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Learned Parameters ===&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Coefficients: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Intercept: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p>The pipeline connects:</p>
<ol>
<li><strong>Data</strong> → Training/test split for generalization testing</li>
<li><strong>Cost Function</strong> (loss) → Defines what "good" means</li>
<li><strong>Optimization</strong> (gradient descent) → Finds best parameters</li>
<li><strong>Learning Rate</strong> → Controls optimization speed</li>
<li><strong>Convergence</strong> → Knows when to stop</li>
<li><strong>Generalization</strong> → Tests on unseen data</li>
</ol>
<h2 id="summary-the-machine-learning-mental-model">Summary: The Machine Learning Mental Model</h2>
<p>You now understand the core concepts that power all of machine learning:</p>
<ul>
<li><strong>Machine learning</strong> teaches computers to learn patterns from data</li>
<li><strong>Supervised learning</strong> learns from labeled examples; <strong>unsupervised learning</strong> discovers structure without labels</li>
<li><strong>Classification</strong> predicts categories; <strong>clustering</strong> finds natural groups</li>
<li><strong>Training</strong> iteratively adjusts parameters to reduce error</li>
<li><strong>Generalization</strong> is the ability to perform well on new data</li>
<li><strong>Loss functions</strong> measure prediction error; <strong>cost functions</strong> aggregate over training data</li>
<li><strong>Gradient descent</strong> finds optimal parameters by following the slope downhill</li>
<li><strong>Learning rate</strong> controls step size; too small is slow, too large is unstable</li>
<li><strong>Convergence</strong> occurs when parameters stabilize</li>
<li><strong>Local minima</strong> can trap optimization; various strategies help escape them</li>
</ul>
<p>This foundation prepares you for the most exciting topic in modern AI: neural networks. Everything you've learned—gradients, optimization, loss functions, generalization—will apply directly. You're ready.</p>
<h2 id="looking-ahead">Looking Ahead</h2>
<p>In the next chapter, we'll build neural networks and use PyTorch. You'll see how the gradient descent and loss function concepts you learned here scale up to millions of parameters. The optimization principles are the same—just with more powerful models that can learn incredibly complex patterns.</p>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<ul>
<li>Machine learning is about learning patterns from data rather than explicitly programming rules</li>
<li>Supervised learning uses labeled data; unsupervised learning discovers structure without labels</li>
<li>The training process iteratively adjusts parameters to minimize a cost function</li>
<li>Generalization—performance on unseen data—is the true measure of model quality</li>
<li>Loss functions measure individual prediction errors; cost functions aggregate over training data</li>
<li>Gradient descent finds optimal parameters by repeatedly stepping in the direction of steepest descent</li>
<li>Learning rate controls step size; finding the right rate requires experimentation</li>
<li>Convergence occurs when optimization has reached a stable solution</li>
<li>For complex models, local minima can trap optimization; strategies exist to escape them</li>
</ul>







  
  



  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../11-nonlinear-models-regularization/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Non-linear Models">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Non-linear Models
              </div>
            </div>
          </a>
        
        
          
          <a href="../13-neural-networks-pytorch/" class="md-footer__link md-footer__link--next" aria-label="Next: Neural Networks and PyTorch">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Neural Networks and PyTorch
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2025 Dan McCreary. Licensed under <a href="license/">CC BY-NC-SA 4.0</a> for non-commercial use.
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.path", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "navigation.footer", "content.action.edit"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../js/extra.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>