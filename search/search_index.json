{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Data Science with Python and AI","text":"<p>Welcome to our website for the course that uses AI to help students learn data science.  This course is designed to help advanced high-school and freshman college students quickly learn the core concepts of data science with extensive use of Micro-Simulations and AI.</p> <p>Please contact me on LinkedIn if you have any questions.</p> <ul> <li>Dan McCreary</li> </ul>"},{"location":"about/","title":"About This Book","text":"<p>In today\u2019s data-rich world, the ability to extract meaningful insights from information isn\u2019t just an advantage\u2014it\u2019s essential. This book, Introduction to Data Science with Python and AI, aims to empower advanced high-school and early college students with the tools, concepts, and critical thinking skills needed to become fluent in the language of data. Through interactive modules\u2014MicroSims\u2014you\u2019ll build a deep understanding of linear regression and progressively more sophisticated predictive models using Python, NumPy, and PyTorch. Our focus remains on balancing interpretability and predictive performance, helping you discover how simple models can often offer powerful\u2014and explainable\u2014solutions.</p>"},{"location":"about/#why-this-book-matters","title":"Why This Book Matters","text":""},{"location":"about/#1-data-science-skills-are-in-high-demand","title":"1. Data Science Skills Are in High Demand","text":"<p>The U.S. Bureau of Labor Statistics projects that jobs for data scientists will grow by 36% from 2021 to 2031\u2014far exceeding average job growth across all occupations (Harvard Extension School). IBM research also forecasts approximately 2.7 million job openings in data science and data engineering in the near future (Medium).</p>"},{"location":"about/#2-businesses-thrive-with-data-expertise","title":"2. Businesses Thrive with Data Expertise","text":"<p>An empirical study spanning 2.9 million employees across 7,408 investment firms (2008\u20132021) found that firms with more data specialists performed better, particularly when local universities introduced data science programs\u2014a pattern suggestive of a causal link (F.N. London).</p>"},{"location":"about/#3-cross-sector-demand-for-data-literacy","title":"3. Cross-Sector Demand for Data Literacy","text":"<p>Beyond specialized roles, basic data literacy is quickly becoming a core competency across all fields. A global study by Forrester Consulting (sponsored by Tableau) found that 82% of decision-makers expect basic data literacy from employees in every department, with projections that by 2025 nearly 70% of employees will heavily rely on data (TIME).</p>"},{"location":"about/#4-ai-increases-demand-for-complementary-human-skills","title":"4. AI Increases Demand for Complementary Human Skills","text":"<p>Research analyzing 12 million job postings from 2018\u20132023 in the U.S. shows that AI is raising the demand\u2014and wage premiums\u2014for complementary human skills such as digital literacy, teamwork, and resilience, especially in AI-impacted roles like data science (arXiv). This means that learning to work effectively with data and AI tools will remain valuable\u2014and increasingly so.</p>"},{"location":"about/#5-data-driven-innovation-boosts-subject-matter-experts","title":"5. Data-Driven Innovation Boosts Subject Matter Experts","text":"<p>A case study of 85 Subject Matter Expertss in the U.K. highlighted how data science techniques like customer forecasting and predictive maintenance drive productivity, innovation, and job creation\u2014though these benefits hinge on developing both skills and infrastructure (Reddit, arXiv).</p> <p>Through this book, you won\u2019t just learn to code\u2014you\u2019ll gain an essential toolkit for navigating, interrogating, and communicating with data in any field you choose. By starting with clear, hands-on simulations and building up to more advanced techniques, we ensure that every concept you encounter is grounded in understanding, not just syntax.</p> <p>Here\u2019s an additional section you can insert right after the \u201cWhy This Book Matters\u201d part of your document. It highlights why using AI-generated interactive simulations (MicroSims) offers a superior learning experience compared to traditional static textbooks.</p>"},{"location":"about/#why-our-ai-powered-interactive-simulations-outperform-static-textbooks","title":"Why Our AI-Powered Interactive Simulations Outperform Static Textbooks","text":"<p>Traditional textbooks, while valuable as references, often present information in a linear, static, and passive format. This approach leaves students reading about concepts without the opportunity to experiment, test, and immediately see the impact of changing parameters. Our methodology fundamentally changes that dynamic by integrating AI-generated interactive simulations (MicroSims) into the core of the learning experience.</p> <p>Key Advantages:</p> <ol> <li> <p>Active Learning Over Passive Reading    Students don\u2019t just read about a concept\u2014they manipulate variables, observe results, and form hypotheses in real time. This active engagement significantly improves retention compared to passive reading.</p> </li> <li> <p>Personalized Exploration    AI adapts simulations to a student\u2019s pace and curiosity. If a learner struggles with a concept, the system can generate additional guided examples or simplify the scenario. Advanced learners can explore \u201cwhat-if\u201d situations beyond the basic curriculum.</p> </li> <li> <p>Immediate Feedback Loop    Instead of waiting until a homework assignment is graded, students get instant feedback within the simulation. This encourages experimentation and builds confidence.</p> </li> <li> <p>Bridging Theory and Practice    Concepts such as statistical distributions, regression coefficients, and the bias-variance tradeoff become tangible. By adjusting sliders or toggling options, abstract equations transform into visual, intuitive insights.</p> </li> <li> <p>Adaptive Content Updates    AI-generated content allows the textbook to evolve with the field. When new methods or datasets emerge, we can rapidly integrate them into simulations\u2014keeping learning materials relevant without waiting for a new print edition.</p> </li> <li> <p>Data-Driven Instruction    Student interactions within MicroSims can be logged (with privacy safeguards) to help educators identify common misunderstandings and adapt teaching strategies accordingly.</p> </li> <li> <p>Democratizing Access    Because simulations run in the browser, they require no expensive software or specialized hardware\u2014making cutting-edge, interactive learning accessible to a global audience.</p> </li> </ol> <p>The Result: Instead of memorizing definitions and formulas in isolation, learners develop conceptual mastery through guided experimentation, preparing them not only to understand data science today but also to apply it creatively to tomorrow\u2019s problems.</p> <p>Here\u2019s the additional section you can place toward the end of the About This Book page, right before the References section.</p>"},{"location":"about/#open-licensing-for-educator-adaptation","title":"Open Licensing for Educator Adaptation","text":"<p>This textbook is published under the Creative Commons Attribution\u2013NonCommercial\u2013ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED) license. This means you are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format.</li> <li>Adapt \u2014 remix, transform, and build upon the material.</li> </ul> <p>As long as you follow these conditions:</p> <ol> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> </ol>"},{"location":"about/#why-this-matters-for-instructors","title":"Why This Matters for Instructors","text":"<p>The open license ensures that educators can quickly adapt this textbook to match their school\u2019s curriculum, pacing, and student needs without seeking additional permissions.</p> <p>Instructors can add new examples relevant to their region, translate content into different languages, or integrate local datasets and case studies\u2014while still contributing back to the global teaching community.</p> <p>For the full legal text of the license, see: https://creativecommons.org/licenses/by-nc-sa/4.0/</p>"},{"location":"about/#references","title":"References","text":"<ol> <li> <p>https://en.wikipedia.org/wiki/Data_science \u2013 today \u2013 Wikipedia \u2013 Provides foundational context and history of data science, framing the evolution of the field (Wikipedia).</p> </li> <li> <p>https://extension.harvard.edu/blog/why-study-data-science/ \u2013 Jul\u202f21\u202f2023 \u2013 Harvard Extension School blog \u2013 Highlights job growth (36\u202f%) and career opportunities in data science (Harvard Extension School).</p> </li> <li> <p>https://medium.com/analysts-corner/bridging-the-data-science-skills-gap-c90b4d994bff \u2013 last 9 months \u2013 Medium article \u2013 Cites IBM prediction of 2.7 million roles in data science and engineering (Medium).</p> </li> <li> <p>https://www.fnlondon.com/articles/firms-say-they-like-arts-graduates-but-its-the-data-geeks-who-perform\u2011e2d6e034 \u2013 May\u202f5\u202f2025 \u2013 Financial News London \u2013 Reports study showing better investment firm performance tied to hiring more data specialists (F.N. London).</p> </li> <li> <p>https://time.com/6290684/data-literacy-us-national-security/ \u2013 Jun\u202f29\u202f2023 \u2013 TIME \u2013 Summarizes Forrester/Tab\u200bleau finding that 82\u202f% of decision\u2011makers expect data literacy, with 70\u202f% of employees using data heavily by 2025 (TIME).</p> </li> <li> <p>https://arxiv.org/abs/2412.19754 \u2013 Dec\u202f27\u202f2024 \u2013 arXiv preprint \u2013 Shows AI increases demand for complementary skills like digital literacy and resilience, especially in data science roles (arXiv).</p> </li> <li> <p>https://arxiv.org/abs/2305.15454 \u2013 May\u202f24\u202f2023 \u2013 arXiv preprint \u2013 Case studies of 85 UK SMEs showing data science supports productivity, innovation, customer insight, but requires investment in skills and infrastructure (arXiv).</p> </li> <li> <p>https://en.wikipedia.org/wiki/Data_literacy \u2013 6 months ago \u2013 Wikipedia \u2013 Defines \u201cdata literacy\u201d as a fundamental capability involving reading, interpreting, evaluating, and communicating data effectively (Wikipedia).</p> </li> <li> <p>https://en.wikipedia.org/wiki/Analytical_skill \u2013 last month \u2013 Wikipedia \u2013 Describes analytical skill set (including data analysis, critical thinking) as critical across professions (Wikipedia).</p> </li> <li> <p>https://sciencedirect.com/science/article/pii/S0263237322000810 \u2013 2022 \u2013 ScienceDirect \u2013 Miller and Hughes study on market demand for data science skills across six industries (ScienceDirect).</p> </li> </ol> <p>Let me know if you'd like help drafting other sections (e.g. Preface, Tutorials, Chapter intros).</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory</li> <li>Test the build</li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> </ol>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn if you have any questions.</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description for Introduction to Data Science with Python","text":"<p>Course Duration: 10 weeks Target Audience: Advanced high school students and college freshmen Prerequisites: Basic algebra and introductory programming experience recommended</p>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This introductory course provides students with foundational knowledge and practical skills in data science using Python.  Through hands-on experience with interactive simulations MicroSims and real-world datasets,  students will develop competency in data analysis, visualization, and predictive modeling.  The course emphasizes the critical balance between model explainability and predictive accuracy,  guiding students to identify the simplest effective solutions to data-driven problems.</p>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <p>Remember (Knowledge) - Recall fundamental data science terminology and concepts - Identify key Python libraries for data science (NumPy, pandas, matplotlib, PyTorch) - Recognize different types of data and measurement scales - List the steps in the data science workflow</p> <p>Understand (Comprehension) - Explain the relationship between independent and dependent variables - Describe how linear regression models make predictions - Interpret basic statistical measures and visualizations - Summarize the trade-offs between model complexity and interpretability</p> <p>Apply (Application) - Implement basic data cleaning and preprocessing techniques - Create visualizations using Python libraries - Build simple linear regression models - Execute standard data science workflows on new datasets</p> <p>Analyze (Analysis) - Examine datasets to identify patterns and relationships - Compare different modeling approaches for the same problem - Distinguish between correlation and causation in data relationships - Evaluate model performance using appropriate metrics</p> <p>Evaluate (Evaluation) - Assess the quality and reliability of data sources - Critique model assumptions and limitations - Judge the appropriateness of different models for specific problems - Validate model performance and identify potential overfitting</p> <p>Create (Synthesis) - Design experiments to test hypotheses using data - Construct predictive models for real-world scenarios - Develop data-driven solutions to complex problems - Generate original insights from exploratory data analysis</p>"},{"location":"course-description/#sample-weekly-schedule","title":"Sample Weekly Schedule","text":""},{"location":"course-description/#week-1-foundations-of-data-science","title":"Week 1: Foundations of Data Science","text":"<ul> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> </ul>"},{"location":"course-description/#week-2-data-exploration-and-visualization","title":"Week 2: Data Exploration and Visualization","text":"<ul> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> </ul>"},{"location":"course-description/#week-3-statistical-foundations","title":"Week 3: Statistical Foundations","text":"<ul> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> </ul>"},{"location":"course-description/#week-4-simple-linear-regression","title":"Week 4: Simple Linear Regression","text":"<ul> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> </ul>"},{"location":"course-description/#week-5-model-evaluation-and-validation","title":"Week 5: Model Evaluation and Validation","text":"<ul> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> </ul>"},{"location":"course-description/#week-6-multiple-linear-regression","title":"Week 6: Multiple Linear Regression","text":"<ul> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> </ul>"},{"location":"course-description/#week-7-introduction-to-numpy-and-advanced-computation","title":"Week 7: Introduction to NumPy and Advanced Computation","text":"<ul> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> </ul>"},{"location":"course-description/#week-8-non-linear-models-and-regularization","title":"Week 8: Non-linear Models and Regularization","text":"<ul> <li>Polynomial regression and feature transformation</li> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> </ul>"},{"location":"course-description/#week-9-introduction-to-machine-learning-with-pytorch","title":"Week 9: Introduction to Machine Learning with PyTorch","text":"<ul> <li>Neural networks and deep learning concepts</li> <li>Building simple networks with PyTorch</li> <li>MicroSim: Neural network playground</li> <li>Comparing traditional and deep learning approaches</li> </ul>"},{"location":"course-description/#week-10-capstone-project-and-model-deployment","title":"Week 10: Capstone Project and Model Deployment","text":"<ul> <li>End-to-end data science project</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> </ul>"},{"location":"course-description/#assessment-methods","title":"Assessment Methods","text":"<p>Formative Assessment (60%) - Weekly MicroSim exercises and reflections (30%) - Homework assignments applying concepts to new datasets (20%) - Peer review activities and collaborative problem-solving (10%)</p> <p>Summative Assessment (40%) - Midterm project: Complete data analysis report (15%) - Final capstone project: Original predictive modeling solution (20%) - Final examination covering theoretical concepts (5%)</p>"},{"location":"course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Computer with Python 3.8+ installed</li> <li>Access to interactive online textbook with MicroSims</li> <li>Jupyter Notebook environment</li> <li>Required Python packages: pandas, NumPy, matplotlib, scikit-learn, PyTorch</li> </ul>"},{"location":"course-description/#key-learning-principles","title":"Key Learning Principles","text":"<p>Interactive Learning: Each week features hands-on MicroSims that allow students to manipulate parameters and observe results in real-time, reinforcing theoretical concepts through experiential learning.</p> <p>Scaffolded Complexity: The course progresses systematically from simple linear relationships to complex neural networks, ensuring students build confidence before tackling advanced topics.</p> <p>Explainable AI Focus: Throughout the course, emphasis is placed on understanding and interpreting models rather than simply achieving high accuracy, preparing students for ethical and transparent data science practice.</p> <p>Real-world Applications: All examples and projects use authentic datasets and scenarios, helping students connect academic learning to practical problem-solving.</p>"},{"location":"course-description/#course-philosophy","title":"Course Philosophy","text":"<p>This course is built on the principle that effective data science requires both technical competence and critical thinking. Students will learn not just how to build predictive models, but when to use them, how to interpret their results, and how to communicate findings to diverse audiences. The integration of interactive simulations ensures that abstract mathematical concepts become concrete and intuitive, while the progression from simple to complex models helps students appreciate the value of parsimony in modeling.</p> <p>By the end of this course, students will have developed both the technical skills and analytical mindset necessary for success in advanced data science coursework or entry-level positions in data-driven fields.</p>"},{"location":"faq/","title":"Introduction to Data Science FAQs","text":""},{"location":"faq/#what-is-data-science-and-why-is-it-important","title":"What is data science, and why is it important?","text":"<p>Data science is an interdisciplinary field that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data. It is important because it enables organizations to make informed decisions, predict trends, and solve complex problems by analyzing large datasets.</p>"},{"location":"faq/#why-is-python-used-in-data-science","title":"Why is Python used in data science?","text":"<p>Python is widely used in data science due to its simplicity and versatility. It offers a vast ecosystem of libraries and frameworks like NumPy, Pandas, Matplotlib, and scikit-learn, which facilitate data manipulation, analysis, visualization, and machine learning tasks.</p>"},{"location":"faq/#what-are-jupyter-notebooks","title":"What are Jupyter Notebooks?","text":"<p>Jupyter Notebooks are interactive web-based computing environments that allow you to create and share documents containing live code, equations, visualizations, and narrative text. They are particularly popular in data science, research, and education for their ability to combine code execution with rich documentation.</p> <p>Key Features: - Interactive Code Execution: Run code cells individually and see results immediately - Multiple Language Support: Primarily Python, but also supports R, Julia, Scala, and 40+ other languages - Rich Output Display: Show plots, tables, HTML, images, and interactive widgets inline - Markdown Support: Write formatted text, equations (LaTeX), and documentation alongside code - Easy Sharing: Export to HTML, PDF, or share via GitHub, making collaboration simple</p> <p>Why Use Jupyter Notebooks for Data Science? - Exploratory Data Analysis: Perfect for iterative data exploration and visualization - Prototyping: Quickly test ideas and algorithms before building production code - Documentation: Combine code with explanations, making analysis reproducible and understandable - Education: Ideal for learning and teaching data science concepts with immediate feedback - Presentation: Create compelling data stories with code, visualizations, and narrative</p> <p>Common Use Cases: - Data cleaning and preprocessing - Statistical analysis and modeling - Machine learning experimentation - Creating reproducible research - Building interactive dashboards - Educational tutorials and assignments</p> <p>In this course, you'll use Jupyter Notebooks extensively for hands-on exercises, MicroSims, and project work, allowing you to experiment with data science concepts in an interactive environment.</p>"},{"location":"faq/#what-are-the-key-python-libraries-for-data-analysis","title":"What are the key Python libraries for data analysis?","text":"<p>The key Python libraries for data analysis include:</p> <ul> <li>NumPy: For numerical computing and array operations.</li> <li>Pandas: For data manipulation and analysis using data structures like DataFrames.</li> <li>Plotly: Modern interactive data visualization within a browser</li> <li>Matplotlib: For creating static, animated, and interactive visualizations.</li> <li>Seaborn: For statistical data visualization built on top of Matplotlib.</li> <li>scikit-learn: For machine learning algorithms and predictive data analysis.</li> </ul>"},{"location":"faq/#how-do-you-import-a-csv-file-into-a-pandas-dataframe","title":"How do you import a CSV file into a Pandas DataFrame?","text":"<p>You can import a CSV file using the <code>read_csv()</code> function from Pandas:</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv('file_name.csv')\n</code></pre> <p>Replace <code>'file_name.csv'</code> with the path to your CSV file.</p>"},{"location":"faq/#what-is-a-dataframe-in-pandas","title":"What is a DataFrame in Pandas?","text":"<p>A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). It is similar to a spreadsheet or SQL table and is the primary data structure used in Pandas for data manipulation.</p>"},{"location":"faq/#how-do-you-handle-missing-data-in-a-dataset","title":"How do you handle missing data in a dataset?","text":"<p>Missing data can be handled by:</p> <ul> <li>Removing missing values: Using <code>dropna()</code> to remove rows or columns with missing values.</li> <li>Imputing missing values: Using <code>fillna()</code> to replace missing values with a specific value, mean, median, or mode.</li> <li>Interpolate missing values: Using <code>interpolate()</code> to estimate missing values based on other data points.</li> </ul>"},{"location":"faq/#what-is-the-difference-between-numpy-arrays-and-python-lists","title":"What is the difference between NumPy arrays and Python lists?","text":"<p>NumPy arrays are fixed-size, homogeneous collections of elements (all of the same data type) optimized for numerical computations. Python lists are dynamic, heterogeneous collections that can contain elements of different data types. NumPy arrays offer better performance for mathematical operations.</p>"},{"location":"faq/#how-do-you-select-a-subset-of-data-from-a-dataframe","title":"How do you select a subset of data from a DataFrame?","text":"<p>You can select subsets using two methods:</p> <ol> <li>Label-based indexing</li> <li> <p>Integer-based indexing</p> </li> <li> <p>Label-based indexing with <code>.loc</code>:</p> </li> </ol> <pre><code>df_subset = df.loc[row_labels, column_labels]\n</code></pre> <ul> <li>Integer-based indexing with <code>.iloc</code>:</li> </ul> <pre><code>df_subset = df.iloc[row_indices, column_indices]\n</code></pre>"},{"location":"faq/#what-is-data-visualization-and-why-is-it-important","title":"What is data visualization, and why is it important?","text":"<p>Data visualization is the graphical representation of data to communicate information clearly and efficiently. It is important because it helps identify patterns, trends, and outliers in data, making complex data more accessible and understandable.</p>"},{"location":"faq/#how-do-you-create-a-simple-line-plot-using-matplotlib","title":"How do you create a simple line plot using Matplotlib?","text":"<pre><code>import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny = [10, 20, 25, 30]\n\nplt.plot(x, y)\nplt.xlabel('X-axis Label')\nplt.ylabel('Y-axis Label')\nplt.title('Simple Line Plot')\nplt.show()\n</code></pre>"},{"location":"faq/#what-is-the-purpose-of-the-groupby-function-in-pandas","title":"What is the purpose of the <code>groupby()</code> function in Pandas?","text":"<p>The <code>groupby()</code> function is used to split data into groups based on some criteria, perform operations on each group independently, and then combine the results. It is useful for aggregation, transformation, and filtration of data.</p>"},{"location":"faq/#how-do-you-merge-two-dataframes-in-pandas","title":"How do you merge two DataFrames in Pandas?","text":"<p>You can merge two DataFrames using the <code>merge()</code> function:</p> <pre><code>merged_df = pd.merge(df1, df2, on='common_column')\n</code></pre> <p>Replace <code>'common_column'</code> with the column name that is common to both DataFrames.</p>"},{"location":"faq/#what-is-the-difference-between-merge-and-concat-in-pandas","title":"What is the difference between <code>merge()</code> and <code>concat()</code> in Pandas?","text":"<ul> <li><code>merge()</code>: Combines two DataFrames based on the values of common columns (similar to SQL joins).</li> <li><code>concat()</code>: Concatenates DataFrames either vertically or horizontally, stacking them along an axis.</li> </ul>"},{"location":"faq/#how-do-you-calculate-basic-statistical-measures-like-mean-and-median-in-pandas","title":"How do you calculate basic statistical measures like mean and median in Pandas?","text":"<p>You can use built-in functions:</p> <ul> <li>Mean:</li> </ul> <pre><code>mean_value = df['column_name'].mean()\n</code></pre> <ul> <li>Median:</li> </ul> <pre><code>median_value = df['column_name'].median()\n</code></pre>"},{"location":"faq/#what-is-the-purpose-of-the-apply-function-in-pandas","title":"What is the purpose of the <code>apply()</code> function in Pandas?","text":"<p>The <code>apply()</code> function allows you to apply a function along an axis of the DataFrame (either rows or columns). It is useful for performing complex operations on DataFrame elements.</p>"},{"location":"faq/#how-do-you-create-a-pivot-table-in-pandas","title":"How do you create a pivot table in Pandas?","text":"<p>You can create a pivot table using the <code>pivot_table()</code> function:</p> <pre><code>pivot = pd.pivot_table(df, values='value_column', index='index_column', columns='columns_column', aggfunc='mean')\n</code></pre>"},{"location":"faq/#what-is-the-difference-between-supervised-and-unsupervised-learning","title":"What is the difference between supervised and unsupervised learning?","text":"<ul> <li>Supervised Learning: Involves training a model on labeled data, where the target outcome is known. Examples include regression and classification.</li> <li>Unsupervised Learning: Involves finding patterns in unlabeled data without predefined outcomes. Examples include clustering and dimensionality reduction.</li> </ul>"},{"location":"faq/#how-do-you-perform-linear-regression-using-scikit-learn","title":"How do you perform linear regression using scikit-learn?","text":"<pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"faq/#what-is-overfitting-in-machine-learning","title":"What is overfitting in machine learning?","text":"<p>Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor generalization to new, unseen data. It performs well on training data but poorly on test data.</p>"},{"location":"faq/#how-can-you-prevent-overfitting","title":"How can you prevent overfitting?","text":"<p>Overfitting can be prevented by:</p> <ul> <li>Cross-validation: Splitting data into training and validation sets.</li> <li>Regularization: Adding penalties for complex models (e.g., Lasso, Ridge).</li> <li>Simplifying the model: Reducing the number of features.</li> <li>Early stopping: Halting training when performance on a validation set starts to degrade.</li> </ul>"},{"location":"faq/#what-is-cross-validation","title":"What is cross-validation?","text":"<p>Cross-validation is a technique for assessing how a model will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets, and validating it on the remaining ones.</p>"},{"location":"faq/#how-do-you-evaluate-the-performance-of-a-regression-model","title":"How do you evaluate the performance of a regression model?","text":"<p>Common metrics include:</p> <ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-squared (Coefficient of Determination)</li> </ul>"},{"location":"faq/#what-is-the-purpose-of-feature-scaling","title":"What is the purpose of feature scaling?","text":"<p>Feature scaling standardizes the range of independent variables, improving the performance and convergence speed of some machine learning algorithms that are sensitive to the scale of data, such as gradient descent optimization.</p>"},{"location":"faq/#how-do-you-perform-feature-scaling-in-python","title":"How do you perform feature scaling in Python?","text":"<p>Using scikit-learn's <code>StandardScaler</code>:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n</code></pre>"},{"location":"faq/#what-is-one-hot-encoding","title":"What is one-hot encoding?","text":"<p>One-hot encoding is a process of converting categorical variables into a binary (0 or 1) representation. Each category becomes a new column, and a value of 1 indicates the presence of that category.</p>"},{"location":"faq/#how-do-you-perform-one-hot-encoding-in-pandas","title":"How do you perform one-hot encoding in Pandas?","text":"<p>Using the <code>get_dummies()</code> function:</p> <pre><code>encoded_df = pd.get_dummies(df, columns=['categorical_column'])\n</code></pre>"},{"location":"faq/#what-is-a-confusion-matrix","title":"What is a confusion matrix?","text":"<p>A confusion matrix is a table used to evaluate the performance of a classification model. It displays the true positives, true negatives, false positives, and false negatives, providing insight into the types of errors made by the model.</p>"},{"location":"faq/#how-do-you-calculate-accuracy-precision-and-recall-from-a-confusion-matrix","title":"How do you calculate accuracy, precision, and recall from a confusion matrix?","text":"<ul> <li>Accuracy: (TP + TN) / (TP + TN + FP + FN)</li> <li>Precision: TP / (TP + FP)</li> <li>Recall: TP / (TP + FN)</li> </ul> <p>where:</p> <ul> <li>TP=True Positives</li> <li>TN=True Negatives</li> <li>FP=False Positives</li> <li>FN=False Negatives.</li> </ul>"},{"location":"faq/#what-is-the-purpose-of-the-train_test_split-function","title":"What is the purpose of the <code>train_test_split</code> function?","text":"<p>The <code>train_test_split</code> function splits a dataset into training and testing sets, allowing you to train the model on one subset and evaluate its performance on another to prevent overfitting.</p>"},{"location":"faq/#how-do-you-split-data-into-training-and-testing-sets-in-scikit-learn","title":"How do you split data into training and testing sets in scikit-learn?","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n</code></pre>"},{"location":"faq/#what-is-the-difference-between-classification-and-regression","title":"What is the difference between classification and regression?","text":"<ul> <li>Classification: Predicts categorical labels or classes.</li> <li>Regression: Predicts continuous numerical values.</li> </ul>"},{"location":"faq/#how-do-you-handle-categorical-variables-in-machine-learning-models","title":"How do you handle categorical variables in machine learning models?","text":"<p>Categorical variables can be handled by:</p> <ul> <li>Label Encoding: Assigning a unique integer to each category.</li> <li>One-Hot Encoding: Creating binary columns for each category.</li> </ul>"},{"location":"faq/#what-is-k-means-clustering","title":"What is k-means clustering?","text":"<p>K-means clustering is an unsupervised learning algorithm that partitions data into k clusters, where each data point belongs to the cluster with the nearest mean. It aims to minimize the within-cluster sum of squares.</p>"},{"location":"faq/#how-do-you-determine-the-optimal-number-of-clusters-in-k-means","title":"How do you determine the optimal number of clusters in k-means?","text":"<p>Common methods include:</p> <ul> <li>Elbow Method: Plotting the explained variance as a function of the number of clusters and looking for an \"elbow\" point.</li> <li>Silhouette Score: Measuring how similar a data point is to its own cluster compared to other clusters.</li> </ul>"},{"location":"faq/#what-is-principal-component-analysis-pca","title":"What is principal component analysis (PCA)?","text":"<p>PCA is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information by identifying the principal components (directions of maximum variance).</p>"},{"location":"faq/#how-do-you-perform-pca-in-scikit-learn","title":"How do you perform PCA in scikit-learn?","text":"<pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data)\n</code></pre>"},{"location":"faq/#what-is-the-bias-variance-tradeoff","title":"What is the bias-variance tradeoff?","text":"<p>The bias-variance tradeoff is the balance between a model's ability to generalize to new data (low variance) and its accuracy on training data (low bias). High bias can lead to underfitting, while high variance can lead to overfitting.</p>"},{"location":"faq/#what-is-regularization-in-machine-learning","title":"What is regularization in machine learning?","text":"<p>Regularization involves adding a penalty term to the loss function to prevent overfitting by discouraging complex models. Common regularization techniques include Lasso (L1) and Ridge (L2) regression.</p>"},{"location":"faq/#how-do-you-implement-ridge-regression-in-scikit-learn","title":"How do you implement Ridge regression in scikit-learn?","text":"<pre><code>from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faq/#what-is-logistic-regression","title":"What is logistic regression?","text":"<p>Logistic regression is a classification algorithm used to predict binary outcomes (0 or 1) by modeling the probability of a certain class using a logistic function.</p>"},{"location":"faq/#how-do-you-evaluate-the-performance-of-a-classification-model","title":"How do you evaluate the performance of a classification model?","text":"<p>Common metrics include:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1 Score</li> <li>ROC AUC Score</li> </ul>"},{"location":"faq/#what-is-the-receiver-operating-characteristic-roc-curve","title":"What is the Receiver Operating Characteristic (ROC) curve?","text":"<p>The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It illustrates the diagnostic ability of a binary classifier.</p>"},{"location":"faq/#how-do-you-handle-imbalanced-datasets","title":"How do you handle imbalanced datasets?","text":"<p>Techniques include:</p> <ul> <li>Resampling Methods: Oversampling the minority class or undersampling the majority class.</li> <li>Synthetic Data Generation: Using methods like SMOTE to generate synthetic examples.</li> <li>Using Appropriate Evaluation Metrics: Focusing on precision, recall, or F1 score instead of accuracy.</li> </ul>"},{"location":"faq/#what-is-time-series-analysis","title":"What is time series analysis?","text":"<p>Time series analysis involves analyzing data points collected or recorded at specific time intervals to identify trends, cycles, and seasonal variations for forecasting and other purposes.</p>"},{"location":"faq/#how-do-you-deal-with-date-and-time-data-in-pandas","title":"How do you deal with date and time data in Pandas?","text":"<p>Pandas provides the <code>to_datetime()</code> function to convert strings to datetime objects, and you can use datetime properties and methods to manipulate date and time data.</p> <pre><code>df['date_column'] = pd.to_datetime(df['date_column'])\n</code></pre>"},{"location":"faq/#what-is-autocorrelation-in-time-series-data","title":"What is autocorrelation in time series data?","text":"<p>Autocorrelation is the correlation of a signal with a delayed copy of itself. In time series data, it measures the relationship between a variable's current value and its past values.</p>"},{"location":"faq/#how-do-you-perform-forecasting-using-arima-models","title":"How do you perform forecasting using ARIMA models?","text":"<p>Using the <code>statsmodels</code> library:</p> <pre><code>from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(time_series_data, order=(p, d, q))\nmodel_fit = model.fit()\nforecast = model_fit.forecast(steps=10)\n</code></pre>"},{"location":"faq/#what-is-natural-language-processing-nlp","title":"What is natural language processing (NLP)?","text":"<p>NLP is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language.</p>"},{"location":"faq/#how-do-you-perform-text-preprocessing-in-nlp","title":"How do you perform text preprocessing in NLP?","text":"<p>Common steps include:</p> <ul> <li>Tokenization: Splitting text into words or sentences.</li> <li>Stop Word Removal: Removing common words that add little meaning.</li> <li>Stemming/Lemmatization: Reducing words to their base or root form.</li> <li>Encoding: Converting text to numerical representation using methods like TF-IDF or word embeddings.</li> </ul>"},{"location":"faq/#what-is-tf-idf","title":"What is TF-IDF?","text":"<p>Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects how important a word is to a document in a collection. It balances the frequency of a word in a document with how common the word is across all documents.</p>"},{"location":"faq/#how-do-you-handle-large-datasets-that-dont-fit-into-memory","title":"How do you handle large datasets that don't fit into memory?","text":"<p>Techniques include:</p> <ul> <li>Data Sampling: Working with a subset of the data.</li> <li>Batch Processing: Processing data in chunks.</li> <li>Distributed Computing: Using tools like Apache Spark.</li> <li>Out-of-core Learning: Using algorithms that can learn from data incrementally.</li> </ul>"},{"location":"faq/#what-is-a-pipeline-in-scikit-learn","title":"What is a pipeline in scikit-learn?","text":"<p>A pipeline is a sequence of data processing steps assembled into a single object. It ensures that all steps are applied consistently during training and testing, simplifying the workflow.</p> <pre><code>from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\n</code></pre>"},{"location":"faq/#how-do-you-save-and-load-trained-models-in-scikit-learn","title":"How do you save and load trained models in scikit-learn?","text":"<p>Using the <code>joblib</code> library:</p> <pre><code>import joblib\n\n# Save model\njoblib.dump(model, 'model.pkl')\n\n# Load model\nmodel = joblib.load('model.pkl')\n</code></pre>"},{"location":"faq/#what-is-gradient-descent","title":"What is gradient descent?","text":"<p>Gradient descent is an optimization algorithm used to minimize the cost function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.</p>"},{"location":"faq/#what-is-the-difference-between-batch-stochastic-and-mini-batch-gradient-descent","title":"What is the difference between batch, stochastic, and mini-batch gradient descent?","text":"<ul> <li>Batch Gradient Descent: Uses the entire dataset to compute gradients.</li> <li>Stochastic Gradient Descent (SGD): Uses one sample at a time.</li> <li>Mini-Batch Gradient Descent: Uses a small subset (batch) of the data.</li> </ul>"},{"location":"faq/#how-do-you-handle-multicollinearity-in-regression-analysis","title":"How do you handle multicollinearity in regression analysis?","text":"<p>Techniques include:</p> <ul> <li>Removing correlated features.</li> <li>Principal Component Analysis (PCA) to reduce dimensionality.</li> <li>Regularization methods like Ridge regression.</li> </ul>"},{"location":"faq/#what-is-the-central-limit-theorem","title":"What is the Central Limit Theorem?","text":"<p>The Central Limit Theorem states that the sampling distribution of the sample means approaches a normal distribution as the sample size becomes large, regardless of the population's distribution.</p>"},{"location":"faq/#what-is-hypothesis-testing","title":"What is hypothesis testing?","text":"<p>Hypothesis testing is a statistical method used to decide whether there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis based on sample data.</p>"},{"location":"faq/#what-is-p-value","title":"What is p-value?","text":"<p>A p-value is the probability of observing results at least as extreme as those measured when the null hypothesis is true. A low p-value indicates that the observed data is unlikely under the null hypothesis.</p>"},{"location":"faq/#how-do-you-perform-a-t-test-in-python","title":"How do you perform a t-test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>from scipy import stats\n\nt_statistic, p_value = stats.ttest_ind(sample1, sample2)\n</code></pre>"},{"location":"faq/#what-is-the-difference-between-type-i-and-type-ii-errors","title":"What is the difference between Type I and Type II errors?","text":"<ul> <li>Type I Error: Rejecting a true null hypothesis (false positive).</li> <li>Type II Error: Failing to reject a false null hypothesis (false negative).</li> </ul>"},{"location":"faq/#what-is-an-anova-test","title":"What is an ANOVA test?","text":"<p>Analysis of Variance (ANOVA) is a statistical method used to compare means across three or more groups to see if at least one mean is different from the others.</p>"},{"location":"faq/#how-do-you-perform-an-anova-test-in-python","title":"How do you perform an ANOVA test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>from scipy import stats\n\nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n</code></pre>"},{"location":"faq/#what-is-bootstrapping-in-statistics","title":"What is bootstrapping in statistics?","text":"<p>Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It allows estimation of the sampling distribution of almost any statistic.</p>"},{"location":"faq/#what-is-the-law-of-large-numbers","title":"What is the law of large numbers?","text":"<p>The law of large numbers states that as the number of trials increases, the sample mean will converge to the expected value (population mean).</p>"},{"location":"faq/#what-is-a-probability-distribution","title":"What is a probability distribution?","text":"<p>A probability distribution describes how the values of a random variable are distributed. It defines the probabilities of different outcomes.</p>"},{"location":"faq/#what-are-common-probability-distributions-used-in-data-science","title":"What are common probability distributions used in data science?","text":"<ul> <li>Normal Distribution</li> <li>Binomial Distribution</li> <li>Poisson Distribution</li> <li>Exponential Distribution</li> </ul>"},{"location":"faq/#how-do-you-generate-random-numbers-following-a-normal-distribution-in-numpy","title":"How do you generate random numbers following a normal distribution in NumPy?","text":"<pre><code>import numpy as np\n\nrandom_numbers = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n</code></pre>"},{"location":"faq/#what-is-the-curse-of-dimensionality","title":"What is the curse of dimensionality?","text":"<p>The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces, often leading to challenges like overfitting and increased computational cost.</p>"},{"location":"faq/#how-do-you-reduce-dimensionality","title":"How do you reduce dimensionality?","text":"<p>Techniques include:</p> <ul> <li>Feature Selection: Choosing a subset of relevant features.</li> <li>Feature Extraction: Transforming data into a lower-dimensional space (e.g., PCA).</li> </ul>"},{"location":"faq/#what-is-the-difference-between-bagging-and-boosting","title":"What is the difference between bagging and boosting?","text":"<ul> <li>Bagging: Combines multiple models (usually of the same type) trained on different subsets of the data to reduce variance.</li> <li>Boosting: Sequentially trains models, where each new model focuses on correcting errors made by previous ones, reducing bias.</li> </ul>"},{"location":"faq/#what-is-a-decision-tree","title":"What is a decision tree?","text":"<p>A decision tree is a flowchart-like structure used for classification and regression that splits data into branches based on feature values to make predictions.</p>"},{"location":"faq/#how-do-you-prevent-a-decision-tree-from-overfitting","title":"How do you prevent a decision tree from overfitting?","text":"<p>By:</p> <ul> <li>Pruning: Removing branches that have little power in predicting target variables.</li> <li>Setting a maximum depth: Limiting the depth of the tree.</li> <li>Setting a minimum number of samples per leaf.</li> </ul>"},{"location":"faq/#what-is-random-forest","title":"What is random forest?","text":"<p>Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.</p>"},{"location":"faq/#how-do-you-implement-a-random-forest-classifier-in-scikit-learn","title":"How do you implement a random forest classifier in scikit-learn?","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faq/#what-is-ensemble-learning","title":"What is ensemble learning?","text":"<p>Ensemble learning combines predictions from multiple machine learning algorithms to produce a more accurate prediction than any individual model.</p>"},{"location":"faq/#what-is-a-neural-network","title":"What is a neural network?","text":"<p>A neural network is a computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons) that can learn complex patterns through training.</p>"},{"location":"faq/#what-is-the-difference-between-deep-learning-and-machine-learning","title":"What is the difference between deep learning and machine learning?","text":"<ul> <li>Machine Learning: Involves algorithms that parse data, learn from it, and make decisions.</li> <li>Deep Learning: A subset of machine learning using neural networks with multiple layers to model complex patterns.</li> </ul>"},{"location":"faq/#how-do-you-handle-class-imbalance-in-classification-problems","title":"How do you handle class imbalance in classification problems?","text":"<p>By:</p> <ul> <li>Resampling the dataset: Oversampling the minority class or undersampling the majority class.</li> <li>Using appropriate evaluation metrics: Such as ROC AUC, precision-recall curve.</li> <li>Using algorithms that handle imbalance: Like XGBoost.</li> </ul>"},{"location":"faq/#what-is-the-purpose-of-the-map-function-in-pandas","title":"What is the purpose of the <code>map()</code> function in Pandas?","text":"<p>The <code>map()</code> function is used to map values of a Series according to an input mapping or function, useful for substituting values or applying a function element-wise.</p>"},{"location":"faq/#how-do-you-sort-a-dataframe-in-pandas","title":"How do you sort a DataFrame in Pandas?","text":"<p>Using the <code>sort_values()</code> function:</p> <pre><code>sorted_df = df.sort_values(by='column_name', ascending=True)\n</code></pre>"},{"location":"faq/#what-is-the-difference-between-apply-and-applymap-in-pandas","title":"What is the difference between <code>apply()</code> and <code>applymap()</code> in Pandas?","text":"<ul> <li><code>apply()</code>: Applies a function along an axis of the DataFrame (rows or columns).</li> <li><code>applymap()</code>: Applies a function element-wise to the entire DataFrame.</li> </ul>"},{"location":"faq/#how-do-you-remove-duplicates-from-a-dataframe","title":"How do you remove duplicates from a DataFrame?","text":"<p>Using the <code>drop_duplicates()</code> function:</p> <pre><code>df_unique = df.drop_duplicates()\n</code></pre>"},{"location":"faq/#what-is-an-outlier-and-how-do-you-detect-them","title":"What is an outlier, and how do you detect them?","text":"<p>An outlier is a data point significantly different from others. Detection methods include:</p> <ul> <li>Statistical methods: Using Z-scores or IQR.</li> <li>Visualization: Box plots or scatter plots.</li> </ul>"},{"location":"faq/#how-do-you-handle-outliers-in-data","title":"How do you handle outliers in data?","text":"<p>By:</p> <ul> <li>Removing them: If they are errors.</li> <li>Transforming data: Using log or square root transformations.</li> <li>Using robust algorithms: That are less sensitive to outliers.</li> </ul>"},{"location":"faq/#what-is-data-normalization","title":"What is data normalization?","text":"<p>Data normalization scales numerical data into a specific range, typically [0,1], ensuring that each feature contributes equally to the analysis.</p>"},{"location":"faq/#how-do-you-perform-data-normalization-in-scikit-learn","title":"How do you perform data normalization in scikit-learn?","text":"<p>Using <code>MinMaxScaler</code>:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(data)\n</code></pre>"},{"location":"faq/#what-is-a-heatmap-and-when-do-you-use-it","title":"What is a heatmap, and when do you use it?","text":"<p>A heatmap is a graphical representation of data where individual values are represented as colors. It is used to visualize correlation matrices or to display patterns in data.</p>"},{"location":"faq/#how-do-you-create-a-heatmap-in-seaborn","title":"How do you create a heatmap in Seaborn?","text":"<pre><code>import seaborn as sns\n\nsns.heatmap(data, annot=True)\n</code></pre>"},{"location":"faq/#what-is-a-pairplot-in-seaborn","title":"What is a pairplot in Seaborn?","text":"<p>A pairplot creates a grid of Axes such that each variable in the data is shared across the y-axes across a single row and the x-axes across a single column, visualizing pairwise relationships.</p> <pre><code>sns.pairplot(df)\n</code></pre>"},{"location":"faq/#how-do-you-interpret-a-correlation-coefficient","title":"How do you interpret a correlation coefficient?","text":"<p>A correlation coefficient measures the strength and direction of a linear relationship between two variables. Values range from -1 to 1:</p> <ul> <li>-1: Perfect negative correlation.</li> <li>0: No correlation.</li> <li>1: Perfect positive correlation.</li> </ul>"},{"location":"faq/#what-is-the-purpose-of-the-describe-function-in-pandas","title":"What is the purpose of the <code>describe()</code> function in Pandas?","text":"<p>The <code>describe()</code> function generates descriptive statistics, including count, mean, standard deviation, min, max, and quartile values for numerical columns.</p>"},{"location":"faq/#how-do-you-handle-datetime-features-for-machine-learning-models","title":"How do you handle date/time features for machine learning models?","text":"<p>By extracting meaningful components:</p> <ul> <li>Year, Month, Day</li> <li>Weekday</li> <li>Hour, Minute, Second</li> <li>Time since a specific date</li> </ul>"},{"location":"faq/#what-is-the-difference-between-loc-and-iloc-in-pandas","title":"What is the difference between <code>.loc</code> and <code>.iloc</code> in Pandas?","text":"<ul> <li><code>.loc</code>: Label-based indexing to select data by row and column labels.</li> <li><code>.iloc</code>: Integer-based indexing to select data by row and column positions.</li> </ul>"},{"location":"faq/#how-do-you-rename-columns-in-a-dataframe","title":"How do you rename columns in a DataFrame?","text":"<p>Using the <code>rename()</code> function:</p> <pre><code>df.rename(columns={'old_name': 'new_name'}, inplace=True)\n</code></pre>"},{"location":"faq/#what-is-the-purpose-of-the-astype-function-in-pandas","title":"What is the purpose of the <code>astype()</code> function in Pandas?","text":"<p>The <code>astype()</code> function is used to change the data type of a Series or DataFrame columns.</p> <pre><code>df['column_name'] = df['column_name'].astype('float')\n</code></pre>"},{"location":"faq/#how-do-you-detect-missing-values-in-a-dataframe","title":"How do you detect missing values in a DataFrame?","text":"<p>Using <code>isnull()</code> or <code>isna()</code> functions:</p> <pre><code>missing_values = df.isnull().sum()\n</code></pre>"},{"location":"faq/#what-is-an-ensemble-method-in-machine-learning","title":"What is an ensemble method in machine learning?","text":"<p>An ensemble method combines predictions from multiple machine learning models to improve performance over a single model. Examples include Random Forest, Gradient Boosting.</p>"},{"location":"faq/#how-do-you-implement-gradient-boosting-in-scikit-learn","title":"How do you implement Gradient Boosting in scikit-learn?","text":"<pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faq/#what-is-xgboost","title":"What is XGBoost?","text":"<p>XGBoost (Extreme Gradient Boosting) is an optimized gradient boosting library designed to be highly efficient, flexible, and portable, widely used for its performance and speed.</p>"},{"location":"faq/#how-do-you-install-and-use-xgboost-in-python","title":"How do you install and use XGBoost in Python?","text":"<p>Install using pip:</p> <pre><code>bash\npip install xgboost\n</code></pre> <p>Use in code:</p> <pre><code>import xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faq/#what-is-cross-entropy-loss","title":"What is cross-entropy loss?","text":"<p>Cross-entropy loss measures the performance of a classification model whose output is a probability between 0 and 1. It increases as the predicted probability diverges from the actual label.</p>"},{"location":"faq/#how-do-you-calculate-the-learning-rate-in-gradient-descent","title":"How do you calculate the learning rate in gradient descent?","text":"<p>The learning rate is a hyperparameter that you set manually. It determines the step size at each iteration while moving toward a minimum of a loss function.</p>"},{"location":"faq/#what-is-the-difference-between-epochs-batches-and-iterations","title":"What is the difference between epochs, batches, and iterations?","text":"<ul> <li>Epoch: One complete pass through the entire training dataset.</li> <li>Batch: A subset of the training data used in one iteration.</li> <li>Iteration: One update of the model's parameters.</li> </ul>"},{"location":"faq/#how-do-you-perform-hyperparameter-tuning","title":"How do you perform hyperparameter tuning?","text":"<p>By:</p> <ul> <li>Grid Search: Exhaustively searching through a specified subset of hyperparameters.</li> <li>Random Search: Randomly sampling hyperparameter combinations.</li> <li>Bayesian Optimization: Using probabilistic models to select hyperparameters.</li> </ul>"},{"location":"faq/#what-is-the-purpose-of-the-pipeline-module-in-scikit-learn","title":"What is the purpose of the <code>pipeline</code> module in scikit-learn?","text":"<p>It allows you to chain preprocessing steps and estimators in a sequential manner, ensuring consistent application of transformations during training and testing.</p>"},{"location":"faq/#how-do-you-evaluate-a-clustering-algorithm","title":"How do you evaluate a clustering algorithm?","text":"<p>Using metrics like:</p> <ul> <li>Silhouette Score</li> <li>Calinski-Harabasz Index</li> <li>Davies-Bouldin Index</li> </ul>"},{"location":"faq/#what-is-a-dummy-variable-trap","title":"What is a dummy variable trap?","text":"<p>The dummy variable trap occurs when multicollinearity is introduced in a regression model due to the inclusion of dummy variables that are linearly dependent. It can be avoided by dropping one dummy variable.</p>"},{"location":"faq/#how-do-you-create-a-correlation-matrix-in-pandas","title":"How do you create a correlation matrix in Pandas?","text":"<pre><code>corr_matrix = df.corr()\n</code></pre>"},{"location":"faq/#what-is-an-roc-curve-and-how-do-you-plot-it","title":"What is an ROC curve, and how do you plot it?","text":"<p>An ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate at various threshold settings. You can plot it using scikit-learn:</p> <pre><code>from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nplt.plot(fpr, tpr)\n</code></pre>"},{"location":"faq/#what-is-a-type-i-error","title":"What is a type I error?","text":"<p>A type I error occurs when the null hypothesis is true, but we incorrectly reject it (false positive).</p>"},{"location":"faq/#what-is-a-type-ii-error","title":"What is a type II error?","text":"<p>A type II error occurs when the null hypothesis is false, but we fail to reject it (false negative).</p>"},{"location":"faq/#how-do-you-calculate-the-p-value-in-a-hypothesis-test","title":"How do you calculate the p-value in a hypothesis test?","text":"<p>Using statistical tests from libraries like <code>scipy.stats</code>, which return the p-value as part of the output.</p>"},{"location":"faq/#what-is-the-difference-between-parametric-and-non-parametric-tests","title":"What is the difference between parametric and non-parametric tests?","text":"<ul> <li>Parametric Tests: Assume underlying statistical distributions (e.g., t-test).</li> <li>Non-Parametric Tests: Do not assume any specific distribution (e.g., Mann-Whitney U test).</li> </ul>"},{"location":"faq/#how-do-you-perform-a-chi-squared-test-in-python","title":"How do you perform a chi-squared test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>from scipy.stats import chi2_contingency\n\nchi2, p, dof, expected = chi2_contingency(observed_values)\n</code></pre>"},{"location":"faq/#what-is-the-purpose-of-the-seaborn-library","title":"What is the purpose of the <code>seaborn</code> library?","text":"<p>Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics.</p>"},{"location":"faq/#how-do-you-create-a-box-plot-in-seaborn","title":"How do you create a box plot in Seaborn?","text":"<pre><code>import seaborn as sns\n\nsns.boxplot(x='categorical_column', y='numerical_column', data=df)\n</code></pre>"},{"location":"faq/#what-is-a-violin-plot","title":"What is a violin plot?","text":"<p>A violin plot combines a box plot and a kernel density plot to provide a richer depiction of the data distribution.</p>"},{"location":"faq/#how-do-you-handle-multivariate-time-series-data","title":"How do you handle multivariate time series data?","text":"<p>By:</p> <ul> <li>Using models designed for multivariate data: Like VAR (Vector Autoregression).</li> <li>Feature engineering: Creating lag features for each variable.</li> </ul>"},{"location":"faq/#what-is-an-ab-test","title":"What is an A/B test?","text":"<p>An A/B test is an experiment comparing two variants (A and B) to determine which one performs better regarding a specific metric.</p>"},{"location":"faq/#how-do-you-analyze-ab-test-results","title":"How do you analyze A/B test results?","text":"<p>By:</p> <ul> <li>Calculating the difference in metrics between groups.</li> <li>Performing statistical tests: Like t-tests or chi-squared tests.</li> <li>Checking for statistical significance: Using p-values and confidence intervals.</li> </ul>"},{"location":"faq/#what-is-the-bonferroni-correction","title":"What is the Bonferroni correction?","text":"<p>A method to adjust p-values when multiple comparisons are made to reduce the chances of obtaining false-positive results (Type I errors).</p>"},{"location":"faq/#what-is-survivorship-bias","title":"What is survivorship bias?","text":"<p>Survivorship bias occurs when analyses are conducted only on surviving subjects, leading to skewed results due to the overlooking of those that did not survive or were not included.</p>"},{"location":"faq/#what-is-data-leakage","title":"What is data leakage?","text":"<p>Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates that won't generalize to new data.</p>"},{"location":"faq/#how-do-you-prevent-data-leakage","title":"How do you prevent data leakage?","text":"<p>By:</p> <ul> <li>Separating training and testing data properly.</li> <li>Performing data preprocessing steps within cross-validation.</li> <li>Avoiding using future data in model training.</li> </ul>"},{"location":"faq/#what-is-a-hash-table","title":"What is a hash table?","text":"<p>A hash table is a data structure that implements an associative array, mapping keys to values using a hash function to compute an index into an array of buckets.</p>"},{"location":"faq/#what-is-memoization","title":"What is memoization?","text":"<p>Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.</p>"},{"location":"faq/#how-do-you-reverse-a-linked-list","title":"How do you reverse a linked list?","text":"<p>By iterating through the list and reversing the pointers of each node to point to the previous node.</p>"},{"location":"faq/#what-is-the-time-complexity-of-quicksort","title":"What is the time complexity of quicksort?","text":"<p>The average time complexity is O(n log n), while the worst-case is O(n^2).</p>"},{"location":"faq/#what-is-a-generator-in-python","title":"What is a generator in Python?","text":"<p>A generator is a special type of function that returns an iterator object which can iterate over a sequence of values. It uses the <code>yield</code> keyword.</p> <pre><code>def my_generator():\n    yield value\n</code></pre>"},{"location":"faq/#how-do-you-handle-exceptions-in-python","title":"How do you handle exceptions in Python?","text":"<p>Using try-except blocks:</p> <pre><code>try:\n    # Code that may raise an exception\nexcept ExceptionType as e:\n    # Code to handle the exception\n</code></pre>"},{"location":"faq/#what-is-a-decorator-in-python","title":"What is a decorator in Python?","text":"<p>A decorator is a function that modifies the behavior of another function or method. It allows for the addition of functionality to existing code in a modular way.</p> <pre><code>def decorator_function(func):\n    def wrapper():\n        # Code before function call\n        func()\n        # Code after function call\n    return wrapper\n</code></pre>"},{"location":"faq/#how-do-you-read-and-write-json-files-in-python","title":"How do you read and write JSON files in Python?","text":"<p>Using the <code>json</code> module:</p> <pre><code>import json\n\n# Read JSON\nwith open('file.json', 'r') as f:\n    data = json.load(f)\n\n# Write JSON\nwith open('file.json', 'w') as f:\n    json.dump(data, f)\n</code></pre>"},{"location":"faq/#what-is-multithreading-and-how-do-you-implement-it-in-python","title":"What is multithreading, and how do you implement it in Python?","text":"<p>Multithreading allows concurrent execution of threads (lightweight processes) to improve performance. In Python, you can use the <code>threading</code> module:</p> <pre><code>import threading\n\ndef function_to_run():\n    pass\n\nthread = threading.Thread(target=function_to_run)\nthread.start()\n</code></pre>"},{"location":"faq/#what-is-the-global-interpreter-lock-gil-in-python","title":"What is the Global Interpreter Lock (GIL) in Python?","text":"<p>The GIL is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. It simplifies memory management but can limit performance in CPU-bound multi-threaded programs.</p>"},{"location":"faq/#how-do-you-handle-file-operations-in-python","title":"How do you handle file operations in Python?","text":"<p>Using <code>open()</code>:</p> <pre><code>with open('file.txt', 'r') as file:\n    content = file.read()\n</code></pre>"},{"location":"faq/#what-are-lambda-functions-in-python","title":"What are lambda functions in Python?","text":"<p>Lambda functions are anonymous functions defined using the <code>lambda</code> keyword, useful for short, simple functions.</p> <pre><code>add = lambda x, y: x + y\n</code></pre>"},{"location":"faq/#how-do-you-work-with-databases-in-python","title":"How do you work with databases in Python?","text":"<p>By using database connectors and ORMs like:</p> <ul> <li>SQLite: Using <code>sqlite3</code> module.</li> <li>MySQL: Using <code>mysql-connector-python</code>.</li> <li>ORMs: Using libraries like SQLAlchemy.</li> </ul>"},{"location":"faq/#what-is-the-purpose-of-virtual-environments-in-python","title":"What is the purpose of virtual environments in Python?","text":"<p>Virtual environments allow you to create isolated Python environments with specific packages and dependencies, preventing conflicts between projects.</p> <pre><code>bash\npython -m venv myenv\n</code></pre>"},{"location":"faq/#how-do-you-install-packages-in-python","title":"How do you install packages in Python?","text":"<p>Using <code>pip</code>:</p> <pre><code>bash\npip install package_name\n</code></pre>"},{"location":"faq/#what-are-the-common-data-types-in-python","title":"What are the common data types in Python?","text":"<ul> <li>Numeric Types: int, float, complex</li> <li>Sequence Types: list, tuple, range</li> <li>Text Type: str</li> <li>Mapping Type: dict</li> <li>Set Types: set, frozenset</li> <li>Boolean Type: bool</li> </ul>"},{"location":"faq/#how-do-you-create-a-class-in-python","title":"How do you create a class in Python?","text":"<pre><code>class MyClass:\n    def __init__(self, attribute):\n        self.attribute = attribute\n</code></pre>"},{"location":"faq/#what-is-inheritance-in-python","title":"What is inheritance in Python?","text":"<p>Inheritance allows a class (child) to inherit attributes and methods from another class (parent), promoting code reusability.</p> <pre><code>class ChildClass(ParentClass):\n    pass\n</code></pre>"},{"location":"faq/#what-is-polymorphism-in-python","title":"What is polymorphism in Python?","text":"<p>Polymorphism allows methods to have the same name but behave differently in different classes. It enables methods to be used interchangeably.</p> <p>o1</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#glossary-of-terms-for-introduction-to-data-science-with-python","title":"Glossary of Terms for Introduction to Data Science with Python","text":""},{"location":"glossary/#ac","title":"A\u2013C","text":""},{"location":"glossary/#accuracy","title":"Accuracy","text":"<p>A metric used to evaluate classification models, representing the proportion of correct predictions over the total number of predictions.</p> <p>Example: In a spam email classifier, if the model correctly identifies 90 out of 100 emails, the accuracy is 90%.</p>"},{"location":"glossary/#algorithm","title":"Algorithm","text":"<p>A step-by-step procedure or set of rules designed to perform a specific task or solve a problem.</p> <p>Example: Implementing the k-means clustering algorithm to group similar data points in an unsupervised learning task.</p>"},{"location":"glossary/#anaconda","title":"Anaconda","text":"<p>A free and open-source distribution of Python and R programming languages for scientific computing and data science.</p> <p>Example: Using Anaconda to manage Python packages and environments for data analysis projects in the course.</p>"},{"location":"glossary/#analytics","title":"Analytics","text":"<p>The process of examining datasets to draw conclusions about the information they contain, often using specialized software and statistical techniques.</p> <p>Example: Performing customer behavior analytics using Pandas and Matplotlib to improve marketing strategies.</p>"},{"location":"glossary/#anomaly-detection","title":"Anomaly Detection","text":"<p>The process of identifying unusual patterns or observations in data that do not conform to expected behavior.</p> <p>Example: Detecting fraudulent credit card transactions using an isolation forest model.</p>"},{"location":"glossary/#api-application-programming-interface","title":"API (Application Programming Interface)","text":"<p>A set of functions and protocols that allows software applications to communicate with each other.</p> <p>Example: Using the OpenWeather API to import real-time temperature data into a Python program.</p>"},{"location":"glossary/#application-layer","title":"Application Layer","text":"<p>The highest level in a computing system that interacts directly with user-facing applications and services.</p> <p>Example: A Python script running in Jupyter Notebook is executed at the application layer.</p>"},{"location":"glossary/#area-under-the-curve-auc","title":"Area Under the Curve (AUC)","text":"<p>A performance metric for classification models that measures the ability to distinguish between classes, calculated from the ROC curve.</p> <p>Example: A model with an AUC of 0.95 is better at distinguishing positive from negative cases than one with 0.70.</p>"},{"location":"glossary/#array","title":"Array","text":"<p>A data structure that stores a collection of items, typically of the same type, at contiguous memory locations.</p> <p>Example: Using NumPy arrays for fast vectorized mathematical computations.</p>"},{"location":"glossary/#artificial-intelligence-ai","title":"Artificial Intelligence (AI)","text":"<p>The field of computer science focused on creating systems capable of performing tasks that normally require human intelligence.</p> <p>Example: Building a neural network to recognize handwritten digits.</p>"},{"location":"glossary/#association-rule-mining","title":"Association Rule Mining","text":"<p>A method for finding relationships between variables in large datasets.</p> <p>Example: Identifying that customers who buy bread often also purchase butter.</p>"},{"location":"glossary/#attribute","title":"Attribute","text":"<p>A property or characteristic of an object, often represented as a column in a dataset.</p> <p>Example: In a dataset of houses, \"number of bedrooms\" is an attribute.</p>"},{"location":"glossary/#autocorrelation","title":"Autocorrelation","text":"<p>A measure of how related a variable is with a lagged version of itself over time.</p> <p>Example: Detecting repeating seasonal patterns in monthly sales data.</p>"},{"location":"glossary/#bagging-bootstrap-aggregating","title":"Bagging (Bootstrap Aggregating)","text":"<p>A machine learning ensemble technique that combines predictions from multiple models trained on different subsets of the data.</p> <p>Example: Using bagging with decision trees to create a random forest classifier.</p>"},{"location":"glossary/#bar-chart","title":"Bar Chart","text":"<p>A visualization that uses rectangular bars to represent data values for different categories.</p> <p>Example: Displaying average exam scores for different majors in a bar chart.</p>"},{"location":"glossary/#baseline-model","title":"Baseline Model","text":"<p>A simple model used as a reference point for comparing the performance of more complex models.</p> <p>Example: Using the mean value of the target variable as predictions to compare against a regression model.</p>"},{"location":"glossary/#batch-processing","title":"Batch Processing","text":"<p>A method of processing large volumes of data in groups instead of in real time.</p> <p>Example: Running a nightly script to clean and aggregate all transaction data from the previous day.</p>"},{"location":"glossary/#bayes-theorem","title":"Bayes\u2019 Theorem","text":"<p>A formula used to update probabilities based on new evidence.</p> <p>Example: Updating the probability of having a disease after a positive test result.</p>"},{"location":"glossary/#bias-in-machine-learning","title":"Bias (in Machine Learning)","text":"<p>The error introduced when a model makes assumptions that oversimplify the problem, potentially leading to underfitting.</p> <p>Example: Using a linear model for a highly non-linear dataset can introduce high bias.</p>"},{"location":"glossary/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The balance between underfitting (high bias) and overfitting (high variance) when building predictive models.</p> <p>Example: Choosing a model complexity that minimizes both bias and variance.</p>"},{"location":"glossary/#big-data","title":"Big Data","text":"<p>Extremely large datasets that require specialized storage and processing methods.</p> <p>Example: Analyzing petabytes of social media data for sentiment analysis.</p>"},{"location":"glossary/#bin","title":"Bin","text":"<p>A grouping of continuous numerical values into intervals for analysis or visualization.</p> <p>Example: Grouping ages into bins like 0\u201318, 19\u201335, and 36\u201360 for analysis.</p>"},{"location":"glossary/#binary-classification","title":"Binary Classification","text":"<p>A classification problem with only two possible outcomes.</p> <p>Example: Predicting whether an email is spam or not spam.</p>"},{"location":"glossary/#binary-variable","title":"Binary Variable","text":"<p>A variable with only two possible values, often represented as 0 and 1.</p> <p>Example: \"Passed Exam\" variable coded as 1 for pass and 0 for fail.</p>"},{"location":"glossary/#box-plot","title":"Box Plot","text":"<p>A graphical summary of data distribution showing median, quartiles, and potential outliers.</p> <p>Example: Using a box plot to compare exam scores across different classes.</p>"},{"location":"glossary/#categorical-data","title":"Categorical Data","text":"<p>Data that represents discrete groups or categories.</p> <p>Example: \"Car color\" with values red, blue, or green.</p>"},{"location":"glossary/#causal-relationship","title":"Causal Relationship","text":"<p>A relationship where a change in one variable directly produces a change in another.</p> <p>Example: Increasing advertising budget directly increasing sales.</p>"},{"location":"glossary/#central-limit-theorem-clt","title":"Central Limit Theorem (CLT)","text":"<p>A statistical theory stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.</p> <p>Example: Using the CLT to justify using normal-based confidence intervals.</p>"},{"location":"glossary/#class-in-classification","title":"Class (in Classification)","text":"<p>A label assigned to a group of similar data points in a classification task.</p> <p>Example: \"Cat\" and \"Dog\" are two classes in an animal image classifier.</p>"},{"location":"glossary/#classification","title":"Classification","text":"<p>The process of predicting a categorical label for new observations based on training data.</p> <p>Example: Using logistic regression to classify emails as spam or not spam.</p>"},{"location":"glossary/#classification-report","title":"Classification Report","text":"<p>A summary of a classification model\u2019s performance, including precision, recall, and F1-score.</p> <p>Example: Generating a classification report in scikit-learn after model training.</p>"},{"location":"glossary/#clustering","title":"Clustering","text":"<p>An unsupervised learning method that groups data points into clusters based on similarity.</p> <p>Example: Segmenting customers into groups based on purchasing habits.</p>"},{"location":"glossary/#coefficient","title":"Coefficient","text":"<p>A numerical value representing the strength and direction of the relationship between a predictor and the response variable in a regression model.</p> <p>Example: In a linear regression, a coefficient of 2.5 for \"hours studied\" means each extra hour increases predicted score by 2.5 points.</p>"},{"location":"glossary/#collinearity","title":"Collinearity","text":"<p>A situation where two or more predictor variables are highly correlated, which can affect model stability.</p> <p>Example: Including both height in inches and height in centimeters in a regression model creates collinearity.</p>"},{"location":"glossary/#column","title":"Column","text":"<p>A vertical arrangement of data values in a table, representing a single attribute.</p> <p>Example: In a Pandas DataFrame, each column holds one variable\u2019s data.</p>"},{"location":"glossary/#confusion-matrix","title":"Confusion Matrix","text":"<p>A table used to evaluate classification performance by showing counts of true positives, false positives, true negatives, and false negatives.</p> <p>Example: A confusion matrix showing that a model misclassified 5 out of 100 test emails.</p>"},{"location":"glossary/#correlation","title":"Correlation","text":"<p>A statistical measure that describes the strength and direction of a linear relationship between two variables.</p> <p>Example: A correlation coefficient of 0.9 between height and weight indicates a strong positive relationship.</p>"},{"location":"glossary/#correlation-coefficient","title":"Correlation Coefficient","text":"<p>A numerical value between -1 and 1 that indicates the degree of linear association between two variables.</p> <p>Example: A value of -0.75 shows a strong negative correlation.</p>"},{"location":"glossary/#cross-validation","title":"Cross-Validation","text":"<p>A technique for assessing how well a model generalizes by dividing data into multiple training and testing subsets.</p> <p>Example: Using 5-fold cross-validation to evaluate a regression model\u2019s accuracy.</p>"},{"location":"glossary/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>A simple file format for storing tabular data where each line represents a row and values are separated by commas.</p> <p>Example: Importing a CSV file of sales data into Pandas for analysis.</p>"},{"location":"glossary/#dh","title":"D\u2013H","text":""},{"location":"glossary/#data-cleaning","title":"Data Cleaning","text":"<p>The process of detecting and correcting inaccurate, incomplete, or irrelevant data in a dataset.</p> <p>Example: Removing duplicate rows and correcting misspelled city names in a dataset.</p>"},{"location":"glossary/#data-exploration","title":"Data Exploration","text":"<p>The initial phase of data analysis where patterns, trends, and relationships are identified through visualization and statistics.</p> <p>Example: Using Pandas and Matplotlib to examine sales trends by month.</p>"},{"location":"glossary/#data-frame","title":"Data Frame","text":"<p>A two-dimensional labeled data structure in Pandas that can hold different data types in columns.</p> <p>Example: Loading a CSV file into a Pandas DataFrame for manipulation.</p>"},{"location":"glossary/#data-mining","title":"Data Mining","text":"<p>The process of discovering patterns, correlations, and insights from large datasets using statistical and computational techniques.</p> <p>Example: Finding product purchase patterns from transaction logs.</p>"},{"location":"glossary/#data-normalization","title":"Data Normalization","text":"<p>Adjusting values in a dataset to a common scale without distorting differences in ranges.</p> <p>Example: Scaling features between 0 and 1 before training a machine learning model.</p>"},{"location":"glossary/#data-preprocessing","title":"Data Preprocessing","text":"<p>Transforming raw data into a clean and usable format for analysis or modeling.</p> <p>Example: Encoding categorical variables as integers before running a regression.</p>"},{"location":"glossary/#data-science","title":"Data Science","text":"<p>An interdisciplinary field that combines statistics, programming, and domain knowledge to extract insights from data.</p> <p>Example: Using Python to analyze and visualize health survey data.</p>"},{"location":"glossary/#data-type","title":"Data Type","text":"<p>A classification specifying the kind of value a variable can hold, such as integer, float, or string.</p> <p>Example: In Python, <code>3</code> is an integer, and <code>3.14</code> is a float.</p>"},{"location":"glossary/#dataset","title":"Dataset","text":"<p>A structured collection of data, typically organized in rows and columns.</p> <p>Example: The Iris dataset is commonly used for classification demonstrations.</p>"},{"location":"glossary/#decision-tree","title":"Decision Tree","text":"<p>A model that makes decisions by splitting data into branches based on feature values.</p> <p>Example: Predicting loan approval using a decision tree classifier.</p>"},{"location":"glossary/#deep-learning","title":"Deep Learning","text":"<p>A subfield of machine learning that uses multi-layered neural networks to model complex patterns.</p> <p>Example: Training a convolutional neural network to recognize handwritten digits.</p>"},{"location":"glossary/#dependent-variable","title":"Dependent Variable","text":"<p>The outcome variable being predicted or explained in a model.</p> <p>Example: Predicting \"house price\" based on features like size and location.</p>"},{"location":"glossary/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Statistical methods for summarizing and describing data features.</p> <p>Example: Calculating mean, median, and standard deviation for exam scores.</p>"},{"location":"glossary/#dimension-reduction","title":"Dimension Reduction","text":"<p>Techniques for reducing the number of variables while preserving relevant information.</p> <p>Example: Using Principal Component Analysis (PCA) to reduce feature count before clustering.</p>"},{"location":"glossary/#discrete-data","title":"Discrete Data","text":"<p>Data that can only take on specific, distinct values.</p> <p>Example: Number of students in a classroom.</p>"},{"location":"glossary/#distribution","title":"Distribution","text":"<p>The way values of a variable are spread or arranged.</p> <p>Example: A bell-shaped distribution for students\u2019 test scores.</p>"},{"location":"glossary/#dummy-variable","title":"Dummy Variable","text":"<p>A binary variable created to represent categorical data for modeling purposes.</p> <p>Example: Encoding \"Gender\" as 0 for male and 1 for female.</p>"},{"location":"glossary/#eda-exploratory-data-analysis","title":"EDA (Exploratory Data Analysis)","text":"<p>The process of visually and statistically summarizing a dataset to uncover patterns and insights.</p> <p>Example: Plotting histograms and scatter plots to explore data before modeling.</p>"},{"location":"glossary/#epoch","title":"Epoch","text":"<p>One complete pass through the entire training dataset in machine learning.</p> <p>Example: Training a neural network for 50 epochs to achieve better accuracy.</p>"},{"location":"glossary/#evaluation-metric","title":"Evaluation Metric","text":"<p>A standard of measurement used to assess model performance.</p> <p>Example: Mean Squared Error (MSE) for regression models.</p>"},{"location":"glossary/#evidence-based-decision","title":"Evidence-Based Decision","text":"<p>A decision guided by analysis and data rather than intuition alone.</p> <p>Example: Allocating marketing budget based on data-driven campaign performance.</p>"},{"location":"glossary/#explanatory-variable","title":"Explanatory Variable","text":"<p>A variable used to predict or explain the dependent variable.</p> <p>Example: In predicting crop yield, rainfall amount is an explanatory variable.</p>"},{"location":"glossary/#exponential-smoothing","title":"Exponential Smoothing","text":"<p>A forecasting method that applies decreasing weights to older observations.</p> <p>Example: Forecasting monthly sales using exponential smoothing.</p>"},{"location":"glossary/#feature","title":"Feature","text":"<p>An individual measurable property of the data used in modeling.</p> <p>Example: \"Age\" and \"Income\" as features in predicting spending habits.</p>"},{"location":"glossary/#feature-engineering","title":"Feature Engineering","text":"<p>The process of creating new features or transforming existing ones to improve model performance.</p> <p>Example: Creating a \"BMI\" feature from height and weight columns.</p>"},{"location":"glossary/#feature-scaling","title":"Feature Scaling","text":"<p>Adjusting feature values to the same scale to ensure fair model training.</p> <p>Example: Standardizing all features to have zero mean and unit variance.</p>"},{"location":"glossary/#feature-selection","title":"Feature Selection","text":"<p>Choosing the most relevant features to improve model efficiency and accuracy.</p> <p>Example: Removing features with low correlation to the target variable.</p>"},{"location":"glossary/#f1-score","title":"F1-Score","text":"<p>A metric that combines precision and recall into a single value.</p> <p>Example: An F1-score of 0.85 indicates a balanced model performance.</p>"},{"location":"glossary/#false-negative","title":"False Negative","text":"<p>A prediction that incorrectly classifies a positive case as negative.</p> <p>Example: A medical test failing to detect a disease when it is present.</p>"},{"location":"glossary/#false-positive","title":"False Positive","text":"<p>A prediction that incorrectly classifies a negative case as positive.</p> <p>Example: A spam filter marking a legitimate email as spam.</p>"},{"location":"glossary/#feature-matrix","title":"Feature Matrix","text":"<p>A two-dimensional array containing feature values for all observations in a dataset.</p> <p>Example: A NumPy array where each row is a sample and each column is a feature.</p>"},{"location":"glossary/#forecasting","title":"Forecasting","text":"<p>Predicting future values based on historical data.</p> <p>Example: Using time series models to forecast next month\u2019s sales.</p>"},{"location":"glossary/#function-python","title":"Function (Python)","text":"<p>A reusable block of code that performs a specific task.</p> <p>Example: Defining a function to calculate the mean of a list.</p>"},{"location":"glossary/#gaussian-distribution","title":"Gaussian Distribution","text":"<p>A symmetric, bell-shaped probability distribution characterized by mean and standard deviation.</p> <p>Example: Heights of adult men following a Gaussian distribution.</p>"},{"location":"glossary/#generalization","title":"Generalization","text":"<p>The ability of a model to perform well on new, unseen data.</p> <p>Example: A model that predicts accurately on both training and test datasets generalizes well.</p>"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"<p>An optimization algorithm used to minimize a function by iteratively moving toward the lowest point.</p> <p>Example: Training linear regression by adjusting weights with gradient descent.</p>"},{"location":"glossary/#ground-truth","title":"Ground Truth","text":"<p>The actual, verified values used to evaluate predictions.</p> <p>Example: Comparing model predictions to actual customer purchase data.</p>"},{"location":"glossary/#groupby-pandas","title":"GroupBy (Pandas)","text":"<p>A Pandas method for splitting data into groups for aggregation or transformation.</p> <p>Example: Grouping sales data by region and summing total revenue.</p>"},{"location":"glossary/#heatmap","title":"Heatmap","text":"<p>A graphical representation of data values using color shading.</p> <p>Example: A correlation heatmap showing relationships between features.</p>"},{"location":"glossary/#heuristic","title":"Heuristic","text":"<p>A rule-of-thumb strategy for solving problems quickly, though not always optimally.</p> <p>Example: Using a decision tree depth limit of 5 as a heuristic for speed.</p>"},{"location":"glossary/#histogram","title":"Histogram","text":"<p>A plot that groups numerical data into intervals (bins) and shows their frequency.</p> <p>Example: Plotting the distribution of exam scores in bins of 10 points.</p>"},{"location":"glossary/#hyperparameter","title":"Hyperparameter","text":"<p>A parameter set before training that controls model behavior.</p> <p>Example: Setting the learning rate in gradient descent.</p>"},{"location":"glossary/#im","title":"I\u2013M","text":""},{"location":"glossary/#imbalanced-dataset","title":"Imbalanced Dataset","text":"<p>A dataset where some classes have significantly more samples than others.</p> <p>Example: A fraud detection dataset with 99% non-fraud and 1% fraud cases.</p>"},{"location":"glossary/#imputation","title":"Imputation","text":"<p>The process of replacing missing data with substituted values.</p> <p>Example: Filling missing ages with the mean age of the dataset.</p>"},{"location":"glossary/#independent-variable","title":"Independent Variable","text":"<p>A variable used as an input to predict another variable.</p> <p>Example: Study time as an independent variable in predicting test scores.</p>"},{"location":"glossary/#index-pandas","title":"Index (Pandas)","text":"<p>Labels that identify rows in a Pandas DataFrame or Series.</p> <p>Example: Setting the \"date\" column as the index for a time series.</p>"},{"location":"glossary/#inferential-statistics","title":"Inferential Statistics","text":"<p>Statistical methods for making predictions or inferences about a population from a sample.</p> <p>Example: Using a sample of voters to predict an election outcome.</p>"},{"location":"glossary/#information-gain","title":"Information Gain","text":"<p>A measure of how much a feature improves classification by reducing uncertainty.</p> <p>Example: A decision tree choosing a feature with the highest information gain for the first split.</p>"},{"location":"glossary/#interpolation","title":"Interpolation","text":"<p>Estimating unknown values within the range of known data points.</p> <p>Example: Estimating missing temperature readings between two known days.</p>"},{"location":"glossary/#iteration","title":"Iteration","text":"<p>A single pass through a loop or process in programming.</p> <p>Example: Each pass through a <code>for</code> loop is an iteration.</p>"},{"location":"glossary/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>An interactive environment for writing and running Python code, combining code, visualizations, and text.</p> <p>Example: Running data analysis and visualizations in a Jupyter Notebook.</p>"},{"location":"glossary/#json-javascript-object-notation","title":"JSON (JavaScript Object Notation)","text":"<p>A lightweight data format for storing and exchanging structured data.</p> <p>Example: Importing JSON-formatted API data into Pandas.</p>"},{"location":"glossary/#k-fold-cross-validation","title":"k-Fold Cross-Validation","text":"<p>A method of splitting data into k subsets for training and testing to evaluate model performance.</p> <p>Example: Using 10-fold cross-validation to assess a regression model.</p>"},{"location":"glossary/#k-means-clustering","title":"k-Means Clustering","text":"<p>An unsupervised learning algorithm that groups data into k clusters based on similarity.</p> <p>Example: Segmenting customers into 4 clusters using purchase history.</p>"},{"location":"glossary/#kernel-in-computing","title":"Kernel (in Computing)","text":"<p>The central component of an operating system or computational environment.</p> <p>Example: Restarting the Python kernel in Jupyter Notebook to clear variables.</p>"},{"location":"glossary/#keyword-argument-python","title":"Keyword Argument (Python)","text":"<p>An argument passed to a function by explicitly naming the parameter.</p> <p>Example: Calling <code>plot(x, y, color=\"blue\")</code> uses a keyword argument.</p>"},{"location":"glossary/#kolmogorov-smirnov-test","title":"Kolmogorov-Smirnov Test","text":"<p>A statistical test to compare a sample distribution with a reference probability distribution.</p> <p>Example: Checking if a dataset follows a normal distribution.</p>"},{"location":"glossary/#label-in-machine-learning","title":"Label (in Machine Learning)","text":"<p>The output or target variable in supervised learning.</p> <p>Example: \"Species\" in the Iris dataset is the label.</p>"},{"location":"glossary/#label-encoding","title":"Label Encoding","text":"<p>Converting categorical values into numeric codes.</p> <p>Example: Encoding \"low\", \"medium\", \"high\" as 0, 1, 2.</p>"},{"location":"glossary/#lasso-regression","title":"Lasso Regression","text":"<p>A linear regression method with L1 regularization that can reduce coefficients to zero.</p> <p>Example: Using Lasso to perform feature selection in a high-dimensional dataset.</p>"},{"location":"glossary/#learning-rate","title":"Learning Rate","text":"<p>A hyperparameter controlling how much to adjust model weights during training.</p> <p>Example: Setting a learning rate of 0.01 for gradient descent.</p>"},{"location":"glossary/#least-squares-method","title":"Least Squares Method","text":"<p>A method for estimating regression coefficients by minimizing the sum of squared residuals.</p> <p>Example: Fitting a line to data points using least squares.</p>"},{"location":"glossary/#legend-in-visualization","title":"Legend (in Visualization)","text":"<p>An element in a chart that explains the meaning of colors, symbols, or line styles.</p> <p>Example: A legend identifying which line corresponds to each city in a temperature chart.</p>"},{"location":"glossary/#library-python","title":"Library (Python)","text":"<p>A collection of modules that provide specific functionality.</p> <p>Example: NumPy is a library for numerical computing.</p>"},{"location":"glossary/#linear-regression","title":"Linear Regression","text":"<p>A method for modeling the relationship between a dependent variable and one or more independent variables using a linear equation.</p> <p>Example: Predicting house prices from square footage.</p>"},{"location":"glossary/#logistic-regression","title":"Logistic Regression","text":"<p>A statistical model used for binary classification that predicts probabilities using a logistic function.</p> <p>Example: Predicting whether a student passes an exam based on study hours.</p>"},{"location":"glossary/#loss-function","title":"Loss Function","text":"<p>A function that measures the error between predicted and actual values.</p> <p>Example: Mean Squared Error is a loss function for regression.</p>"},{"location":"glossary/#machine-learning","title":"Machine Learning","text":"<p>A subset of AI where models improve their performance by learning from data.</p> <p>Example: Training a decision tree to classify loan applications.</p>"},{"location":"glossary/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"<p>A regression metric measuring the average absolute difference between predicted and actual values.</p> <p>Example: An MAE of 5 means predictions are off by 5 units on average.</p>"},{"location":"glossary/#map-function-python","title":"Map Function (Python)","text":"<p>A function that applies another function to each item in an iterable.</p> <p>Example: Using <code>map(str.upper, words)</code> to convert all words to uppercase.</p>"},{"location":"glossary/#matplotlib","title":"Matplotlib","text":"<p>A Python library for creating static, animated, and interactive visualizations.</p> <p>Example: Plotting a line chart of sales over time using Matplotlib.</p>"},{"location":"glossary/#matrix","title":"Matrix","text":"<p>A two-dimensional array of numbers arranged in rows and columns.</p> <p>Example: A 3\u00d73 NumPy array representing a matrix.</p>"},{"location":"glossary/#mean","title":"Mean","text":"<p>The average value of a dataset, found by summing all values and dividing by the count.</p> <p>Example: The mean of 2, 4, and 6 is 4.</p>"},{"location":"glossary/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>A regression metric that measures the average squared difference between predicted and actual values.</p> <p>Example: An MSE of 9 means the average squared error is 9 units.</p>"},{"location":"glossary/#median","title":"Median","text":"<p>The middle value when data is ordered from smallest to largest.</p> <p>Example: The median of 1, 3, 5 is 3.</p>"},{"location":"glossary/#microsimulation","title":"MicroSimulation","text":"<p>An interactive simulation that runs in your browser that helps you learn a data science concept.</p> <p>Example: You can visualize the least squares method of liner regression by changing the slope and intercept of a line through a series of points.</p>"},{"location":"glossary/#min-max-scaling","title":"Min-Max Scaling","text":"<p>A feature scaling technique that rescales values to a given range, usually [0, 1].</p> <p>Example: Converting exam scores from a 0\u2013100 scale to 0\u20131.</p>"},{"location":"glossary/#missing-data","title":"Missing Data","text":"<p>Values not recorded in a dataset.</p> <p>Example: A blank entry for \"age\" in a survey response.</p>"},{"location":"glossary/#mode","title":"Mode","text":"<p>The most frequently occurring value in a dataset.</p> <p>Example: In 2, 3, 3, 4, 5, the mode is 3.</p> <p>Example: Creating a scikit-learn pipeline to standardize data and train a model in a single workflow.</p> <p>A mathematical representation of a process or system built from data to make predictions or decisions.</p> <p>Example: A linear regression equation predicting house prices.</p>"},{"location":"glossary/#model-complexity","title":"Model Complexity","text":"<p>The level of detail and number of parameters in a model.</p> <p>Example: A deep neural network is more complex than a simple linear regression.</p>"},{"location":"glossary/#model-evaluation","title":"Model Evaluation","text":"<p>The process of assessing a model\u2019s performance using metrics and validation techniques.</p> <p>Example: Evaluating a classification model with precision and recall.</p>"},{"location":"glossary/#model-overfitting","title":"Model Overfitting","text":"<p>When a model learns noise in the training data, reducing performance on new data.</p> <p>Example: A decision tree that memorizes training examples but fails on test data.</p>"},{"location":"glossary/#model-underfitting","title":"Model Underfitting","text":"<p>When a model is too simple to capture the underlying structure of the data.</p> <p>Example: Using a straight line to fit highly curved data.</p>"},{"location":"glossary/#module-python","title":"Module (Python)","text":"<p>A file containing Python definitions and functions.</p> <p>Example: Importing the <code>math</code> module to use mathematical functions.</p>"},{"location":"glossary/#multicollinearity","title":"Multicollinearity","text":"<p>A situation where predictor variables in a regression model are highly correlated with each other.</p> <p>Example: Including both temperature in Celsius and Fahrenheit in the same model.</p>"},{"location":"glossary/#multiple-linear-regression","title":"Multiple Linear Regression","text":"<p>A linear regression model with two or more independent variables.</p> <p>Example: Predicting house price using square footage, number of bedrooms, and location.</p>"},{"location":"glossary/#ns","title":"N\u2013S","text":""},{"location":"glossary/#naive-bayes","title":"Naive Bayes","text":"<p>A probabilistic classification algorithm based on Bayes\u2019 theorem with the assumption of feature independence.</p> <p>Example: Classifying text documents into categories using Naive Bayes.</p>"},{"location":"glossary/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>A field of AI focused on enabling computers to understand and process human language.</p> <p>Example: Using NLP to analyze sentiment in customer reviews.</p>"},{"location":"glossary/#neural-network","title":"Neural Network","text":"<p>A computational model inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers.</p> <p>Example: A neural network classifying handwritten digits.</p>"},{"location":"glossary/#noise-data","title":"Noise (Data)","text":"<p>Random variations or irrelevant information in data that obscure patterns.</p> <p>Example: Sensor measurement errors in temperature readings.</p>"},{"location":"glossary/#nominal-data","title":"Nominal Data","text":"<p>Categorical data without an inherent order.</p> <p>Example: Car brands such as Toyota, Ford, and Honda.</p>"},{"location":"glossary/#normal-distribution","title":"Normal Distribution","text":"<p>A symmetric, bell-shaped probability distribution characterized by mean and standard deviation.</p> <p>Example: Adult human heights often approximate a normal distribution.</p>"},{"location":"glossary/#normalization","title":"Normalization","text":"<p>The process of scaling numerical data to a standard range or format.</p> <p>Example: Scaling all features to a 0\u20131 range before training.</p>"},{"location":"glossary/#null-hypothesis","title":"Null Hypothesis","text":"<p>A default statistical assumption that there is no effect or relationship between variables.</p> <p>Example: Testing whether two teaching methods result in the same average test score.</p>"},{"location":"glossary/#numpy","title":"NumPy","text":"<p>A Python library for numerical computing, providing array objects and mathematical functions.</p> <p>Example: Performing matrix multiplication using NumPy arrays.</p>"},{"location":"glossary/#observation","title":"Observation","text":"<p>A single record or row in a dataset.</p> <p>Example: One student's exam scores in a class dataset.</p>"},{"location":"glossary/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>A method of converting categorical variables into binary indicator variables.</p> <p>Example: Encoding \"red\", \"blue\", \"green\" into three separate binary columns.</p>"},{"location":"glossary/#open-source-software","title":"Open Source Software","text":"<p>Software with source code made freely available for modification and distribution.</p> <p>Example: Pandas is open source.</p>"},{"location":"glossary/#optimization","title":"Optimization","text":"<p>The process of adjusting model parameters to minimize or maximize an objective function.</p> <p>Example: Using gradient descent to minimize prediction error.</p>"},{"location":"glossary/#outlier","title":"Outlier","text":"<p>A data point significantly different from other observations.</p> <p>Example: A monthly sales value 10 times higher than all others.</p>"},{"location":"glossary/#overfitting","title":"Overfitting","text":"<p>When a model fits training data too closely, capturing noise rather than general patterns.</p> <p>Example: A polynomial regression curve that perfectly passes through all training points but fails on test data.</p>"},{"location":"glossary/#overfitting-prevention","title":"Overfitting Prevention","text":"<p>Techniques used to reduce overfitting, such as regularization, pruning, and cross-validation.</p> <p>Example: Using Lasso regression to reduce unnecessary model complexity.</p>"},{"location":"glossary/#p-value","title":"p-Value","text":"<p>A statistical measure of the probability of obtaining results at least as extreme as the observed results, assuming the null hypothesis is true.</p> <p>Example: A p-value of 0.03 suggests statistical significance at the 5% level.</p>"},{"location":"glossary/#pandas","title":"Pandas","text":"<p>A Python library for data manipulation and analysis with DataFrame and Series objects.</p> <p>Example: Loading and cleaning a CSV dataset with Pandas.</p>"},{"location":"glossary/#parameter-model","title":"Parameter (Model)","text":"<p>A value learned from data that defines how the model makes predictions.</p> <p>Example: The slope in a linear regression equation.</p>"},{"location":"glossary/#parameter-tuning","title":"Parameter Tuning","text":"<p>Adjusting model hyperparameters to optimize performance.</p> <p>Example: Testing multiple learning rates to find the best one.</p>"},{"location":"glossary/#parsing","title":"Parsing","text":"<p>Breaking down data or text into structured components for processing.</p> <p>Example: Parsing JSON data from an API response.</p>"},{"location":"glossary/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":"<p>A statistic measuring the linear relationship between two variables.</p> <p>Example: A Pearson coefficient of 0.8 indicates a strong positive relationship.</p>"},{"location":"glossary/#performance-metric","title":"Performance Metric","text":"<p>A quantitative measure used to evaluate the effectiveness of a model.</p> <p>Example: Accuracy, precision, and recall are performance metrics for classification.</p>"},{"location":"glossary/#pipeline-machine-learning","title":"Pipeline (Machine Learning)","text":"<p>A sequence of data processing and modeling steps applied consistently to training and test data.</p> <p>Example: A pipeline for scaling features and then applying logistic regression.</p>"},{"location":"glossary/#pivot-table","title":"Pivot Table","text":"<p>A table that summarizes data by grouping and aggregating values.</p> <p>Example: Summarizing sales by region and month using a Pandas pivot table.</p>"},{"location":"glossary/#polynomial-regression","title":"Polynomial Regression","text":"<p>A regression method where the relationship between variables is modeled as an nth-degree polynomial.</p> <p>Example: Fitting a quadratic curve to non-linear data.</p>"},{"location":"glossary/#population","title":"Population","text":"<p>The entire set of items or individuals of interest in a study.</p> <p>Example: All registered voters in a country.</p>"},{"location":"glossary/#precision","title":"Precision","text":"<p>The proportion of positive predictions that are actually correct.</p> <p>Example: A precision of 0.9 means 90% of predicted positives were correct.</p>"},{"location":"glossary/#predictive-modeling","title":"Predictive Modeling","text":"<p>The process of creating models to forecast outcomes based on historical data.</p> <p>Example: Using past sales data to predict next month\u2019s revenue.</p>"},{"location":"glossary/#predictor-variable","title":"Predictor Variable","text":"<p>An independent variable used to forecast the dependent variable.</p> <p>Example: Advertising spend as a predictor of sales.</p>"},{"location":"glossary/#preprocessing","title":"Preprocessing","text":"<p>Preparing raw data for analysis or modeling through cleaning, transformation, and encoding.</p> <p>Example: Removing missing values and scaling features.</p>"},{"location":"glossary/#pre-trained-model","title":"Pre-Trained Model","text":"<p>A model that has been previously trained on a large dataset and can be fine-tuned for a new task.</p> <p>Example: Using a pre-trained image classifier for a smaller dataset.</p>"},{"location":"glossary/#probability","title":"Probability","text":"<p>A measure of the likelihood that an event will occur, ranging from 0 to 1.</p> <p>Example: A probability of 0.75 means a 75% chance of occurrence.</p>"},{"location":"glossary/#probability-distribution","title":"Probability Distribution","text":"<p>A function that describes the likelihood of all possible outcomes.</p> <p>Example: A uniform distribution gives equal probability to all values.</p>"},{"location":"glossary/#pytorch","title":"PyTorch","text":"<p>An open-source machine learning library for deep learning and tensor computation.</p> <p>Example: Building a neural network in PyTorch for image recognition.</p>"},{"location":"glossary/#quantile","title":"Quantile","text":"<p>A cut point dividing data into equal-sized intervals.</p> <p>Example: The median is the 0.5 quantile.</p>"},{"location":"glossary/#quartile","title":"Quartile","text":"<p>One of three points that divide data into four equal groups.</p> <p>Example: The first quartile is the 25th percentile.</p>"},{"location":"glossary/#query-database","title":"Query (Database)","text":"<p>A request for specific information from a database.</p> <p>Example: Using SQL to retrieve rows where sales exceed $500.</p>"},{"location":"glossary/#r2-coefficient-of-determination","title":"R\u00b2 (Coefficient of Determination)","text":"<p>A metric indicating the proportion of variance in the dependent variable explained by the model.</p> <p>Example: An R\u00b2 of 0.85 means 85% of the variation is explained by the model.</p>"},{"location":"glossary/#random-forest","title":"Random Forest","text":"<p>An ensemble learning method combining multiple decision trees for better performance.</p> <p>Example: Classifying emails using a random forest model.</p>"},{"location":"glossary/#recall","title":"Recall","text":"<p>The proportion of actual positives correctly identified by the model.</p> <p>Example: A recall of 0.8 means the model identified 80% of actual positives.</p>"},{"location":"glossary/#recursive-feature-elimination-rfe","title":"Recursive Feature Elimination (RFE)","text":"<p>A method for selecting features by recursively removing the least important ones.</p> <p>Example: Using RFE to reduce feature count before training.</p>"},{"location":"glossary/#regression","title":"Regression","text":"<p>A statistical method for modeling relationships between variables to predict numerical outcomes.</p> <p>Example: Predicting car prices from mileage and age.</p>"},{"location":"glossary/#regularization","title":"Regularization","text":"<p>A technique for reducing model complexity to prevent overfitting by adding a penalty term to the loss function.</p> <p>Example: L2 regularization in Ridge regression.</p>"},{"location":"glossary/#residual","title":"Residual","text":"<p>The difference between the observed value and the value predicted by a model.</p> <p>Example: If actual sales are 200 and predicted sales are 180, the residual is 20.</p>"},{"location":"glossary/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>A regression metric measuring the square root of the average squared prediction error.</p> <p>Example: An RMSE of 3 means predictions are off by 3 units on average.</p>"},{"location":"glossary/#sample","title":"Sample","text":"<p>A subset of a population used for analysis.</p> <p>Example: Surveying 500 people from a city of 100,000 residents.</p>"},{"location":"glossary/#sampling","title":"Sampling","text":"<p>Selecting a subset of data from a larger dataset or population.</p> <p>Example: Randomly selecting 100 customers for a feedback survey.</p>"},{"location":"glossary/#sampling-bias","title":"Sampling Bias","text":"<p>A bias introduced when the sample is not representative of the population.</p> <p>Example: Surveying only morning shoppers to estimate daily spending.</p>"},{"location":"glossary/#scatter-plot","title":"Scatter Plot","text":"<p>A graph showing data points on an x-y axis to visualize relationships between variables.</p> <p>Example: Plotting height against weight to explore correlation.</p>"},{"location":"glossary/#scikit-learn","title":"Scikit-learn","text":"<p>A Python library for machine learning, providing tools for modeling, preprocessing, and evaluation.</p> <p>Example: Building a decision tree classifier with Scikit-learn.</p>"},{"location":"glossary/#scripting","title":"Scripting","text":"<p>Writing and executing code to automate tasks.</p> <p>Example: Writing a Python script to download and clean data files.</p>"},{"location":"glossary/#series-pandas","title":"Series (Pandas)","text":"<p>A one-dimensional labeled array in Pandas.</p> <p>Example: A Series of student grades with names as labels.</p>"},{"location":"glossary/#standard-deviation","title":"Standard Deviation","text":"<p>A measure of the dispersion of values around the mean.</p> <p>Example: A standard deviation of 2 means most values fall within \u00b12 units of the mean.</p>"},{"location":"glossary/#standardization","title":"Standardization","text":"<p>Scaling features so they have zero mean and unit variance.</p> <p>Example: Standardizing exam scores before analysis.</p>"},{"location":"glossary/#tz","title":"T\u2013Z","text":""},{"location":"glossary/#target-variable","title":"Target Variable","text":"<p>The variable a model aims to predict.</p> <p>Example: Predicting \"exam score\" as the target variable using study hours and sleep.</p>"},{"location":"glossary/#t-statistic","title":"t-Statistic","text":"<p>A value used in hypothesis testing to determine if sample means differ significantly.</p> <p>Example: Using a t-statistic to compare average weights of two groups.</p>"},{"location":"glossary/#tensor","title":"Tensor","text":"<p>A multi-dimensional array used in deep learning frameworks like PyTorch.</p> <p>Example: Representing a batch of images as a 4D tensor.</p>"},{"location":"glossary/#test-data","title":"Test Data","text":"<p>A dataset used to evaluate model performance after training.</p> <p>Example: Testing a spam filter on previously unseen emails.</p>"},{"location":"glossary/#test-train-split","title":"Test-Train Split","text":"<p>The process of dividing data into separate training and testing subsets.</p> <p>Example: Using 80% of data for training and 20% for testing.</p>"},{"location":"glossary/#text-mining","title":"Text Mining","text":"<p>Extracting useful information from text data through statistical and computational methods.</p> <p>Example: Identifying frequent keywords in news articles.</p>"},{"location":"glossary/#time-series","title":"Time Series","text":"<p>A sequence of data points collected over time intervals.</p> <p>Example: Daily stock prices over a year.</p>"},{"location":"glossary/#tokenization","title":"Tokenization","text":"<p>Breaking text into smaller units such as words or phrases.</p> <p>Example: Splitting \"Data Science is fun\" into [\"Data\", \"Science\", \"is\", \"fun\"].</p>"},{"location":"glossary/#training-data","title":"Training Data","text":"<p>The dataset used to fit or train a model.</p> <p>Example: Using past sales data to train a revenue prediction model.</p>"},{"location":"glossary/#training-epoch","title":"Training Epoch","text":"<p>One complete pass of the training dataset through the model during learning.</p> <p>Example: Training a neural network for 10 epochs.</p>"},{"location":"glossary/#transformation","title":"Transformation","text":"<p>Applying a mathematical function to change data format or scale.</p> <p>Example: Applying a logarithmic transformation to skewed data.</p>"},{"location":"glossary/#tree-based-models","title":"Tree-Based Models","text":"<p>Models that use decision trees or ensembles of trees for prediction.</p> <p>Example: Random forest and gradient boosting are tree-based models.</p>"},{"location":"glossary/#trend-time-series","title":"Trend (Time Series)","text":"<p>A long-term movement or pattern in time series data.</p> <p>Example: Increasing monthly sales over several years.</p>"},{"location":"glossary/#true-negative","title":"True Negative","text":"<p>A case where a model correctly predicts a negative outcome.</p> <p>Example: A medical test correctly identifying a healthy patient.</p>"},{"location":"glossary/#true-positive","title":"True Positive","text":"<p>A case where a model correctly predicts a positive outcome.</p> <p>Example: A cancer detection test correctly identifying a patient with cancer.</p>"},{"location":"glossary/#t-test","title":"t-Test","text":"<p>A statistical test comparing the means of two groups.</p> <p>Example: Testing whether two different diets result in different average weight loss.</p>"},{"location":"glossary/#underfitting","title":"Underfitting","text":"<p>When a model is too simple to capture the underlying data patterns.</p> <p>Example: Using a straight line to predict a highly curved dataset.</p>"},{"location":"glossary/#univariate-analysis","title":"Univariate Analysis","text":"<p>Analysis of a single variable to summarize and find patterns.</p> <p>Example: Plotting a histogram of exam scores.</p>"},{"location":"glossary/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>A machine learning approach where models find structure in data without labeled outcomes.</p> <p>Example: Using k-means clustering to group customers.</p>"},{"location":"glossary/#validation-data","title":"Validation Data","text":"<p>A dataset used during training to tune model hyperparameters.</p> <p>Example: Using validation data to choose the number of decision tree splits.</p>"},{"location":"glossary/#value-counts-pandas","title":"Value Counts (Pandas)","text":"<p>A method to count occurrences of each unique value in a Pandas Series.</p> <p>Example: Counting how many times each grade appears in a class.</p>"},{"location":"glossary/#variable","title":"Variable","text":"<p>A storage location identified by a name that holds a value.</p> <p>Example: In Python, <code>age = 20</code> assigns 20 to the variable <code>age</code>.</p>"},{"location":"glossary/#variance","title":"Variance","text":"<p>A measure of the spread of data points around the mean.</p> <p>Example: A variance of 4 means average squared deviations are 4 units\u00b2.</p>"},{"location":"glossary/#vectorization","title":"Vectorization","text":"<p>Performing operations on entire arrays rather than using loops.</p> <p>Example: Adding two NumPy arrays without explicit loops.</p>"},{"location":"glossary/#visualization","title":"Visualization","text":"<p>The graphical representation of data to reveal patterns and insights.</p> <p>Example: Using Matplotlib to plot a scatter diagram of height vs. weight.</p>"},{"location":"glossary/#visual-studio-code","title":"Visual Studio Code","text":"<p>A free integrated development environment that is used widely by programmers and data science science professionals.</p> <p>Visual Studio Code (VS-Code) also has many extensions for editing different languages and data types.</p> <p>Other examples of AI-based IDEs include Cursor and Windsurf.  The Cline extension is also useful.</p> <p>Example: We use VS-Code to edit, run, debug and test our Python code.</p>"},{"location":"glossary/#weighted-average","title":"Weighted Average","text":"<p>An average where each value contributes according to its assigned weight.</p> <p>Example: Calculating a final grade with different weights for assignments and exams.</p>"},{"location":"glossary/#weight-model","title":"Weight (Model)","text":"<p>A parameter that determines the influence of a feature in a model\u2019s prediction.</p> <p>Example: A weight of 0.5 for \"hours studied\" in a regression model.</p>"},{"location":"glossary/#white-noise-time-series","title":"White Noise (Time Series)","text":"<p>A random sequence of values with no predictable pattern.</p> <p>Example: Fluctuations in daily stock prices without a trend.</p>"},{"location":"glossary/#wilcoxon-test","title":"Wilcoxon Test","text":"<p>A non-parametric test for comparing paired samples.</p> <p>Example: Comparing pre- and post-training test scores without assuming normality.</p>"},{"location":"glossary/#yaml-yaml-aint-markup-language","title":"YAML (YAML Ain\u2019t Markup Language)","text":"<p>A human-readable data serialization format.</p> <p>Example: Storing configuration settings in a YAML file.</p>"},{"location":"glossary/#z-score","title":"z-Score","text":"<p>A standardized score indicating how many standard deviations a value is from the mean.</p> <p>Example: A z-score of 2 means the value is two standard deviations above the mean.</p>"},{"location":"glossary/#zero-inflated-data","title":"Zero-Inflated Data","text":"<p>Data with an excess number of zero values compared to a standard distribution.</p> <p>Example: A dataset of monthly purchases where many customers have zero purchases.</p>"},{"location":"glossary/#zero-shot-learning","title":"Zero-Shot Learning","text":"<p>A machine learning method where a model makes predictions for classes it was not explicitly trained on.</p> <p>Example: A text classifier identifying a new topic without prior examples.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"license/#commercial-licensing","title":"Commercial Licensing","text":"<p>Commercial rights are reserved by the copyright holder. For commercial licensing, publication inquiries, or permission to use this work in commercial contexts, please contact Dan McCreary on LinkedIn.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 13 chapters covering 300 concepts in data science with Python.</p>"},{"location":"chapters/#chapter-overview","title":"Chapter Overview","text":"<ol> <li> <p>Introduction to Data Science - Foundational concepts including data science workflow, types of data, and measurement scales.</p> </li> <li> <p>Python Environment and Setup - Python installation, package management, Jupyter notebooks, and IDE configuration.</p> </li> <li> <p>Python Data Structures - Python native structures and pandas DataFrames for data manipulation.</p> </li> <li> <p>Data Cleaning and Preprocessing - Handling missing values, outliers, and data transformation techniques.</p> </li> <li> <p>Data Visualization with Matplotlib - Creating effective visualizations with matplotlib and Seaborn.</p> </li> <li> <p>Statistical Foundations - Descriptive statistics, distributions, probability, and correlation.</p> </li> <li> <p>Simple Linear Regression - Regression fundamentals, least squares method, and scikit-learn implementation.</p> </li> <li> <p>Model Evaluation and Validation - Performance metrics, cross-validation, and bias-variance tradeoff.</p> </li> <li> <p>Multiple Linear Regression - Multiple predictors, feature selection, and handling categorical variables.</p> </li> <li> <p>NumPy and Numerical Computing - NumPy arrays, vectorization, and matrix operations.</p> </li> <li> <p>Non-linear Models and Regularization - Polynomial regression and regularization techniques.</p> </li> <li> <p>Introduction to Machine Learning - Machine learning paradigms, optimization, and gradient descent.</p> </li> <li> <p>Neural Networks and PyTorch - Neural network architecture, PyTorch, and capstone projects.</p> </li> <li> <p>Capstone Project - Working in teams to create a final data science project and presenting to your classmates.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>This textbook is designed to be read sequentially, as each chapter builds upon concepts from previous chapters. The learning graph ensures that prerequisite concepts are always introduced before they are needed. However, if you have prior experience with certain topics, you may skip ahead while referring back to earlier chapters as needed.</p> <p>Each chapter includes interactive MicroSims to reinforce learning through hands-on exploration.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-intro-to-data-science/","title":"Introduction to Data Science","text":""},{"location":"chapters/01-intro-to-data-science/#summary","title":"Summary","text":"<p>This chapter introduces the foundational concepts of data science, establishing the vocabulary and mental models needed for the rest of the course. Students will learn what data science is, understand the data science workflow from problem definition to data collection, and master the different types of data and measurement scales. By the end of this chapter, students will be able to identify variables, distinguish between independent and dependent variables, and understand the structure of datasets.</p>"},{"location":"chapters/01-intro-to-data-science/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Data Science</li> <li>Data</li> <li>Variables</li> <li>Data Types</li> <li>Numerical Data</li> <li>Categorical Data</li> <li>Ordinal Data</li> <li>Nominal Data</li> <li>Measurement Scales</li> <li>Independent Variable</li> <li>Dependent Variable</li> <li>Dataset</li> <li>Observation</li> <li>Feature</li> <li>Target Variable</li> <li>Data Science Workflow</li> <li>Problem Definition</li> <li>Data Collection</li> <li>Python Programming</li> <li>Documentation</li> </ol>"},{"location":"chapters/01-intro-to-data-science/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description. No prior data science knowledge is required.</p>"},{"location":"chapters/01-intro-to-data-science/#welcome-to-your-data-science-origin-story","title":"Welcome to Your Data Science Origin Story","text":"<p>Every superhero has an origin story. Spider-Man got bitten by a radioactive spider. Iron Man built a suit in a cave. You? You're about to unlock something even more powerful\u2014the ability to see patterns in chaos, predict the future from the past, and make decisions that actually make sense. Welcome to data science, your new superpower.</p> <p>Here's the thing: we live in a world absolutely drowning in data. Every swipe, click, purchase, and late-night snack run generates information. Most people just swim through this ocean without a second thought. But data scientists? They're the ones who can read the currents, spot the hidden treasure, and navigate to exactly where they want to go.</p> <p>By the end of this book, you'll be able to:</p> <ul> <li>Predict trends before they happen</li> <li>Find patterns nobody else can see</li> <li>Build models that learn from experience</li> <li>Make decisions backed by evidence, not just gut feelings</li> </ul> <p>Sound like a superpower? That's because it is.</p> <p>The concept map below previews the key skills and ideas you'll master throughout this course. Hover over each node to see how these superpowers connect and build upon each other.</p>"},{"location":"chapters/01-intro-to-data-science/#data-science-superpower-concept-map","title":"Data Science Superpower Concept Map","text":""},{"location":"chapters/01-intro-to-data-science/#what-exactly-is-data-science","title":"What Exactly Is Data Science?","text":"<p>Data science is the art and science of extracting meaningful insights from data. It combines statistics, computer programming, and domain expertise to answer questions, solve problems, and predict outcomes. Think of it as being a detective, mathematician, and storyteller all rolled into one.</p> <p>But wait\u2014isn't that just statistics? Or computer science? Or business analytics? Here's what makes data science special: it's all of those things working together. A statistician might crunch numbers. A programmer might write code. A business analyst might make recommendations. A data scientist does all three, often before lunch.</p> Role Primary Focus Data Scientist's Take Statistician Mathematical analysis \"I use stats AND make it work in code\" Programmer Building software \"I code AND understand what the numbers mean\" Business Analyst Strategic recommendations \"I advise AND prove it with evidence\" Data Scientist All of the above \"I do it all\u2014and make it look easy\" <p>The best part? Data science isn't just for tech companies. Hospitals use it to save lives. Sports teams use it to win championships. Musicians use it to create hit songs. Even farmers use it to grow better crops. Whatever you're passionate about, data science can make you better at it.</p>"},{"location":"chapters/01-intro-to-data-science/#understanding-data-the-raw-material-of-your-power","title":"Understanding Data: The Raw Material of Your Power","text":"<p>Before you can wield your new superpower, you need to understand its source: data. Data is simply recorded information. It's the raw material that, when processed correctly, becomes insight, knowledge, and wisdom.</p> <p>Data is everywhere. Right now, your phone is collecting data about your location, battery level, and which apps you use most. Your school tracks data about attendance, grades, and lunch purchases. Even your heartbeat generates data that a smartwatch can record.</p> <p>Here's a mind-bending thought: by the time you finish reading this sentence, the world will have generated approximately 2.5 quintillion bytes of new data. That's 2,500,000,000,000,000,000 bytes. Every. Single. Day.</p> <p>Data Science Superpower Tip</p> <p>The difference between ordinary people and data scientists isn't access to data\u2014everyone has that now. It's knowing what to DO with data that makes you powerful.</p> <p>But not all data is created equal. Some data is clean and ready to use. Some is messy and needs work. Some is numerical, some is categorical. Understanding these differences is your first step toward data science mastery.</p>"},{"location":"chapters/01-intro-to-data-science/#variables-the-building-blocks-of-data","title":"Variables: The Building Blocks of Data","text":"<p>In data science, we organize data using variables. A variable is any characteristic, number, or quantity that can be measured or counted. Variables are called \"variables\" because they vary\u2014they can take on different values.</p> <p>Think about describing your classmates. You might note:</p> <ul> <li>Their height (a number that varies from person to person)</li> <li>Their eye color (a category that varies)</li> <li>Their grade level (a rank that varies)</li> <li>Their student ID (a unique identifier that varies)</li> </ul> <p>Each of these is a variable. And here's where it gets interesting: the TYPE of variable determines what you can do with it.</p> <p>How do you figure out what type a variable is? Use the decision tree below. Start at the top and answer each question\u2014the flowchart will guide you to the correct classification. This simple process works for ANY variable you'll ever encounter.</p>"},{"location":"chapters/01-intro-to-data-science/#diagram-variable-types-decision-tree","title":"Diagram: Variable Types Decision Tree","text":"<p>Run the Variable Types Decision Tree Fullscreen</p> Variable Types Decision Tree <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Enable students to correctly classify any variable they encounter into its proper type</p> <p>Purpose: Guide students through identifying variable types with a simple decision tree</p> <p>Visual style: Flowchart with decision diamonds and endpoint rectangles</p> <p>Starting point: \"What kind of variable is this?\"</p> <p>Decision 1: \"Can you do math with it?\" - YES \u2192 \"NUMERICAL DATA\" path - NO \u2192 \"CATEGORICAL DATA\" path</p> <p>NUMERICAL path continues: Decision 2: \"Can it be any value (including decimals)?\" - YES \u2192 Endpoint: \"Continuous\" (example: height = 5.7 feet) - NO \u2192 Endpoint: \"Discrete\" (example: number of siblings = 2)</p> <p>CATEGORICAL path continues: Decision 3: \"Is there a meaningful order?\" - YES \u2192 Endpoint: \"Ordinal\" (example: grade levels Freshman &lt; Senior) - NO \u2192 Endpoint: \"Nominal\" (example: eye color - no order)</p> <p>Color coding: - Decision diamonds: Yellow - Numerical endpoints: Blue - Categorical endpoints: Green - Examples: Gray italic text</p> <p>Interactive elements: - Hover over each endpoint to see 3 more examples - Click \"Test Yourself\" button to get a random variable to classify</p> <p>Implementation: HTML/CSS/JavaScript flowchart</p>"},{"location":"chapters/01-intro-to-data-science/#the-four-types-of-data","title":"The Four Types of Data","text":"<p>Let's break down the data types you'll encounter. Mastering these is like learning to identify different materials\u2014you wouldn't use the same technique on wood as you would on metal.</p>"},{"location":"chapters/01-intro-to-data-science/#numerical-data","title":"Numerical Data","text":"<p>Numerical data consists of numbers that represent quantities. You can perform mathematical operations on numerical data\u2014add it, subtract it, find averages, and more.</p> <p>Numerical data comes in two flavors:</p> <ul> <li>Continuous: Can take ANY value within a range (height, temperature, time)</li> <li>Discrete: Can only take specific values, usually whole numbers (number of pets, test score out of 100)</li> </ul>"},{"location":"chapters/01-intro-to-data-science/#categorical-data","title":"Categorical Data","text":"<p>Categorical data represents groups or categories. You can't do math with it in the traditional sense\u2014what's the average of \"blue\" plus \"green\"? Exactly.</p> <p>Categorical data also comes in two flavors:</p> <ul> <li>Nominal data: Categories with NO inherent order (colors, countries, types of music)</li> <li>Ordinal data: Categories WITH a meaningful order (rankings, satisfaction levels, education levels)</li> </ul> <p>Here's a handy comparison:</p> Data Type Can Do Math? Has Order? Examples Numerical (Continuous) Yes Yes Height, Weight, Temperature Numerical (Discrete) Yes Yes Age in years, Number of friends Categorical (Ordinal) No* Yes Star ratings, T-shirt sizes (S, M, L) Categorical (Nominal) No No Favorite color, Country of birth <p>*You can count ordinal categories and find modes, but traditional arithmetic doesn't apply.</p>"},{"location":"chapters/01-intro-to-data-science/#measurement-scales-precision-levels-for-your-data","title":"Measurement Scales: Precision Levels for Your Data","text":"<p>Measurement scales describe the precision and type of information a variable provides. Think of them as different levels of detail, like the difference between knowing someone is \"tall\" versus knowing they're \"exactly 6 feet 2 inches.\"</p> <p>There are four measurement scales, from least to most informative:</p> <ol> <li>Nominal: Names or labels only (no order, no math)</li> <li>Ordinal: Ordered categories (rankings exist, but intervals aren't equal)</li> <li>Interval: Equal intervals, but no true zero (temperature in Celsius)</li> <li>Ratio: Equal intervals WITH a true zero (height, weight, age)</li> </ol> <p>Why Does This Matter?</p> <p>Knowing your measurement scale determines which analyses you can perform. You can calculate a mean height (ratio scale), but you can't calculate a \"mean favorite color\" (nominal scale). Using the wrong analysis on the wrong scale is like trying to measure your height with a thermometer\u2014technically using a measurement tool, but completely useless.</p> <p>The pyramid below shows how measurement scales build on each other\u2014each level adds new mathematical operations you can perform. Hover over each level to see examples and learn which statistical operations are valid at that scale.</p>"},{"location":"chapters/01-intro-to-data-science/#diagram-measurement-scales-pyramid","title":"Diagram: Measurement Scales Pyramid","text":"<p>View the Measurement Scales Pyramid Fullscreen</p> Measurement Scales Pyramid <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Help students remember the four measurement scales and what operations each allows</p> <p>Purpose: Visualize measurement scales as a hierarchy where each level adds capabilities</p> <p>Layout: Pyramid with four layers, widest at bottom</p> <p>Layers (bottom to top):</p>"},{"location":"chapters/01-intro-to-data-science/#1-nominal-base-widest","title":"1. NOMINAL (base, widest)","text":"<ul> <li>Operations: = \u2260 (equality only)</li> <li>Example: \"Jersey numbers\"</li> <li>Color: Light gray</li> </ul>"},{"location":"chapters/01-intro-to-data-science/#2-ordinal","title":"2. ORDINAL","text":"<ul> <li>Operations: = \u2260 &lt; &gt; (equality + ordering)</li> <li>Example: \"Race finishing positions\"</li> <li>Color: Light blue</li> </ul>"},{"location":"chapters/01-intro-to-data-science/#3-interval","title":"3. INTERVAL","text":"<ul> <li>Operations: = \u2260 &lt; &gt; + - (add equal intervals)</li> <li>Example: \"Temperature in Fahrenheit\"</li> <li>Color: Light green</li> </ul>"},{"location":"chapters/01-intro-to-data-science/#4-ratio-top-narrowest","title":"4. RATIO (top, narrowest)","text":"<ul> <li>Operations: = \u2260 &lt; &gt; + - \u00d7 \u00f7 (full arithmetic)</li> <li>Example: \"Height in centimeters\"</li> <li>Color: Gold (representing \"gold standard\")</li> </ul> <p>Side annotations: - Arrow pointing up: \"More mathematical operations possible\" - Arrow pointing down: \"More variables fit this category\"</p> <p>Interactive elements: - Hover over each level to see 5 example variables - Click to see which statistical tests are appropriate for each level</p> <p>Visual style: 3D pyramid with glowing edges, game achievement aesthetic</p> <p>Implementation: CSS 3D transforms or SVG</p>"},{"location":"chapters/01-intro-to-data-science/#practice-makes-perfect-sorting-variables","title":"Practice Makes Perfect: Sorting Variables","text":"<p>Now that you understand the different data types and measurement scales, it's time to test your skills. The best way to master variable classification is through practice\u2014the more variables you categorize, the more automatic it becomes.</p> <p>Use the interactive sorter below to drag variables into their correct categories. Don't worry if you make mistakes at first\u2014that's how learning works! Pay attention to the feedback and think about WHY each variable belongs in its category.</p>"},{"location":"chapters/01-intro-to-data-science/#diagram-variable-type-sorter","title":"Diagram: Variable Type Sorter","text":"<p>Run the Variable Types Sorter Fullscreen</p>"},{"location":"chapters/01-intro-to-data-science/#independent-and-dependent-variables-cause-and-effect","title":"Independent and Dependent Variables: Cause and Effect","text":"<p>Now we get to one of the most important concepts in all of science: the relationship between independent variables and dependent variables.</p> <p>The independent variable is what you change or control in an experiment. It's the \"cause\" in cause-and-effect. The dependent variable is what you measure\u2014it \"depends\" on the independent variable. It's the \"effect.\"</p> <p>Here's an easy way to remember:</p> <ul> <li>Independent = \"I change this\" (I for Independent)</li> <li>Dependent = \"This depends on what I changed\"</li> </ul> <p>Real-world examples:</p> Scenario Independent Variable Dependent Variable Studying for a test Hours studied Test score Growing plants Amount of sunlight Plant height Running a business Advertising budget Sales revenue Video game practice Hours played Skill level <p>Understanding this relationship is HUGE. It's the foundation of prediction. If you know how an independent variable affects a dependent variable, you can predict outcomes. That's literally seeing the future.</p>"},{"location":"chapters/01-intro-to-data-science/#diagram-independent-vs-dependent-variable-microsim","title":"Diagram: Independent vs Dependent Variable MicroSim","text":"<p>Run the Independent vs Dependent Variable MicroSim Fullscreen</p> Study Time vs Test Score Simulator <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students experiment with changing an independent variable and observe how the dependent variable responds, building intuition for cause-and-effect relationships</p> <p>Canvas layout (700x450px): - Main area (500x400): Scatter plot visualization - Right panel (200x400): Controls and stats</p> <p>Visual elements: - Scatter plot with \"Hours Studied\" on X-axis (0-10 hours) - \"Test Score\" on Y-axis (0-100 points) - Data points appear as student emoji icons - Trend line showing relationship - Confidence band around trend line</p> <p>Interactive controls: - Slider: \"Add Student\" - Click to add a new random data point - Slider: \"Relationship Strength\" (0-100%) - Controls how predictable scores are from study time - Button: \"Generate Class of 30\" - Creates realistic dataset - Button: \"Clear All\" - Reset visualization - Toggle: \"Show Trend Line\" - Display/hide regression line - Toggle: \"Show Prediction\" - Highlight predicted vs actual</p> <p>Display stats: - Number of students - Average study time - Average test score - Correlation strength (with emoji: \ud83d\udcaa strong, \ud83e\udd37 weak)</p> <p>Default parameters: - Relationship strength: 70% - Start with 10 sample students - Trend line visible</p> <p>Behavior: - When relationship strength is high, points cluster tightly around trend line - When strength is low, points scatter widely (more randomness) - Clicking on the plot shows \"If you study X hours, you'd expect a score around Y\" - Fun messages appear: \"Whoa, that student beat the odds!\" for outliers</p> <p>Educational message displayed: \"The INDEPENDENT variable (study time) is what you control. The DEPENDENT variable (test score) is what you measure. Notice: More study time TENDS to mean better scores, but there's always variation!\"</p> <p>Implementation: p5.js with simple linear regression</p>"},{"location":"chapters/01-intro-to-data-science/#datasets-your-data-organized-and-ready","title":"Datasets: Your Data Organized and Ready","text":"<p>A dataset is a structured collection of data, organized in a way that makes analysis possible. Think of it as a spreadsheet with rows and columns, where each row is one observation and each column is one variable.</p> <p>An observation (also called a record or instance) is a single unit of data. If you're collecting data about students, each student is one observation. If you're tracking daily weather, each day is one observation.</p> <p>Here's what a simple dataset looks like:</p> Student ID Name Hours Studied Test Score Grade Level Favorite Subject 001 Alex 3.5 82 Junior Math 002 Jordan 5.0 91 Senior Science 003 Sam 2.0 74 Junior Art 004 Taylor 4.5 88 Senior History <p>In this dataset:</p> <ul> <li>Each row is one observation (one student)</li> <li>Each column is one variable</li> <li>We have 4 observations and 6 variables</li> </ul>"},{"location":"chapters/01-intro-to-data-science/#features-and-target-variables-setting-up-for-prediction","title":"Features and Target Variables: Setting Up for Prediction","text":"<p>When we're building predictive models, we use special terms for our variables:</p> <p>Features (also called predictors or input variables) are the variables we use to make predictions. They're the information we feed INTO our model.</p> <p>The target variable (also called the outcome, response, or label) is what we're trying to predict. It's the OUTPUT of our model.</p> <p>Using our student example:</p> <ul> <li>Features: Hours Studied, Grade Level, Favorite Subject</li> <li>Target Variable: Test Score (what we want to predict)</li> </ul> <p>Feature vs. Target: A Sports Analogy</p> <p>Imagine predicting which team will win a game. Your features might be: each team's win record, star player stats, home vs. away, and weather conditions. Your target variable is the game outcome (win/lose). Features are your clues; the target is what you're solving for.</p> <p>Here's how features and targets relate to independent/dependent variables:</p> Predictive Modeling Term Experimental Term Role Features Independent Variables Input to model Target Variable Dependent Variable Output to predict"},{"location":"chapters/01-intro-to-data-science/#the-data-science-workflow-your-heros-journey","title":"The Data Science Workflow: Your Hero's Journey","text":"<p>Every great hero follows a journey, and data scientists are no different. The data science workflow is your roadmap from \"I have a question\" to \"I have an answer (with evidence).\"</p> <p>The workflow has several key stages:</p> <ol> <li>Problem Definition: What question are you trying to answer?</li> <li>Data Collection: Gathering the information you need</li> <li>Data Cleaning: Fixing errors and handling missing information</li> <li>Exploratory Analysis: Getting to know your data through visualization</li> <li>Modeling: Building mathematical representations to make predictions</li> <li>Evaluation: Testing how well your model works</li> <li>Communication: Sharing your findings with others</li> </ol> <p>The visualization below maps this workflow to the classic hero's journey from storytelling. Just like any great adventure, your data science quest will have challenges to overcome, mentors to guide you, and ultimately a transformation that changes how you see the world. Explore each stage to see how the hero's journey parallels your path to becoming a data scientist.</p>"},{"location":"chapters/01-intro-to-data-science/#data-science-workflow-heros-journey","title":"Data Science Workflow Hero's Journey","text":"<p>Now let's dive deeper into the first two stages of the workflow, which you'll encounter in every data science project.</p>"},{"location":"chapters/01-intro-to-data-science/#problem-definition-starting-your-quest","title":"Problem Definition: Starting Your Quest","text":"<p>Problem definition is where every data science project begins. Before you touch any data, you need a clear question. Vague questions lead to vague answers (or worse, wrong answers).</p> <p>Bad problem definitions:</p> <ul> <li>\"I want to analyze data\" (About what? Why?)</li> <li>\"Make me a machine learning model\" (To do what?)</li> <li>\"Find something interesting\" (That's not a question!)</li> </ul> <p>Good problem definitions:</p> <ul> <li>\"Which factors most influence student test performance?\"</li> <li>\"Can we predict which customers will cancel their subscription?\"</li> <li>\"How does weather affect daily bike rental demand?\"</li> </ul> <p>The difference? Good questions are specific, measurable, and actionable.</p>"},{"location":"chapters/01-intro-to-data-science/#data-collection-gathering-your-resources","title":"Data Collection: Gathering Your Resources","text":"<p>Data collection is the process of gathering the information you need to answer your question. Data can come from many sources:</p> <ul> <li>Surveys and forms: Directly asking people questions</li> <li>Sensors and devices: Automatic measurements (temperature, motion, etc.)</li> <li>Databases: Stored records from transactions, events, or activities</li> <li>Web scraping: Automatically extracting data from websites</li> <li>APIs: Requesting data from other software systems</li> <li>Experiments: Controlled tests you design and run</li> </ul> <p>Data Collection Reality Check</p> <p>Here's a secret professionals don't always tell you: data collection is often the HARDEST part. The data you want might not exist. The data you can get might be messy. You might need permission you don't have. Real data science is 80% wrestling with data and 20% building cool models.</p>"},{"location":"chapters/01-intro-to-data-science/#python-programming-your-superpower-tool","title":"Python Programming: Your Superpower Tool","text":"<p>If data science is your superpower, then Python programming is your utility belt. Python is a programming language that's become the go-to tool for data scientists worldwide.</p> <p>Why Python? Several reasons:</p> <ul> <li>Readable: Python code looks almost like English</li> <li>Powerful libraries: Tools like pandas, NumPy, and matplotlib do the heavy lifting</li> <li>Huge community: Millions of users means tons of help and resources</li> <li>Free and open source: No expensive licenses required</li> <li>Versatile: Works for data analysis, web apps, automation, and more</li> </ul> <p>Here's a sneak peek at what Python code looks like:</p> <pre><code># Load a dataset\nimport pandas as pd\nstudents = pd.read_csv(\"students.csv\")\n\n# Calculate average test score\naverage_score = students[\"test_score\"].mean()\nprint(f\"Class average: {average_score}\")\n\n# Find students who studied more than 4 hours\nhard_workers = students[students[\"hours_studied\"] &gt; 4]\n</code></pre> <p>Don't worry if this looks foreign right now. By the end of this course, you'll be writing code like this in your sleep.</p>"},{"location":"chapters/01-intro-to-data-science/#documentation-recording-your-journey","title":"Documentation: Recording Your Journey","text":"<p>Documentation is the practice of recording what you did, why you did it, and what you learned. It might sound boring, but trust me\u2014future you will thank present you for good documentation.</p> <p>Documentation serves several purposes:</p> <ul> <li>Memory aid: You WILL forget what you did three weeks ago</li> <li>Collaboration: Others need to understand your work</li> <li>Reproducibility: Science requires that results can be replicated</li> <li>Debugging: Finding errors is easier when you know what you did</li> <li>Portfolio: Documentation becomes proof of your skills</li> </ul> <p>Good documentation includes:</p> <ul> <li>Clear comments in your code</li> <li>Explanation of your reasoning and decisions</li> <li>Notes about data sources and limitations</li> <li>Visualizations of key findings</li> <li>Summary of results and conclusions</li> </ul> <p>The Superhero's Journal</p> <p>Even Batman keeps records in the Batcomputer. Your documentation is your version of this\u2014a record of your adventures, discoveries, and hard-won knowledge. Start good habits now, and you'll stand out as a professional later.</p>"},{"location":"chapters/01-intro-to-data-science/#putting-it-all-together-your-data-science-toolkit","title":"Putting It All Together: Your Data Science Toolkit","text":"<p>Congratulations! You've just acquired the foundational vocabulary of data science. Let's recap your new powers:</p> <p>Core Concepts:</p> <ul> <li>Data Science: Extracting insights from data using statistics, programming, and domain expertise</li> <li>Data: Recorded information that serves as raw material for analysis</li> <li>Variables: Characteristics that can take different values</li> <li>Data Types: Numerical (continuous, discrete) and Categorical (nominal, ordinal)</li> <li>Measurement Scales: Nominal \u2192 Ordinal \u2192 Interval \u2192 Ratio</li> </ul> <p>Relationships:</p> <ul> <li>Independent Variable: What you change (the cause)</li> <li>Dependent Variable: What you measure (the effect)</li> <li>Features: Input variables for prediction</li> <li>Target Variable: What you're trying to predict</li> </ul> <p>Structure:</p> <ul> <li>Dataset: Organized collection of data</li> <li>Observation: Single data point (one row)</li> <li>Data Science Workflow: Problem \u2192 Collect \u2192 Clean \u2192 Explore \u2192 Model \u2192 Evaluate \u2192 Communicate</li> </ul> <p>Tools:</p> <ul> <li>Python Programming: Your primary weapon for data analysis</li> <li>Documentation: Your record of the journey</li> </ul> <p>The concept map below shows how all 20 concepts from this chapter connect to each other. Notice how Data Science sits at the center, with branches extending to data types, variable roles, the workflow, and tools. Click on any concept to highlight its connections and see how the pieces fit together.</p>"},{"location":"chapters/01-intro-to-data-science/#diagram-chapter-1-concept-mind-map","title":"Diagram: Chapter 1 Concept Mind Map","text":"<p>View the Chapter 1 Concept Map Fullscreen</p> Introduction to Data Science Concept Map <p>Type: graph-model</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help students see how all concepts from Chapter 1 connect and relate to each other</p> <p>Purpose: Visual summary showing relationships between all 20 concepts covered</p> <p>Node types: 1. Core Concept (large gold circles)    - Data Science, Data, Variables, Dataset</p> <ol> <li>Data Types (medium blue squares)</li> <li> <p>Numerical Data, Categorical Data, Ordinal Data, Nominal Data</p> </li> <li> <p>Variable Roles (medium green diamonds)</p> </li> <li> <p>Independent Variable, Dependent Variable, Feature, Target Variable</p> </li> <li> <p>Workflow (medium purple hexagons)</p> </li> <li> <p>Data Science Workflow, Problem Definition, Data Collection</p> </li> <li> <p>Tools &amp; Practices (medium orange rectangles)</p> </li> <li> <p>Python Programming, Documentation</p> </li> <li> <p>Structure (medium teal triangles)</p> </li> <li>Measurement Scales, Observation</li> </ol> <p>Edge types: 1. \"is a type of\" (solid gray) - e.g., Numerical \u2192 Data Types 2. \"contains\" (dashed blue) - e.g., Dataset \u2192 Observation 3. \"uses\" (dotted green) - e.g., Data Science \u2192 Python Programming 4. \"same as\" (double line) - e.g., Feature \u2194 Independent Variable</p> <p>Layout: Hierarchical with Data Science at top</p> <p>Key relationships: - Data Science \u2192 Data (works with) - Data \u2192 Variables (organized into) - Variables \u2192 Data Types (classified by) - Data Types \u2192 splits to Numerical and Categorical - Numerical/Categorical \u2192 further subtypes - Variables \u2192 splits to Independent/Dependent - Feature = Independent Variable (in prediction context) - Target Variable = Dependent Variable (in prediction context) - Data Science Workflow \u2192 Problem Definition \u2192 Data Collection - Data Science \u2192 Python Programming (implemented using) - All activities \u2192 Documentation (recorded through)</p> <p>Interactive features: - Hover: Show concept definition - Click: Highlight all connected concepts - Double-click: Expand to show examples - Filter buttons: Show only certain node types</p> <p>Visual styling: - Node size reflects importance (centrality) - Edge thickness reflects strength of relationship - Animated pulses along edges when hovering</p> <p>Implementation: vis-network JavaScript library Canvas size: 800x600px</p>"},{"location":"chapters/01-intro-to-data-science/#whats-next","title":"What's Next?","text":"<p>You've completed your origin story. You understand what data science is, how data is organized, and the workflow you'll follow on every project. You've met Python, your trusty sidekick.</p> <p>But here's the exciting part: this is just the beginning. In the chapters ahead, you'll:</p> <ul> <li>Set up your Python environment and write your first code</li> <li>Load real datasets and explore them hands-on</li> <li>Create beautiful visualizations that tell stories</li> <li>Learn statistical techniques that reveal hidden patterns</li> <li>Build models that actually predict the future</li> <li>And ultimately, complete a capstone project that showcases your new superpowers</li> </ul> <p>The world needs more people who can make sense of data. It needs people who can separate signal from noise, truth from fiction, and insight from confusion. By the time you finish this book, you'll be one of those people.</p> <p>Ready to continue your training?</p> Chapter 1 Checkpoint: Test Your Understanding <p>Question: A researcher collects data on students including their GPA (0.0-4.0), major (Biology, Chemistry, Physics), year in school (Freshman through Senior), and number of classes taken. Classify each variable by its data type and measurement scale.</p> <p>Click to reveal answer:</p> <ul> <li>GPA: Numerical (continuous), Ratio scale</li> <li>Major: Categorical (nominal), Nominal scale</li> <li>Year in school: Categorical (ordinal), Ordinal scale</li> <li>Number of classes: Numerical (discrete), Ratio scale</li> </ul>"},{"location":"chapters/01-intro-to-data-science/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Data science is your superpower\u2014it combines statistics, programming, and domain knowledge to extract insights from data.</p> </li> <li> <p>Data is everywhere, and understanding its types (numerical vs. categorical, ordinal vs. nominal) determines what you can do with it.</p> </li> <li> <p>Variables are the building blocks of data, and knowing the difference between independent and dependent variables is crucial for understanding cause and effect.</p> </li> <li> <p>Datasets organize data into rows (observations) and columns (variables), making analysis possible.</p> </li> <li> <p>The data science workflow is your hero's journey: define the problem, collect data, clean it, explore it, model it, evaluate it, and communicate your findings.</p> </li> <li> <p>Python is your primary tool, and documentation is how you record your journey for yourself and others.</p> </li> </ol> <p>Now turn the page and let's set up your Python environment. Your training continues!</p>"},{"location":"chapters/02-python-environment/","title":"Python Environment and Setup","text":""},{"location":"chapters/02-python-environment/#summary","title":"Summary","text":"<p>This chapter guides students through setting up a complete Python development environment for data science. Students will install Python, learn about package management with pip and conda, set up virtual environments, and configure their IDE. The chapter also covers Jupyter notebooks in depth, including working with cells, kernels, and importing libraries. By the end of this chapter, students will have a fully functional data science development environment.</p>"},{"location":"chapters/02-python-environment/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>Python Installation</li> <li>Package Management</li> <li>Pip</li> <li>Conda Environment</li> <li>Virtual Environment</li> <li>IDE Setup</li> <li>VS Code</li> <li>Jupyter Notebooks</li> <li>Notebook Cells</li> <li>Code Cell</li> <li>Markdown Cell</li> <li>Cell Execution</li> <li>Kernel</li> <li>Import Statement</li> <li>Python Libraries</li> </ol>"},{"location":"chapters/02-python-environment/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Introduction to Data Science</li> </ul>"},{"location":"chapters/02-python-environment/#building-your-data-science-headquarters","title":"Building Your Data Science Headquarters","text":"<p>Every superhero needs a headquarters. Batman has the Batcave. Iron Man has Stark Tower. The Avengers have their compound. You? You're about to build something just as cool\u2014your personal data science command center.</p> <p>In Chapter 1, you discovered your new superpower: the ability to extract insights from data. But a superpower without the right tools is like having super strength but nowhere to punch. This chapter is where you assemble your utility belt, stock your armory, and set up the base of operations where all your data science magic will happen.</p> <p>By the end of this chapter, you'll have:</p> <ul> <li>Python installed and ready to go</li> <li>A professional code editor configured for data science</li> <li>Jupyter notebooks for interactive exploration</li> <li>Package management skills to add new tools whenever you need them</li> <li>A clean, organized environment that won't cause headaches later</li> </ul> <p>Let's build your headquarters!</p>"},{"location":"chapters/02-python-environment/#diagram-data-science-environment-architecture","title":"Diagram: Data Science Environment Architecture","text":"Data Science Environment Architecture <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students visualize how all components of their data science environment fit together</p> <p>Purpose: Show the layered architecture of a data science setup</p> <p>Layout: Vertical stack diagram showing layers from bottom to top</p> <p>Layers (bottom to top): 1. OPERATING SYSTEM (base layer)    - Windows, macOS, or Linux    - Color: Dark gray    - Icon: Computer</p> <ol> <li>PYTHON INSTALLATION</li> <li>Python interpreter (the engine)</li> <li>Color: Blue (Python blue)</li> <li> <p>Icon: Python logo</p> </li> <li> <p>PACKAGE MANAGER</p> </li> <li>pip or conda (the supply chain)</li> <li>Arrows showing packages flowing in</li> <li>Color: Orange</li> <li> <p>Icon: Package box</p> </li> <li> <p>VIRTUAL ENVIRONMENT</p> </li> <li>Isolated workspace (the clean room)</li> <li>Shows boundary separating from other environments</li> <li>Color: Green</li> <li> <p>Icon: Bubble/container</p> </li> <li> <p>PYTHON LIBRARIES</p> </li> <li>pandas, numpy, matplotlib (the tools)</li> <li>Multiple small icons representing different libraries</li> <li> <p>Color: Various colors for each library</p> </li> <li> <p>IDE / JUPYTER (top layer)</p> </li> <li>VS Code or Jupyter Notebook (the cockpit)</li> <li>Color: Purple</li> <li>Icon: Code editor window</li> </ol> <p>Side annotations: - Arrow from user to IDE: \"You work here\" - Arrow from libraries to IDE: \"Tools you use\" - Bracket around virtual environment: \"Keeps projects separate\"</p> <p>Interactive elements: - Hover over each layer to see description and purpose - Click to see common problems at each layer</p> <p>Visual style: Modern, clean boxes with rounded corners, superhero HQ aesthetic</p> <p>Implementation: SVG with CSS hover effects</p>"},{"location":"chapters/02-python-environment/#python-installation-powering-up-your-system","title":"Python Installation: Powering Up Your System","text":"<p>Python installation is the first and most critical step. Python is the programming language that powers your data science work\u2014it's the engine of your entire operation. Without it, nothing else works.</p> <p>Think of Python like electricity in your headquarters. You can have the fanciest equipment in the world, but without power, it's all just expensive furniture. Installing Python gives your computer the ability to understand and execute Python code.</p>"},{"location":"chapters/02-python-environment/#choosing-your-python-distribution","title":"Choosing Your Python Distribution","text":"<p>Here's where it gets interesting: there are actually several ways to install Python. The two main approaches are:</p> Approach Best For Includes Python.org (Official) Minimalists, learning basics Just Python, nothing extra Anaconda Distribution Data scientists (that's you!) Python + 250+ data science packages <p>For this course, we strongly recommend Anaconda. Why? Because it comes pre-loaded with almost everything you'll need\u2014pandas, NumPy, matplotlib, Jupyter, and hundreds of other tools. It's like buying a fully furnished headquarters instead of an empty warehouse.</p> <p>Superhero Shortcut</p> <p>Installing Anaconda is like getting a starter kit with all the gadgets already assembled. You could build everything from scratch, but why? Batman didn't forge his own Batarangs (okay, maybe he did, but you get the point).</p>"},{"location":"chapters/02-python-environment/#installation-steps","title":"Installation Steps","text":"<p>The installation process is straightforward:</p> <ol> <li>Visit anaconda.com and download the installer for your operating system</li> <li>Run the installer and accept the default options</li> <li>Wait for installation to complete (it might take a few minutes\u2014grab a snack)</li> <li>Open a terminal or Anaconda Prompt to verify it worked</li> </ol> <p>To verify your installation, open a terminal and type:</p> <pre><code>python --version\n</code></pre> <p>You should see something like <code>Python 3.11.5</code> (the exact version may differ). If you see an error, don't panic\u2014check the troubleshooting section at the end of this chapter.</p>"},{"location":"chapters/02-python-environment/#package-management-your-supply-chain","title":"Package Management: Your Supply Chain","text":"<p>Package management is how you add new tools and capabilities to your Python installation. In the superhero world, this is like having access to a warehouse full of gadgets you can requisition whenever you need them.</p> <p>Python's real power comes from its ecosystem of Python libraries\u2014pre-written code packages that handle specific tasks. Need to work with data tables? There's a library for that (pandas). Need to create visualizations? There's a library for that (matplotlib). Need to do machine learning? Yep, library for that too (scikit-learn).</p> <p>But how do you get these libraries? That's where package managers come in.</p>"},{"location":"chapters/02-python-environment/#pip-the-original-package-manager","title":"Pip: The Original Package Manager","text":"<p>Pip stands for \"Pip Installs Packages\" (yes, it's a recursive acronym\u2014programmers think they're funny). It's the original Python package manager and comes built into Python.</p> <p>Using pip is simple. Open a terminal and type:</p> <pre><code>pip install pandas\n</code></pre> <p>That's it! Pip will download pandas and all its dependencies, install them, and you're ready to go. Need to install multiple packages? Just list them:</p> <pre><code>pip install pandas numpy matplotlib seaborn\n</code></pre> <p>Common pip commands you'll use:</p> Command What It Does <code>pip install package_name</code> Install a package <code>pip uninstall package_name</code> Remove a package <code>pip list</code> Show all installed packages <code>pip show package_name</code> Show details about a package <code>pip install --upgrade package_name</code> Update to latest version"},{"location":"chapters/02-python-environment/#conda-the-data-scientists-choice","title":"Conda: The Data Scientist's Choice","text":"<p>While pip is great, data scientists often prefer conda\u2014the package manager that comes with Anaconda. Conda does everything pip does, plus it manages non-Python dependencies and creates isolated environments (more on that soon).</p> <pre><code>conda install pandas\n</code></pre> <p>The syntax is almost identical to pip. So why use conda? Because some data science packages have complicated dependencies involving C libraries, Fortran code, or other system-level components. Conda handles all of that automatically, while pip sometimes struggles.</p> <p>Pick One (Mostly)</p> <p>Using both pip and conda in the same environment can sometimes cause conflicts\u2014like having two quarterbacks calling different plays. In general, prefer conda for data science packages. Use pip only when a package isn't available through conda.</p>"},{"location":"chapters/02-python-environment/#diagram-package-manager-workflow","title":"Diagram: Package Manager Workflow","text":"How Package Managers Work <p>Type: workflow</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students understand the flow of installing and using packages</p> <p>Purpose: Visualize the package installation process from command to usage</p> <p>Visual style: Horizontal flowchart with icons</p> <p>Steps:</p> <ol> <li> <p>USER TYPES COMMAND    Icon: Keyboard    Example: \"pip install pandas\"    Color: Blue</p> </li> <li> <p>PACKAGE MANAGER SEARCHES    Icon: Magnifying glass    Label: \"Searches PyPI (pip) or Anaconda Cloud (conda)\"    Color: Orange</p> </li> <li> <p>DOWNLOADS PACKAGE    Icon: Download arrow    Label: \"Downloads package + all dependencies\"    Color: Green</p> </li> <li> <p>INSTALLS TO ENVIRONMENT    Icon: Folder with checkmark    Label: \"Saves files to your Python environment\"    Color: Purple</p> </li> <li> <p>READY TO IMPORT    Icon: Python logo with sparkles    Label: \"import pandas as pd\"    Color: Gold</p> </li> </ol> <p>Annotations: - Between steps 2-3: \"Internet connection required\" - Below step 3: \"May download multiple packages (dependencies)\"</p> <p>Interactive elements: - Hover each step to see common errors and solutions - Animation: Package icon traveling through pipeline</p> <p>Implementation: SVG with CSS animations</p>"},{"location":"chapters/02-python-environment/#virtual-environments-your-clean-room","title":"Virtual Environments: Your Clean Room","text":"<p>Here's a scenario that trips up many beginners: You're working on two different projects. Project A needs pandas version 1.5, but Project B needs pandas version 2.0. If you install version 2.0, Project A breaks. Install version 1.5, and Project B breaks. What do you do?</p> <p>The answer: virtual environments.</p> <p>A virtual environment is an isolated Python installation where you can install packages without affecting your main system or other projects. Think of it as having multiple separate headquarters, each set up exactly how one specific mission requires.</p>"},{"location":"chapters/02-python-environment/#why-virtual-environments-matter","title":"Why Virtual Environments Matter","text":"<p>Without virtual environments:</p> <ul> <li>Package conflicts are inevitable</li> <li>Upgrading one project can break another</li> <li>Your system Python gets cluttered with random packages</li> <li>Reproducing your work on another computer becomes a nightmare</li> </ul> <p>With virtual environments:</p> <ul> <li>Each project has exactly the packages it needs</li> <li>No conflicts between projects</li> <li>Easy to share your exact setup with others</li> <li>Clean, organized, professional workflow</li> </ul>"},{"location":"chapters/02-python-environment/#conda-environments-your-mission-specific-loadout","title":"Conda Environments: Your Mission-Specific Loadout","text":"<p>Since you're using Anaconda, you'll create environments using conda. Here's how:</p> <pre><code># Create a new environment named \"datascience\" with Python 3.11\nconda create --name datascience python=3.11\n\n# Activate the environment (start using it)\nconda activate datascience\n\n# Install packages into this environment\nconda install pandas numpy matplotlib jupyter\n\n# When you're done, deactivate\nconda deactivate\n</code></pre> <p>Once activated, anything you install goes into that environment only. Your other environments stay clean and untouched.</p> <p>Real-World Analogy</p> <p>Imagine you're a chef who cooks Italian food, Japanese food, and Mexican food. You COULD keep all your ingredients in one giant pantry. But it's much easier to have three separate stations, each stocked with exactly what that cuisine needs. Virtual environments are your separate cooking stations.</p>"},{"location":"chapters/02-python-environment/#diagram-virtual-environment-isolation-microsim","title":"Diagram: Virtual Environment Isolation MicroSim","text":"Virtual Environment Isolation Simulator <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students experiment with creating environments and installing packages to see how isolation works</p> <p>Canvas layout (750x500px): - Left side (500x500): Visual representation of environments - Right side (250x500): Controls and terminal simulation</p> <p>Visual elements: - Base system shown as gray platform at bottom - Virtual environments as colored bubbles floating above - Packages shown as small icons inside environments - Conflict indicators (red X) when same package different versions</p> <p>Interactive controls: - Button: \"Create Environment\" - Adds new bubble with name input - Dropdown: \"Select Environment\" - Choose which env to work in - Button: \"Install Package\" - Shows package picker - Package picker: pandas, numpy, matplotlib with version selector - Button: \"Delete Environment\" - Removes selected environment - Toggle: \"Show Conflicts\" - Highlights version conflicts</p> <p>Terminal simulation (right panel): - Shows conda commands being \"typed\" - Displays output messages - Command history</p> <p>Default state: - Base system with Python - One environment \"project-a\" with pandas 1.5 - One environment \"project-b\" with pandas 2.0 - No conflicts (isolated!)</p> <p>Behavior: - Creating environment adds new bubble - Installing package adds icon to current environment bubble - Installing conflicting versions in same env shows warning - Different versions in different envs shows green checkmarks - Hover over package shows version and description</p> <p>Educational messages: - \"Notice: Each environment is completely separate!\" - \"Try installing different pandas versions in different environments\" - \"See? No conflicts when properly isolated!\"</p> <p>Implementation: p5.js with interactive elements</p>"},{"location":"chapters/02-python-environment/#ide-setup-your-command-center","title":"IDE Setup: Your Command Center","text":"<p>An IDE (Integrated Development Environment) is your primary workspace\u2014the command center where you'll write code, run experiments, and analyze results. While you could technically write Python in Notepad, that's like trying to fight crime with a flashlight instead of the Batcomputer.</p> <p>A good IDE provides:</p> <ul> <li>Syntax highlighting (code is color-coded for readability)</li> <li>Auto-completion (suggests code as you type)</li> <li>Error detection (catches mistakes before you run)</li> <li>Integrated terminal (run commands without switching windows)</li> <li>Debugging tools (find and fix problems)</li> <li>Extension ecosystem (add new features)</li> </ul>"},{"location":"chapters/02-python-environment/#vs-code-the-modern-heros-choice","title":"VS Code: The Modern Hero's Choice","text":"<p>VS Code (Visual Studio Code) is our recommended IDE. It's free, fast, incredibly powerful, and loved by millions of developers worldwide. Microsoft makes it, but don't hold that against it\u2014it's genuinely excellent.</p> <p>Why VS Code for data science?</p> Feature Benefit Python Extension First-class Python support with IntelliSense Jupyter Integration Run notebooks directly in VS Code Git Integration Version control built right in Extensions Marketplace Thousands of add-ons available Remote Development Code on servers, containers, WSL Free Forever No subscriptions, no premium tiers"},{"location":"chapters/02-python-environment/#setting-up-vs-code-for-data-science","title":"Setting Up VS Code for Data Science","text":"<p>After installing VS Code, you'll want to add some extensions. Think of extensions as upgrades to your equipment\u2014each one adds new capabilities.</p> <p>Essential extensions for data science:</p> <ol> <li>Python (by Microsoft) - Core Python support</li> <li>Jupyter (by Microsoft) - Notebook support in VS Code</li> <li>Pylance - Advanced Python language features</li> <li>Python Indent - Fixes indentation automatically</li> <li>Rainbow CSV - Makes CSV files readable</li> </ol> <p>To install an extension:</p> <ol> <li>Click the Extensions icon in the left sidebar (looks like four squares)</li> <li>Search for the extension name</li> <li>Click \"Install\"</li> <li>That's it\u2014no restart required!</li> </ol> <p>Keyboard Shortcut Superpower</p> <p>Press <code>Ctrl+Shift+P</code> (Windows/Linux) or <code>Cmd+Shift+P</code> (Mac) to open the Command Palette\u2014your gateway to every VS Code feature. Start typing what you want to do, and VS Code will find it.</p>"},{"location":"chapters/02-python-environment/#diagram-vs-code-interface-tour","title":"Diagram: VS Code Interface Tour","text":"VS Code Interface Guided Tour <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Help students identify and remember the key parts of the VS Code interface</p> <p>Purpose: Interactive labeled diagram of VS Code interface</p> <p>Layout: Screenshot-style representation of VS Code with numbered callouts</p> <p>Main areas:</p> <ol> <li>ACTIVITY BAR (far left vertical strip)</li> <li>Explorer, Search, Git, Debug, Extensions icons</li> <li>Label: \"Quick access to major features\"</li> <li> <p>Color highlight: Blue</p> </li> <li> <p>SIDE BAR (left panel)</p> </li> <li>File explorer showing project structure</li> <li>Label: \"Your project files and folders\"</li> <li> <p>Color highlight: Green</p> </li> <li> <p>EDITOR AREA (center, main area)</p> </li> <li>Code with syntax highlighting</li> <li>Label: \"Where you write code\"</li> <li> <p>Color highlight: Purple</p> </li> <li> <p>TABS (top of editor)</p> </li> <li>Multiple file tabs</li> <li>Label: \"Switch between open files\"</li> <li> <p>Color highlight: Orange</p> </li> <li> <p>MINIMAP (right edge of editor)</p> </li> <li>Zoomed-out code preview</li> <li>Label: \"Navigate large files quickly\"</li> <li> <p>Color highlight: Teal</p> </li> <li> <p>TERMINAL (bottom panel)</p> </li> <li>Integrated command line</li> <li>Label: \"Run commands without leaving VS Code\"</li> <li> <p>Color highlight: Red</p> </li> <li> <p>STATUS BAR (bottom strip)</p> </li> <li>Python version, line number, encoding</li> <li>Label: \"Current file info and settings\"</li> <li>Color highlight: Gray</li> </ol> <p>Interactive elements: - Hover over each numbered area to see detailed description - Click to see common tasks performed in that area - \"Hide Labels\" toggle to test recall</p> <p>Visual style: Clean, modern, matching VS Code dark theme</p> <p>Implementation: HTML/CSS with image map and tooltips</p>"},{"location":"chapters/02-python-environment/#jupyter-notebooks-your-interactive-lab","title":"Jupyter Notebooks: Your Interactive Lab","text":"<p>While VS Code is great for writing Python scripts, data scientists have a special tool that's become essential to the craft: Jupyter Notebooks.</p> <p>A Jupyter Notebook is an interactive document that combines:</p> <ul> <li>Live code you can run piece by piece</li> <li>Rich text explanations with formatting</li> <li>Visualizations and charts</li> <li>Output from your code, displayed inline</li> </ul> <p>It's like having a lab notebook that can actually DO the experiments, not just record them. You write some code, run it, see the results immediately, then write more code based on what you learned. It's perfect for exploration and experimentation.</p>"},{"location":"chapters/02-python-environment/#why-notebooks-are-perfect-for-data-science","title":"Why Notebooks Are Perfect for Data Science","text":"<p>Traditional programming is linear: write all the code, then run it all at once. But data science is iterative: load data, look at it, clean something, look again, try an analysis, adjust, repeat.</p> <p>Notebooks support this workflow beautifully:</p> <ul> <li>Immediate feedback: See results instantly after each step</li> <li>Documentation built-in: Explain your thinking as you go</li> <li>Easy sharing: Send a notebook, and others see your code AND results</li> <li>Visual output: Charts appear right where you create them</li> <li>Reproducibility: Anyone can re-run your analysis step by step</li> </ul> <p>A Data Scientist's Best Friend</p> <p>\"I never understood data until I started using Jupyter. Being able to see my data, tweak my code, and immediately see what changed\u2014that's when everything clicked.\" \u2014 Every data scientist, basically</p>"},{"location":"chapters/02-python-environment/#starting-jupyter","title":"Starting Jupyter","text":"<p>If you installed Anaconda, you already have Jupyter. To launch it:</p> <pre><code># Make sure your environment is activated\nconda activate datascience\n\n# Launch Jupyter Notebook\njupyter notebook\n</code></pre> <p>This opens a browser window with the Jupyter interface. From there, you can create new notebooks, open existing ones, and organize your files.</p> <p>Alternatively, in VS Code with the Jupyter extension, you can create and run notebooks directly without the browser interface.</p>"},{"location":"chapters/02-python-environment/#notebook-cells-building-blocks-of-discovery","title":"Notebook Cells: Building Blocks of Discovery","text":"<p>Notebook cells are the fundamental units of a Jupyter notebook. Think of cells as individual building blocks\u2014each one contains either code or text, and you can rearrange, add, or delete them as needed.</p> <p>There are two main types of cells:</p>"},{"location":"chapters/02-python-environment/#code-cells","title":"Code Cells","text":"<p>A code cell contains Python code that you can execute. When you run a code cell, Python processes the code and displays any output directly below the cell.</p> <pre><code># This is a code cell\nx = 5\ny = 10\nprint(f\"The sum is: {x + y}\")\n</code></pre> <p>Output: <pre><code>The sum is: 15\n</code></pre></p> <p>Code cells have some special features:</p> <ul> <li>The last expression in a cell is automatically displayed (no <code>print()</code> needed)</li> <li>Variables created in one cell are available in all other cells</li> <li>You can run cells in any order (though running in order is usually best)</li> </ul>"},{"location":"chapters/02-python-environment/#markdown-cells","title":"Markdown Cells","text":"<p>A markdown cell contains formatted text using Markdown syntax. Use these to:</p> <ul> <li>Explain what your code does</li> <li>Document your analysis process</li> <li>Add headers and structure</li> <li>Include images or links</li> <li>Write conclusions and insights</li> </ul> <p>Here's what Markdown looks like:</p> <pre><code># This is a Header\n\nThis is regular text with **bold** and *italic* words.\n\n- Bullet point one\n- Bullet point two\n\n&gt; This is a quote block\n</code></pre> <p>The beauty of Markdown is that it's readable even before rendering. But when you \"run\" a Markdown cell, Jupyter converts it to beautifully formatted text.</p> Cell Type Contains Run Behavior Code Cell Python code Executes code, shows output Markdown Cell Formatted text Renders as HTML"},{"location":"chapters/02-python-environment/#diagram-notebook-cell-types-interactive-demo","title":"Diagram: Notebook Cell Types Interactive Demo","text":"Notebook Cell Types Interactive Demo <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students practice creating, editing, and running different cell types</p> <p>Canvas layout (700x550px): - Main area (700x450): Notebook simulation - Bottom panel (700x100): Controls and instructions</p> <p>Visual elements: - Simulated notebook interface with cells - Cell type indicator (Code/Markdown) on left side - Run button for each cell - Add cell buttons between cells - Cell highlight when selected</p> <p>Starting cells: 1. Markdown cell: \"# My First Notebook\\nWelcome to data science!\" 2. Code cell: \"x = 42\\nprint(f'The answer is {x}')\" 3. Empty code cell (ready for input)</p> <p>Interactive controls: - Click cell to select - Button: \"Run Cell\" (or Shift+Enter simulation) - Button: \"Add Code Cell\" - Button: \"Add Markdown Cell\" - Button: \"Delete Cell\" - Button: \"Change Cell Type\" - Text input area for editing selected cell</p> <p>Behavior: - Running code cell shows output below - Running markdown cell renders formatted text - Variables persist between cells (running cell 2 sets x=42 for later use) - Error messages shown if code has bugs - Cell execution order numbers appear [1], [2], etc.</p> <p>Instructions panel: - \"Click a cell to select it\" - \"Press Run Cell to execute\" - \"Try changing the value of x and re-running!\"</p> <p>Implementation: p5.js with text rendering and simple Python interpreter simulation</p>"},{"location":"chapters/02-python-environment/#cell-execution-bringing-your-code-to-life","title":"Cell Execution: Bringing Your Code to Life","text":"<p>Cell execution is the process of running a cell and getting results. This is where the magic happens\u2014where your ideas become reality.</p>"},{"location":"chapters/02-python-environment/#running-cells","title":"Running Cells","text":"<p>There are several ways to run a cell:</p> <ul> <li>Shift + Enter: Run current cell and move to next cell</li> <li>Ctrl + Enter: Run current cell and stay on it</li> <li>Run button: Click the play icon next to the cell</li> <li>Run All: Execute all cells in order (from menu)</li> </ul>"},{"location":"chapters/02-python-environment/#execution-order-matters","title":"Execution Order Matters","text":"<p>Here's something crucial to understand: cells can be run in any order, but the ORDER you run them in determines the result. Watch this:</p> <pre><code># Cell 1\nx = 5\n\n# Cell 2\nx = x + 10\n\n# Cell 3\nprint(x)\n</code></pre> <p>If you run cells 1, 2, 3 in order: output is <code>15</code> If you run cells 1, 3, 2, 3 in order: output is <code>15</code> then <code>25</code> If you run cell 3 first: ERROR! (x doesn't exist yet)</p> <p>The Restart Trap</p> <p>A common mistake: you run cells out of order, get confused, then can't reproduce your results. Solution? Use Kernel &gt; Restart &amp; Run All regularly to verify your notebook runs correctly from top to bottom.</p>"},{"location":"chapters/02-python-environment/#execution-numbers","title":"Execution Numbers","text":"<p>Notice those numbers in brackets next to code cells? Like <code>[1]</code>, <code>[2]</code>, <code>[3]</code>? Those tell you:</p> <ol> <li>Which cells have been run</li> <li>What order they were run in</li> </ol> <p>If you see <code>[5]</code> followed by <code>[3]</code> followed by <code>[7]</code>, that's a red flag that cells were run out of order. In a clean, reproducible notebook, numbers should be sequential: <code>[1]</code>, <code>[2]</code>, <code>[3]</code>, etc.</p>"},{"location":"chapters/02-python-environment/#the-kernel-your-python-brain","title":"The Kernel: Your Python Brain","text":"<p>The kernel is the computational engine behind your notebook. It's a running Python process that:</p> <ul> <li>Executes your code cells</li> <li>Keeps track of variables and their values</li> <li>Remembers function definitions</li> <li>Maintains the state of your session</li> </ul> <p>Think of the kernel as Python's brain. When you run a cell, you're asking the brain to process that code and remember the results. All cells share the same brain, which is why a variable defined in one cell is available in all others.</p>"},{"location":"chapters/02-python-environment/#kernel-operations","title":"Kernel Operations","text":"<p>Sometimes you need to control the kernel directly:</p> Operation What It Does When to Use Restart Clear all variables, fresh start When things get confusing Restart &amp; Clear Output Restart + clear all outputs Clean slate for sharing Restart &amp; Run All Fresh start, then run everything Verify reproducibility Interrupt Stop a running cell When code takes too long <p>The most common kernel operation: Restart &amp; Run All. This verifies that your notebook works from scratch\u2014essential before sharing your work or submitting an assignment.</p> <p>Kernel Health Check</p> <p>See a circular icon in the top right of Jupyter? That's the kernel status indicator. Empty circle = idle (ready). Filled circle = busy (running code). If it's stuck on busy forever, you might need to interrupt or restart.</p>"},{"location":"chapters/02-python-environment/#diagram-kernel-state-visualization","title":"Diagram: Kernel State Visualization","text":"How the Kernel Remembers Variables <p>Type: infographic</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students understand that the kernel maintains state across cell executions</p> <p>Purpose: Visualize the kernel as a memory bank that persists between cell runs</p> <p>Layout: Split view - notebook cells on left, kernel memory on right</p> <p>Left side (Notebook cells): Cell 1: <code>name = \"Alice\"</code> Cell 2: <code>age = 25</code> Cell 3: <code>greeting = f\"Hello {name}, you are {age}\"</code> Cell 4: <code>print(greeting)</code></p> <p>Right side (Kernel Memory Bank): Visual representation of memory slots: - After Cell 1: name \u2192 \"Alice\" - After Cell 2: name \u2192 \"Alice\", age \u2192 25 - After Cell 3: name \u2192 \"Alice\", age \u2192 25, greeting \u2192 \"Hello Alice, you are 25\"</p> <p>Animation flow: - Running each cell shows variable flowing into memory bank - Memory bank glows briefly when accessed - Clear visual that all cells share the same memory</p> <p>Bottom section: \"Restart Kernel\" button \u2192 Memory bank empties \u2192 Variables gone Message: \"After restart, you must re-run cells to recreate variables\"</p> <p>Interactive elements: - Step through button to simulate running each cell - Restart button to clear memory visualization - Hover over variable in memory to see when it was created</p> <p>Visual style: Clean, bright, \"mind palace\" aesthetic</p> <p>Implementation: HTML/CSS/JavaScript with animation</p>"},{"location":"chapters/02-python-environment/#import-statements-summoning-your-tools","title":"Import Statements: Summoning Your Tools","text":"<p>An import statement tells Python to load a library so you can use its features. Without imports, you're limited to basic Python. With imports, you have access to the entire data science arsenal.</p>"},{"location":"chapters/02-python-environment/#basic-import-syntax","title":"Basic Import Syntax","text":"<p>There are several ways to import libraries:</p> <pre><code># Import the entire library\nimport pandas\n\n# Now use it with the full name\ndata = pandas.read_csv(\"file.csv\")\n</code></pre> <pre><code># Import with a nickname (alias) - MOST COMMON\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Now use the shorter names\ndata = pd.read_csv(\"file.csv\")\n</code></pre> <pre><code># Import specific functions only\nfrom math import sqrt, pi\n\n# Now use them directly\nresult = sqrt(16)  # No \"math.\" prefix needed\n</code></pre>"},{"location":"chapters/02-python-environment/#standard-data-science-imports","title":"Standard Data Science Imports","text":"<p>The data science community has agreed on standard aliases. Using these makes your code readable to others:</p> Library Standard Import What It's For pandas <code>import pandas as pd</code> Data manipulation numpy <code>import numpy as np</code> Numerical computing matplotlib <code>import matplotlib.pyplot as plt</code> Basic plotting seaborn <code>import seaborn as sns</code> Statistical visualization scikit-learn <code>from sklearn import ...</code> Machine learning <p>Import Etiquette</p> <p>Always put imports at the TOP of your notebook or script, not scattered throughout. This makes it easy to see what libraries your code requires. It's like listing ingredients at the start of a recipe\u2014polite and helpful.</p>"},{"location":"chapters/02-python-environment/#what-happens-during-import","title":"What Happens During Import?","text":"<p>When you write <code>import pandas as pd</code>, Python:</p> <ol> <li>Searches for the pandas library (in your environment)</li> <li>Loads the library's code into memory</li> <li>Creates a reference called <code>pd</code> that points to it</li> <li>Makes all pandas functions available as <code>pd.something()</code></li> </ol> <p>If you get an error like <code>ModuleNotFoundError: No module named 'pandas'</code>, it means the library isn't installed in your current environment. Solution: <code>conda install pandas</code> or <code>pip install pandas</code>.</p>"},{"location":"chapters/02-python-environment/#python-libraries-your-superpower-extensions","title":"Python Libraries: Your Superpower Extensions","text":"<p>Python libraries (also called packages or modules) are collections of pre-written code that extend Python's capabilities. They're the reason Python dominates data science\u2014thousands of brilliant people have written code you can use for free.</p> <p>The data science ecosystem includes hundreds of libraries, but you'll start with a core set:</p>"},{"location":"chapters/02-python-environment/#the-essential-five","title":"The Essential Five","text":"Library Superpower Example Use pandas Data manipulation Load CSVs, filter rows, calculate statistics numpy Fast math Array operations, linear algebra matplotlib Visualization Line plots, bar charts, histograms seaborn Beautiful stats plots Distribution plots, heatmaps scikit-learn Machine learning Classification, regression, clustering"},{"location":"chapters/02-python-environment/#the-broader-ecosystem","title":"The Broader Ecosystem","text":"<p>Beyond the essential five, you'll encounter:</p> <ul> <li>jupyter - The notebook system itself</li> <li>scipy - Scientific computing</li> <li>statsmodels - Statistical modeling</li> <li>plotly - Interactive visualizations</li> <li>pytorch / tensorflow - Deep learning</li> <li>requests - Web data fetching</li> <li>beautifulsoup - Web scraping</li> </ul> <p>The beauty of package management: when you need a new capability, you can probably find a library for it. Someone else has already solved your problem\u2014you just need to <code>pip install</code> their solution.</p>"},{"location":"chapters/02-python-environment/#diagram-python-data-science-ecosystem-map","title":"Diagram: Python Data Science Ecosystem Map","text":"Python Data Science Library Ecosystem <p>Type: graph-model</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Help students understand the landscape of Python data science libraries and how they relate</p> <p>Purpose: Show relationships between major libraries and their purposes</p> <p>Node types: 1. Core (large gold hexagons)    - Python, NumPy</p> <ol> <li>Data (blue rectangles)</li> <li> <p>pandas, SQL connectors</p> </li> <li> <p>Visualization (green circles)</p> </li> <li> <p>matplotlib, seaborn, plotly</p> </li> <li> <p>Machine Learning (purple diamonds)</p> </li> <li> <p>scikit-learn, XGBoost</p> </li> <li> <p>Deep Learning (red stars)</p> </li> <li> <p>PyTorch, TensorFlow</p> </li> <li> <p>Utilities (gray rounded rectangles)</p> </li> <li>Jupyter, requests, BeautifulSoup</li> </ol> <p>Relationships (edges): - NumPy \u2192 pandas (built on) - NumPy \u2192 matplotlib (uses arrays) - pandas \u2192 seaborn (data source) - matplotlib \u2192 seaborn (built on) - NumPy \u2192 scikit-learn (data format) - scikit-learn \u2192 XGBoost (similar API) - NumPy \u2192 PyTorch (similar arrays) - NumPy \u2192 TensorFlow (similar arrays)</p> <p>Layout: Hierarchical with Python/NumPy at center</p> <p>Interactive features: - Hover node: See library description and common use cases - Click node: See example import statement - Filter by category (checkboxes)</p> <p>Visual styling: - Node size indicates popularity/importance - Edge thickness shows strength of dependency - Cluster by function area</p> <p>Implementation: vis-network JavaScript Canvas size: 800x600px</p>"},{"location":"chapters/02-python-environment/#putting-it-all-together-your-first-complete-setup","title":"Putting It All Together: Your First Complete Setup","text":"<p>Let's walk through setting up a complete data science environment from scratch. Follow along on your own computer!</p>"},{"location":"chapters/02-python-environment/#step-1-install-anaconda","title":"Step 1: Install Anaconda","text":"<ol> <li>Download Anaconda from anaconda.com/download</li> <li>Run the installer (accept defaults)</li> <li>Open Anaconda Prompt (Windows) or Terminal (Mac/Linux)</li> </ol>"},{"location":"chapters/02-python-environment/#step-2-create-your-environment","title":"Step 2: Create Your Environment","text":"<pre><code># Create a new environment for this course\nconda create --name datascience python=3.11\n\n# Activate it\nconda activate datascience\n\n# Install essential packages\nconda install pandas numpy matplotlib seaborn jupyter scikit-learn\n</code></pre>"},{"location":"chapters/02-python-environment/#step-3-install-vs-code","title":"Step 3: Install VS Code","text":"<ol> <li>Download VS Code from code.visualstudio.com</li> <li>Run the installer</li> <li>Open VS Code</li> <li>Install the Python and Jupyter extensions</li> </ol>"},{"location":"chapters/02-python-environment/#step-4-create-your-first-notebook","title":"Step 4: Create Your First Notebook","text":"<ol> <li>In VS Code, press <code>Ctrl+Shift+P</code> and type \"Create New Jupyter Notebook\"</li> <li>Select your <code>datascience</code> environment as the kernel</li> <li>Add a markdown cell with a title</li> <li>Add a code cell with your first imports:</li> </ol> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"Setup complete! Your data science headquarters is ready.\")\n</code></pre> <ol> <li>Run the cell. If you see the message, congratulations\u2014you're ready!</li> </ol> <p>Achievement Unlocked: Data Science HQ Online!</p> <p>You now have a professional-grade data science environment. The same tools used at Google, Netflix, and NASA are now at your fingertips. The only difference between you and a professional data scientist? Practice. Let's get started!</p>"},{"location":"chapters/02-python-environment/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>Even superheroes face setbacks. Here are solutions to common setup problems:</p>"},{"location":"chapters/02-python-environment/#python-not-found","title":"\"Python not found\"","text":"<p>Cause: Python isn't in your system PATH. Solution: Reinstall Anaconda and check \"Add to PATH\" option, OR always use Anaconda Prompt.</p>"},{"location":"chapters/02-python-environment/#module-not-found","title":"\"Module not found\"","text":"<p>Cause: Package not installed in current environment. Solution: Activate your environment (<code>conda activate datascience</code>) then install the package.</p>"},{"location":"chapters/02-python-environment/#kernel-died-in-jupyter","title":"\"Kernel died\" in Jupyter","text":"<p>Cause: Usually a memory issue or package conflict. Solution: Restart the kernel. If persistent, restart your computer or recreate the environment.</p>"},{"location":"chapters/02-python-environment/#vs-code-doesnt-see-my-environment","title":"VS Code doesn't see my environment","text":"<p>Cause: VS Code hasn't refreshed its environment list. Solution: Press <code>Ctrl+Shift+P</code>, type \"Python: Select Interpreter\", and choose your environment manually.</p>"},{"location":"chapters/02-python-environment/#everything-is-slow","title":"Everything is slow","text":"<p>Cause: Possibly too many packages or old hardware. Solution: Make sure you're using a clean environment with only the packages you need.</p> Chapter 2 Checkpoint: Test Your Understanding <p>Question: You're starting a new project and need pandas version 2.0, but your existing project uses pandas 1.5. How do you handle this without breaking either project?</p> <p>Click to reveal answer:</p> <p>Create a separate virtual environment for each project!</p> <pre><code># For the new project\nconda create --name new_project python=3.11\nconda activate new_project\nconda install pandas=2.0\n\n# For the old project (already exists)\nconda activate old_project\n# pandas 1.5 is already there, unchanged\n</code></pre> <p>Virtual environments keep projects isolated so different versions don't conflict.</p>"},{"location":"chapters/02-python-environment/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Python installation (via Anaconda) is your foundation\u2014everything else builds on it.</p> </li> <li> <p>Package managers (pip and conda) let you install new tools whenever you need them.</p> </li> <li> <p>Virtual environments keep your projects isolated and conflict-free\u2014use them for every project.</p> </li> <li> <p>VS Code is your command center\u2014customize it with extensions for data science.</p> </li> <li> <p>Jupyter Notebooks combine code, text, and visualizations for interactive exploration.</p> </li> <li> <p>Cells (code and markdown) are the building blocks of notebooks; run them with Shift+Enter.</p> </li> <li> <p>The kernel is Python's brain\u2014restart it when things get confusing.</p> </li> <li> <p>Import statements load libraries; use standard aliases like <code>pd</code>, <code>np</code>, <code>plt</code>.</p> </li> <li> <p>Python libraries are your superpower extensions\u2014pandas, numpy, matplotlib are essential.</p> </li> </ol> <p>Your headquarters is built. Your tools are ready. In the next chapter, you'll learn to wield them by working with Python's most important data structures. The real adventure begins now!</p>"},{"location":"chapters/03-python-data-structures/","title":"Python Data Structures","text":""},{"location":"chapters/03-python-data-structures/#summary","title":"Summary","text":"<p>This chapter introduces the essential data structures used in Python data science workflows. Students will learn Python's native data structures (lists, dictionaries, tuples, arrays) and then progress to pandas, the primary library for data manipulation. The chapter covers DataFrames and Series, data loading from CSV files, and methods for inspecting and selecting data. By the end of this chapter, students will be able to load, explore, and navigate datasets using pandas.</p>"},{"location":"chapters/03-python-data-structures/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Lists</li> <li>Dictionaries</li> <li>Tuples</li> <li>Arrays</li> <li>Pandas Library</li> <li>DataFrame</li> <li>Series</li> <li>Index</li> <li>Column</li> <li>Row</li> <li>Data Loading</li> <li>CSV Files</li> <li>Read CSV</li> <li>Data Inspection</li> <li>Head Method</li> <li>Tail Method</li> <li>Shape Attribute</li> <li>Info Method</li> <li>Describe Method</li> <li>Data Selection</li> </ol>"},{"location":"chapters/03-python-data-structures/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Introduction to Data Science</li> <li>Chapter 2: Python Environment and Setup</li> </ul>"},{"location":"chapters/03-python-data-structures/#your-data-needs-a-home","title":"Your Data Needs a Home","text":"<p>In Chapter 1, you learned that data is the raw material of your superpower. In Chapter 2, you built your headquarters and assembled your tools. But here's the thing about data\u2014it doesn't just float around in thin air. Data needs to live somewhere. It needs containers, organization, and structure.</p> <p>Think about it: even superheroes need ways to organize their stuff. Batman has labeled compartments in his utility belt. Iron Man has JARVIS cataloging his suit designs. The X-Men have Cerebro organizing mutant data. You? You're about to learn the containers that will hold YOUR data\u2014and trust me, these containers are more powerful than they might first appear.</p> <p>This chapter is where you graduate from \"knowing about data science\" to actually DOING data science. By the end, you'll be loading real datasets, exploring them like a detective, and selecting exactly the information you need. Let's go!</p>"},{"location":"chapters/03-python-data-structures/#diagram-data-structure-hierarchy","title":"Diagram: Data Structure Hierarchy","text":"Python Data Structure Hierarchy <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students visualize the relationship between Python's native data structures and pandas structures</p> <p>Purpose: Show how basic Python structures lead to more powerful pandas structures</p> <p>Layout: Hierarchical tree diagram, top to bottom</p> <p>Level 1 (Top): \"DATA STRUCTURES\" - Color: Gold - Central hub</p> <p>Level 2 (Split into two branches):</p> <p>Branch A: \"Python Built-in\" - Color: Blue - Children:   - Lists (ordered, mutable)   - Dictionaries (key-value pairs)   - Tuples (ordered, immutable)</p> <p>Branch B: \"Pandas Structures\" - Color: Green - Children:   - Series (1D labeled array)   - DataFrame (2D labeled table)</p> <p>Level 3 (Under pandas): - DataFrame components: Index, Columns, Rows - Series: labeled as \"Single column of DataFrame\"</p> <p>Arrows showing: - \"List \u2192 can become \u2192 Series\" - \"Dictionary \u2192 can become \u2192 DataFrame\" - \"Series + Series \u2192 DataFrame\"</p> <p>Visual style: Modern flowchart with rounded boxes</p> <p>Interactive elements: - Hover over each structure to see quick definition - Click to see code example - Color coding: Blue for Python native, Green for pandas</p> <p>Implementation: SVG with CSS hover effects</p>"},{"location":"chapters/03-python-data-structures/#pythons-built-in-data-structures","title":"Python's Built-in Data Structures","text":"<p>Before we dive into the fancy pandas library, you need to understand Python's native data structures. These are the building blocks\u2014simple containers that Python provides out of the box. Think of them as the basic training before you get the advanced gear.</p>"},{"location":"chapters/03-python-data-structures/#lists-your-ordered-collection","title":"Lists: Your Ordered Collection","text":"<p>A list is Python's most versatile data structure. It's an ordered collection of items that can hold anything\u2014numbers, strings, even other lists. Lists are mutable, meaning you can change them after creation.</p> <pre><code># Creating a list\nsuperheroes = [\"Batman\", \"Iron Man\", \"Spider-Man\", \"Wonder Woman\"]\n\n# Lists can hold different types\nmixed_data = [42, \"hello\", 3.14, True]\n\n# Lists are ordered - position matters!\nprint(superheroes[0])  # Output: \"Batman\" (first item)\nprint(superheroes[-1]) # Output: \"Wonder Woman\" (last item)\n</code></pre> <p>Lists are incredibly flexible:</p> <ul> <li>Add items with <code>append()</code> or <code>insert()</code></li> <li>Remove items with <code>remove()</code> or <code>pop()</code></li> <li>Sort with <code>sort()</code></li> <li>Get the length with <code>len()</code></li> </ul> <pre><code># Modifying lists\nsuperheroes.append(\"Black Panther\")      # Add to end\nsuperheroes.insert(0, \"Superman\")        # Add at position 0\nsuperheroes.remove(\"Spider-Man\")         # Remove specific item\nlast_hero = superheroes.pop()            # Remove and return last item\n</code></pre> <p>When to Use Lists</p> <p>Use lists when you have a collection of items and their ORDER matters. Lists are perfect for sequences, rankings, or any situation where position is important.</p>"},{"location":"chapters/03-python-data-structures/#dictionaries-your-key-value-powerhouse","title":"Dictionaries: Your Key-Value Powerhouse","text":"<p>A dictionary is a collection of key-value pairs. Instead of accessing items by position (like lists), you access them by a unique key. Dictionaries are like having labeled drawers instead of numbered ones.</p> <pre><code># Creating a dictionary\nhero_powers = {\n    \"Batman\": \"Intelligence and gadgets\",\n    \"Iron Man\": \"Powered armor suit\",\n    \"Spider-Man\": \"Spider abilities\",\n    \"Wonder Woman\": \"Divine powers\"\n}\n\n# Accessing values by key\nprint(hero_powers[\"Batman\"])  # Output: \"Intelligence and gadgets\"\n\n# Adding new key-value pairs\nhero_powers[\"Black Panther\"] = \"Vibranium suit\"\n\n# Checking if key exists\nif \"Superman\" in hero_powers:\n    print(hero_powers[\"Superman\"])\nelse:\n    print(\"Superman not found!\")\n</code></pre> <p>Dictionaries are blazingly fast for lookups. Whether you have 10 items or 10 million, finding a value by its key takes roughly the same amount of time. That's why they're used everywhere in Python.</p> Operation List Dictionary Access by position \u2713 Fast \u2717 Not supported Access by key/name \u2717 Slow (must search) \u2713 Instant Maintains order \u2713 Yes \u2713 Yes (Python 3.7+) Duplicate keys \u2713 Allowed \u2717 Keys must be unique"},{"location":"chapters/03-python-data-structures/#tuples-your-immutable-record","title":"Tuples: Your Immutable Record","text":"<p>A tuple is like a list, but immutable\u2014once created, it cannot be changed. This might sound like a limitation, but it's actually a feature. Tuples are perfect for data that shouldn't change.</p> <pre><code># Creating a tuple\ncoordinates = (40.7128, -74.0060)  # NYC latitude, longitude\n\n# Accessing elements (just like lists)\nlatitude = coordinates[0]\nlongitude = coordinates[1]\n\n# Tuples can be unpacked\nlat, lon = coordinates  # Assigns both at once!\n\n# This would cause an error:\n# coordinates[0] = 41.0  # TypeError: tuples are immutable\n</code></pre> <p>Why use tuples?</p> <ul> <li>They're slightly faster than lists</li> <li>They signal \"this data won't change\"</li> <li>They can be used as dictionary keys (lists cannot)</li> <li>They're perfect for returning multiple values from functions</li> </ul>"},{"location":"chapters/03-python-data-structures/#arrays-numeric-power-preview","title":"Arrays: Numeric Power (Preview)","text":"<p>Arrays are specialized containers for numeric data. While Python has a built-in <code>array</code> module, data scientists typically use NumPy arrays, which we'll explore in depth in Chapter 10.</p> <p>The key difference: lists can hold any type of data, but arrays are optimized for numbers. This makes mathematical operations on arrays incredibly fast\u2014often 10-100x faster than the same operations on lists.</p> <pre><code># Preview: NumPy arrays (covered fully in Chapter 10)\nimport numpy as np\n\n# Create an array of numbers\nscores = np.array([85, 92, 78, 95, 88])\n\n# Mathematical operations are fast and easy\naverage = scores.mean()\nhighest = scores.max()\nscaled = scores * 1.1  # Multiply all by 1.1\n</code></pre> <p>For now, just know that arrays exist and are important. We'll come back to them with full superhero treatment later!</p>"},{"location":"chapters/03-python-data-structures/#diagram-python-data-structures-comparison-microsim","title":"Diagram: Python Data Structures Comparison MicroSim","text":"Data Structure Selection Helper <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Help students choose the right data structure for different scenarios</p> <p>Canvas layout (700x500px): - Left side (400x500): Scenario display and answer area - Right side (300x500): Score tracker and hints</p> <p>Visual elements: - Scenario card showing a data storage problem - Four buttons: List, Dictionary, Tuple, Array - Feedback indicator (correct/incorrect with explanation) - Progress bar showing scenarios completed - Score counter</p> <p>Scenarios (10 total): 1. \"Store student names in order of arrival\" \u2192 List 2. \"Map employee IDs to their salaries\" \u2192 Dictionary 3. \"Store GPS coordinates that won't change\" \u2192 Tuple 4. \"Hold 1 million numbers for fast math\" \u2192 Array 5. \"Keep a to-do list you'll modify\" \u2192 List 6. \"Store configuration settings by name\" \u2192 Dictionary 7. \"Return both min and max from a function\" \u2192 Tuple 8. \"Store pixel values for image processing\" \u2192 Array 9. \"Track items and their quantities\" \u2192 Dictionary 10. \"Keep records of (x, y) point pairs\" \u2192 Tuple or List</p> <p>Interactive controls: - Click data structure button to answer - \"Next Scenario\" button - \"Show Hint\" button - \"Reset Quiz\" button</p> <p>Behavior: - Correct answer: Green flash, +10 points, explanation shown - Incorrect: Red flash, correct answer revealed with explanation - End: Summary showing areas to review</p> <p>Visual style: Quiz game aesthetic with achievement badges</p> <p>Implementation: p5.js with scenario array and scoring logic</p>"},{"location":"chapters/03-python-data-structures/#enter-pandas-your-data-science-superweapon","title":"Enter Pandas: Your Data Science Superweapon","text":"<p>Now we're getting to the good stuff. Pandas is the Python library that transformed data science. Before pandas, working with tabular data in Python was painful. After pandas, it became almost enjoyable.</p> <p>The Pandas library provides two main data structures:</p> <ul> <li>Series: A one-dimensional labeled array</li> <li>DataFrame: A two-dimensional labeled table (like a spreadsheet)</li> </ul> <p>These aren't just containers\u2014they're containers with superpowers. Filtering, grouping, merging, reshaping... pandas makes operations that would take dozens of lines of code happen in just one.</p> <pre><code># The standard way to import pandas\nimport pandas as pd\n\n# Now you can use pd.DataFrame, pd.Series, pd.read_csv, etc.\n</code></pre> <p>Why 'pandas'?</p> <p>The name comes from \"panel data\"\u2014a term from statistics for multi-dimensional data. It has nothing to do with the cute bears, though the library is just as cuddly once you get to know it.</p>"},{"location":"chapters/03-python-data-structures/#series-the-one-dimensional-wonder","title":"Series: The One-Dimensional Wonder","text":"<p>A Series is like a supercharged list. It's a one-dimensional array with labels (called an index). Think of it as a single column from a spreadsheet, but with its own identity.</p> <pre><code>import pandas as pd\n\n# Create a Series from a list\nscores = pd.Series([85, 92, 78, 95, 88])\nprint(scores)\n</code></pre> <p>Output: <pre><code>0    85\n1    92\n2    78\n3    95\n4    88\ndtype: int64\n</code></pre></p> <p>Notice those numbers on the left? That's the index\u2014labels for each value. By default, pandas uses 0, 1, 2, etc., but you can use custom labels:</p> <pre><code># Series with custom index\nscores = pd.Series(\n    [85, 92, 78, 95, 88],\n    index=[\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"]\n)\nprint(scores)\n</code></pre> <p>Output: <pre><code>Alice      85\nBob        92\nCharlie    78\nDiana      95\nEve        88\ndtype: int64\n</code></pre></p> <p>Now you can access data by name: <code>scores[\"Bob\"]</code> returns <code>92</code>. This is incredibly powerful when working with real data.</p>"},{"location":"chapters/03-python-data-structures/#dataframe-the-main-event","title":"DataFrame: The Main Event","text":"<p>The DataFrame is pandas' flagship data structure. It's a two-dimensional table with labeled rows and columns\u2014essentially, a super-powered spreadsheet that you can manipulate with code.</p> Student Math Science English Alice 85 90 88 Bob 92 88 95 Charlie 78 82 80 <p>In pandas, this becomes:</p> <pre><code>import pandas as pd\n\n# Create a DataFrame from a dictionary\ndata = {\n    \"Math\": [85, 92, 78],\n    \"Science\": [90, 88, 82],\n    \"English\": [88, 95, 80]\n}\ndf = pd.DataFrame(data, index=[\"Alice\", \"Bob\", \"Charlie\"])\nprint(df)\n</code></pre> <p>Output: <pre><code>         Math  Science  English\nAlice      85       90       88\nBob        92       88       95\nCharlie    78       82       80\n</code></pre></p> <p>This is the structure you'll work with 90% of the time in data science. Let's break down its components.</p>"},{"location":"chapters/03-python-data-structures/#diagram-dataframe-anatomy","title":"Diagram: DataFrame Anatomy","text":"DataFrame Anatomy Interactive Diagram <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Help students identify and remember the components of a DataFrame</p> <p>Purpose: Visual breakdown of DataFrame structure with labeled components</p> <p>Layout: Central DataFrame table with callouts pointing to each component</p> <p>Main visual: A 4x3 DataFrame table displaying: <pre><code>          Col_A   Col_B   Col_C\nrow_0      10      20      30\nrow_1      40      50      60\nrow_2      70      80      90\nrow_3     100     110     120\n</code></pre></p> <p>Callouts (numbered, with leader lines):</p> <ol> <li>INDEX (pointing to row labels on left)</li> <li>\"Row labels - can be numbers, strings, or dates\"</li> <li>\"Access with: df.index\"</li> <li> <p>Color: Blue</p> </li> <li> <p>COLUMNS (pointing to column headers)</p> </li> <li>\"Column names - usually strings\"</li> <li>\"Access with: df.columns\"</li> <li> <p>Color: Green</p> </li> <li> <p>ROW (highlighting entire row_1)</p> </li> <li>\"One observation/record\"</li> <li>\"Access with: df.loc['row_1'] or df.iloc[1]\"</li> <li> <p>Color: Orange</p> </li> <li> <p>COLUMN (highlighting entire Col_B)</p> </li> <li>\"One variable/feature\"</li> <li>\"Access with: df['Col_B']\"</li> <li> <p>Color: Purple</p> </li> <li> <p>CELL (highlighting intersection of row_1 and Col_B = 50)</p> </li> <li>\"Single value\"</li> <li>\"Access with: df.loc['row_1', 'Col_B']\"</li> <li> <p>Color: Red</p> </li> <li> <p>VALUES (pointing to all numbers)</p> </li> <li>\"The actual data (NumPy array underneath)\"</li> <li>\"Access with: df.values\"</li> <li>Color: Gray</li> </ol> <p>Interactive elements: - Hover over each component to highlight it - Click to see code example for accessing that component - Toggle \"Show Code\" to see access patterns</p> <p>Visual style: Clean spreadsheet look with color highlights</p> <p>Implementation: HTML/CSS with JavaScript interactivity</p>"},{"location":"chapters/03-python-data-structures/#understanding-index-columns-and-rows","title":"Understanding Index, Columns, and Rows","text":"<p>Let's clarify these fundamental concepts:</p> <p>Index: The row labels of a DataFrame. Think of it as the \"name tag\" for each row.</p> <pre><code>print(df.index)  # Index(['Alice', 'Bob', 'Charlie'], dtype='object')\n</code></pre> <p>Columns: The column headers\u2014the names of your variables.</p> <pre><code>print(df.columns)  # Index(['Math', 'Science', 'English'], dtype='object')\n</code></pre> <p>Row: A single horizontal slice\u2014one observation with values for all columns.</p> <p>Column: A single vertical slice\u2014one variable with values for all observations. When you extract a single column, you get a Series!</p> <pre><code># Get a single column (returns a Series)\nmath_scores = df[\"Math\"]\nprint(type(math_scores))  # &lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <p>DataFrame vs Series</p> <p>A DataFrame is essentially a collection of Series that share the same index. Each column is a Series. Understanding this relationship will help you navigate pandas intuitively.</p>"},{"location":"chapters/03-python-data-structures/#loading-real-data-csv-files","title":"Loading Real Data: CSV Files","text":"<p>Theory is great, but data science is about DOING. Let's load some real data!</p> <p>Data loading is the process of reading data from external sources into Python. The most common format you'll encounter is CSV.</p>"},{"location":"chapters/03-python-data-structures/#what-are-csv-files","title":"What Are CSV Files?","text":"<p>CSV stands for Comma-Separated Values. It's a simple text format where:</p> <ul> <li>Each line is one row of data</li> <li>Values in a row are separated by commas</li> <li>The first line usually contains column names</li> </ul> <p>Here's what a CSV file looks like inside:</p> <pre><code>name,age,city,score\nAlice,25,New York,85\nBob,30,Los Angeles,92\nCharlie,22,Chicago,78\nDiana,28,Houston,95\n</code></pre> <p>CSV files are everywhere because they're simple, human-readable, and work with almost any software. Excel, Google Sheets, databases\u2014they all speak CSV.</p>"},{"location":"chapters/03-python-data-structures/#the-read_csv-method","title":"The read_csv Method","text":"<p>The read_csv function is your gateway to loading CSV data:</p> <pre><code>import pandas as pd\n\n# Load a CSV file into a DataFrame\ndf = pd.read_csv(\"students.csv\")\n\n# That's it! df now contains all your data\nprint(df)\n</code></pre> <p>Output: <pre><code>      name  age         city  score\n0    Alice   25     New York     85\n1      Bob   30  Los Angeles     92\n2  Charlie   22      Chicago     78\n3    Diana   28      Houston     95\n</code></pre></p> <p><code>read_csv</code> is incredibly smart. It automatically:</p> <ul> <li>Detects the delimiter (comma, tab, etc.)</li> <li>Identifies the header row</li> <li>Infers data types for each column</li> <li>Handles missing values</li> </ul> <p>For special situations, you have options:</p> <pre><code># Specify a different delimiter\ndf = pd.read_csv(\"data.tsv\", sep=\"\\t\")  # Tab-separated\n\n# Use a different row as header\ndf = pd.read_csv(\"data.csv\", header=1)  # Second row as header\n\n# No header in file\ndf = pd.read_csv(\"data.csv\", header=None, names=[\"A\", \"B\", \"C\"])\n\n# Set a column as index\ndf = pd.read_csv(\"data.csv\", index_col=\"name\")\n</code></pre> <p>File Paths Matter</p> <p><code>read_csv(\"data.csv\")</code> looks for the file in your current working directory. If the file is elsewhere, use the full path: <code>read_csv(\"/path/to/data.csv\")</code> or a relative path: <code>read_csv(\"../data/data.csv\")</code>.</p>"},{"location":"chapters/03-python-data-structures/#diagram-data-loading-workflow","title":"Diagram: Data Loading Workflow","text":"CSV Loading Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students understand the complete process from CSV file to usable DataFrame</p> <p>Purpose: Visualize the journey of data from file to DataFrame</p> <p>Visual style: Horizontal flowchart with icons</p> <p>Steps:</p> <ol> <li> <p>CSV FILE ON DISK    Icon: Document with \".csv\" label    Label: \"Raw text file with comma-separated values\"    Color: Gray</p> </li> <li> <p>pd.read_csv()    Icon: Pandas logo with arrow    Label: \"Pandas reads and parses the file\"    Color: Orange</p> </li> <li> <p>PARSING HAPPENS    Icon: Gears turning    Sub-steps shown below:</p> </li> <li>\"Detect delimiter\"</li> <li>\"Read header row\"</li> <li>\"Infer data types\"</li> <li> <p>\"Handle missing values\"    Color: Blue</p> </li> <li> <p>DATAFRAME CREATED    Icon: Table grid    Label: \"Data now in memory as DataFrame\"    Color: Green</p> </li> <li> <p>READY FOR ANALYSIS    Icon: Sparkles/magic wand    Label: \"Filter, analyze, visualize!\"    Color: Gold</p> </li> </ol> <p>Annotations: - Below step 1: \"Could be 100 rows or 100 million\" - Below step 4: \"Lives in computer memory (RAM)\" - Below step 5: \"This is where the fun begins\"</p> <p>Error path (branching from step 2): - \"FileNotFoundError\" \u2192 \"Check your file path!\" - \"ParserError\" \u2192 \"Check file format and delimiter\"</p> <p>Interactive elements: - Hover each step for detailed explanation - Click to see common errors at each stage</p> <p>Implementation: SVG with CSS animations</p>"},{"location":"chapters/03-python-data-structures/#data-inspection-getting-to-know-your-data","title":"Data Inspection: Getting to Know Your Data","text":"<p>You've loaded a dataset. Now what? Before doing any analysis, you need to understand what you're working with. This is data inspection\u2014exploring your data to understand its structure, size, types, and content.</p> <p>Think of it like this: you've received a mysterious package. Before using what's inside, you'd want to know how big it is, what's in it, and whether anything's broken. Same with data.</p>"},{"location":"chapters/03-python-data-structures/#the-head-method-peek-at-the-beginning","title":"The head Method: Peek at the Beginning","text":"<p>The head method shows you the first few rows of your DataFrame. It's usually the first thing you'll do after loading data.</p> <pre><code># Show first 5 rows (default)\ndf.head()\n\n# Show first 10 rows\ndf.head(10)\n</code></pre> <p>Output: <pre><code>      name  age         city  score\n0    Alice   25     New York     85\n1      Bob   30  Los Angeles     92\n2  Charlie   22      Chicago     78\n3    Diana   28      Houston     95\n4      Eve   26       Boston     88\n</code></pre></p> <p>Why is this useful? Real datasets often have thousands or millions of rows. You can't print the whole thing. <code>head()</code> gives you a quick preview without overwhelming your screen.</p>"},{"location":"chapters/03-python-data-structures/#the-tail-method-check-the-end","title":"The tail Method: Check the End","text":"<p>The tail method shows the last few rows. It's great for verifying that your data loaded completely and for seeing recent entries in time-series data.</p> <pre><code># Show last 5 rows (default)\ndf.tail()\n\n# Show last 3 rows\ndf.tail(3)\n</code></pre> <p>Head and Tail Together</p> <p>Use <code>head()</code> AND <code>tail()</code> together. Sometimes data has different formatting at the beginning vs. end, or there are footer rows you need to handle. Looking at both ends catches issues early.</p>"},{"location":"chapters/03-python-data-structures/#the-shape-attribute-know-your-dimensions","title":"The shape Attribute: Know Your Dimensions","text":"<p>The shape attribute tells you the dimensions of your DataFrame\u2014how many rows and columns it has.</p> <pre><code>print(df.shape)  # Output: (1000, 15)\n# This means: 1000 rows, 15 columns\n</code></pre> <p>Shape is crucial because:</p> <ul> <li>It tells you how much data you're working with</li> <li>It helps identify if data loaded correctly</li> <li>It's used for splitting data in machine learning</li> </ul> <pre><code># Unpack shape into variables\nnum_rows, num_cols = df.shape\nprint(f\"Dataset has {num_rows} observations and {num_cols} variables\")\n</code></pre>"},{"location":"chapters/03-python-data-structures/#the-info-method-get-the-full-picture","title":"The info Method: Get the Full Picture","text":"<p>The info method provides a complete summary of your DataFrame:</p> <pre><code>df.info()\n</code></pre> <p>Output: <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   name    1000 non-null   object\n 1   age     998 non-null    int64\n 2   city    1000 non-null   object\n 3   score   995 non-null    float64\nDtypes: float64(1), int64(1), object(2)\nmemory usage: 31.4+ KB\n</code></pre></p> <p>This tells you:</p> <ul> <li>Total number of entries (rows)</li> <li>Column names and their order</li> <li>Non-null counts (how many values aren't missing)</li> <li>Data types for each column</li> <li>Memory usage</li> </ul> <p>See those \"non-null\" counts? If they're less than the total rows, you have missing data. The <code>age</code> column has 998 non-null out of 1000\u2014meaning 2 values are missing. That's important to know!</p>"},{"location":"chapters/03-python-data-structures/#the-describe-method-statistical-summary","title":"The describe Method: Statistical Summary","text":"<p>The describe method gives you summary statistics for all numeric columns:</p> <pre><code>df.describe()\n</code></pre> <p>Output: <pre><code>              age        score\ncount  998.000000   995.000000\nmean    27.500000    85.200000\nstd      5.230000    10.150000\nmin     18.000000    55.000000\n25%     23.000000    78.000000\n50%     27.000000    85.000000\n75%     32.000000    93.000000\nmax     45.000000   100.000000\n</code></pre></p> <p>In one command, you get:</p> <ul> <li>count: Number of non-null values</li> <li>mean: Average value</li> <li>std: Standard deviation (spread)</li> <li>min/max: Smallest and largest values</li> <li>25%, 50%, 75%: Quartiles (distribution shape)</li> </ul> <p>This is your first glimpse at what the data looks like statistically. Is the average score 85 or 50? Are ages ranging from 18-25 or 18-80? <code>describe()</code> answers these questions instantly.</p>"},{"location":"chapters/03-python-data-structures/#diagram-data-inspection-command-center","title":"Diagram: Data Inspection Command Center","text":"Data Inspection Dashboard MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students practice using inspection methods and see results immediately</p> <p>Canvas layout (800x600px): - Top area (800x100): Sample DataFrame display - Left panel (300x500): Method buttons - Right panel (500x500): Output display area</p> <p>Sample DataFrame (displayed at top): Small 8x5 DataFrame with realistic data including: - name (strings) - age (integers with 1 missing) - city (strings) - score (floats with 1 missing) - active (booleans)</p> <p>Method buttons (left panel): - <code>.head()</code> with slider for n (1-8) - <code>.tail()</code> with slider for n (1-8) - <code>.shape</code> - <code>.info()</code> - <code>.describe()</code> - <code>.columns</code> - <code>.dtypes</code></p> <p>Output display (right panel): - Shows result of selected method - Formatted to look like Jupyter output - Syntax highlighting for code</p> <p>Interactive controls: - Click method button to execute - Adjust slider for head/tail n parameter - \"Show Code\" toggle displays the Python code - \"Reset\" button returns to initial state</p> <p>Behavior: - Clicking method shows its output - Output updates immediately when parameters change - Code panel shows exact command used - Tooltips explain what each method returns</p> <p>Educational annotations: - For head/tail: \"Notice: only shows [n] rows!\" - For shape: \"(rows, columns) - easy to remember!\" - For info: \"Look for non-null counts to find missing data\" - For describe: \"Only numeric columns shown by default\"</p> <p>Visual style: IDE/notebook aesthetic with dark mode option</p> <p>Implementation: p5.js with precomputed outputs</p>"},{"location":"chapters/03-python-data-structures/#data-selection-getting-what-you-need","title":"Data Selection: Getting What You Need","text":"<p>Now for the really powerful stuff. Data selection is how you extract specific pieces from your DataFrame\u2014particular rows, columns, or combinations. This is like having X-ray vision for your data.</p>"},{"location":"chapters/03-python-data-structures/#selecting-columns","title":"Selecting Columns","text":"<p>The most common selection: grabbing specific columns.</p> <pre><code># Select a single column (returns a Series)\nages = df[\"age\"]\n\n# Select multiple columns (returns a DataFrame)\nsubset = df[[\"name\", \"score\"]]\n</code></pre> <p>Notice the double brackets for multiple columns: <code>df[[\"col1\", \"col2\"]]</code>. This is a common source of errors\u2014remember it!</p>"},{"location":"chapters/03-python-data-structures/#selecting-rows-by-position-iloc","title":"Selecting Rows by Position: iloc","text":"<p>Use iloc (integer location) to select rows by their position number:</p> <pre><code># First row\nfirst_row = df.iloc[0]\n\n# First three rows\nfirst_three = df.iloc[0:3]\n\n# Specific rows\nsome_rows = df.iloc[[0, 2, 5]]\n\n# Rows and columns by position\ncell = df.iloc[0, 1]  # First row, second column\n</code></pre> <p>Think of <code>iloc</code> as \"integer location\"\u2014it works with numbers.</p>"},{"location":"chapters/03-python-data-structures/#selecting-rows-by-label-loc","title":"Selecting Rows by Label: loc","text":"<p>Use loc to select rows by their index labels:</p> <pre><code># If index is default (0, 1, 2...)\nrow = df.loc[0]  # Same as iloc[0]\n\n# If index is custom (names)\nalice_data = df.loc[\"Alice\"]\n\n# Multiple labels\nsome_data = df.loc[[\"Alice\", \"Charlie\"]]\n\n# Rows AND columns by label\nscore = df.loc[\"Alice\", \"score\"]  # Alice's score\n</code></pre> <p>Think of <code>loc</code> as \"label location\"\u2014it works with names.</p>"},{"location":"chapters/03-python-data-structures/#boolean-selection-the-real-superpower","title":"Boolean Selection: The Real Superpower","text":"<p>Here's where it gets amazing. You can select rows based on conditions:</p> <pre><code># All rows where age &gt; 25\nolder_students = df[df[\"age\"] &gt; 25]\n\n# All rows where score &gt;= 90\nhigh_scorers = df[df[\"score\"] &gt;= 90]\n\n# Combine conditions with &amp; (and) or | (or)\nyoung_high_scorers = df[(df[\"age\"] &lt; 25) &amp; (df[\"score\"] &gt;= 90)]\n\n# Select specific cities\nny_and_la = df[df[\"city\"].isin([\"New York\", \"Los Angeles\"])]\n</code></pre> <p>This is incredibly powerful. Need all customers who spent over $100 in the last month? One line. All students who failed the exam? One line. All products with low inventory? One line.</p> Selection Type Syntax Returns Single column <code>df[\"col\"]</code> Series Multiple columns <code>df[[\"col1\", \"col2\"]]</code> DataFrame Rows by position <code>df.iloc[0]</code> or <code>df.iloc[0:5]</code> Series or DataFrame Rows by label <code>df.loc[\"label\"]</code> Series or DataFrame Boolean filter <code>df[df[\"col\"] &gt; value]</code> DataFrame"},{"location":"chapters/03-python-data-structures/#diagram-data-selection-playground-microsim","title":"Diagram: Data Selection Playground MicroSim","text":"Interactive Data Selection Playground <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students practice different selection methods and see results in real-time</p> <p>Canvas layout (850x600px): - Top area (850x200): Full DataFrame display (10 rows x 5 cols) - Bottom left (400x400): Selection builder - Bottom right (450x400): Results display</p> <p>Sample DataFrame: 10 rows with columns: name, age, city, score, active Mix of data types and some interesting patterns to query</p> <p>Selection builder (tabs):</p> <p>Tab 1: \"Column Selection\" - Checkboxes for each column - Shows: df[[\"selected\", \"columns\"]]</p> <p>Tab 2: \"Row Selection (iloc)\" - Start index input - End index input - Shows: df.iloc[start:end]</p> <p>Tab 3: \"Row Selection (loc)\" - Dropdown of index labels - Multi-select for multiple labels - Shows: df.loc[[labels]]</p> <p>Tab 4: \"Boolean Filter\" - Column dropdown - Operator dropdown (&gt;, &lt;, ==, &gt;=, &lt;=, !=) - Value input - \"Add condition\" button (for combining) - Shows: df[df[\"col\"] &gt; value]</p> <p>Results display: - Shows selected data as formatted table - Shows shape of result - Shows code used to generate selection - Highlights selected cells in original DataFrame</p> <p>Interactive controls: - Real-time update as selections change - \"Copy Code\" button - \"Reset\" button - Syntax examples shown as hints</p> <p>Educational features: - Color coding: selected rows/columns highlighted in original - Error messages for invalid selections - Hints: \"Try selecting students with score &gt; 80\"</p> <p>Challenges (optional): - \"Select all students from New York\" - \"Find the youngest student\" - \"Get names of students who passed (score &gt;= 70)\"</p> <p>Implementation: p5.js or React with pandas-like simulation</p>"},{"location":"chapters/03-python-data-structures/#putting-it-all-together-a-complete-workflow","title":"Putting It All Together: A Complete Workflow","text":"<p>Let's walk through a real data exploration workflow, combining everything you've learned:</p> <pre><code>import pandas as pd\n\n# Step 1: Load the data\ndf = pd.read_csv(\"student_data.csv\")\n\n# Step 2: Quick peek\nprint(\"First few rows:\")\nprint(df.head())\n\n# Step 3: Check dimensions\nprint(f\"\\nDataset shape: {df.shape}\")\n\n# Step 4: Get detailed info\nprint(\"\\nDataset info:\")\ndf.info()\n\n# Step 5: Statistical summary\nprint(\"\\nStatistical summary:\")\nprint(df.describe())\n\n# Step 6: Ask questions with selection\n# Who are the high performers?\ntop_students = df[df[\"score\"] &gt;= 90]\nprint(f\"\\nTop students: {len(top_students)}\")\n\n# What's the average score by city?\ncity_scores = df.groupby(\"city\")[\"score\"].mean()\nprint(\"\\nAverage score by city:\")\nprint(city_scores)\n</code></pre> <p>This workflow\u2014load, inspect, explore, select\u2014is the foundation of every data science project. Master it, and you're ready for anything.</p> <p>Achievement Unlocked: Data Wrangler</p> <p>You can now load data, inspect its structure, and select exactly what you need. These skills alone put you ahead of most people who just \"open files in Excel.\" You're thinking like a data scientist!</p>"},{"location":"chapters/03-python-data-structures/#common-patterns-and-pitfalls","title":"Common Patterns and Pitfalls","text":"<p>Before we wrap up, here are patterns you'll use constantly and pitfalls to avoid:</p>"},{"location":"chapters/03-python-data-structures/#patterns-to-remember","title":"Patterns to Remember","text":"<pre><code># Load and immediately inspect\ndf = pd.read_csv(\"data.csv\")\ndf.head()  # or df.info() or df.shape\n\n# Chain methods for quick exploration\ndf.head().describe()\n\n# Check for missing values\ndf.isnull().sum()\n\n# Get unique values in a column\ndf[\"column\"].unique()\ndf[\"column\"].nunique()  # count of unique values\ndf[\"column\"].value_counts()  # frequency of each value\n</code></pre>"},{"location":"chapters/03-python-data-structures/#pitfalls-to-avoid","title":"Pitfalls to Avoid","text":"<pre><code># WRONG: Single brackets for multiple columns\ndf[\"col1\", \"col2\"]  # Error!\n\n# RIGHT: Double brackets\ndf[[\"col1\", \"col2\"]]\n\n# WRONG: Using iloc with labels\ndf.iloc[\"Alice\"]  # Error!\n\n# RIGHT: Use loc for labels\ndf.loc[\"Alice\"]\n\n# WRONG: Forgetting parentheses in boolean logic\ndf[df[\"age\"] &gt; 25 &amp; df[\"score\"] &gt; 80]  # Error!\n\n# RIGHT: Wrap each condition in parentheses\ndf[(df[\"age\"] &gt; 25) &amp; (df[\"score\"] &gt; 80)]\n</code></pre> Chapter 3 Checkpoint: Test Your Understanding <p>Question: You have a DataFrame <code>df</code> with columns: student_id, name, major, gpa, credits. Write code to: 1. Find all students with GPA above 3.5 2. Select only the name and major columns 3. Count how many students are in each major</p> <p>Click to reveal answer:</p> <pre><code># 1. High GPA students\nhigh_gpa = df[df[\"gpa\"] &gt; 3.5]\n\n# 2. Name and major only\nnames_majors = df[[\"name\", \"major\"]]\n\n# 3. Count by major\nmajor_counts = df[\"major\"].value_counts()\n</code></pre>"},{"location":"chapters/03-python-data-structures/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Python's built-in structures (lists, dictionaries, tuples) are the building blocks, but pandas structures are purpose-built for data science.</p> </li> <li> <p>Lists are ordered and mutable\u2014use them when order matters and you need flexibility.</p> </li> <li> <p>Dictionaries provide instant key-based lookup\u2014perfect for mapping relationships.</p> </li> <li> <p>Tuples are immutable\u2014use them for data that shouldn't change.</p> </li> <li> <p>Pandas Series is a 1D labeled array\u2014like a list with an index attached.</p> </li> <li> <p>Pandas DataFrame is a 2D labeled table\u2014the workhorse of data science.</p> </li> <li> <p>CSV files are the universal data exchange format\u2014<code>pd.read_csv()</code> is your friend.</p> </li> <li> <p>Inspection methods (<code>head()</code>, <code>tail()</code>, <code>shape</code>, <code>info()</code>, <code>describe()</code>) help you understand your data before analyzing it.</p> </li> <li> <p>Data selection with <code>loc</code>, <code>iloc</code>, and boolean indexing lets you extract exactly what you need.</p> </li> <li> <p>The load \u2192 inspect \u2192 select \u2192 analyze workflow is the foundation of every data science project.</p> </li> </ol> <p>You now have the containers for your data superpower. In the next chapter, you'll learn to clean and prepare that data\u2014because real-world data is messy, and heroes clean up messes. Onward!</p>"},{"location":"chapters/04-data-cleaning/","title":"Data Cleaning and Preprocessing","text":""},{"location":"chapters/04-data-cleaning/#summary","title":"Summary","text":"<p>This chapter covers the critical skills of preparing raw data for analysis. Students will learn to identify and handle missing values, detect and remove duplicates, identify outliers, and validate data quality. The chapter also covers data transformation techniques including filtering, type conversion, and feature scaling. By the end of this chapter, students will be able to clean messy real-world datasets and prepare them for visualization and modeling.</p>"},{"location":"chapters/04-data-cleaning/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Missing Values</li> <li>NaN</li> <li>Null Detection</li> <li>Dropna Method</li> <li>Fillna Method</li> <li>Imputation</li> <li>Data Type Conversion</li> <li>Duplicate Detection</li> <li>Duplicate Removal</li> <li>Outliers</li> <li>Outlier Detection</li> <li>Data Validation</li> <li>String Cleaning</li> <li>Column Renaming</li> <li>Data Filtering</li> <li>Boolean Indexing</li> <li>Query Method</li> <li>Data Transformation</li> <li>Feature Scaling</li> <li>Normalization</li> </ol>"},{"location":"chapters/04-data-cleaning/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Python Data Structures</li> </ul>"},{"location":"chapters/04-data-cleaning/#the-art-of-cleaning-up-messes","title":"The Art of Cleaning Up Messes","text":"<p>In the last chapter, you learned to load data and explore it like a detective. You probably felt pretty good about yourself\u2014data was flowing, <code>head()</code> was working, and everything seemed under control. Then reality hit: the data was messy.</p> <p>Welcome to the real world of data science.</p> <p>Real-world data is never clean. It's full of gaps where values should be, duplicates that snuck in somehow, numbers that shouldn't exist (like someone being -5 years old), and formatting disasters that would make an English teacher weep. This isn't a bug in the data pipeline\u2014it's just how data works in the wild.</p> <p>Here's the thing: messy data will destroy your analysis. Train a machine learning model on dirty data? It learns the wrong patterns. Calculate averages with missing values? Your numbers lie. Build a report with duplicates? Everything is inflated. Garbage in, garbage out\u2014that's the first law of data science.</p> <p>But here's the good news: cleaning data is a superpower in itself. Most people don't know how to do it well. Master this chapter, and you'll be the hero who transforms chaotic datasets into pristine, analysis-ready gold. Let's get scrubbing!</p>"},{"location":"chapters/04-data-cleaning/#diagram-data-cleaning-pipeline-overview","title":"Diagram: Data Cleaning Pipeline Overview","text":"Data Cleaning Pipeline Overview <p>Type: workflow</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students visualize the complete data cleaning workflow from raw data to analysis-ready data</p> <p>Purpose: Show the sequential steps in a typical data cleaning process</p> <p>Visual style: Horizontal flowchart with icons for each stage</p> <p>Steps (left to right):</p> <ol> <li> <p>RAW DATA    Icon: Messy document with question marks    Color: Red    Hover text: \"Data as received - full of problems\"</p> </li> <li> <p>MISSING VALUES    Icon: Grid with empty cells highlighted    Color: Orange    Hover text: \"Identify and handle NaN, None, empty strings\"</p> </li> <li> <p>DUPLICATES    Icon: Two identical rows with X on one    Color: Yellow    Hover text: \"Find and remove duplicate records\"</p> </li> <li> <p>OUTLIERS    Icon: Box plot with point far outside    Color: Yellow-green    Hover text: \"Detect and decide how to handle extreme values\"</p> </li> <li> <p>DATA TYPES    Icon: Type conversion symbol (A\u21921)    Color: Green    Hover text: \"Convert columns to appropriate types\"</p> </li> <li> <p>VALIDATION    Icon: Checkmark in shield    Color: Blue    Hover text: \"Verify data meets business rules\"</p> </li> <li> <p>TRANSFORMATION    Icon: Gear with arrows    Color: Purple    Hover text: \"Scale, normalize, and prepare for analysis\"</p> </li> <li> <p>CLEAN DATA    Icon: Sparkly document with checkmark    Color: Gold    Hover text: \"Analysis-ready dataset!\"</p> </li> </ol> <p>Annotations below pipeline: - \"Each step catches different problems\" - \"Order matters: missing values before duplicates\" - \"Always document your cleaning decisions\"</p> <p>Error feedback loops: - Dashed arrows from steps 2-6 back to \"Log Issues\" box - \"Log Issues\" connects to \"Data Quality Report\"</p> <p>Implementation: SVG with CSS hover effects</p>"},{"location":"chapters/04-data-cleaning/#missing-values-the-silent-killers","title":"Missing Values: The Silent Killers","text":"<p>Missing values are the most common data quality problem you'll encounter. They're sneaky\u2014sometimes they look like empty cells, sometimes they're the word \"NULL,\" and sometimes they're special numeric codes like -999.</p>"},{"location":"chapters/04-data-cleaning/#understanding-nan-and-null","title":"Understanding NaN and Null","text":"<p>In pandas, missing values are represented as NaN (Not a Number). This is a special value from the NumPy library that indicates \"something should be here, but isn't.\"</p> <pre><code>import pandas as pd\nimport numpy as np\n\n# Creating data with missing values\ndf = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", None, \"Diana\"],\n    \"age\": [25, np.nan, 22, 28],\n    \"score\": [85, 92, 78, np.nan]\n})\nprint(df)\n</code></pre> <p>Output: <pre><code>    name   age  score\n0  Alice  25.0   85.0\n1    Bob   NaN   92.0\n2   None  22.0   78.0\n3  Diana  28.0    NaN\n</code></pre></p> <p>Notice how <code>None</code> and <code>np.nan</code> both represent missing data, but they're handled slightly differently. Pandas is smart enough to recognize both as missing values.</p> Missing Value Type Where You'll See It Pandas Representation Empty cell CSV files, Excel NaN None Python code, JSON NaN (converted) NULL Databases NaN (converted) Empty string (\"\") Text files Not automatically NaN! Special codes (-999, 9999) Legacy systems Not automatically NaN! <p>Hidden Missing Values</p> <p>Be careful! Empty strings and special codes like -999 are NOT automatically treated as missing. You need to identify and convert them manually. Always inspect your data after loading.</p>"},{"location":"chapters/04-data-cleaning/#null-detection-finding-the-gaps","title":"Null Detection: Finding the Gaps","text":"<p>Null detection is the process of finding where your missing values hide. Pandas provides several methods for this:</p> <pre><code># Check for missing values - returns True/False for each cell\ndf.isnull()\n\n# Count missing values in each column\ndf.isnull().sum()\n\n# Percentage of missing values\n(df.isnull().sum() / len(df)) * 100\n\n# Find rows with ANY missing values\ndf[df.isnull().any(axis=1)]\n\n# Find rows with missing values in a specific column\ndf[df[\"age\"].isnull()]\n</code></pre> <p>The <code>isnull()</code> method is your first line of defense. Run it immediately after loading any dataset\u2014it tells you exactly where the problems are.</p> <pre><code># Real-world pattern: Quick missing value assessment\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\nprint(f\"\\nTotal missing: {df.isnull().sum().sum()}\")\nprint(f\"Percentage missing: {df.isnull().sum().sum() / df.size * 100:.1f}%\")\n</code></pre>"},{"location":"chapters/04-data-cleaning/#diagram-missing-value-detection-microsim","title":"Diagram: Missing Value Detection MicroSim","text":"Missing Value Detective MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Help students practice identifying and counting missing values in different scenarios</p> <p>Canvas layout (750x500px): - Top (750x200): DataFrame display with highlighted missing values - Bottom left (350x300): Detection code panel - Bottom right (400x300): Results and quiz area</p> <p>Visual elements: - DataFrame grid showing 8 rows \u00d7 5 columns - Missing values highlighted in red when detected - Color legend showing: Present (green), NaN (red), None (orange), Empty string (yellow) - Running count of missing values by column</p> <p>Sample data scenarios (rotate through): 1. Simple NaN values only 2. Mix of NaN and None 3. Hidden missing values (empty strings, -999) 4. Missing values with pattern (all in one column) 5. Sparse data (&gt;50% missing)</p> <p>Interactive controls: - Button: \"Show isnull()\" - highlights all missing - Button: \"Count by Column\" - shows bar chart of missing counts - Button: \"Find Hidden Missing\" - identifies non-standard missing values - Dropdown: \"Select Scenario\" - changes dataset - Quiz mode: \"How many missing in column X?\" with input field</p> <p>Behavior: - Clicking detection buttons animates the detection process - Correct quiz answers earn points and unlock harder scenarios - Hints available after wrong answers - Progress tracker shows scenarios completed</p> <p>Visual style: Data detective theme with magnifying glass cursor</p> <p>Implementation: p5.js with animated highlighting</p>"},{"location":"chapters/04-data-cleaning/#handling-missing-values-your-three-options","title":"Handling Missing Values: Your Three Options","text":"<p>Once you've found missing values, you have three main strategies:</p> <ol> <li>Drop them - Remove rows or columns with missing data</li> <li>Fill them - Replace missing values with something reasonable</li> <li>Leave them - Some algorithms can handle missing values directly</li> </ol>"},{"location":"chapters/04-data-cleaning/#the-dropna-method-clean-sweep","title":"The dropna Method: Clean Sweep","text":"<p>The dropna method removes rows or columns containing missing values:</p> <pre><code># Drop any row with missing values\ndf_clean = df.dropna()\n\n# Drop rows only if ALL values are missing\ndf_clean = df.dropna(how=\"all\")\n\n# Drop rows with missing values in specific columns\ndf_clean = df.dropna(subset=[\"name\", \"age\"])\n\n# Drop columns (instead of rows) with missing values\ndf_clean = df.dropna(axis=1)\n</code></pre> <p>When to use <code>dropna()</code>:</p> <ul> <li>Missing values are rare (less than 5% of data)</li> <li>Rows with missing data are truly unusable</li> <li>You have plenty of data to spare</li> <li>Missing values are random (not systematic)</li> </ul> <p>When NOT to use <code>dropna()</code>:</p> <ul> <li>It would remove too much data</li> <li>Missing values follow a pattern (systematic missingness)</li> <li>The columns with missing data are important</li> <li>You suspect the missing data contains signal</li> </ul>"},{"location":"chapters/04-data-cleaning/#the-fillna-method-filling-the-gaps","title":"The fillna Method: Filling the Gaps","text":"<p>The fillna method replaces missing values with specified values:</p> <pre><code># Fill with a constant value\ndf[\"age\"].fillna(0)\n\n# Fill with the mean of the column\ndf[\"age\"].fillna(df[\"age\"].mean())\n\n# Fill with the median (more robust to outliers)\ndf[\"age\"].fillna(df[\"age\"].median())\n\n# Fill with the most common value (mode) for categorical data\ndf[\"city\"].fillna(df[\"city\"].mode()[0])\n\n# Fill with the previous value (forward fill) - great for time series\ndf[\"temperature\"].fillna(method=\"ffill\")\n\n# Fill with the next value (backward fill)\ndf[\"temperature\"].fillna(method=\"bfill\")\n</code></pre> Fill Strategy Best For Example Mean Normally distributed numeric data Age, height Median Skewed numeric data, outliers present Income, prices Mode Categorical data City, category Forward fill Time series data Stock prices, temperatures Constant (0) When zero has meaning Transaction amounts Constant (\"Unknown\") Categorical placeholders Status fields"},{"location":"chapters/04-data-cleaning/#imputation-the-smart-approach","title":"Imputation: The Smart Approach","text":"<p>Imputation is the fancy term for filling missing values based on patterns in the data. It's more sophisticated than simple mean/median filling.</p> <pre><code># Simple imputation: use column statistics\nfrom sklearn.impute import SimpleImputer\n\n# Create imputer that uses mean for numeric columns\nimputer = SimpleImputer(strategy=\"mean\")\ndf_imputed = pd.DataFrame(\n    imputer.fit_transform(df[[\"age\", \"score\"]]),\n    columns=[\"age\", \"score\"]\n)\n</code></pre> <p>Advanced imputation strategies include:</p> <ul> <li>K-Nearest Neighbors (KNN): Fill based on similar rows</li> <li>Regression imputation: Predict missing values from other columns</li> <li>Multiple imputation: Create multiple filled versions and combine</li> </ul> <p>The Golden Rule of Missing Data</p> <p>Always ask: WHY is the data missing? Random missing values can often be filled. But if data is missing for a reason (like patients who missed follow-up because they got worse), filling it can introduce bias. When in doubt, document your decision and consider sensitivity analysis.</p>"},{"location":"chapters/04-data-cleaning/#duplicate-detection-and-removal","title":"Duplicate Detection and Removal","text":"<p>Duplicates are copies of data that shouldn't exist. They inflate your counts, skew your statistics, and make your analyses unreliable. They sneak in through:</p> <ul> <li>Data entry mistakes</li> <li>Multiple data imports</li> <li>System glitches</li> <li>Merging datasets incorrectly</li> </ul>"},{"location":"chapters/04-data-cleaning/#duplicate-detection-finding-the-clones","title":"Duplicate Detection: Finding the Clones","text":"<p>Duplicate detection identifies rows that appear more than once:</p> <pre><code># Check for exact duplicate rows\ndf.duplicated()  # Returns True/False for each row\n\n# Count total duplicates\ndf.duplicated().sum()\n\n# View the duplicate rows\ndf[df.duplicated()]\n\n# View all copies (including the first occurrence)\ndf[df.duplicated(keep=False)]\n\n# Check duplicates based on specific columns only\ndf.duplicated(subset=[\"name\", \"email\"])\n</code></pre> <p>The <code>duplicated()</code> method marks duplicates starting from the second occurrence. Use <code>keep=False</code> to see ALL copies, including the first one.</p> <pre><code># Example: Finding partial duplicates\n# Maybe the same person appears with slightly different data\ndf_people = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Alice\", \"Charlie\"],\n    \"email\": [\"a@mail.com\", \"b@mail.com\", \"a@mail.com\", \"c@mail.com\"],\n    \"age\": [25, 30, 26, 28]  # Alice's age changed!\n})\n\n# Find duplicates by name and email\nprint(df_people[df_people.duplicated(subset=[\"name\", \"email\"], keep=False)])\n</code></pre>"},{"location":"chapters/04-data-cleaning/#duplicate-removal-eliminating-the-clones","title":"Duplicate Removal: Eliminating the Clones","text":"<p>Duplicate removal keeps only unique rows:</p> <pre><code># Remove exact duplicates (keep first occurrence)\ndf_unique = df.drop_duplicates()\n\n# Keep the last occurrence instead\ndf_unique = df.drop_duplicates(keep=\"last\")\n\n# Remove duplicates based on specific columns\ndf_unique = df.drop_duplicates(subset=[\"email\"])\n\n# Remove ALL copies of duplicated rows (nuclear option)\ndf_unique = df.drop_duplicates(keep=False)\n</code></pre> <p>When removing duplicates, ask yourself:</p> <ul> <li>Should you keep the first or last occurrence?</li> <li>Are you sure the duplicates are truly identical?</li> <li>Should you merge duplicate information instead of just dropping?</li> </ul>"},{"location":"chapters/04-data-cleaning/#diagram-duplicate-handling-decision-tree","title":"Diagram: Duplicate Handling Decision Tree","text":"Duplicate Handling Decision Tree <p>Type: diagram</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help students decide the appropriate strategy for handling different types of duplicates</p> <p>Purpose: Guide decision-making process for duplicate handling</p> <p>Visual style: Decision tree flowchart</p> <p>Start: \"Duplicates Detected\"</p> <p>Decision 1: \"Are rows EXACTLY identical?\" - Yes \u2192 \"Safe to drop_duplicates()\" - No \u2192 Decision 2</p> <p>Decision 2: \"Which columns make rows 'the same'?\" - Identify key columns (ID, email, etc.) - \u2192 Decision 3</p> <p>Decision 3: \"Do non-key columns differ?\" - No \u2192 \"drop_duplicates(subset=[key_cols])\" - Yes \u2192 Decision 4</p> <p>Decision 4: \"Which version is correct?\" - First occurrence \u2192 \"keep='first'\" - Last occurrence \u2192 \"keep='last'\" - Need to merge \u2192 \"Use groupby().agg()\" - Can't determine \u2192 \"Flag for manual review\"</p> <p>Special case branch: \"Is duplication intentional?\" - Yes (e.g., same product bought twice) \u2192 \"Don't remove!\" - No \u2192 Continue with removal</p> <p>Color coding: - Green: Safe actions - Yellow: Need investigation - Red: Be careful - Blue: Decision points</p> <p>Annotations: - \"Always examine duplicates before removing\" - \"Document which strategy you used and why\"</p> <p>Implementation: SVG decision tree with interactive highlights</p>"},{"location":"chapters/04-data-cleaning/#outliers-the-extreme-values","title":"Outliers: The Extreme Values","text":"<p>Outliers are data points that are unusually far from other observations. They might be:</p> <ul> <li>Legitimate extremes: A billionaire in income data\u2014rare but real</li> <li>Data errors: Someone's age recorded as 999</li> <li>Measurement errors: A sensor glitch recording impossible values</li> </ul> <p>Outliers matter because they can dramatically affect your statistics. One outlier can shift your mean, expand your standard deviation, and confuse your machine learning models.</p>"},{"location":"chapters/04-data-cleaning/#outlier-detection-finding-the-extremes","title":"Outlier Detection: Finding the Extremes","text":"<p>Outlier detection identifies values that don't fit the normal pattern. Common methods include:</p> <p>1. Visual inspection (always start here!): <pre><code>import matplotlib.pyplot as plt\n\n# Box plot - shows outliers as dots beyond the whiskers\ndf[\"score\"].plot(kind=\"box\")\nplt.show()\n\n# Histogram - outliers appear as isolated bars\ndf[\"score\"].hist(bins=30)\nplt.show()\n</code></pre></p> <p>2. Statistical methods: <pre><code># Z-score method: outliers are &gt;3 standard deviations from mean\nfrom scipy import stats\nz_scores = stats.zscore(df[\"score\"].dropna())\noutliers = df[(abs(z_scores) &gt; 3)]\n\n# IQR method: outliers are beyond 1.5 \u00d7 IQR from quartiles\nQ1 = df[\"score\"].quantile(0.25)\nQ3 = df[\"score\"].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = df[(df[\"score\"] &lt; lower_bound) | (df[\"score\"] &gt; upper_bound)]\n</code></pre></p> Method How It Works Best For Z-score Distance from mean in standard deviations Normal distributions IQR Distance from quartiles Any distribution, robust to extremes Domain rules Based on what's possible When you know valid ranges <p>3. Domain knowledge rules: <pre><code># Age can't be negative or over 120\ninvalid_ages = df[(df[\"age\"] &lt; 0) | (df[\"age\"] &gt; 120)]\n\n# Test scores must be 0-100\ninvalid_scores = df[(df[\"score\"] &lt; 0) | (df[\"score\"] &gt; 100)]\n</code></pre></p>"},{"location":"chapters/04-data-cleaning/#diagram-outlier-detection-methods-microsim","title":"Diagram: Outlier Detection Methods MicroSim","text":"Outlier Detection Playground <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students experiment with different outlier detection methods and see how they identify different points</p> <p>Canvas layout (800x550px): - Left (550x550): Scatter plot / histogram visualization - Right (250x550): Controls and detected outliers list</p> <p>Visual elements: - Data points displayed as circles - Normal points in blue - Detected outliers highlighted in red with labels - Detection threshold lines/zones shown - Summary statistics displayed</p> <p>Sample datasets (toggle between): 1. Normal distribution with obvious outliers 2. Skewed distribution (income-like) 3. Multi-modal distribution 4. Real-world messy data with errors</p> <p>Interactive controls: - Radio buttons: Detection method (Z-score, IQR, Custom range) - Slider: Z-score threshold (1.5 to 4.0) - Slider: IQR multiplier (1.0 to 3.0) - Number inputs: Custom min/max values - Dropdown: Dataset selector - Toggle: Show/hide threshold lines</p> <p>Display panels: - Count of outliers detected - Outlier values and indices - Percentage of data flagged - Before/after mean comparison</p> <p>Behavior: - Adjusting thresholds immediately updates highlighting - Hovering over points shows their values - Clicking outliers adds them to \"investigate\" list - Show how different methods catch different outliers</p> <p>Educational annotations: - \"Z-score catches 3 outliers, IQR catches 5 - which is right?\" - \"Lower threshold = more outliers flagged\" - \"Some 'outliers' might be valid data!\"</p> <p>Visual style: Clean statistical visualization with gridlines</p> <p>Implementation: p5.js with dynamic data visualization</p>"},{"location":"chapters/04-data-cleaning/#handling-outliers-what-to-do","title":"Handling Outliers: What To Do","text":"<p>Once you've identified outliers, you have options:</p> <pre><code># Option 1: Remove them\ndf_clean = df[(df[\"score\"] &gt;= lower_bound) &amp; (df[\"score\"] &lt;= upper_bound)]\n\n# Option 2: Cap them (winsorization)\ndf[\"score_capped\"] = df[\"score\"].clip(lower=lower_bound, upper=upper_bound)\n\n# Option 3: Replace with NaN (treat as missing)\ndf.loc[(df[\"score\"] &lt; lower_bound) | (df[\"score\"] &gt; upper_bound), \"score\"] = np.nan\n\n# Option 4: Log transform (reduces impact of extremes)\ndf[\"score_log\"] = np.log1p(df[\"score\"])\n\n# Option 5: Keep them! (if they're legitimate)\n</code></pre> <p>Don't Just Delete Outliers!</p> <p>Before removing outliers, ask: Are they errors or legitimate extremes? Removing real data points because they're \"inconvenient\" is bad science. Document every outlier decision.</p>"},{"location":"chapters/04-data-cleaning/#data-validation-enforcing-the-rules","title":"Data Validation: Enforcing the Rules","text":"<p>Data validation checks that data meets expected criteria. It's the quality control checkpoint that catches problems before they contaminate your analysis.</p> <p>Common validation checks:</p> <pre><code># Check data types are correct\nassert df[\"age\"].dtype in [\"int64\", \"float64\"], \"Age should be numeric\"\n\n# Check value ranges\nassert df[\"age\"].between(0, 120).all(), \"Invalid ages found\"\n\n# Check for required values\nassert df[\"customer_id\"].notna().all(), \"Missing customer IDs\"\n\n# Check uniqueness\nassert df[\"email\"].is_unique, \"Duplicate emails found\"\n\n# Check referential integrity\nvalid_categories = [\"A\", \"B\", \"C\", \"D\"]\nassert df[\"category\"].isin(valid_categories).all(), \"Invalid categories\"\n\n# Check date logic\nassert (df[\"end_date\"] &gt;= df[\"start_date\"]).all(), \"End before start\"\n</code></pre> <p>Building a validation function:</p> <pre><code>def validate_student_data(df):\n    \"\"\"Validate student dataset and return issues found.\"\"\"\n    issues = []\n\n    # Check required columns exist\n    required = [\"student_id\", \"name\", \"age\", \"gpa\"]\n    missing_cols = set(required) - set(df.columns)\n    if missing_cols:\n        issues.append(f\"Missing columns: {missing_cols}\")\n\n    # Check data types\n    if df[\"age\"].dtype not in [\"int64\", \"float64\"]:\n        issues.append(\"Age is not numeric\")\n\n    # Check value ranges\n    if (df[\"age\"] &lt; 0).any() or (df[\"age\"] &gt; 100).any():\n        issues.append(f\"Invalid ages: {df[(df['age'] &lt; 0) | (df['age'] &gt; 100)]['age'].tolist()}\")\n\n    if (df[\"gpa\"] &lt; 0).any() or (df[\"gpa\"] &gt; 4.0).any():\n        issues.append(\"GPA out of range [0, 4.0]\")\n\n    # Check for duplicates\n    if df[\"student_id\"].duplicated().any():\n        issues.append(f\"Duplicate student_ids: {df[df['student_id'].duplicated()]['student_id'].tolist()}\")\n\n    if issues:\n        print(\"\u26a0\ufe0f Validation failed:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n        return False\n    else:\n        print(\"\u2705 All validations passed!\")\n        return True\n\n# Use it\nvalidate_student_data(df)\n</code></pre>"},{"location":"chapters/04-data-cleaning/#data-transformation-techniques","title":"Data Transformation Techniques","text":"<p>Now that your data is clean, it's time to transform it into the shape your analysis needs. Data transformation includes filtering, selecting, renaming, type conversion, and more.</p>"},{"location":"chapters/04-data-cleaning/#data-filtering-selecting-subsets","title":"Data Filtering: Selecting Subsets","text":"<p>Data filtering extracts rows that meet specific criteria. You learned the basics in Chapter 3\u2014now let's master advanced techniques.</p> <pre><code># Basic filtering review\nhigh_scorers = df[df[\"score\"] &gt; 90]\n\n# Multiple conditions (AND)\nyoung_high_scorers = df[(df[\"age\"] &lt; 25) &amp; (df[\"score\"] &gt; 90)]\n\n# Multiple conditions (OR)\nspecial_cases = df[(df[\"score\"] &gt; 95) | (df[\"age\"] &gt; 50)]\n\n# NOT condition\nnot_from_ny = df[~(df[\"city\"] == \"New York\")]\n</code></pre>"},{"location":"chapters/04-data-cleaning/#boolean-indexing-the-power-tool","title":"Boolean Indexing: The Power Tool","text":"<p>Boolean indexing is the technique behind filtering\u2014using True/False arrays to select data:</p> <pre><code># Create a boolean mask\nmask = df[\"score\"] &gt; 80\nprint(mask)  # Shows True/False for each row\n\n# Use the mask to filter\nhigh_scorers = df[mask]\n\n# Combine masks\nmask1 = df[\"score\"] &gt; 80\nmask2 = df[\"age\"] &lt; 30\ncombined = df[mask1 &amp; mask2]  # Both conditions\neither = df[mask1 | mask2]    # Either condition\n\n# Invert a mask\nlow_scorers = df[~mask1]  # NOT high scorers\n</code></pre> <p>Understanding boolean indexing makes complex filtering intuitive. You can build masks step-by-step, test them separately, and combine them logically.</p>"},{"location":"chapters/04-data-cleaning/#the-query-method-sql-like-filtering","title":"The Query Method: SQL-Like Filtering","text":"<p>The query method offers a cleaner syntax for complex filters:</p> <pre><code># Instead of this:\nresult = df[(df[\"age\"] &gt; 25) &amp; (df[\"city\"] == \"New York\") &amp; (df[\"score\"] &gt;= 80)]\n\n# You can write this:\nresult = df.query(\"age &gt; 25 and city == 'New York' and score &gt;= 80\")\n\n# Using variables in queries\nmin_age = 25\ntarget_city = \"New York\"\nresult = df.query(\"age &gt; @min_age and city == @target_city\")\n\n# Complex conditions read more naturally\nresult = df.query(\"(score &gt; 90) or (age &lt; 20 and score &gt; 80)\")\n</code></pre> Approach Syntax Best For Boolean indexing <code>df[df[\"col\"] &gt; val]</code> Simple conditions, programmatic use Query method <code>df.query(\"col &gt; val\")</code> Complex conditions, readability"},{"location":"chapters/04-data-cleaning/#string-cleaning-taming-text-data","title":"String Cleaning: Taming Text Data","text":"<p>String cleaning standardizes text data that comes in many messy forms:</p> <pre><code># Convert to lowercase\ndf[\"name\"] = df[\"name\"].str.lower()\n\n# Remove leading/trailing whitespace\ndf[\"city\"] = df[\"city\"].str.strip()\n\n# Replace characters\ndf[\"phone\"] = df[\"phone\"].str.replace(\"-\", \"\")\n\n# Extract patterns with regex\ndf[\"zipcode\"] = df[\"address\"].str.extract(r\"(\\d{5})\")\n\n# Check for patterns\ndf[\"has_email\"] = df[\"contact\"].str.contains(\"@\")\n</code></pre> <p>Common string cleaning operations:</p> <pre><code># All-in-one cleaning function\ndef clean_text(text):\n    if pd.isna(text):\n        return text\n    return (text\n            .strip()                      # Remove whitespace\n            .lower()                       # Lowercase\n            .replace(\"  \", \" \")           # Fix double spaces\n           )\n\ndf[\"name\"] = df[\"name\"].apply(clean_text)\n</code></pre>"},{"location":"chapters/04-data-cleaning/#column-renaming-clear-names-matter","title":"Column Renaming: Clear Names Matter","text":"<p>Column renaming makes your data self-documenting:</p> <pre><code># Rename specific columns\ndf = df.rename(columns={\n    \"cust_id\": \"customer_id\",\n    \"amt\": \"amount\",\n    \"dt\": \"transaction_date\"\n})\n\n# Rename all columns at once\ndf.columns = [\"id\", \"name\", \"age\", \"score\"]\n\n# Apply a function to all column names\ndf.columns = df.columns.str.lower()\ndf.columns = df.columns.str.replace(\" \", \"_\")\n\n# Chain operations\ndf = df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\"))\n</code></pre> <p>Good column names are:</p> <ul> <li>Lowercase with underscores (snake_case)</li> <li>Descriptive but not too long</li> <li>Free of spaces and special characters</li> <li>Consistent across your project</li> </ul>"},{"location":"chapters/04-data-cleaning/#data-type-conversion-getting-types-right","title":"Data Type Conversion: Getting Types Right","text":"<p>Data type conversion ensures columns have appropriate types for analysis:</p> <pre><code># Check current types\nprint(df.dtypes)\n\n# Convert to numeric\ndf[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")  # Invalid \u2192 NaN\n\n# Convert to string\ndf[\"zipcode\"] = df[\"zipcode\"].astype(str)\n\n# Convert to datetime\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Convert to categorical (saves memory, enables ordering)\ndf[\"grade\"] = pd.Categorical(df[\"grade\"], categories=[\"F\", \"D\", \"C\", \"B\", \"A\"], ordered=True)\n\n# Convert to integer (handling NaN)\ndf[\"age\"] = df[\"age\"].astype(\"Int64\")  # Nullable integer type\n</code></pre> Original Type Convert To Why String numbers <code>float64</code> or <code>int64</code> Enable math operations Dates as strings <code>datetime64</code> Enable date arithmetic Numeric codes <code>category</code> Save memory, show meaning Float IDs <code>Int64</code> Preserve as integers with NaN support"},{"location":"chapters/04-data-cleaning/#diagram-data-type-conversion-guide-infographic","title":"Diagram: Data Type Conversion Guide Infographic","text":"Data Type Conversion Reference <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Quick reference for common data type conversions and when to use them</p> <p>Purpose: Visual guide for choosing the right data type conversion method</p> <p>Layout: Two-column reference card style</p> <p>Section 1: \"Converting TO Numeric\" - <code>pd.to_numeric(col)</code> - Basic conversion - <code>pd.to_numeric(col, errors='coerce')</code> - Invalid \u2192 NaN - <code>col.astype(float)</code> - When you're sure it's clean - Visual: String \"42\" \u2192 Integer 42</p> <p>Section 2: \"Converting TO Datetime\" - <code>pd.to_datetime(col)</code> - Smart parsing - <code>pd.to_datetime(col, format='%Y-%m-%d')</code> - Specific format - Visual: String \"2024-03-15\" \u2192 Datetime object</p> <p>Section 3: \"Converting TO Categorical\" - <code>col.astype('category')</code> - Basic categorical - <code>pd.Categorical(col, categories=[...], ordered=True)</code> - Ordered - Visual: Repeating strings \u2192 Category codes</p> <p>Section 4: \"Converting TO String\" - <code>col.astype(str)</code> - Simple conversion - <code>col.map('{:.2f}'.format)</code> - With formatting - Visual: Number 3.14159 \u2192 String \"3.14\"</p> <p>Common pitfalls callout box: - \"Integer columns with NaN need Int64 (capital I)\" - \"datetime parsing can be slow on large datasets\" - \"Category type saves memory but changes behavior\"</p> <p>Color coding: - Blue: Numeric conversions - Green: Date conversions - Purple: Categorical conversions - Orange: String conversions</p> <p>Interactive elements: - Hover over conversion methods to see code examples - Click to copy code snippet</p> <p>Implementation: HTML/CSS with JavaScript tooltips</p>"},{"location":"chapters/04-data-cleaning/#feature-scaling-and-normalization","title":"Feature Scaling and Normalization","text":"<p>When you're preparing data for machine learning, feature scaling becomes critical. Different features might have vastly different ranges\u2014age might be 18-80 while income might be 20,000-500,000. Without scaling, the larger numbers dominate the analysis.</p>"},{"location":"chapters/04-data-cleaning/#feature-scaling-bringing-features-to-same-scale","title":"Feature Scaling: Bringing Features to Same Scale","text":"<p>Feature scaling transforms features to comparable ranges:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Min-Max scaling: transforms to [0, 1] range\nscaler = MinMaxScaler()\ndf[[\"age_scaled\", \"income_scaled\"]] = scaler.fit_transform(df[[\"age\", \"income\"]])\n\n# Standard scaling: transforms to mean=0, std=1\nscaler = StandardScaler()\ndf[[\"age_standard\", \"income_standard\"]] = scaler.fit_transform(df[[\"age\", \"income\"]])\n</code></pre>"},{"location":"chapters/04-data-cleaning/#normalization-statistical-standardization","title":"Normalization: Statistical Standardization","text":"<p>Normalization typically refers to scaling to unit norm or standard distribution:</p> <pre><code># Z-score normalization (same as StandardScaler)\ndf[\"score_normalized\"] = (df[\"score\"] - df[\"score\"].mean()) / df[\"score\"].std()\n\n# Min-max normalization (manual)\ndf[\"score_minmax\"] = (df[\"score\"] - df[\"score\"].min()) / (df[\"score\"].max() - df[\"score\"].min())\n\n# Robust scaling (uses median/IQR, resistant to outliers)\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ndf[\"income_robust\"] = scaler.fit_transform(df[[\"income\"]])\n</code></pre> Scaling Method Formula When to Use Min-Max \\((x - min) / (max - min)\\) Neural networks, bounded algorithms Z-score/Standard \\((x - \\mu) / \\sigma\\) Most ML algorithms, normally distributed data Robust \\((x - median) / IQR\\) Data with outliers Log transform \\(\\log(x + 1)\\) Right-skewed data (income, counts) <p>When to Scale</p> <p>Scale features when using distance-based algorithms (KNN, SVM, K-means) or gradient descent (neural networks). Tree-based algorithms (Random Forest, XGBoost) usually don't need scaling.</p>"},{"location":"chapters/04-data-cleaning/#diagram-feature-scaling-comparison-microsim","title":"Diagram: Feature Scaling Comparison MicroSim","text":"Feature Scaling Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students visualize how different scaling methods transform data distributions</p> <p>Canvas layout (800x500px): - Top (800x200): Original data distribution histogram - Bottom left (400x300): Scaled distribution histogram - Bottom right (400x300): Controls and comparison stats</p> <p>Visual elements: - Original data histogram with descriptive statistics - Scaled data histogram (updates with scaling method) - Before/after comparison statistics table - Visual axis showing value ranges</p> <p>Sample datasets: - Normal distribution (symmetric) - Right-skewed (income-like) - With outliers - Bimodal distribution</p> <p>Interactive controls: - Radio buttons: Scaling method   - None (original)   - Min-Max [0,1]   - Standard (Z-score)   - Robust (median/IQR)   - Log transform - Dropdown: Dataset selector - Checkbox: Show outliers highlighted - Checkbox: Show before/after overlay</p> <p>Comparison statistics displayed: - Min, Max, Range - Mean, Median - Std Dev, IQR - Visual indicator of how outliers are affected</p> <p>Behavior: - Switching scaling method animates the transformation - Hover over bars to see exact values - Toggle overlay to see original vs scaled superimposed - Outliers maintain highlighting through transformation</p> <p>Educational annotations: - \"Notice: Min-Max squishes outliers to 0 or 1\" - \"Standard scaling keeps outliers as extreme z-scores\" - \"Robust scaling ignores outliers!\" - \"Log transform pulls in right tail\"</p> <p>Visual style: Statistical visualization with clean grid</p> <p>Implementation: p5.js with real-time distribution updates</p>"},{"location":"chapters/04-data-cleaning/#complete-data-cleaning-workflow","title":"Complete Data Cleaning Workflow","text":"<p>Let's put it all together with a complete data cleaning workflow:</p> <pre><code>import pandas as pd\nimport numpy as np\n\ndef clean_dataset(df, config=None):\n    \"\"\"\n    Complete data cleaning pipeline.\n\n    Parameters:\n    -----------\n    df : DataFrame\n        Raw data to clean\n    config : dict, optional\n        Cleaning configuration options\n\n    Returns:\n    --------\n    DataFrame : Cleaned data\n    dict : Cleaning report\n    \"\"\"\n    report = {\"original_shape\": df.shape, \"issues_found\": [], \"actions_taken\": []}\n\n    # Step 1: Make a copy\n    df_clean = df.copy()\n\n    # Step 2: Handle missing values\n    missing_counts = df_clean.isnull().sum()\n    cols_with_missing = missing_counts[missing_counts &gt; 0]\n    if len(cols_with_missing) &gt; 0:\n        report[\"issues_found\"].append(f\"Missing values in {len(cols_with_missing)} columns\")\n\n        # Fill numeric columns with median\n        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n        for col in numeric_cols:\n            if df_clean[col].isnull().any():\n                df_clean[col].fillna(df_clean[col].median(), inplace=True)\n                report[\"actions_taken\"].append(f\"Filled {col} with median\")\n\n        # Fill categorical columns with mode\n        cat_cols = df_clean.select_dtypes(include=[\"object\", \"category\"]).columns\n        for col in cat_cols:\n            if df_clean[col].isnull().any():\n                df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n                report[\"actions_taken\"].append(f\"Filled {col} with mode\")\n\n    # Step 3: Remove duplicates\n    n_duplicates = df_clean.duplicated().sum()\n    if n_duplicates &gt; 0:\n        report[\"issues_found\"].append(f\"{n_duplicates} duplicate rows\")\n        df_clean = df_clean.drop_duplicates()\n        report[\"actions_taken\"].append(f\"Removed {n_duplicates} duplicates\")\n\n    # Step 4: Clean string columns\n    string_cols = df_clean.select_dtypes(include=[\"object\"]).columns\n    for col in string_cols:\n        df_clean[col] = df_clean[col].str.strip()\n        df_clean[col] = df_clean[col].str.lower()\n    report[\"actions_taken\"].append(\"Cleaned string columns (strip, lowercase)\")\n\n    # Step 5: Standardize column names\n    df_clean.columns = (df_clean.columns\n                        .str.strip()\n                        .str.lower()\n                        .str.replace(\" \", \"_\")\n                        .str.replace(\"-\", \"_\"))\n    report[\"actions_taken\"].append(\"Standardized column names\")\n\n    # Final report\n    report[\"final_shape\"] = df_clean.shape\n    report[\"rows_removed\"] = report[\"original_shape\"][0] - report[\"final_shape\"][0]\n\n    return df_clean, report\n\n# Use the function\ndf_raw = pd.read_csv(\"messy_data.csv\")\ndf_clean, cleaning_report = clean_dataset(df_raw)\n\nprint(f\"Cleaned {cleaning_report['original_shape'][0]} rows \u2192 {cleaning_report['final_shape'][0]} rows\")\nprint(\"Actions taken:\", cleaning_report[\"actions_taken\"])\n</code></pre> <p>Achievement Unlocked: Data Janitor</p> <p>You now have the skills to transform any messy dataset into clean, analysis-ready data. This isn't glamorous work, but it's where real data scientists spend 60-80% of their time. You're now equipped for the real world!</p>"},{"location":"chapters/04-data-cleaning/#common-patterns-and-best-practices","title":"Common Patterns and Best Practices","text":""},{"location":"chapters/04-data-cleaning/#the-cleaning-checklist","title":"The Cleaning Checklist","text":"<p>Before any analysis, run through this checklist:</p> <ul> <li>[ ] Load data and check <code>shape</code></li> <li>[ ] View <code>head()</code> and <code>tail()</code> for anomalies</li> <li>[ ] Check <code>dtypes</code> for correct types</li> <li>[ ] Run <code>isnull().sum()</code> for missing values</li> <li>[ ] Run <code>duplicated().sum()</code> for duplicates</li> <li>[ ] Check <code>describe()</code> for impossible values</li> <li>[ ] Validate against business rules</li> <li>[ ] Document all cleaning decisions</li> </ul>"},{"location":"chapters/04-data-cleaning/#anti-patterns-to-avoid","title":"Anti-Patterns to Avoid","text":"<pre><code># DON'T: Modify data without inspection\ndf = df.dropna()  # How much did you just lose?\n\n# DO: Inspect first, then decide\nprint(f\"Missing values: {df.isnull().sum().sum()}\")\nprint(f\"Would drop {df.isnull().any(axis=1).sum()} rows\")\n\n# DON'T: Hardcode cleaning values\ndf[\"status\"].fillna(\"Unknown\")  # What if this changes?\n\n# DO: Use data-driven or configurable values\ndefault_status = df[\"status\"].mode()[0]  # Most common value\n\n# DON'T: Silently modify the original\ndf.dropna(inplace=True)\n\n# DO: Create cleaned versions\ndf_clean = df.dropna().copy()\n</code></pre> Chapter 4 Checkpoint: Test Your Understanding <p>Question: You receive a dataset with the following issues: - 5% of rows have missing ages - 2% of rows have duplicate emails - Some scores are recorded as -1 (meaning \"not applicable\") - Income column is stored as strings like \"$50,000\"</p> <p>Write a cleaning plan for this data.</p> <p>Click to reveal answer:</p> <pre><code># 1. Handle missing ages (5% is acceptable to fill)\ndf[\"age\"].fillna(df[\"age\"].median(), inplace=True)\n\n# 2. Handle duplicates based on email\ndf = df.drop_duplicates(subset=[\"email\"], keep=\"last\")\n\n# 3. Handle -1 scores (replace with NaN, not a real value)\ndf[\"score\"] = df[\"score\"].replace(-1, np.nan)\n\n# 4. Clean income column\ndf[\"income\"] = (df[\"income\"]\n                .str.replace(\"$\", \"\")\n                .str.replace(\",\", \"\")\n                .astype(float))\n</code></pre>"},{"location":"chapters/04-data-cleaning/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Missing values (NaN) must be found with <code>isnull()</code> and handled with <code>dropna()</code>, <code>fillna()</code>, or imputation strategies.</p> </li> <li> <p>Duplicates inflate your data\u2014detect with <code>duplicated()</code> and remove with <code>drop_duplicates()</code>.</p> </li> <li> <p>Outliers can be errors or legitimate extremes\u2014detect with IQR or z-scores, then decide whether to remove, cap, or keep.</p> </li> <li> <p>Data validation enforces business rules\u2014build validation functions to catch problems early.</p> </li> <li> <p>Boolean indexing and the query method let you filter data with complex conditions.</p> </li> <li> <p>String cleaning standardizes text\u2014use <code>.str</code> methods for cleaning operations.</p> </li> <li> <p>Data type conversion ensures columns have appropriate types for analysis.</p> </li> <li> <p>Feature scaling (Min-Max, Standard, Robust) brings features to comparable ranges for machine learning.</p> </li> <li> <p>Document everything\u2014cleaning decisions affect your entire analysis, so keep a record.</p> </li> <li> <p>The cleaning workflow (missing \u2192 duplicates \u2192 outliers \u2192 types \u2192 validation \u2192 transformation) should become second nature.</p> </li> </ol> <p>You've mastered the art of data cleaning\u2014arguably the most valuable practical skill in data science. In the next chapter, you'll learn to visualize your clean data, turning numbers into insights that everyone can understand. The glamorous part is coming!</p>"},{"location":"chapters/05-data-visualization/","title":"Data Visualization with Matplotlib and Plotly","text":""},{"location":"chapters/05-data-visualization/#summary","title":"Summary","text":"<p>This chapter teaches students how to create effective data visualizations using matplotlib, Seaborn, and the modern interactive library Plotly. Students will learn visualization architecture (figures and axes), create various plot types (line, scatter, bar, histogram, box, pie), and customize their visualizations with titles, labels, legends, and colors. The chapter emphasizes choosing appropriate visualizations for different data types and creating interactive charts that engage viewers. By the end of this chapter, students will be able to create publication-quality visualizations that effectively communicate insights from data.</p>"},{"location":"chapters/05-data-visualization/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 25 concepts from the learning graph:</p> <ol> <li>Data Visualization</li> <li>Matplotlib Library</li> <li>Figure</li> <li>Axes</li> <li>Plot Function</li> <li>Line Plot</li> <li>Scatter Plot</li> <li>Bar Chart</li> <li>Histogram</li> <li>Box Plot</li> <li>Pie Chart</li> <li>Subplot</li> <li>Figure Size</li> <li>Title</li> <li>Axis Labels</li> <li>Legend</li> <li>Color</li> <li>Markers</li> <li>Line Styles</li> <li>Grid</li> <li>Annotations</li> <li>Save Figure</li> <li>Plot Customization</li> <li>Seaborn Library</li> <li>Statistical Plots</li> </ol>"},{"location":"chapters/05-data-visualization/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Python Data Structures</li> <li>Chapter 4: Data Cleaning and Preprocessing</li> </ul>"},{"location":"chapters/05-data-visualization/#show-dont-tell-the-visual-superpower","title":"Show, Don't Tell: The Visual Superpower","text":"<p>You've loaded data. You've cleaned it. You've wrangled it into perfect shape. But here's the thing\u2014a spreadsheet full of numbers is about as exciting as reading the phone book. Nobody ever changed the world by emailing a CSV file.</p> <p>Data visualization is where data science becomes VISIBLE. It's the superpower that lets you take thousands of numbers and transform them into a single image that tells a story. A well-crafted chart can reveal patterns that would take hours to find in a table. It can convince skeptics, inspire action, and make the invisible visible.</p> <p>Think about it: every powerful presentation you've ever seen probably had a chart. Every news story about trends shows a graph. Every scientific breakthrough gets communicated through visualization. This is the skill that takes your analysis from \"interesting to me\" to \"interesting to everyone.\"</p> <p>In this chapter, you'll learn multiple visualization tools\u2014from the classic Matplotlib to the beautiful Seaborn to the modern, interactive Plotly. By the end, you'll be creating charts that don't just display data\u2014they ENGAGE with it.</p>"},{"location":"chapters/05-data-visualization/#diagram-visualization-library-comparison","title":"Diagram: Visualization Library Comparison","text":"Python Visualization Library Landscape <p>Type: infographic</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students understand when to use different visualization libraries</p> <p>Purpose: Compare the major Python visualization libraries and their strengths</p> <p>Layout: Three-column comparison card layout</p> <p>Column 1: MATPLOTLIB - Icon: Classic line graph - Tagline: \"The Foundation\" - Color: Blue - Strengths:   - Complete control over every element   - Publication-quality static images   - Huge community and documentation   - Works everywhere - Best for:   - Academic papers   - Print publications   - Maximum customization - Learning curve: Medium-High - Interactivity: Limited (static by default)</p> <p>Column 2: SEABORN - Icon: Statistical plot with confidence intervals - Tagline: \"Beautiful Statistics\" - Color: Teal - Strengths:   - Beautiful defaults   - Built-in statistical visualizations   - Works with pandas DataFrames   - Less code for common plots - Best for:   - Statistical analysis   - Exploratory data analysis   - Quick beautiful plots - Learning curve: Low-Medium - Interactivity: Limited (built on matplotlib)</p> <p>Column 3: PLOTLY - Icon: Interactive 3D scatter plot with cursor - Tagline: \"Interactive &amp; Modern\" - Color: Purple - Strengths:   - Interactive by default (zoom, pan, hover)   - Web-ready (HTML output)   - Beautiful modern aesthetics   - 3D visualizations   - Dashboards with Dash - Best for:   - Web applications   - Presentations   - Data exploration   - User engagement - Learning curve: Low-Medium - Interactivity: Full (native)</p> <p>Bottom section: Decision flowchart - \"Need print/PDF?\" \u2192 Matplotlib - \"Statistical focus?\" \u2192 Seaborn - \"Need interactivity?\" \u2192 Plotly - \"Quick exploration?\" \u2192 Seaborn or Plotly</p> <p>Interactive elements: - Hover over each library to see code examples - Click to see sample output images</p> <p>Implementation: HTML/CSS grid with JavaScript hover effects</p>"},{"location":"chapters/05-data-visualization/#the-classic-matplotlib-library","title":"The Classic: Matplotlib Library","text":"<p>Let's start with the grandfather of Python visualization: the Matplotlib library. Created in 2003, matplotlib is the foundation that most other Python visualization libraries build upon. It's powerful, flexible, and gives you complete control over every pixel.</p> <pre><code>import matplotlib.pyplot as plt\n\n# The classic matplotlib import\n# pyplot gives you a MATLAB-like interface\n</code></pre>"},{"location":"chapters/05-data-visualization/#understanding-figures-and-axes","title":"Understanding Figures and Axes","text":"<p>Matplotlib has a specific architecture you need to understand. A figure is the entire window or page\u2014think of it as your canvas. Axes are the actual plots within that figure (yes, the name is confusing\u2014it's not about x-axis and y-axis, but the plot area itself).</p> <pre><code>import matplotlib.pyplot as plt\n\n# Create a figure and axes\nfig, ax = plt.subplots()\n\n# The figure is the container\n# The axes (ax) is where you actually draw\nax.plot([1, 2, 3, 4], [1, 4, 2, 3])\nplt.show()\n</code></pre> <p>This figure/axes separation becomes important when you create multiple plots. You can have one figure with many axes (subplots), giving you complete control over complex layouts.</p> <pre><code># Create a figure with 2x2 grid of subplots\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n# axes is now a 2x2 array of Axes objects\naxes[0, 0].plot([1, 2, 3], [1, 2, 3])  # Top-left\naxes[0, 1].bar([1, 2, 3], [3, 1, 2])   # Top-right\naxes[1, 0].scatter([1, 2, 3], [2, 3, 1])  # Bottom-left\naxes[1, 1].hist([1, 1, 2, 3, 3, 3, 4])    # Bottom-right\n\nplt.tight_layout()  # Prevent overlapping\nplt.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#the-plot-function","title":"The Plot Function","text":"<p>The plot function is your basic drawing tool. At its simplest, it connects points with lines:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Basic plot: x values, y values\nx = [0, 1, 2, 3, 4, 5]\ny = [0, 1, 4, 9, 16, 25]  # y = x\u00b2\n\nplt.plot(x, y)\nplt.show()\n</code></pre> <p>But <code>plot()</code> can do much more with its many parameters for line styles, markers, and colors.</p> Parameter Example Description <code>color</code> or <code>c</code> <code>'red'</code>, <code>'#FF5733'</code>, <code>'C0'</code> Line/marker color <code>linestyle</code> or <code>ls</code> <code>'-'</code>, <code>'--'</code>, <code>':'</code>, <code>'-.'</code> Line pattern <code>linewidth</code> or <code>lw</code> <code>2</code>, <code>0.5</code> Line thickness <code>marker</code> <code>'o'</code>, <code>'s'</code>, <code>'^'</code>, <code>'*'</code> Point markers <code>markersize</code> or <code>ms</code> <code>10</code>, <code>5</code> Marker size <pre><code># Customized plot\nplt.plot(x, y,\n         color='purple',\n         linestyle='--',\n         linewidth=2,\n         marker='o',\n         markersize=8)\nplt.show()\n</code></pre> <p>The Format String Shortcut</p> <p>Matplotlib has a shortcut: <code>plt.plot(x, y, 'ro--')</code> means red (<code>r</code>), circles (<code>o</code>), dashed line (<code>--</code>). It's compact but can be cryptic\u2014use named parameters for clarity in your code.</p>"},{"location":"chapters/05-data-visualization/#essential-plot-types","title":"Essential Plot Types","text":"<p>Different data calls for different visualizations. Let's master the essential types.</p>"},{"location":"chapters/05-data-visualization/#line-plot-trends-over-time","title":"Line Plot: Trends Over Time","text":"<p>A line plot connects data points with lines, perfect for showing how values change over time or across a sequence.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Stock price over 30 days\ndays = np.arange(1, 31)\nstock_price = 100 + np.cumsum(np.random.randn(30) * 2)\n\nplt.figure(figsize=(10, 5))\nplt.plot(days, stock_price, color='blue', linewidth=2)\nplt.title('Stock Price Over 30 Days')\nplt.xlabel('Day')\nplt.ylabel('Price ($)')\nplt.grid(True)\nplt.show()\n</code></pre> <p>When to use: Time series, trends, continuous data, showing progression.</p>"},{"location":"chapters/05-data-visualization/#scatter-plot-relationships-between-variables","title":"Scatter Plot: Relationships Between Variables","text":"<p>A scatter plot shows individual data points without connecting them, revealing relationships between two variables.</p> <pre><code># Height vs Weight\nheight = [160, 165, 170, 175, 180, 185, 190]\nweight = [55, 62, 68, 75, 82, 88, 95]\n\nplt.figure(figsize=(8, 6))\nplt.scatter(height, weight, color='green', s=100, alpha=0.7)\nplt.title('Height vs Weight')\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.show()\n</code></pre> <p>When to use: Correlation analysis, comparing two numeric variables, finding clusters or outliers.</p>"},{"location":"chapters/05-data-visualization/#bar-chart-comparing-categories","title":"Bar Chart: Comparing Categories","text":"<p>A bar chart uses rectangular bars to compare values across categories.</p> <pre><code># Programming language popularity\nlanguages = ['Python', 'JavaScript', 'Java', 'C++', 'Go']\npopularity = [28, 25, 18, 12, 8]\n\nplt.figure(figsize=(10, 6))\nplt.bar(languages, popularity, color=['#3776AB', '#F7DF1E', '#ED8B00', '#00599C', '#00ADD8'])\nplt.title('Programming Language Popularity')\nplt.xlabel('Language')\nplt.ylabel('Popularity (%)')\nplt.show()\n</code></pre> <p>When to use: Comparing categories, showing rankings, discrete data.</p>"},{"location":"chapters/05-data-visualization/#histogram-distribution-of-values","title":"Histogram: Distribution of Values","text":"<p>A histogram shows how values are distributed across ranges (bins). Unlike bar charts, histograms show continuous data grouped into intervals.</p> <pre><code>import numpy as np\n\n# Test scores\nscores = np.random.normal(75, 10, 1000)  # Mean=75, std=10\n\nplt.figure(figsize=(10, 6))\nplt.hist(scores, bins=20, color='coral', edgecolor='black', alpha=0.7)\nplt.title('Distribution of Test Scores')\nplt.xlabel('Score')\nplt.ylabel('Number of Students')\nplt.axvline(x=75, color='red', linestyle='--', label='Mean')\nplt.legend()\nplt.show()\n</code></pre> <p>When to use: Understanding distribution shape, finding outliers, comparing to normal distribution.</p>"},{"location":"chapters/05-data-visualization/#box-plot-statistical-summary","title":"Box Plot: Statistical Summary","text":"<p>A box plot (or box-and-whisker plot) shows the five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It's excellent for comparing distributions and spotting outliers.</p> <pre><code># Compare scores across three classes\nclass_a = np.random.normal(75, 8, 30)\nclass_b = np.random.normal(70, 12, 30)\nclass_c = np.random.normal(80, 5, 30)\n\nplt.figure(figsize=(8, 6))\nplt.boxplot([class_a, class_b, class_c], labels=['Class A', 'Class B', 'Class C'])\nplt.title('Score Distribution by Class')\nplt.ylabel('Score')\nplt.show()\n</code></pre> <p>When to use: Comparing distributions, identifying outliers, showing spread and central tendency.</p>"},{"location":"chapters/05-data-visualization/#pie-chart-parts-of-a-whole","title":"Pie Chart: Parts of a Whole","text":"<p>A pie chart shows proportions of a whole. Use them sparingly\u2014they're often harder to read than bar charts.</p> <pre><code># Budget allocation\ncategories = ['Rent', 'Food', 'Transport', 'Entertainment', 'Savings']\namounts = [1200, 400, 200, 150, 250]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\n\nplt.figure(figsize=(8, 8))\nplt.pie(amounts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.title('Monthly Budget Allocation')\nplt.show()\n</code></pre> <p>When to use: Showing proportions of a whole (when you have 2-5 categories). Avoid for comparisons.</p>"},{"location":"chapters/05-data-visualization/#diagram-chart-type-selection-guide","title":"Diagram: Chart Type Selection Guide","text":"Which Chart Should I Use? <p>Type: infographic</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Help students choose the appropriate chart type for their data and question</p> <p>Purpose: Decision guide for selecting visualization types</p> <p>Layout: Flowchart/decision tree with visual examples</p> <p>Starting question: \"What do you want to show?\"</p> <p>Branch 1: \"Comparison\" - Few categories \u2192 Bar Chart (vertical) - Many categories \u2192 Bar Chart (horizontal) - Over time \u2192 Line Chart (multiple lines) - Visual: Small example of each</p> <p>Branch 2: \"Distribution\" - Single variable \u2192 Histogram - Compare distributions \u2192 Box Plot - Density estimate \u2192 KDE Plot - Visual: Small example of each</p> <p>Branch 3: \"Relationship\" - Two variables \u2192 Scatter Plot - Three variables \u2192 Bubble Chart (size = 3rd var) - Many variables \u2192 Pair Plot - Visual: Small example of each</p> <p>Branch 4: \"Composition\" - Static \u2192 Pie Chart (2-5 parts only!) - Over time \u2192 Stacked Area Chart - Many parts \u2192 Treemap - Visual: Small example of each</p> <p>Branch 5: \"Trend\" - Over time \u2192 Line Chart - With uncertainty \u2192 Line + Confidence Band - Multiple series \u2192 Multiple Lines + Legend - Visual: Small example of each</p> <p>Warning callouts: - \"Pie charts: Only use with 2-5 categories\" - \"3D charts: Avoid! They distort perception\" - \"Dual y-axes: Use carefully, can mislead\"</p> <p>Interactive elements: - Hover over each chart type to see larger example - Click to see code snippet</p> <p>Visual style: Clean flowchart with colorful chart thumbnails</p> <p>Implementation: SVG with interactive JavaScript</p>"},{"location":"chapters/05-data-visualization/#customizing-your-visualizations","title":"Customizing Your Visualizations","text":"<p>Raw plots are just the beginning. Professional visualizations need polish. Let's master plot customization.</p>"},{"location":"chapters/05-data-visualization/#title-and-axis-labels","title":"Title and Axis Labels","text":"<p>Every chart needs a title that explains what it shows and axis labels that explain the variables:</p> <pre><code>plt.figure(figsize=(10, 6))\nplt.plot(x, y)\n\n# Title with size and style\nplt.title('Quadratic Growth: y = x\u00b2', fontsize=16, fontweight='bold')\n\n# Axis labels\nplt.xlabel('Input Value (x)', fontsize=12)\nplt.ylabel('Output Value (y)', fontsize=12)\n\nplt.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#legend","title":"Legend","text":"<p>A legend identifies multiple data series. Position it where it doesn't obscure data:</p> <pre><code>plt.figure(figsize=(10, 6))\nplt.plot(x, y, label='y = x\u00b2')\nplt.plot(x, [i**3/10 for i in x], label='y = x\u00b3/10')\nplt.plot(x, [2**i for i in x], label='y = 2^x')\n\nplt.legend(loc='upper left')  # Options: 'best', 'upper right', 'lower left', etc.\nplt.title('Growth Functions Comparison')\nplt.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#grid-and-annotations","title":"Grid and Annotations","text":"<p>A grid helps readers estimate values. Annotations highlight specific points:</p> <pre><code>plt.figure(figsize=(10, 6))\nplt.plot(x, y, 'b-o')\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.7)\n\n# Add annotation pointing to a specific point\nplt.annotate('Maximum value',\n             xy=(5, 25),           # Point to annotate\n             xytext=(3.5, 20),     # Text position\n             fontsize=12,\n             arrowprops=dict(arrowstyle='-&gt;', color='red'))\n\nplt.title('Annotated Plot')\nplt.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#figure-size-and-saving","title":"Figure Size and Saving","text":"<p>Control figure size for different outputs and save your work:</p> <pre><code># Create figure with specific size (width, height in inches)\nplt.figure(figsize=(12, 6))\nplt.plot(x, y)\nplt.title('Wide Format Plot')\n\n# Save to file (before plt.show()!)\nplt.savefig('my_plot.png', dpi=300, bbox_inches='tight')\nplt.savefig('my_plot.pdf')  # Vector format for publications\nplt.savefig('my_plot.svg')  # Vector format for web\n\nplt.show()\n</code></pre> Format Best For File Size PNG Web, presentations Medium PDF Publications, print Small SVG Web (scalable) Small JPG Photos (avoid for charts) Small"},{"location":"chapters/05-data-visualization/#subplots-multiple-views","title":"Subplots: Multiple Views","text":"<p>Subplots let you show multiple related visualizations together:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create sample data\nx = np.linspace(0, 10, 100)\n\n# Create 2x2 subplot grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Top-left: Line plot\naxes[0, 0].plot(x, np.sin(x), 'b-')\naxes[0, 0].set_title('Sine Wave')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('sin(x)')\n\n# Top-right: Scatter plot\naxes[0, 1].scatter(np.random.rand(50), np.random.rand(50), c='green', alpha=0.6)\naxes[0, 1].set_title('Random Scatter')\n\n# Bottom-left: Bar chart\naxes[1, 0].bar(['A', 'B', 'C', 'D'], [23, 45, 56, 78], color='coral')\naxes[1, 0].set_title('Category Comparison')\n\n# Bottom-right: Histogram\naxes[1, 1].hist(np.random.normal(0, 1, 1000), bins=30, color='purple', alpha=0.7)\naxes[1, 1].set_title('Normal Distribution')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Sharing Axes</p> <p>Use <code>sharex=True</code> or <code>sharey=True</code> in <code>subplots()</code> to align axes across plots\u2014essential for fair comparisons.</p>"},{"location":"chapters/05-data-visualization/#seaborn-beautiful-statistics","title":"Seaborn: Beautiful Statistics","text":"<p>The Seaborn library builds on matplotlib to provide beautiful default styles and specialized statistical plots. It's perfect for exploratory data analysis.</p> <pre><code>import seaborn as sns\nimport pandas as pd\n\n# Seaborn works beautifully with DataFrames\ndf = pd.DataFrame({\n    'x': np.random.normal(0, 1, 100),\n    'y': np.random.normal(0, 1, 100),\n    'category': np.random.choice(['A', 'B', 'C'], 100)\n})\n\n# Set the style\nsns.set_style('whitegrid')\n\n# Create a scatter plot with regression line\nsns.lmplot(data=df, x='x', y='y', hue='category', height=6)\nplt.title('Scatter Plot with Regression by Category')\nplt.show()\n</code></pre> <p>Seaborn's statistical plots include:</p> <ul> <li><code>sns.histplot()</code> - Enhanced histograms with KDE</li> <li><code>sns.boxplot()</code> - Box plots with category support</li> <li><code>sns.violinplot()</code> - Distribution shape visualization</li> <li><code>sns.heatmap()</code> - Correlation matrices</li> <li><code>sns.pairplot()</code> - All pairwise relationships</li> </ul> <pre><code># Correlation heatmap\ncorrelation_matrix = df[['x', 'y']].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap')\nplt.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#plotly-interactive-visualization","title":"Plotly: Interactive Visualization","text":"<p>Now for the exciting part! Plotly creates interactive visualizations that users can explore\u2014zoom, pan, hover for details, and more. This is what modern data visualization looks like.</p> <pre><code>import plotly.express as px\nimport pandas as pd\n\n# Create sample data\ndf = pd.DataFrame({\n    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],\n    'Sales': [100, 120, 140, 135, 160, 180],\n    'Profit': [20, 25, 30, 28, 35, 40]\n})\n\n# Create interactive line chart - it's this simple!\nfig = px.line(df, x='Month', y='Sales',\n              title='Monthly Sales Trend',\n              markers=True)\nfig.show()\n</code></pre> <p>When you run this, you get a chart where you can:</p> <ul> <li>Hover over points to see exact values</li> <li>Zoom by clicking and dragging</li> <li>Pan by holding shift and dragging</li> <li>Download as PNG with one click</li> <li>Toggle data series on/off via legend</li> </ul>"},{"location":"chapters/05-data-visualization/#why-plotly-changes-everything","title":"Why Plotly Changes Everything","text":"Feature Matplotlib Plotly Default output Static image Interactive HTML Hover tooltips Manual coding Automatic Zoom/Pan Not available Built-in Web embedding Export as image Native HTML Learning curve Medium-High Low-Medium Customization Maximum High"},{"location":"chapters/05-data-visualization/#plotly-express-the-fast-lane","title":"Plotly Express: The Fast Lane","text":"<p><code>plotly.express</code> (imported as <code>px</code>) provides high-level functions for common chart types:</p> <pre><code>import plotly.express as px\n\n# Interactive scatter plot\ndf = px.data.iris()  # Built-in sample dataset\nfig = px.scatter(df, x='sepal_width', y='sepal_length',\n                 color='species',\n                 size='petal_length',\n                 hover_data=['petal_width'],\n                 title='Iris Dataset: Sepal Dimensions')\nfig.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#diagram-plotly-interactive-features-microsim","title":"Diagram: Plotly Interactive Features MicroSim","text":"Interactive Chart Exploration Playground <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students experience and practice using Plotly's interactive features</p> <p>Canvas layout (850x600px): - Main area (850x450): Interactive Plotly chart - Bottom panel (850x150): Feature buttons and instructions</p> <p>Visual elements: - Sample scatter plot with 50+ data points - Multiple colored categories - Visible toolbar (zoom, pan, select, download) - Hover tooltip showing data values</p> <p>Interactive features to demonstrate: 1. HOVER: Move mouse over points to see tooltips 2. ZOOM: Click-drag to zoom into a region 3. PAN: Shift+drag to pan around 4. BOX SELECT: Draw box to select points 5. LASSO SELECT: Freeform selection 6. RESET: Double-click to reset view 7. DOWNLOAD: Click camera icon to save PNG 8. LEGEND: Click legend items to toggle series</p> <p>Challenge tasks (bottom panel): - \"Zoom into the cluster in the upper right\" - \"Select all points in category A\" - \"Find the outlier with the highest y-value\" - \"Download the chart as PNG\"</p> <p>Progress tracker: - Checkboxes for each feature used - \"You've explored X of 8 interactive features!\"</p> <p>Behavior: - Track which features student has used - Provide hints for unexplored features - Celebrate when all features discovered</p> <p>Visual style: Modern dashboard aesthetic</p> <p>Implementation: Embedded Plotly.js chart with custom tracking overlay</p>"},{"location":"chapters/05-data-visualization/#interactive-line-charts","title":"Interactive Line Charts","text":"<pre><code>import plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# Multiple time series\ndates = pd.date_range('2024-01-01', periods=100, freq='D')\ndf = pd.DataFrame({\n    'Date': dates,\n    'Product A': np.cumsum(np.random.randn(100)) + 100,\n    'Product B': np.cumsum(np.random.randn(100)) + 100,\n    'Product C': np.cumsum(np.random.randn(100)) + 100\n})\n\n# Melt for plotly format\ndf_melted = df.melt(id_vars='Date', var_name='Product', value_name='Sales')\n\nfig = px.line(df_melted, x='Date', y='Sales', color='Product',\n              title='Product Sales Over Time')\n\n# Customize interactivity\nfig.update_layout(\n    hovermode='x unified',  # Show all values at same x position\n    legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01)\n)\n\nfig.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#interactive-bar-charts","title":"Interactive Bar Charts","text":"<pre><code># Animated bar chart\ndf = px.data.gapminder()\ndf_2007 = df[df['year'] == 2007].nlargest(10, 'pop')\n\nfig = px.bar(df_2007, x='country', y='pop', color='continent',\n             title='Top 10 Countries by Population (2007)',\n             labels={'pop': 'Population', 'country': 'Country'},\n             hover_data=['gdpPercap', 'lifeExp'])\n\nfig.update_layout(xaxis_tickangle=-45)\nfig.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#interactive-scatter-plots","title":"Interactive Scatter Plots","text":"<pre><code># Bubble chart with animation\ndf = px.data.gapminder()\n\nfig = px.scatter(df, x='gdpPercap', y='lifeExp',\n                 animation_frame='year',\n                 animation_group='country',\n                 size='pop', color='continent',\n                 hover_name='country',\n                 log_x=True,\n                 size_max=60,\n                 range_x=[100, 100000],\n                 range_y=[25, 90],\n                 title='Global Development: GDP vs Life Expectancy')\n\nfig.show()\n</code></pre> <p>This creates the famous \"Gapminder\" visualization that Hans Rosling made famous\u2014an animated bubble chart showing how countries develop over time!</p>"},{"location":"chapters/05-data-visualization/#interactive-histograms-and-box-plots","title":"Interactive Histograms and Box Plots","text":"<pre><code># Interactive histogram with marginal plots\ndf = px.data.tips()\n\nfig = px.histogram(df, x='total_bill', color='sex',\n                   marginal='box',  # Add box plot on margin\n                   hover_data=df.columns,\n                   title='Distribution of Total Bills')\n\nfig.show()\n</code></pre> <pre><code># Interactive box plot\nfig = px.box(df, x='day', y='total_bill', color='smoker',\n             notched=True,  # Show confidence interval\n             title='Bill Distribution by Day and Smoking Status')\n\nfig.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#customizing-plotly-charts","title":"Customizing Plotly Charts","text":"<p>Plotly offers extensive customization through <code>update_layout()</code> and <code>update_traces()</code>:</p> <pre><code>fig = px.scatter(df, x='total_bill', y='tip', color='day')\n\n# Customize layout\nfig.update_layout(\n    title=dict(text='Tips vs Total Bill', font=dict(size=24)),\n    xaxis_title='Total Bill ($)',\n    yaxis_title='Tip ($)',\n    legend_title='Day of Week',\n    font=dict(family='Arial', size=14),\n    plot_bgcolor='white',\n    paper_bgcolor='white'\n)\n\n# Customize the data points\nfig.update_traces(\n    marker=dict(size=12, line=dict(width=1, color='DarkSlateGrey')),\n    selector=dict(mode='markers')\n)\n\n# Add gridlines\nfig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\nfig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n\nfig.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#subplots-in-plotly","title":"Subplots in Plotly","text":"<pre><code>from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Create subplot grid\nfig = make_subplots(rows=2, cols=2,\n                    subplot_titles=('Line', 'Bar', 'Scatter', 'Histogram'))\n\n# Add traces to each subplot\nfig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6], mode='lines'), row=1, col=1)\nfig.add_trace(go.Bar(x=['A', 'B', 'C'], y=[1, 3, 2]), row=1, col=2)\nfig.add_trace(go.Scatter(x=[1, 2, 3], y=[1, 3, 2], mode='markers'), row=2, col=1)\nfig.add_trace(go.Histogram(x=np.random.randn(500)), row=2, col=2)\n\nfig.update_layout(height=600, width=800, title_text=\"Multiple Plot Types\")\nfig.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#saving-plotly-charts","title":"Saving Plotly Charts","text":"<pre><code># Save as interactive HTML (can be opened in browser)\nfig.write_html('my_interactive_chart.html')\n\n# Save as static image (requires kaleido package)\nfig.write_image('my_chart.png', scale=2)  # scale=2 for higher resolution\nfig.write_image('my_chart.pdf')\nfig.write_image('my_chart.svg')\n</code></pre>"},{"location":"chapters/05-data-visualization/#diagram-plotly-code-pattern-reference","title":"Diagram: Plotly Code Pattern Reference","text":"Plotly Express Quick Reference Card <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Provide quick reference for common Plotly Express patterns</p> <p>Purpose: Cheat sheet for Plotly Express functions and parameters</p> <p>Layout: Four-quadrant reference card</p> <p>Quadrant 1: \"Common Chart Functions\" <pre><code>px.line()      - Line charts\npx.scatter()   - Scatter plots\npx.bar()       - Bar charts\npx.histogram() - Histograms\npx.box()       - Box plots\npx.pie()       - Pie charts\npx.area()      - Area charts\npx.violin()    - Violin plots\n</code></pre></p> <p>Quadrant 2: \"Essential Parameters\" <pre><code>x, y           - Data columns\ncolor          - Color by category\nsize           - Size by value\nhover_data     - Extra tooltip info\ntitle          - Chart title\nlabels         - Rename axis labels\nfacet_col      - Small multiples (columns)\nfacet_row      - Small multiples (rows)\nanimation_frame - Animate over values\n</code></pre></p> <p>Quadrant 3: \"Layout Customization\" <pre><code>fig.update_layout(\n    title='My Title',\n    xaxis_title='X Label',\n    yaxis_title='Y Label',\n    legend_title='Legend',\n    template='plotly_white',\n    height=500,\n    width=800\n)\n</code></pre></p> <p>Quadrant 4: \"Saving Options\" <pre><code># Interactive HTML\nfig.write_html('chart.html')\n\n# Static images\nfig.write_image('chart.png')\nfig.write_image('chart.pdf')\nfig.write_image('chart.svg')\n\n# In Jupyter\nfig.show()\n</code></pre></p> <p>Bottom strip: \"Templates\" - plotly, plotly_white, plotly_dark - ggplot2, seaborn, simple_white - Visual swatches of each</p> <p>Color scheme: Purple gradient (Plotly brand color)</p> <p>Interactive elements: - Hover for expanded code examples - Click to copy code snippet</p> <p>Implementation: HTML/CSS grid with copy-to-clipboard JavaScript</p>"},{"location":"chapters/05-data-visualization/#real-world-visualization-workflow","title":"Real-World Visualization Workflow","text":"<p>Let's put it all together with a complete workflow:</p> <pre><code>import plotly.express as px\nimport pandas as pd\n\n# Step 1: Load and clean data\ndf = pd.read_csv('sales_data.csv')\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.dropna()\n\n# Step 2: Explore with quick visualizations\nfig1 = px.histogram(df, x='revenue', title='Revenue Distribution')\nfig1.show()\n\n# Step 3: Create main visualization\nfig2 = px.scatter(df,\n                  x='marketing_spend',\n                  y='revenue',\n                  color='region',\n                  size='units_sold',\n                  hover_name='product',\n                  trendline='ols',  # Add regression line\n                  title='Marketing Spend vs Revenue by Region')\n\n# Step 4: Customize for presentation\nfig2.update_layout(\n    title=dict(font=dict(size=24)),\n    xaxis_title='Marketing Spend ($)',\n    yaxis_title='Revenue ($)',\n    legend_title='Region',\n    template='plotly_white'\n)\n\n# Step 5: Save for sharing\nfig2.write_html('marketing_analysis.html')  # Interactive version\nfig2.write_image('marketing_analysis.png', scale=2)  # For presentations\n\nfig2.show()\n</code></pre>"},{"location":"chapters/05-data-visualization/#diagram-visualization-design-microsim","title":"Diagram: Visualization Design MicroSim","text":"Chart Design Playground <p>Type: microsim</p> <p>Bloom Taxonomy: Create (L6)</p> <p>Learning Objective: Let students design and customize their own visualizations interactively</p> <p>Canvas layout (900x650px): - Left panel (300x650): Controls and options - Right panel (600x650): Live chart preview</p> <p>Control panel sections:</p> <p>Section 1: \"Chart Type\" - Radio buttons: Line, Scatter, Bar, Histogram, Box - Visual icon for each type</p> <p>Section 2: \"Data Selection\" - Dropdown: X-axis variable - Dropdown: Y-axis variable - Dropdown: Color by (optional) - Dropdown: Size by (optional)</p> <p>Section 3: \"Customization\" - Text input: Title - Text input: X-axis label - Text input: Y-axis label - Color picker: Primary color - Dropdown: Color palette (categorical) - Slider: Marker size (5-50) - Slider: Line width (1-5) - Toggle: Show grid - Toggle: Show legend</p> <p>Section 4: \"Export\" - Button: \"Copy Code\" - Button: \"Download PNG\" - Button: \"Download HTML\"</p> <p>Sample dataset: - Pre-loaded \"tips\" style dataset - Columns: total_bill, tip, day, time, size, smoker</p> <p>Chart preview: - Updates in real-time as controls change - Fully interactive (zoom, pan, hover) - Shows Plotly toolbar</p> <p>Code panel (collapsible): - Shows Python code that would generate current chart - Updates dynamically with changes - Syntax highlighted</p> <p>Behavior: - Every control change immediately updates preview - Code panel reflects exact current configuration - Copy code button copies to clipboard - Download buttons generate files</p> <p>Educational features: - Tooltips explaining each option - \"Design tips\" suggestions based on data types selected - Warnings for bad practices (pie chart with too many categories, etc.)</p> <p>Visual style: Modern design tool interface (think Canva/Figma)</p> <p>Implementation: p5.js for controls + embedded Plotly.js for preview</p>"},{"location":"chapters/05-data-visualization/#choosing-the-right-visualization","title":"Choosing the Right Visualization","text":"<p>The most important skill isn't knowing how to make a chart\u2014it's knowing WHICH chart to make. Here's your decision framework:</p> Your Question Best Chart Type Why \"How does X change over time?\" Line chart Shows trends and patterns \"How are X and Y related?\" Scatter plot Reveals correlations \"How do categories compare?\" Bar chart Easy comparison \"What's the distribution?\" Histogram Shows shape and spread \"How do groups compare statistically?\" Box plot Shows median, quartiles, outliers \"What's the composition?\" Pie chart (2-5 parts) Shows parts of whole \"How do multiple variables relate?\" Pair plot / Scatter matrix See all relationships <p>Visualization Pitfalls to Avoid</p> <ul> <li>Truncated axes: Starting y-axis at non-zero exaggerates differences</li> <li>3D charts: They look cool but distort perception\u2014avoid them</li> <li>Too many colors: Stick to 5-7 distinct colors maximum</li> <li>Missing labels: Every chart needs title, axis labels, and legend (if needed)</li> <li>Pie charts with many slices: More than 5 categories? Use a bar chart instead</li> </ul>"},{"location":"chapters/05-data-visualization/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"chapters/05-data-visualization/#the-visualization-checklist","title":"The Visualization Checklist","text":"<p>Before sharing any visualization, verify:</p> <ul> <li>[ ] Clear, descriptive title</li> <li>[ ] Labeled axes with units</li> <li>[ ] Legend (if multiple series)</li> <li>[ ] Appropriate chart type for the data</li> <li>[ ] Accessible colors (colorblind-friendly)</li> <li>[ ] No unnecessary 3D effects</li> <li>[ ] Source cited (if using external data)</li> <li>[ ] Interactive features work (for Plotly)</li> </ul>"},{"location":"chapters/05-data-visualization/#code-organization-patterns","title":"Code Organization Patterns","text":"<pre><code># Good: Organized, readable, reusable\ndef create_sales_chart(df, x_col, y_col, title):\n    \"\"\"Create a customized sales visualization.\"\"\"\n    fig = px.scatter(df, x=x_col, y=y_col,\n                     color='region',\n                     title=title)\n\n    fig.update_layout(\n        template='plotly_white',\n        font=dict(family='Arial')\n    )\n\n    return fig\n\n# Use the function\nchart = create_sales_chart(sales_df, 'spend', 'revenue', 'Marketing ROI')\nchart.show()\n</code></pre> Chapter 5 Checkpoint: Test Your Understanding <p>Question: You have a dataset with columns: <code>date</code>, <code>sales</code>, <code>region</code>, <code>product_category</code>. You want to show: 1. How sales change over time 2. Sales comparison across regions 3. The distribution of sales values</p> <p>What chart types would you use for each, and would you use matplotlib or Plotly?</p> <p>Click to reveal answer:</p> <pre><code>import plotly.express as px\n\n# 1. Sales over time \u2192 Line chart (Plotly for interactivity)\nfig1 = px.line(df, x='date', y='sales', color='region',\n               title='Sales Over Time by Region')\n\n# 2. Compare regions \u2192 Bar chart\nregion_totals = df.groupby('region')['sales'].sum().reset_index()\nfig2 = px.bar(region_totals, x='region', y='sales',\n              title='Total Sales by Region')\n\n# 3. Distribution \u2192 Histogram\nfig3 = px.histogram(df, x='sales', nbins=30,\n                    title='Distribution of Sales Values')\n</code></pre> <p>Why Plotly? Interactive features let viewers explore the data themselves\u2014hover for details, zoom into interesting regions, and click legend items to focus on specific categories.</p> <p>Achievement Unlocked: Visual Storyteller</p> <p>You can now transform raw numbers into compelling visual narratives. Whether you need static publication graphics (matplotlib), beautiful statistical plots (Seaborn), or interactive web-ready visualizations (Plotly), you have the tools. This is the skill that gets your insights SEEN.</p>"},{"location":"chapters/05-data-visualization/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Data visualization transforms numbers into insights that everyone can understand\u2014it's how data science becomes visible.</p> </li> <li> <p>Matplotlib is the foundational library with complete control; understand figures (canvas) and axes (plot areas).</p> </li> <li> <p>Seaborn provides beautiful statistical plots with minimal code\u2014great for exploration.</p> </li> <li> <p>Plotly creates interactive visualizations with zoom, pan, hover tooltips\u2014the modern standard for web and presentations.</p> </li> <li> <p>Choose chart types based on your question: line for trends, scatter for relationships, bar for comparisons, histogram for distributions, box for statistical summaries.</p> </li> <li> <p>Customize your plots: meaningful titles, axis labels with units, legends for multiple series, appropriate colors.</p> </li> <li> <p>Subplots let you show multiple related views together for comprehensive analysis.</p> </li> <li> <p>Save your work: PNG/PDF for static uses, HTML for interactive sharing.</p> </li> <li> <p>Plotly Express (<code>px</code>) provides high-level functions that create professional interactive charts in one line.</p> </li> <li> <p>The best visualization is one that answers a question clearly\u2014not the fanciest chart, but the most appropriate one.</p> </li> </ol> <p>You've now mastered the art of visual communication. In the next chapter, you'll learn the statistical foundations that give your visualizations mathematical backing\u2014the numbers behind the pictures!</p>"},{"location":"chapters/06-statistical-foundations/","title":"Statistical Foundations","text":""},{"location":"chapters/06-statistical-foundations/#summary","title":"Summary","text":"<p>This chapter establishes the statistical foundation essential for data science and machine learning. Students will learn descriptive statistics (mean, median, mode, variance, standard deviation), understand distributions and probability, and explore sampling concepts. The chapter covers hypothesis testing, confidence intervals, and measures of association including correlation and covariance. By the end of this chapter, students will be able to summarize datasets statistically, understand the relationship between variables, and make probabilistic inferences.</p>"},{"location":"chapters/06-statistical-foundations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 30 concepts from the learning graph:</p> <ol> <li>Descriptive Statistics</li> <li>Mean</li> <li>Median</li> <li>Mode</li> <li>Range</li> <li>Variance</li> <li>Standard Deviation</li> <li>Quartiles</li> <li>Percentiles</li> <li>Interquartile Range</li> <li>Skewness</li> <li>Kurtosis</li> <li>Distribution</li> <li>Normal Distribution</li> <li>Probability</li> <li>Random Variables</li> <li>Expected Value</li> <li>Sample</li> <li>Population</li> <li>Sampling</li> <li>Central Limit Theorem</li> <li>Confidence Interval</li> <li>Hypothesis Testing</li> <li>P-Value</li> <li>Statistical Significance</li> <li>Correlation</li> <li>Covariance</li> <li>Pearson Correlation</li> <li>Spearman Correlation</li> <li>Correlation Matrix</li> </ol>"},{"location":"chapters/06-statistical-foundations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Introduction to Data Science</li> <li>Chapter 5: Data Visualization with Matplotlib</li> </ul>"},{"location":"chapters/06-statistical-foundations/#the-language-of-uncertainty","title":"The Language of Uncertainty","text":"<p>Here's a secret that surprises most people: the world runs on statistics. Every medical treatment you take was proven effective through statistics. Every recommendation Netflix makes uses statistical patterns. Every weather forecast is a statistical prediction. Every poll predicting election results? Statistics.</p> <p>Statistics is the mathematical language for dealing with uncertainty and variation. And in a world drowning in data, those who speak this language fluently have an incredible advantage.</p> <p>This chapter gives you that fluency. You'll learn to summarize thousands of data points with a handful of numbers, understand how data is distributed, measure relationships between variables, and make confident statements about populations based on samples. These aren't just academic exercises\u2014they're the core tools that power everything from A/B testing at tech companies to clinical trials for new medicines.</p> <p>Fair warning: this chapter is packed with concepts. But don't worry\u2014each one builds on the last, and by the end, you'll have a complete statistical toolkit. Let's start with the basics.</p>"},{"location":"chapters/06-statistical-foundations/#descriptive-statistics-summarizing-data","title":"Descriptive Statistics: Summarizing Data","text":"<p>Descriptive statistics are numbers that summarize and describe a dataset. Instead of looking at thousands of individual values, descriptive statistics give you the big picture in just a few numbers.</p> <p>Think of it like this: if someone asks \"How tall are the students in your school?\", you don't list every student's height. You say something like \"The average is 5'7\", ranging from 4'11\" to 6'4\".\" That's descriptive statistics in action.</p> <p>There are two main categories:</p> <ul> <li>Measures of central tendency: Where is the \"center\" of the data? (mean, median, mode)</li> <li>Measures of spread: How spread out is the data? (range, variance, standard deviation)</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\n\n# Sample dataset: test scores\nscores = [72, 85, 90, 78, 88, 92, 76, 84, 89, 95, 70, 82, 87, 91, 79]\n\n# Quick descriptive statistics with pandas\ndf = pd.DataFrame({'score': scores})\nprint(df.describe())\n</code></pre> <p>Output: <pre><code>            score\ncount   15.000000\nmean    83.866667\nstd      7.577722\nmin     70.000000\n25%     78.000000\n50%     85.000000\n75%     90.000000\nmax     95.000000\n</code></pre></p> <p>In one command, you get count, mean, standard deviation, min, max, and quartiles. Let's understand each of these.</p>"},{"location":"chapters/06-statistical-foundations/#measures-of-central-tendency","title":"Measures of Central Tendency","text":""},{"location":"chapters/06-statistical-foundations/#mean-the-arithmetic-average","title":"Mean: The Arithmetic Average","text":"<p>The mean is what most people call \"the average.\" Add up all the values and divide by how many there are:</p> \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + ... + x_n}{n}\\] <pre><code>scores = [72, 85, 90, 78, 88]\n\n# Calculate mean\nmean_score = sum(scores) / len(scores)\nprint(f\"Mean: {mean_score}\")  # Output: 82.6\n\n# Or use numpy/pandas\nimport numpy as np\nprint(f\"Mean: {np.mean(scores)}\")  # Output: 82.6\n</code></pre> <p>The mean is intuitive and mathematically convenient, but it has a weakness: it's sensitive to outliers.</p> <pre><code># What happens with an outlier?\nscores_with_outlier = [72, 85, 90, 78, 88, 250]  # Someone scored 250? (data error)\nprint(f\"Mean with outlier: {np.mean(scores_with_outlier)}\")  # Output: 110.5 (misleading!)\n</code></pre> <p>One extreme value pulled the mean way up. For this reason, we sometimes use the median instead.</p>"},{"location":"chapters/06-statistical-foundations/#median-the-middle-value","title":"Median: The Middle Value","text":"<p>The median is the middle value when data is sorted. Half the values are below it, half are above.</p> <pre><code>scores = [72, 85, 90, 78, 88]\n\n# Sort: [72, 78, 85, 88, 90]\n# Middle value: 85\n\nprint(f\"Median: {np.median(scores)}\")  # Output: 85.0\n\n# With the outlier\nscores_with_outlier = [72, 85, 90, 78, 88, 250]\n# Sort: [72, 78, 85, 88, 90, 250]\n# Middle: average of 85 and 88 = 86.5\n\nprint(f\"Median with outlier: {np.median(scores_with_outlier)}\")  # Output: 86.5 (much more reasonable!)\n</code></pre> <p>The median is robust to outliers\u2014extreme values don't affect it much. Use median when your data might have outliers or is skewed.</p>"},{"location":"chapters/06-statistical-foundations/#mode-the-most-common-value","title":"Mode: The Most Common Value","text":"<p>The mode is the value that appears most frequently. It's the only measure of central tendency that works for categorical data.</p> <pre><code>from scipy import stats\n\n# Numeric mode\ntest_scores = [85, 90, 85, 78, 85, 92, 90, 85]\nprint(f\"Mode: {stats.mode(test_scores, keepdims=True).mode[0]}\")  # Output: 85\n\n# Categorical mode\nfavorite_colors = ['blue', 'red', 'blue', 'green', 'blue', 'red']\nprint(f\"Mode: {stats.mode(favorite_colors, keepdims=True).mode[0]}\")  # Output: blue\n</code></pre> <p>Data can have multiple modes (bimodal, multimodal) or no mode at all if every value appears once.</p> Measure Best For Sensitive to Outliers? Mean Symmetric data, further calculations Yes Median Skewed data, outliers present No Mode Categorical data, finding most common No"},{"location":"chapters/06-statistical-foundations/#diagram-central-tendency-comparison-microsim","title":"Diagram: Central Tendency Comparison MicroSim","text":"Mean, Median, Mode Interactive Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students visualize how mean, median, and mode respond differently to data changes and outliers</p> <p>Canvas layout (800x500px): - Top (800x300): Interactive histogram with draggable data points - Bottom (800x200): Statistics display and controls</p> <p>Visual elements: - Histogram showing data distribution - Vertical lines for mean (red), median (green), mode (blue) - Individual data points displayed as draggable circles below histogram - Statistics panel showing current values</p> <p>Interactive controls: - Draggable data points: Click and drag any point to change its value - \"Add Point\" button: Add new data point - \"Add Outlier\" button: Add extreme value - \"Remove Point\" button: Click to remove - \"Reset\" button: Return to original dataset - Dropdown: Preset distributions (symmetric, left-skewed, right-skewed, bimodal)</p> <p>Initial dataset: - 20 points normally distributed around 50</p> <p>Behavior: - All three measures update in real-time as points are dragged - Visual indication when mean and median diverge significantly - Highlight which measure is \"best\" for current distribution - Animation when adding outliers to show mean shifting</p> <p>Educational annotations: - \"Notice how the mean moves toward the outlier\" - \"The median stays stable!\" - \"Mode shows the peak of the distribution\"</p> <p>Challenge tasks: - \"Make the mean equal to the median\" - \"Create a distribution where mode \u2260 median \u2260 mean\" - \"Add an outlier that changes the mean by at least 10\"</p> <p>Visual style: Clean statistical visualization with color-coded measures</p> <p>Implementation: p5.js with real-time statistical calculations</p>"},{"location":"chapters/06-statistical-foundations/#measures-of-spread","title":"Measures of Spread","text":"<p>Knowing the center isn't enough\u2014you also need to know how spread out the data is. Two datasets can have the same mean but very different spreads.</p>"},{"location":"chapters/06-statistical-foundations/#range-simplest-spread-measure","title":"Range: Simplest Spread Measure","text":"<p>The range is simply the difference between the maximum and minimum values:</p> \\[\\text{Range} = \\text{Max} - \\text{Min}\\] <pre><code>scores = [72, 85, 90, 78, 88, 92, 76]\n\nrange_value = max(scores) - min(scores)\nprint(f\"Range: {range_value}\")  # Output: 20\n</code></pre> <p>Range is easy to understand but has limitations: it only uses two values and is very sensitive to outliers.</p>"},{"location":"chapters/06-statistical-foundations/#variance-average-squared-deviation","title":"Variance: Average Squared Deviation","text":"<p>Variance measures how far each value is from the mean, on average. It squares the deviations (so negatives don't cancel positives):</p> \\[\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\] <pre><code>import numpy as np\n\nscores = [72, 85, 90, 78, 88]\nmean = np.mean(scores)\n\n# Calculate variance step by step\ndeviations = [(x - mean) for x in scores]  # How far from mean\nsquared_deviations = [d**2 for d in deviations]  # Square them\nvariance = sum(squared_deviations) / len(scores)  # Average\n\nprint(f\"Variance: {variance}\")  # Output: 41.04\n\n# Or simply:\nprint(f\"Variance: {np.var(scores)}\")  # Output: 41.04\n</code></pre> <p>The problem with variance? The units are squared. If your data is in meters, variance is in meters\u00b2. That's hard to interpret.</p>"},{"location":"chapters/06-statistical-foundations/#standard-deviation-the-useful-spread-measure","title":"Standard Deviation: The Useful Spread Measure","text":"<p>Standard deviation is the square root of variance, bringing us back to the original units:</p> \\[\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\] <pre><code>import numpy as np\n\nscores = [72, 85, 90, 78, 88]\n\nstd_dev = np.std(scores)\nprint(f\"Standard Deviation: {std_dev:.2f}\")  # Output: 6.41\n</code></pre> <p>Standard deviation tells you how much values typically deviate from the mean. In a normal distribution:</p> <ul> <li>~68% of data falls within 1 standard deviation of the mean</li> <li>~95% falls within 2 standard deviations</li> <li>~99.7% falls within 3 standard deviations</li> </ul> <p>This is called the 68-95-99.7 rule (or empirical rule).</p> <p>Population vs Sample</p> <p>When calculating variance/standard deviation for a sample (subset of data), divide by \\(n-1\\) instead of \\(n\\). This corrects for bias. Use <code>np.var(data, ddof=1)</code> or <code>np.std(data, ddof=1)</code> for sample statistics.</p> <pre><code># Population standard deviation (divide by n)\nnp.std(scores, ddof=0)\n\n# Sample standard deviation (divide by n-1) - use this most of the time!\nnp.std(scores, ddof=1)\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#quartiles-percentiles-and-iqr","title":"Quartiles, Percentiles, and IQR","text":""},{"location":"chapters/06-statistical-foundations/#quartiles-dividing-data-into-fourths","title":"Quartiles: Dividing Data into Fourths","text":"<p>Quartiles divide sorted data into four equal parts:</p> <ul> <li>Q1 (25th percentile): 25% of data is below this value</li> <li>Q2 (50th percentile): The median\u201450% below, 50% above</li> <li>Q3 (75th percentile): 75% of data is below this value</li> </ul> <pre><code>import numpy as np\n\nscores = [72, 75, 78, 80, 82, 85, 88, 90, 92, 95, 98]\n\nq1 = np.percentile(scores, 25)\nq2 = np.percentile(scores, 50)  # Same as median\nq3 = np.percentile(scores, 75)\n\nprint(f\"Q1: {q1}, Q2: {q2}, Q3: {q3}\")  # Output: Q1: 78.5, Q2: 85.0, Q3: 92.5\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#percentiles-any-division-you-want","title":"Percentiles: Any Division You Want","text":"<p>Percentiles generalize quartiles\u2014the Pth percentile is the value below which P% of the data falls.</p> <pre><code># What score puts you in the top 10%?\ntop_10_cutoff = np.percentile(scores, 90)\nprint(f\"90th percentile: {top_10_cutoff}\")\n\n# What percentile is a score of 85?\n# Use scipy for this\nfrom scipy import stats\npercentile_of_85 = stats.percentileofscore(scores, 85)\nprint(f\"85 is at the {percentile_of_85}th percentile\")\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#interquartile-range-iqr","title":"Interquartile Range (IQR)","text":"<p>The interquartile range is the range of the middle 50% of data:</p> \\[\\text{IQR} = Q3 - Q1\\] <pre><code>iqr = q3 - q1\nprint(f\"IQR: {iqr}\")  # Output: 14.0\n\n# Or use scipy\nfrom scipy.stats import iqr as calc_iqr\nprint(f\"IQR: {calc_iqr(scores)}\")\n</code></pre> <p>IQR is robust to outliers and is used to detect them: any value below \\(Q1 - 1.5 \\times IQR\\) or above \\(Q3 + 1.5 \\times IQR\\) is considered an outlier.</p> <pre><code># Outlier detection using IQR\nlower_fence = q1 - 1.5 * iqr\nupper_fence = q3 + 1.5 * iqr\n\noutliers = [x for x in scores if x &lt; lower_fence or x &gt; upper_fence]\nprint(f\"Outliers: {outliers}\")\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#diagram-box-plot-anatomy","title":"Diagram: Box Plot Anatomy","text":"Interactive Box Plot Anatomy <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Help students identify and remember the components of a box plot</p> <p>Purpose: Visual breakdown of box plot structure with labeled components</p> <p>Layout: Central box plot with callouts pointing to each component</p> <p>Main visual: A horizontal box plot with sample data showing: - Whisker extending left to minimum (non-outlier) - Box from Q1 to Q3 - Median line inside box - Whisker extending right to maximum (non-outlier) - Two outlier points beyond whiskers</p> <p>Callouts (numbered with leader lines):</p> <ol> <li>MINIMUM (pointing to left whisker end)</li> <li>\"Smallest non-outlier value\"</li> <li>\"= Q1 - 1.5\u00d7IQR or actual min, whichever is larger\"</li> <li> <p>Color: Blue</p> </li> <li> <p>Q1 / FIRST QUARTILE (pointing to left edge of box)</p> </li> <li>\"25% of data below this\"</li> <li>\"Left edge of box\"</li> <li> <p>Color: Green</p> </li> <li> <p>MEDIAN / Q2 (pointing to line inside box)</p> </li> <li>\"50% of data below this\"</li> <li>\"Center line in box\"</li> <li> <p>Color: Red</p> </li> <li> <p>Q3 / THIRD QUARTILE (pointing to right edge of box)</p> </li> <li>\"75% of data below this\"</li> <li>\"Right edge of box\"</li> <li> <p>Color: Green</p> </li> <li> <p>MAXIMUM (pointing to right whisker end)</p> </li> <li>\"Largest non-outlier value\"</li> <li>\"= Q3 + 1.5\u00d7IQR or actual max, whichever is smaller\"</li> <li> <p>Color: Blue</p> </li> <li> <p>IQR (bracket spanning the box)</p> </li> <li>\"Interquartile Range = Q3 - Q1\"</li> <li>\"Contains middle 50% of data\"</li> <li> <p>Color: Orange</p> </li> <li> <p>OUTLIERS (pointing to dots beyond whiskers)</p> </li> <li>\"Values beyond 1.5\u00d7IQR from box\"</li> <li>\"Shown as individual points\"</li> <li>Color: Purple</li> </ol> <p>Bottom section: \"What box plots tell you at a glance\" - Center (median position) - Spread (box width) - Symmetry (median position within box) - Outliers (individual points)</p> <p>Interactive elements: - Hover over each component to highlight it - Click to see formula or code to calculate - Toggle between horizontal and vertical orientation</p> <p>Implementation: SVG with CSS hover effects and JavaScript interactivity</p>"},{"location":"chapters/06-statistical-foundations/#distribution-shape-skewness-and-kurtosis","title":"Distribution Shape: Skewness and Kurtosis","text":""},{"location":"chapters/06-statistical-foundations/#skewness-leaning-left-or-right","title":"Skewness: Leaning Left or Right","text":"<p>Skewness measures asymmetry in a distribution:</p> <ul> <li>Negative skew (left-skewed): Tail extends to the left; mean &lt; median</li> <li>Zero skew: Symmetric; mean \u2248 median</li> <li>Positive skew (right-skewed): Tail extends to the right; mean &gt; median</li> </ul> <pre><code>from scipy.stats import skew\n\n# Symmetric data\nsymmetric = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(f\"Symmetric skewness: {skew(symmetric):.3f}\")  # Close to 0\n\n# Right-skewed (like income data)\nright_skewed = [1, 2, 2, 3, 3, 3, 4, 4, 5, 10, 15, 20]\nprint(f\"Right-skewed: {skew(right_skewed):.3f}\")  # Positive\n\n# Left-skewed\nleft_skewed = [1, 5, 10, 15, 16, 16, 17, 17, 17, 18, 18, 19]\nprint(f\"Left-skewed: {skew(left_skewed):.3f}\")  # Negative\n</code></pre> <p>Real-world examples:</p> <ul> <li>Right-skewed: Income, house prices, social media followers</li> <li>Left-skewed: Age at death in developed countries, exam scores (if test is easy)</li> </ul>"},{"location":"chapters/06-statistical-foundations/#kurtosis-tails-and-peaks","title":"Kurtosis: Tails and Peaks","text":"<p>Kurtosis measures the \"tailedness\" of a distribution\u2014how much data is in the extreme tails versus the center:</p> <ul> <li>Positive kurtosis (leptokurtic): Heavy tails, sharp peak, more outliers</li> <li>Zero kurtosis (mesokurtic): Normal distribution</li> <li>Negative kurtosis (platykurtic): Light tails, flat peak, fewer outliers</li> </ul> <pre><code>from scipy.stats import kurtosis\n\n# Normal distribution has kurtosis \u2248 0 (with Fisher's definition)\nnormal_data = np.random.normal(0, 1, 10000)\nprint(f\"Normal kurtosis: {kurtosis(normal_data):.3f}\")  # Close to 0\n\n# Heavy tails (more extreme values)\nheavy_tails = np.concatenate([np.random.normal(0, 1, 9000),\n                              np.random.normal(0, 5, 1000)])\nprint(f\"Heavy tails kurtosis: {kurtosis(heavy_tails):.3f}\")  # Positive\n</code></pre> Skewness Distribution Shape Mean vs Median Negative Left tail longer Mean &lt; Median Zero Symmetric Mean \u2248 Median Positive Right tail longer Mean &gt; Median"},{"location":"chapters/06-statistical-foundations/#understanding-distributions","title":"Understanding Distributions","text":"<p>A distribution describes how values in a dataset are spread across different possible values. It shows the frequency or probability of each value occurring.</p>"},{"location":"chapters/06-statistical-foundations/#the-normal-distribution","title":"The Normal Distribution","text":"<p>The normal distribution (also called Gaussian or bell curve) is the most important distribution in statistics. It appears everywhere:</p> <ul> <li>Heights of people</li> <li>Measurement errors</li> <li>Test scores (often)</li> <li>Many natural phenomena</li> </ul> <pre><code>import numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Generate normal distribution\nnp.random.seed(42)\ndata = np.random.normal(loc=100, scale=15, size=10000)  # Mean=100, StdDev=15\n\n# Interactive histogram with Plotly\nfig = px.histogram(data, nbins=50, title='Normal Distribution (\u03bc=100, \u03c3=15)')\nfig.update_layout(\n    xaxis_title='Value',\n    yaxis_title='Frequency',\n    showlegend=False\n)\nfig.show()\n</code></pre> <p>Key properties of normal distributions:</p> <ul> <li>Symmetric around the mean</li> <li>Mean = Median = Mode</li> <li>Defined by two parameters: mean (\u03bc) and standard deviation (\u03c3)</li> <li>The 68-95-99.7 rule applies</li> </ul> <pre><code># Visualize the 68-95-99.7 rule\nmean, std = 100, 15\n\nwithin_1_std = np.sum((data &gt;= mean - std) &amp; (data &lt;= mean + std)) / len(data)\nwithin_2_std = np.sum((data &gt;= mean - 2*std) &amp; (data &lt;= mean + 2*std)) / len(data)\nwithin_3_std = np.sum((data &gt;= mean - 3*std) &amp; (data &lt;= mean + 3*std)) / len(data)\n\nprint(f\"Within 1 std: {within_1_std:.1%}\")  # ~68%\nprint(f\"Within 2 std: {within_2_std:.1%}\")  # ~95%\nprint(f\"Within 3 std: {within_3_std:.1%}\")  # ~99.7%\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#diagram-normal-distribution-explorer-microsim","title":"Diagram: Normal Distribution Explorer MicroSim","text":"Interactive Normal Distribution Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Let students manipulate mean and standard deviation to understand how they affect the normal distribution shape</p> <p>Canvas layout (850x550px): - Main area (850x400): Interactive normal distribution plot - Control panel (850x150): Sliders and statistics</p> <p>Visual elements: - Smooth normal distribution curve - Shaded regions showing 1\u03c3, 2\u03c3, 3\u03c3 areas - Vertical line at mean - Axis labels and tick marks - Current \u03bc and \u03c3 displayed prominently</p> <p>Interactive controls: - Slider: Mean (\u03bc) range: 0 to 200, default: 100 - Slider: Standard Deviation (\u03c3) range: 1 to 50, default: 15 - Toggle: Show 68-95-99.7 regions - Toggle: Show probability density values - Button: \"Add second distribution\" (for comparison) - Dropdown: Preset examples (IQ scores, heights, test scores)</p> <p>Display panels: - Probability within 1\u03c3: 68.27% - Probability within 2\u03c3: 95.45% - Probability within 3\u03c3: 99.73% - Current curve equation</p> <p>Behavior: - Curve updates smoothly as sliders move - Shaded regions resize with \u03c3 changes - Curve shifts horizontally with \u03bc changes - Comparison mode overlays two distributions</p> <p>Educational annotations: - \"Larger \u03c3 = wider, flatter curve\" - \"Smaller \u03c3 = narrower, taller curve\" - \"\u03bc shifts the center, \u03c3 changes the spread\"</p> <p>Challenge tasks: - \"Set parameters to match IQ distribution (\u03bc=100, \u03c3=15)\" - \"What \u03c3 makes 95% fall between 60 and 140?\" - \"Compare two distributions: same mean, different spread\"</p> <p>Visual style: Clean mathematical visualization with Plotly-like aesthetics</p> <p>Implementation: p5.js or Plotly.js with real-time updates</p>"},{"location":"chapters/06-statistical-foundations/#probability-fundamentals","title":"Probability Fundamentals","text":"<p>Probability is the mathematical framework for quantifying uncertainty. It assigns a number between 0 and 1 to events:</p> <ul> <li>P = 0: Impossible</li> <li>P = 1: Certain</li> <li>P = 0.5: Equal chance of happening or not</li> </ul> <pre><code># Probability of rolling a 6 on a fair die\np_roll_6 = 1 / 6\nprint(f\"P(roll 6) = {p_roll_6:.4f}\")  # 0.1667\n\n# Probability of flipping heads\np_heads = 1 / 2\nprint(f\"P(heads) = {p_heads}\")  # 0.5\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#random-variables-and-expected-value","title":"Random Variables and Expected Value","text":"<p>A random variable is a variable whose value depends on random outcomes. It can be:</p> <ul> <li>Discrete: Takes specific values (dice roll: 1, 2, 3, 4, 5, 6)</li> <li>Continuous: Takes any value in a range (height: 5.5, 5.51, 5.512...)</li> </ul> <p>The expected value (E[X]) is the long-run average\u2014what you'd expect on average over many repetitions:</p> \\[E[X] = \\sum_{i} x_i \\cdot P(x_i)\\] <pre><code># Expected value of a fair die roll\noutcomes = [1, 2, 3, 4, 5, 6]\nprobabilities = [1/6] * 6\n\nexpected_value = sum(x * p for x, p in zip(outcomes, probabilities))\nprint(f\"Expected value of die roll: {expected_value:.2f}\")  # 3.5\n\n# Verify with simulation\nrolls = np.random.randint(1, 7, 100000)\nprint(f\"Simulated average: {rolls.mean():.2f}\")  # ~3.5\n</code></pre> <p>The expected value of a fair die is 3.5\u2014you can never actually roll 3.5, but it's the average outcome over time.</p>"},{"location":"chapters/06-statistical-foundations/#sampling-from-population-to-sample","title":"Sampling: From Population to Sample","text":""},{"location":"chapters/06-statistical-foundations/#population-vs-sample","title":"Population vs Sample","text":"<p>A population is the entire group you want to study. A sample is a subset you actually measure.</p> <ul> <li>Population: All high school students in the US</li> <li>Sample: 1,000 randomly selected high school students</li> </ul> <p>Sampling is the process of selecting a sample from a population. Good sampling is crucial\u2014a biased sample leads to wrong conclusions.</p> <pre><code>import numpy as np\n\n# Population: All test scores (imagine this is millions of values)\nnp.random.seed(42)\npopulation = np.random.normal(75, 10, 1000000)  # Mean=75, StdDev=10\n\n# Sample: We can only survey 100 students\nsample = np.random.choice(population, size=100, replace=False)\n\nprint(f\"Population mean: {population.mean():.2f}\")\nprint(f\"Sample mean: {sample.mean():.2f}\")\nprint(f\"Difference: {abs(population.mean() - sample.mean()):.2f}\")\n</code></pre> <p>The sample mean estimates the population mean, but there's always some error. That's where the Central Limit Theorem helps.</p>"},{"location":"chapters/06-statistical-foundations/#the-central-limit-theorem-clt","title":"The Central Limit Theorem (CLT)","text":"<p>The Central Limit Theorem is one of the most important results in statistics. It says:</p> <p>When you take many random samples from ANY population and calculate the mean of each sample, those sample means will be approximately normally distributed\u2014regardless of the original population's distribution.</p> <p>This is magical because:</p> <ol> <li>It works for any population shape (uniform, skewed, bimodal...)</li> <li>The distribution of sample means gets more normal as sample size increases</li> <li>It lets us make probability statements about sample means</li> </ol> <pre><code>import numpy as np\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nnp.random.seed(42)\n\n# Start with a NON-normal population (uniform distribution)\npopulation = np.random.uniform(0, 100, 100000)\n\n# Take many samples and calculate their means\nsample_size = 30\nnum_samples = 1000\n\nsample_means = [np.random.choice(population, size=sample_size).mean()\n                for _ in range(num_samples)]\n\n# The sample means are normally distributed!\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=['Original Population (Uniform)',\n                                   'Distribution of Sample Means (Normal!)'])\n\nfig.add_trace(go.Histogram(x=population, nbinsx=50, name='Population'),\n              row=1, col=1)\nfig.add_trace(go.Histogram(x=sample_means, nbinsx=30, name='Sample Means'),\n              row=1, col=2)\n\nfig.update_layout(title='Central Limit Theorem in Action')\nfig.show()\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#diagram-central-limit-theorem-simulator-microsim","title":"Diagram: Central Limit Theorem Simulator MicroSim","text":"Central Limit Theorem Interactive Demonstration <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help students understand the CLT by visualizing how sample means become normally distributed regardless of population shape</p> <p>Canvas layout (900x600px): - Left panel (450x600): Population distribution - Right panel (450x600): Distribution of sample means - Bottom strip (900x100): Controls</p> <p>Visual elements: - Left: Histogram of original population - Right: Histogram of sample means (builds up over time) - Normal curve overlay on right panel - Running statistics display</p> <p>Interactive controls: - Dropdown: Population distribution type   - Normal   - Uniform   - Exponential (right-skewed)   - Bimodal   - Custom (draw your own!) - Slider: Sample size (5, 10, 30, 50, 100) - Button: \"Take One Sample\" (animated) - Button: \"Take 100 Samples\" (fast) - Button: \"Take 1000 Samples\" (bulk) - Button: \"Reset\" - Slider: Animation speed</p> <p>Display panels: - Population mean and std - Mean of sample means - Std of sample means (should \u2248 \u03c3/\u221an) - Number of samples taken</p> <p>Behavior: - \"Take One Sample\" animates: highlight sample from population, calculate mean, add to right histogram - Sample means histogram builds up gradually - Normal curve overlay adjusts to fit data - Show how larger sample sizes make sample means distribution narrower</p> <p>Educational annotations: - \"Notice: Even though population is [skewed/uniform], sample means are normal!\" - \"Larger samples \u2192 narrower distribution of means\" - \"Standard error = \u03c3/\u221an\"</p> <p>Challenge tasks: - \"Which sample size makes sample means most normal?\" - \"Predict the std of sample means for n=100\" - \"Try the most extreme distribution\u2014CLT still works!\"</p> <p>Visual style: Side-by-side comparison with animation</p> <p>Implementation: p5.js with smooth animations and Plotly for histograms</p>"},{"location":"chapters/06-statistical-foundations/#confidence-intervals-quantifying-uncertainty","title":"Confidence Intervals: Quantifying Uncertainty","text":"<p>A confidence interval gives a range that likely contains the true population parameter. Instead of saying \"the average is 75,\" you say \"I'm 95% confident the average is between 72 and 78.\"</p> <pre><code>import numpy as np\nfrom scipy import stats\n\n# Sample data\nsample = np.random.normal(75, 10, 100)\n\n# Calculate 95% confidence interval for the mean\nconfidence = 0.95\nmean = sample.mean()\nse = stats.sem(sample)  # Standard error\nci = stats.t.interval(confidence, len(sample)-1, loc=mean, scale=se)\n\nprint(f\"Sample mean: {mean:.2f}\")\nprint(f\"95% CI: ({ci[0]:.2f}, {ci[1]:.2f})\")\n</code></pre> <p>The interpretation: If we repeated this sampling process many times, 95% of the confidence intervals we calculate would contain the true population mean.</p> <p>Common Misconception</p> <p>A 95% confidence interval does NOT mean \"there's a 95% probability the true value is in this range.\" The true value either is or isn't in the range\u2014we just don't know which. The 95% refers to the long-run success rate of the method.</p>"},{"location":"chapters/06-statistical-foundations/#hypothesis-testing-making-decisions-with-data","title":"Hypothesis Testing: Making Decisions with Data","text":"<p>Hypothesis testing is a framework for making decisions based on data. You start with a hypothesis and use data to evaluate whether the evidence supports it.</p>"},{"location":"chapters/06-statistical-foundations/#the-process","title":"The Process","text":"<ol> <li>State the null hypothesis (H\u2080): The default assumption (usually \"no effect\" or \"no difference\")</li> <li>State the alternative hypothesis (H\u2081): What you're testing for</li> <li>Collect data and calculate a test statistic</li> <li>Calculate the p-value</li> <li>Make a decision based on significance level</li> </ol>"},{"location":"chapters/06-statistical-foundations/#p-value-the-evidence-measure","title":"P-Value: The Evidence Measure","text":"<p>The p-value is the probability of seeing results at least as extreme as yours, assuming the null hypothesis is true.</p> <ul> <li>Small p-value (&lt; 0.05): Evidence against H\u2080; reject it</li> <li>Large p-value (\u2265 0.05): Not enough evidence; fail to reject H\u2080</li> </ul> <pre><code>from scipy import stats\n\n# Example: Testing if a coin is fair\n# Flip 100 times, get 60 heads\n# H\u2080: coin is fair (p = 0.5)\n# H\u2081: coin is not fair (p \u2260 0.5)\n\nn_flips = 100\nn_heads = 60\n\n# Binomial test\nresult = stats.binomtest(n_heads, n_flips, p=0.5, alternative='two-sided')\nprint(f\"P-value: {result.pvalue:.4f}\")\n\nif result.pvalue &lt; 0.05:\n    print(\"Reject H\u2080: The coin appears to be unfair\")\nelse:\n    print(\"Fail to reject H\u2080: No evidence the coin is unfair\")\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#statistical-significance","title":"Statistical Significance","text":"<p>Statistical significance means the p-value is below a predetermined threshold (usually 0.05). It indicates the result is unlikely to have occurred by chance alone.</p> P-value Interpretation &lt; 0.001 Very strong evidence against H\u2080 &lt; 0.01 Strong evidence against H\u2080 &lt; 0.05 Evidence against H\u2080 (significant) \u2265 0.05 Insufficient evidence against H\u2080 <p>Statistical vs Practical Significance</p> <p>A result can be statistically significant but practically meaningless. If a drug reduces blood pressure by 0.1 mmHg and it's significant with p &lt; 0.001, so what? That's too small to matter clinically. Always consider effect size, not just p-values.</p>"},{"location":"chapters/06-statistical-foundations/#diagram-hypothesis-testing-workflow","title":"Diagram: Hypothesis Testing Workflow","text":"Hypothesis Testing Decision Flowchart <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Guide students through the hypothesis testing process step by step</p> <p>Purpose: Visual decision tree for conducting hypothesis tests</p> <p>Visual style: Vertical flowchart with decision diamonds and process rectangles</p> <p>Steps (top to bottom):</p> <ol> <li> <p>START: \"Research Question\"    Hover text: \"What are you trying to determine?\"    Color: Blue</p> </li> <li> <p>PROCESS: \"State Hypotheses\"</p> </li> <li>H\u2080: Null hypothesis (no effect/difference)</li> <li> <p>H\u2081: Alternative hypothesis (effect exists)    Hover text: \"H\u2080 is what you're trying to disprove\"    Color: Green</p> </li> <li> <p>PROCESS: \"Choose Significance Level (\u03b1)\"</p> </li> <li> <p>Usually \u03b1 = 0.05    Hover text: \"This is your threshold for 'unlikely'\"    Color: Green</p> </li> <li> <p>PROCESS: \"Collect Data &amp; Calculate Test Statistic\"    Hover text: \"t-test, chi-square, etc. depending on your data\"    Color: Green</p> </li> <li> <p>PROCESS: \"Calculate P-value\"    Hover text: \"Probability of seeing this result if H\u2080 is true\"    Color: Orange</p> </li> <li> <p>DECISION: \"Is p-value &lt; \u03b1?\"    Color: Yellow</p> </li> </ol> <p>7a. YES PATH: \"Reject H\u2080\"    - \"Results are statistically significant\"    - \"Evidence supports H\u2081\"    Hover text: \"But also check effect size!\"    Color: Red</p> <p>7b. NO PATH: \"Fail to Reject H\u2080\"    - \"Results are not statistically significant\"    - \"Insufficient evidence for H\u2081\"    Hover text: \"This doesn't prove H\u2080 is true!\"    Color: Gray</p> <ol> <li>END: \"Report Results\"</li> <li>Include: test statistic, p-value, effect size, confidence interval    Color: Blue</li> </ol> <p>Side annotations: - \"Type I Error (\u03b1): Rejecting H\u2080 when it's actually true\" - \"Type II Error (\u03b2): Failing to reject H\u2080 when it's actually false\"</p> <p>Interactive elements: - Hover over each step for detailed explanation - Click to see Python code for that step - Example problems that walk through the flowchart</p> <p>Implementation: SVG with JavaScript interactivity</p>"},{"location":"chapters/06-statistical-foundations/#correlation-measuring-relationships","title":"Correlation: Measuring Relationships","text":"<p>Correlation measures the strength and direction of a linear relationship between two variables. It ranges from -1 to +1:</p> <ul> <li>+1: Perfect positive correlation (as X increases, Y increases)</li> <li>0: No linear correlation</li> <li>-1: Perfect negative correlation (as X increases, Y decreases)</li> </ul>"},{"location":"chapters/06-statistical-foundations/#covariance-the-building-block","title":"Covariance: The Building Block","text":"<p>Covariance measures how two variables change together:</p> \\[\\text{Cov}(X, Y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\\] <pre><code>import numpy as np\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 4, 5]\n\ncovariance = np.cov(x, y)[0, 1]\nprint(f\"Covariance: {covariance:.2f}\")\n</code></pre> <p>The problem with covariance: it's affected by the scale of the variables. Covariance between height in inches and weight in pounds will be different from height in centimeters and weight in kilograms.</p>"},{"location":"chapters/06-statistical-foundations/#pearson-correlation-the-standard-measure","title":"Pearson Correlation: The Standard Measure","text":"<p>Pearson correlation standardizes covariance to a -1 to +1 scale:</p> \\[r = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\\] <pre><code>import numpy as np\nfrom scipy import stats\n\n# Example: Study hours vs exam score\nstudy_hours = [1, 2, 3, 4, 5, 6, 7, 8]\nexam_scores = [50, 55, 65, 70, 72, 80, 85, 90]\n\n# Calculate Pearson correlation\nr, p_value = stats.pearsonr(study_hours, exam_scores)\nprint(f\"Pearson r: {r:.3f}\")\nprint(f\"P-value: {p_value:.4f}\")\n</code></pre> <p>Pearson correlation assumes:</p> <ul> <li>Linear relationship</li> <li>Both variables are continuous</li> <li>Data is normally distributed (roughly)</li> </ul>"},{"location":"chapters/06-statistical-foundations/#spearman-correlation-for-non-linear-relationships","title":"Spearman Correlation: For Non-Linear Relationships","text":"<p>Spearman correlation uses ranks instead of raw values, making it robust to:</p> <ul> <li>Non-linear relationships (as long as monotonic)</li> <li>Outliers</li> <li>Non-normal distributions</li> </ul> <pre><code>from scipy import stats\n\n# Data with a monotonic but non-linear relationship\nx = [1, 2, 3, 4, 5, 6, 7, 8]\ny = [1, 2, 4, 8, 16, 32, 64, 128]  # Exponential\n\npearson_r, _ = stats.pearsonr(x, y)\nspearman_r, _ = stats.spearmanr(x, y)\n\nprint(f\"Pearson r: {pearson_r:.3f}\")   # Lower because relationship isn't linear\nprint(f\"Spearman r: {spearman_r:.3f}\")  # 1.0 because relationship is monotonic\n</code></pre> Measure Measures Assumptions Best For Pearson Linear relationship Normal, continuous Linear relationships Spearman Monotonic relationship Ordinal or continuous Non-linear, ordinal data"},{"location":"chapters/06-statistical-foundations/#correlation-matrix-many-variables-at-once","title":"Correlation Matrix: Many Variables at Once","text":"<p>A correlation matrix shows correlations between all pairs of variables:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\n# Create sample data\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'hours_studied': np.random.normal(5, 2, 100),\n    'hours_sleep': np.random.normal(7, 1, 100),\n    'exam_score': np.random.normal(75, 10, 100)\n})\n# Add correlations\ndf['exam_score'] = df['exam_score'] + df['hours_studied'] * 3 - df['hours_sleep'] * 0.5\n\n# Calculate correlation matrix\ncorr_matrix = df.corr()\nprint(corr_matrix)\n\n# Interactive heatmap with Plotly\nfig = px.imshow(corr_matrix,\n                text_auto='.2f',\n                color_continuous_scale='RdBu',\n                title='Correlation Matrix')\nfig.show()\n</code></pre>"},{"location":"chapters/06-statistical-foundations/#diagram-correlation-visualizer-microsim","title":"Diagram: Correlation Visualizer MicroSim","text":"Interactive Correlation Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help students understand correlation through interactive visualization of scatter plots with different correlation strengths</p> <p>Canvas layout (850x550px): - Main area (550x500): Interactive scatter plot - Right panel (300x500): Controls and statistics - Bottom strip (850x50): Correlation strength indicator</p> <p>Visual elements: - Scatter plot with data points - Best-fit line (toggleable) - Correlation coefficient displayed prominently - Correlation strength meter (-1 to +1 scale)</p> <p>Interactive controls: - Slider: Target correlation (-1.0 to +1.0) - Button: \"Generate Data\" with current correlation - Slider: Number of points (20-200) - Slider: Noise level - Toggle: Show regression line - Toggle: Show confidence band - Dropdown: Preset examples (perfect positive, perfect negative, no correlation, moderate) - Draggable points: Move individual points to see effect</p> <p>Display panels: - Pearson r - Spearman r - P-value - R\u00b2 (coefficient of determination) - Sample size</p> <p>Behavior: - Adjusting correlation slider regenerates data with target correlation - Dragging individual points updates all statistics in real-time - Adding outliers shows how they affect Pearson vs Spearman - Noise slider shows how correlation degrades with noise</p> <p>Educational annotations: - \"r = 0.8 means strong positive relationship\" - \"Notice Spearman handles the outlier better!\" - \"R\u00b2 = 0.64 means 64% of variance in Y is explained by X\"</p> <p>Challenge tasks: - \"Create data with r \u2248 0 but clear pattern (try a curve!)\" - \"Add an outlier that changes r by at least 0.2\" - \"Find the minimum sample size for statistical significance\"</p> <p>Visual style: Clean Plotly-like scatter plot with interactive elements</p> <p>Implementation: p5.js with statistical calculations</p> <p>Correlation \u2260 Causation</p> <p>The most important rule in statistics: correlation does not imply causation. Ice cream sales and drowning deaths are correlated (both increase in summer), but ice cream doesn't cause drowning. Always consider confounding variables and look for experimental evidence before claiming causation.</p>"},{"location":"chapters/06-statistical-foundations/#putting-statistics-into-practice","title":"Putting Statistics into Practice","text":"<p>Let's combine everything in a real analysis:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport plotly.express as px\n\n# Load or create dataset\nnp.random.seed(42)\nn = 200\ndf = pd.DataFrame({\n    'age': np.random.normal(35, 10, n).clip(18, 65).astype(int),\n    'income': np.random.normal(50000, 15000, n).clip(20000, 150000),\n    'education_years': np.random.normal(14, 3, n).clip(8, 22).astype(int),\n    'satisfaction': np.random.normal(7, 1.5, n).clip(1, 10)\n})\n# Add relationships\ndf['income'] = df['income'] + df['education_years'] * 2000 + np.random.normal(0, 5000, n)\n\n# 1. DESCRIPTIVE STATISTICS\nprint(\"=== Descriptive Statistics ===\")\nprint(df.describe())\nprint(f\"\\nSkewness:\\n{df.apply(stats.skew)}\")\n\n# 2. DISTRIBUTION VISUALIZATION\nfig = px.histogram(df, x='income', marginal='box',\n                   title='Income Distribution with Box Plot')\nfig.show()\n\n# 3. CORRELATION ANALYSIS\nprint(\"\\n=== Correlation Matrix ===\")\nprint(df.corr().round(3))\n\n# Interactive correlation heatmap\nfig = px.imshow(df.corr(), text_auto='.2f',\n                color_continuous_scale='RdBu',\n                title='Variable Correlations')\nfig.show()\n\n# 4. HYPOTHESIS TEST\n# Is there a significant correlation between education and income?\nr, p = stats.pearsonr(df['education_years'], df['income'])\nprint(f\"\\n=== Hypothesis Test: Education vs Income ===\")\nprint(f\"Pearson r: {r:.3f}\")\nprint(f\"P-value: {p:.4e}\")\nprint(f\"Result: {'Significant' if p &lt; 0.05 else 'Not significant'} correlation\")\n\n# 5. CONFIDENCE INTERVAL for mean income\nmean_income = df['income'].mean()\nse = stats.sem(df['income'])\nci = stats.t.interval(0.95, len(df)-1, loc=mean_income, scale=se)\nprint(f\"\\n=== 95% CI for Mean Income ===\")\nprint(f\"Mean: ${mean_income:,.0f}\")\nprint(f\"95% CI: (${ci[0]:,.0f}, ${ci[1]:,.0f})\")\n</code></pre> <p>This workflow demonstrates the complete statistical analysis pipeline:</p> <ol> <li>Summarize with descriptive statistics</li> <li>Visualize distributions</li> <li>Explore relationships with correlations</li> <li>Test hypotheses about relationships</li> <li>Quantify uncertainty with confidence intervals</li> </ol> Chapter 6 Checkpoint: Test Your Understanding <p>Question 1: A dataset has mean = 50 and median = 65. What can you infer about the distribution?</p> <p>Question 2: You test whether a new teaching method improves scores. The p-value is 0.03. What do you conclude at \u03b1 = 0.05?</p> <p>Question 3: Two variables have Pearson r = 0.85 and Spearman r = 0.60. What might explain this difference?</p> <p>Click to reveal answers:</p> <p>Answer 1: The distribution is left-skewed (negative skew). When mean &lt; median, the tail extends to the left, pulling the mean down.</p> <p>Answer 2: Since p = 0.03 &lt; \u03b1 = 0.05, you reject the null hypothesis. There is statistically significant evidence that the new teaching method affects scores. But check the effect size to see if the improvement is practically meaningful!</p> <p>Answer 3: The relationship is likely non-linear. Pearson measures linear correlation (strong here), but Spearman measures monotonic correlation (weaker). There might be outliers affecting Spearman, or the relationship curves rather than being perfectly monotonic.</p> <p>Achievement Unlocked: Statistical Thinker</p> <p>You now speak the language of uncertainty. You can summarize data with the right measures, understand distributions, measure relationships, and make probabilistic inferences. These skills separate people who \"look at data\" from people who truly understand what data is telling them.</p>"},{"location":"chapters/06-statistical-foundations/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Descriptive statistics summarize data: measures of central tendency (mean, median, mode) and spread (range, variance, standard deviation).</p> </li> <li> <p>Mean is sensitive to outliers; median is robust. Choose based on your data.</p> </li> <li> <p>Standard deviation measures typical distance from the mean; use the 68-95-99.7 rule for normal distributions.</p> </li> <li> <p>Quartiles and IQR divide data into parts and help identify outliers.</p> </li> <li> <p>Skewness measures asymmetry; kurtosis measures tail heaviness.</p> </li> <li> <p>The normal distribution is central to statistics\u2014the bell curve appears everywhere.</p> </li> <li> <p>Probability quantifies uncertainty; expected value is the long-run average.</p> </li> <li> <p>Samples estimate population parameters; sampling must be done carefully to avoid bias.</p> </li> <li> <p>The Central Limit Theorem says sample means are approximately normal, regardless of population shape.</p> </li> <li> <p>Confidence intervals quantify uncertainty about estimates; p-values measure evidence against hypotheses.</p> </li> <li> <p>Correlation measures relationship strength; Pearson for linear, Spearman for monotonic. Remember: correlation \u2260 causation!</p> </li> <li> <p>A correlation matrix shows all pairwise relationships at once.</p> </li> </ol> <p>You've built a solid statistical foundation. In the next chapter, you'll use these concepts to build your first predictive model with linear regression\u2014where the statistical concepts you just learned become the engine for making predictions!</p>"},{"location":"chapters/07-simple-linear-regression/","title":"Simple Linear Regression","text":""},{"location":"chapters/07-simple-linear-regression/#summary","title":"Summary","text":"<p>This chapter introduces regression analysis, the foundation of predictive modeling. Students will learn the mathematics behind linear regression, including the least squares method, interpreting coefficients (slope and intercept), and understanding residuals. The chapter covers regression assumptions and teaches students to implement linear regression using scikit-learn. By the end of this chapter, students will be able to build simple linear regression models, interpret their outputs, and make predictions.</p>"},{"location":"chapters/07-simple-linear-regression/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 25 concepts from the learning graph:</p> <ol> <li>Regression Analysis</li> <li>Linear Regression</li> <li>Simple Linear Regression</li> <li>Regression Line</li> <li>Slope</li> <li>Intercept</li> <li>Least Squares Method</li> <li>Residuals</li> <li>Sum of Squared Errors</li> <li>Ordinary Least Squares</li> <li>Regression Coefficients</li> <li>Coefficient Interpretation</li> <li>Prediction</li> <li>Fitted Values</li> <li>Regression Equation</li> <li>Line of Best Fit</li> <li>Assumptions of Regression</li> <li>Linearity Assumption</li> <li>Homoscedasticity</li> <li>Independence Assumption</li> <li>Normality of Residuals</li> <li>Scikit-learn Library</li> <li>LinearRegression Class</li> <li>Fit Method</li> <li>Predict Method</li> </ol>"},{"location":"chapters/07-simple-linear-regression/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 5: Data Visualization with Matplotlib</li> <li>Chapter 6: Statistical Foundations</li> </ul>"},{"location":"chapters/07-simple-linear-regression/#from-description-to-prediction","title":"From Description to Prediction","text":"<p>Everything you've learned so far has been about understanding data that already exists. Descriptive statistics summarize the past. Visualizations reveal patterns in historical data. Correlation shows relationships between variables.</p> <p>But here's where data science gets really exciting: prediction.</p> <p>What if, instead of just describing what happened, you could predict what will happen? What if you could look at a student's study hours and predict their exam score? Or see a house's square footage and estimate its price? Or know a car's age and forecast its fuel efficiency?</p> <p>This is the superpower of regression analysis\u2014the ability to draw a line through data that extends into the unknown future. It's the foundation of machine learning, the backbone of forecasting, and your first step into predictive modeling.</p> <p>In this chapter, you'll learn to build your first predictive model. It's surprisingly simple\u2014just a line\u2014but don't let that fool you. This humble line is one of the most powerful tools in all of data science.</p>"},{"location":"chapters/07-simple-linear-regression/#what-is-regression-analysis","title":"What is Regression Analysis?","text":"<p>Regression analysis is a statistical method for modeling the relationship between variables. It lets you:</p> <ol> <li>Understand how one variable affects another</li> <li>Quantify the strength of that relationship</li> <li>Predict values you haven't observed</li> </ol> <p>The term \"regression\" has a historical origin. In the 1880s, Francis Galton studied the heights of parents and children. He noticed that very tall parents tended to have children shorter than themselves, and very short parents had taller children. Heights \"regressed\" toward the average. The name stuck, even though modern regression is used for much more than studying heights.</p>"},{"location":"chapters/07-simple-linear-regression/#linear-regression-the-straight-line-model","title":"Linear Regression: The Straight-Line Model","text":"<p>Linear regression is the simplest form of regression\u2014it assumes the relationship between variables is a straight line. Despite its simplicity, linear regression is:</p> <ul> <li>Easy to understand and interpret</li> <li>Fast to compute</li> <li>Surprisingly effective for many real problems</li> <li>The foundation for more complex models</li> </ul> <p>When you have one input variable predicting one output variable, it's called simple linear regression. That's what we'll master in this chapter.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n# Example: Study hours vs exam score\nstudy_hours = [1, 2, 3, 4, 5, 6, 7, 8]\nexam_scores = [52, 58, 65, 71, 75, 82, 87, 92]\n\n# Create a DataFrame\ndf = pd.DataFrame({'study_hours': study_hours, 'exam_scores': exam_scores})\n\n# Scatter plot with trendline\nfig = px.scatter(df, x='study_hours', y='exam_scores',\n                 trendline='ols',\n                 title='Study Hours vs Exam Scores',\n                 labels={'study_hours': 'Hours Studied', 'exam_scores': 'Exam Score'})\nfig.show()\n</code></pre> <p>See that line Plotly drew through the data? That's a regression line\u2014your first predictive model! With it, you can predict the exam score for someone who studied 4.5 hours, even though you don't have that exact data point.</p>"},{"location":"chapters/07-simple-linear-regression/#the-regression-equation","title":"The Regression Equation","text":"<p>Every straight line can be described by an equation. You probably remember from algebra:</p> \\[y = mx + b\\] <p>In statistics, we write the regression equation as:</p> \\[\\hat{y} = \\beta_0 + \\beta_1 x\\] <p>Where:</p> <ul> <li>\\(\\hat{y}\\) (y-hat) = the predicted value</li> <li>\\(x\\) = the input variable (predictor, independent variable)</li> <li>\\(\\beta_0\\) = the intercept (where the line crosses the y-axis)</li> <li>\\(\\beta_1\\) = the slope (how much y changes for each unit increase in x)</li> </ul> <p>The \\(\\beta\\) values are called regression coefficients\u2014they define your model.</p>"},{"location":"chapters/07-simple-linear-regression/#understanding-slope","title":"Understanding Slope","text":"<p>The slope (\\(\\beta_1\\)) tells you the rate of change: for every one-unit increase in x, how much does y change?</p> <ul> <li>Positive slope: As x increases, y increases (uphill line)</li> <li>Negative slope: As x increases, y decreases (downhill line)</li> <li>Zero slope: x has no effect on y (horizontal line)</li> </ul> <pre><code># Example interpretation\nslope = 5.5  # Our model's slope\nprint(f\"Slope: {slope}\")\nprint(f\"Interpretation: For each additional hour of studying,\")\nprint(f\"the exam score increases by {slope} points on average.\")\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#understanding-intercept","title":"Understanding Intercept","text":"<p>The intercept (\\(\\beta_0\\)) is the predicted value when x = 0. It's where the line crosses the y-axis.</p> <pre><code>intercept = 47.5  # Our model's intercept\nprint(f\"Intercept: {intercept}\")\nprint(f\"Interpretation: A student who studies 0 hours\")\nprint(f\"would be predicted to score {intercept} points.\")\n</code></pre> <p>Intercept Interpretation Caution</p> <p>The intercept doesn't always have a meaningful interpretation. If x = 0 is outside your data range (like predicting house price for 0 square feet), don't interpret the intercept literally\u2014it's just a mathematical necessity for the line equation.</p>"},{"location":"chapters/07-simple-linear-regression/#diagram-regression-line-anatomy","title":"Diagram: Regression Line Anatomy","text":"Interactive Regression Line Components <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Help students identify and remember the components of a regression line and equation</p> <p>Purpose: Visual breakdown of regression line with labeled components</p> <p>Layout: Scatter plot with regression line and labeled callouts</p> <p>Main visual: Scatter plot (600x400px) showing: - 10-15 data points with clear linear trend - Regression line through points - Y-axis intercept clearly marked - Rise and run triangle showing slope</p> <p>Callouts (numbered with leader lines):</p> <ol> <li>INTERCEPT (\u03b2\u2080) (pointing to y-axis crossing)</li> <li>\"Where line crosses y-axis\"</li> <li>\"Predicted y when x = 0\"</li> <li>\"In equation: the constant term\"</li> <li> <p>Color: Blue</p> </li> <li> <p>SLOPE (\u03b2\u2081) (pointing to rise/run triangle)</p> </li> <li>\"Rise over run\"</li> <li>\"Change in y per unit change in x\"</li> <li>\"Positive = uphill, Negative = downhill\"</li> <li>Shows: \u0394y / \u0394x calculation</li> <li> <p>Color: Red</p> </li> <li> <p>PREDICTED VALUE (\u0177) (pointing to a point on the line)</p> </li> <li>\"Value predicted by the model\"</li> <li>\"Falls exactly on the line\"</li> <li>\"\u0177 = \u03b2\u2080 + \u03b2\u2081x\"</li> <li> <p>Color: Green</p> </li> <li> <p>ACTUAL VALUE (y) (pointing to a data point off the line)</p> </li> <li>\"Real observed value\"</li> <li>\"Usually not exactly on line\"</li> <li> <p>Color: Orange</p> </li> <li> <p>RESIDUAL (pointing to vertical line between actual and predicted)</p> </li> <li>\"Distance from actual to predicted\"</li> <li>\"Residual = y - \u0177\"</li> <li>\"What the model got wrong\"</li> <li>Color: Purple</li> </ol> <p>Bottom equation display: \u0177 = \u03b2\u2080 + \u03b2\u2081x With arrows pointing to each component in the equation</p> <p>Interactive elements: - Hover over each component for detailed explanation - Click to highlight related elements - Toggle to show/hide residuals for all points</p> <p>Implementation: SVG with JavaScript interactivity</p>"},{"location":"chapters/07-simple-linear-regression/#finding-the-best-line-least-squares-method","title":"Finding the Best Line: Least Squares Method","text":"<p>There are infinite lines you could draw through a scatter plot. So how do we find the best one? We use the least squares method.</p>"},{"location":"chapters/07-simple-linear-regression/#residuals-measuring-errors","title":"Residuals: Measuring Errors","text":"<p>A residual is the difference between an actual observed value and the value predicted by the model:</p> \\[\\text{Residual} = y - \\hat{y} = \\text{Actual} - \\text{Predicted}\\] <pre><code># Calculate residuals for our example\nactual_scores = [52, 58, 65, 71, 75, 82, 87, 92]\npredicted_scores = [47.5 + 5.5*h for h in [1, 2, 3, 4, 5, 6, 7, 8]]\n\nresiduals = [actual - predicted for actual, predicted in zip(actual_scores, predicted_scores)]\n\nfor i, (actual, pred, resid) in enumerate(zip(actual_scores, predicted_scores, residuals)):\n    print(f\"Point {i+1}: Actual={actual}, Predicted={pred:.1f}, Residual={resid:.1f}\")\n</code></pre> <p>Residuals tell us how wrong our predictions are:</p> <ul> <li>Positive residual: Model under-predicted (actual &gt; predicted)</li> <li>Negative residual: Model over-predicted (actual &lt; predicted)</li> <li>Zero residual: Perfect prediction (actual = predicted)</li> </ul>"},{"location":"chapters/07-simple-linear-regression/#sum-of-squared-errors-sse","title":"Sum of Squared Errors (SSE)","text":"<p>To find the best line, we want to minimize total error. But we can't just add up residuals\u2014positive and negative would cancel out! Instead, we square them first:</p> \\[\\text{SSE} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n}\\text{residual}_i^2\\] <p>This is the sum of squared errors (also called sum of squared residuals).</p> <pre><code># Calculate SSE\nsse = sum([r**2 for r in residuals])\nprint(f\"Sum of Squared Errors: {sse:.2f}\")\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#ordinary-least-squares-ols","title":"Ordinary Least Squares (OLS)","text":"<p>Ordinary Least Squares (OLS) is the method that finds the line minimizing SSE. It's the standard algorithm for linear regression.</p> <p>The math gives us formulas for the optimal coefficients:</p> \\[\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}\\] \\[\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\\] <p>Don't worry about memorizing these\u2014Python will calculate them for you. The important thing is understanding the concept: OLS finds the line that makes the squared prediction errors as small as possible.</p>"},{"location":"chapters/07-simple-linear-regression/#diagram-least-squares-microsim","title":"Diagram: Least Squares MicroSim","text":"Interactive Least Squares Line Fitting <p>Type: microsim</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help students understand how the least squares method finds the best-fit line by minimizing squared errors</p> <p>Canvas layout (900x600px): - Main area (650x550): Interactive scatter plot with adjustable line - Right panel (250x550): Controls and error display - Bottom strip (900x50): SSE meter</p> <p>Visual elements: - Scatter plot with 8-12 data points - Adjustable regression line (can drag slope and intercept) - Vertical lines from points to line showing residuals - Squares drawn at each residual (area = squared error) - Running SSE total displayed prominently</p> <p>Interactive controls: - Draggable line: Adjust slope by rotating, intercept by vertical drag - Slider: Slope (-5 to +5) - Slider: Intercept (0 to 100) - Button: \"Show Optimal Line\" - animates to best fit - Button: \"Reset\" - return to initial position - Toggle: Show/hide residual squares - Toggle: Show/hide residual values</p> <p>Display panels: - Current slope and intercept - Current SSE - Optimal SSE (shown after clicking \"Show Optimal\") - Percentage improvement from current to optimal</p> <p>SSE Meter (bottom): - Visual bar showing current SSE - Marker showing optimal SSE - Color gradient: red (high error) \u2192 green (low error)</p> <p>Behavior: - As line is adjusted, SSE updates in real-time - Residual squares resize dynamically - \"Show Optimal Line\" smoothly animates to least squares solution - Highlight when current SSE is close to optimal</p> <p>Educational annotations: - \"Each square's area = squared error for that point\" - \"Total area of all squares = SSE\" - \"OLS minimizes this total area\"</p> <p>Challenge tasks: - \"Can you get SSE below 50?\" - \"Find a line where all residuals are positive\" - \"Match the optimal line within 5% SSE\"</p> <p>Visual style: Clean mathematical visualization</p> <p>Implementation: p5.js with real-time calculations</p>"},{"location":"chapters/07-simple-linear-regression/#the-line-of-best-fit","title":"The Line of Best Fit","text":"<p>The line of best fit (also called the regression line or trend line) is the line that minimizes SSE. It's the \"best\" line in the sense that no other straight line would have smaller total squared errors.</p> <pre><code>import numpy as np\nfrom scipy import stats\n\n# Calculate line of best fit\nstudy_hours = np.array([1, 2, 3, 4, 5, 6, 7, 8])\nexam_scores = np.array([52, 58, 65, 71, 75, 82, 87, 92])\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(study_hours, exam_scores)\n\nprint(f\"Line of Best Fit:\")\nprint(f\"  Slope (\u03b2\u2081): {slope:.2f}\")\nprint(f\"  Intercept (\u03b2\u2080): {intercept:.2f}\")\nprint(f\"  Equation: \u0177 = {intercept:.2f} + {slope:.2f}x\")\n</code></pre> <p>Properties of the line of best fit:</p> <ul> <li>It always passes through the point \\((\\bar{x}, \\bar{y})\\) (the means)</li> <li>The sum of residuals equals zero (positive and negative cancel)</li> <li>It minimizes SSE among all possible straight lines</li> </ul>"},{"location":"chapters/07-simple-linear-regression/#fitted-values-and-predictions","title":"Fitted Values and Predictions","text":"<p>Fitted values are the predictions your model makes for the data points you used to build it. They're the y-values on the regression line at each x in your training data.</p> <pre><code># Calculate fitted values\nfitted_values = intercept + slope * study_hours\n\nprint(\"Fitted Values (Predictions for Training Data):\")\nfor hours, actual, fitted in zip(study_hours, exam_scores, fitted_values):\n    print(f\"  {hours} hours: Actual={actual}, Fitted={fitted:.1f}\")\n</code></pre> <p>Prediction uses the regression equation to estimate y for new x values\u2014values you haven't observed yet.</p> <pre><code># Make predictions for new data\nnew_hours = [4.5, 9, 10]\n\nfor hours in new_hours:\n    predicted_score = intercept + slope * hours\n    print(f\"Predicted score for {hours} hours: {predicted_score:.1f}\")\n</code></pre> <p>Extrapolation Warning</p> <p>Be careful predicting far outside your data range! If your data goes from 1-8 hours, predicting for 20 hours is risky. The linear relationship might not hold for extreme values. This is called extrapolation and can lead to unreliable predictions.</p>"},{"location":"chapters/07-simple-linear-regression/#interpreting-regression-coefficients","title":"Interpreting Regression Coefficients","text":"<p>Coefficient interpretation is crucial\u2014it's how you extract meaning from your model.</p>"},{"location":"chapters/07-simple-linear-regression/#interpreting-the-slope","title":"Interpreting the Slope","text":"<p>The slope tells you the effect size: how much y changes per unit change in x.</p> <pre><code># Our model: score = 47.5 + 5.5 * hours\nprint(f\"Slope Interpretation:\")\nprint(f\"  For each additional hour of studying,\")\nprint(f\"  exam score increases by {slope:.1f} points on average.\")\nprint(f\"\")\nprint(f\"  Study 2 more hours \u2192 expect {2 * slope:.1f} more points\")\nprint(f\"  Study 3 more hours \u2192 expect {3 * slope:.1f} more points\")\n</code></pre> <p>The slope also tells you direction:</p> <ul> <li>Positive slope (5.5): More studying \u2192 higher scores (positive relationship)</li> <li>If slope were negative: More of x \u2192 less of y (inverse relationship)</li> </ul>"},{"location":"chapters/07-simple-linear-regression/#interpreting-the-intercept","title":"Interpreting the Intercept","text":"<p>The intercept is the predicted value when x = 0.</p> <pre><code>print(f\"Intercept Interpretation:\")\nprint(f\"  A student who studies 0 hours is predicted\")\nprint(f\"  to score {intercept:.1f} points.\")\n</code></pre> <p>But context matters! Does x = 0 make sense?</p> Scenario x = 0 Meaningful? Intercept Interpretation Study hours \u2192 Score Maybe Baseline score without studying House sq ft \u2192 Price No Price of 0 sq ft house? Nonsense! Age \u2192 Height (children) No Height at age 0? (birth height, maybe) Temperature \u2192 Ice cream sales Maybe Sales at 0\u00b0F (very cold!) Coefficient Symbol Interpretation Slope \u03b2\u2081 Change in y per unit change in x Intercept \u03b2\u2080 Predicted y when x = 0"},{"location":"chapters/07-simple-linear-regression/#assumptions-of-regression","title":"Assumptions of Regression","text":"<p>For linear regression to give reliable results, certain assumptions should hold. Think of these as the \"fine print\" of your model.</p>"},{"location":"chapters/07-simple-linear-regression/#1-linearity-assumption","title":"1. Linearity Assumption","text":"<p>The linearity assumption requires that the relationship between x and y is actually linear (a straight line fits well).</p> <pre><code>import plotly.express as px\n\n# Check linearity with scatter plot\nfig = px.scatter(df, x='study_hours', y='exam_scores',\n                 trendline='ols',\n                 title='Checking Linearity: Do Points Follow a Line?')\nfig.show()\n</code></pre> <p>If the relationship is curved, linear regression will give poor predictions. You'd need polynomial regression or other techniques.</p>"},{"location":"chapters/07-simple-linear-regression/#2-independence-assumption","title":"2. Independence Assumption","text":"<p>The independence assumption requires that observations are independent of each other. One data point shouldn't affect another.</p> <p>Violations occur when:</p> <ul> <li>Time series data (today's value depends on yesterday's)</li> <li>Clustered data (students in same class aren't independent)</li> <li>Repeated measurements on same subjects</li> </ul>"},{"location":"chapters/07-simple-linear-regression/#3-homoscedasticity","title":"3. Homoscedasticity","text":"<p>Homoscedasticity (homo = same, scedasticity = scatter) means the spread of residuals is constant across all x values.</p> <pre><code># Check homoscedasticity with residual plot\nresiduals = exam_scores - fitted_values\n\nfig = px.scatter(x=fitted_values, y=residuals,\n                 title='Residual Plot: Checking Homoscedasticity',\n                 labels={'x': 'Fitted Values', 'y': 'Residuals'})\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\nfig.show()\n</code></pre> <ul> <li>Good: Residuals form a random horizontal band around zero</li> <li>Bad: Residuals fan out (spread increases with x) = heteroscedasticity</li> </ul>"},{"location":"chapters/07-simple-linear-regression/#4-normality-of-residuals","title":"4. Normality of Residuals","text":"<p>The normality of residuals assumption requires that residuals follow a normal distribution. This matters for confidence intervals and hypothesis tests.</p> <pre><code>import plotly.figure_factory as ff\n\n# Check normality with histogram\nfig = px.histogram(x=residuals, nbins=10,\n                   title='Distribution of Residuals',\n                   labels={'x': 'Residual', 'y': 'Count'})\nfig.show()\n</code></pre> <p>For small samples, use a Q-Q plot:</p> <pre><code>from scipy import stats\nimport plotly.graph_objects as go\n\n# Q-Q plot\ntheoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(residuals)))\nsorted_residuals = np.sort(residuals)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=theoretical_quantiles, y=sorted_residuals, mode='markers'))\nfig.add_trace(go.Scatter(x=[-3, 3], y=[-3*np.std(residuals), 3*np.std(residuals)],\n                         mode='lines', line=dict(dash='dash')))\nfig.update_layout(title='Q-Q Plot: Checking Normality of Residuals',\n                  xaxis_title='Theoretical Quantiles',\n                  yaxis_title='Sample Quantiles')\nfig.show()\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#diagram-regression-assumptions-checker-microsim","title":"Diagram: Regression Assumptions Checker MicroSim","text":"Interactive Assumption Diagnostic Tool <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help students diagnose regression assumption violations through interactive visualizations</p> <p>Canvas layout (900x650px): - Top left (450x300): Original scatter plot with regression line - Top right (450x300): Residual vs fitted plot - Bottom left (450x300): Residual histogram - Bottom right (450x300): Q-Q plot of residuals</p> <p>Visual elements: - All four diagnostic plots update together - Traffic light indicators (green/yellow/red) for each assumption - Assumption status panel</p> <p>Interactive controls: - Dropdown: Dataset selector   - \"Good Data\" (all assumptions met)   - \"Non-linear\" (curved relationship)   - \"Heteroscedastic\" (fan-shaped residuals)   - \"Non-normal residuals\" (skewed errors)   - \"Outliers present\"   - \"Custom\" (add/drag points) - Button: \"Diagnose\" - highlights violations - Toggle: Show/hide assumption guidelines - Draggable points in custom mode</p> <p>Assumption indicators: 1. LINEARITY    - Green: Points follow line well    - Yellow: Slight curvature    - Red: Clear non-linear pattern</p> <ol> <li>INDEPENDENCE</li> <li>Note: \"Cannot diagnose from plot alone\"</li> <li> <p>Checkbox: \"Data is from independent observations\"</p> </li> <li> <p>HOMOSCEDASTICITY</p> </li> <li>Green: Constant spread in residual plot</li> <li>Yellow: Slight fanning</li> <li> <p>Red: Clear funnel shape</p> </li> <li> <p>NORMALITY</p> </li> <li>Green: Histogram bell-shaped, Q-Q on line</li> <li>Yellow: Slight deviation</li> <li>Red: Clear non-normality</li> </ol> <p>Behavior: - Selecting dataset updates all four plots - Traffic lights update based on diagnostic rules - Tooltips explain what each violation means - \"Diagnose\" button highlights specific problem areas</p> <p>Educational annotations: - \"Look for patterns in the residual plot\" - \"Points should follow the diagonal in Q-Q plot\" - \"Residuals should be roughly bell-shaped\"</p> <p>Visual style: Dashboard layout with coordinated plots</p> <p>Implementation: p5.js with Plotly.js for statistical plots</p> Assumption What to Check Good Sign Bad Sign Linearity Scatter plot Points follow line Curved pattern Independence Study design Random sampling Clustered/time data Homoscedasticity Residual plot Even spread Fan/funnel shape Normality Histogram/Q-Q Bell curve, diagonal line Skewed, curved Q-Q <p>When Assumptions Are Violated</p> <p>Minor violations often don't matter much. Linear regression is fairly robust. But serious violations require action: transform variables, use robust regression, or try different models. Always check assumptions!</p>"},{"location":"chapters/07-simple-linear-regression/#implementing-linear-regression-with-scikit-learn","title":"Implementing Linear Regression with Scikit-learn","text":"<p>Now let's build regression models the professional way using the scikit-learn library (also called sklearn). It's the most popular machine learning library in Python.</p> <pre><code># Install if needed: pip install scikit-learn\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#the-linearregression-class","title":"The LinearRegression Class","text":"<p>The LinearRegression class is scikit-learn's implementation of ordinary least squares regression.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\n# Create a LinearRegression object\nmodel = LinearRegression()\n\nprint(type(model))  # &lt;class 'sklearn.linear_model._base.LinearRegression'&gt;\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#the-fit-method","title":"The Fit Method","text":"<p>The fit method trains the model\u2014it calculates the optimal coefficients from your data.</p> <pre><code># Prepare data (sklearn needs 2D array for X)\nX = np.array(study_hours).reshape(-1, 1)  # Reshape to column vector\ny = np.array(exam_scores)\n\n# Fit the model\nmodel.fit(X, y)\n\n# Access the learned coefficients\nprint(f\"Intercept (\u03b2\u2080): {model.intercept_:.2f}\")\nprint(f\"Slope (\u03b2\u2081): {model.coef_[0]:.2f}\")\n</code></pre> <p>The <code>.fit()</code> method:</p> <ol> <li>Takes X (features) and y (target)</li> <li>Calculates optimal coefficients using OLS</li> <li>Stores them in <code>model.intercept_</code> and <code>model.coef_</code></li> <li>Returns the model object (for method chaining)</li> </ol>"},{"location":"chapters/07-simple-linear-regression/#the-predict-method","title":"The Predict Method","text":"<p>The predict method uses the trained model to make predictions.</p> <pre><code># Make predictions for training data\ny_pred = model.predict(X)\nprint(\"Fitted values:\", y_pred)\n\n# Make predictions for new data\nX_new = np.array([[4.5], [9], [10]])  # Note: 2D array\npredictions = model.predict(X_new)\n\nfor hours, score in zip([4.5, 9, 10], predictions):\n    print(f\"Predicted score for {hours} hours: {score:.1f}\")\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#complete-scikit-learn-workflow","title":"Complete Scikit-learn Workflow","text":"<p>Here's the standard pattern you'll use for all sklearn models:</p> <pre><code>from sklearn.linear_model import LinearRegression\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# 1. Prepare data\ndf = pd.DataFrame({\n    'study_hours': [1, 2, 3, 4, 5, 6, 7, 8],\n    'exam_scores': [52, 58, 65, 71, 75, 82, 87, 92]\n})\n\nX = df[['study_hours']]  # Features (2D DataFrame or array)\ny = df['exam_scores']     # Target (1D)\n\n# 2. Create model\nmodel = LinearRegression()\n\n# 3. Fit model\nmodel.fit(X, y)\n\n# 4. Make predictions\ny_pred = model.predict(X)\n\n# 5. Inspect results\nprint(f\"Equation: \u0177 = {model.intercept_:.2f} + {model.coef_[0]:.2f}x\")\n\n# 6. Visualize\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df['study_hours'], y=df['exam_scores'],\n                         mode='markers', name='Actual'))\nfig.add_trace(go.Scatter(x=df['study_hours'], y=y_pred,\n                         mode='lines', name='Predicted'))\nfig.update_layout(title='Linear Regression with Scikit-learn',\n                  xaxis_title='Study Hours',\n                  yaxis_title='Exam Score')\nfig.show()\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#diagram-scikit-learn-workflow","title":"Diagram: Scikit-learn Workflow","text":"Machine Learning Pipeline Flowchart <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Help students memorize and apply the standard scikit-learn workflow</p> <p>Purpose: Visual guide for the fit-predict pattern</p> <p>Visual style: Horizontal flowchart with code snippets</p> <p>Steps (left to right):</p> <ol> <li> <p>IMPORT    Icon: Package/box    Code: <code>from sklearn.linear_model import LinearRegression</code>    Hover text: \"Import the model class you need\"    Color: Blue</p> </li> <li> <p>PREPARE DATA    Icon: Table/spreadsheet    Code: <code>X = df[['feature']]</code> and <code>y = df['target']</code>    Hover text: \"X must be 2D, y is 1D\"    Color: Green    Warning note: \"X needs double brackets!\"</p> </li> <li> <p>CREATE MODEL    Icon: Gear/factory    Code: <code>model = LinearRegression()</code>    Hover text: \"Instantiate the model object\"    Color: Orange</p> </li> <li> <p>FIT MODEL    Icon: Brain/learning    Code: <code>model.fit(X, y)</code>    Hover text: \"Train on your data - learns coefficients\"    Color: Purple    Output: \"model.coef_, model.intercept_\"</p> </li> <li> <p>PREDICT    Icon: Crystal ball    Code: <code>y_pred = model.predict(X_new)</code>    Hover text: \"Generate predictions for any X\"    Color: Red</p> </li> <li> <p>EVALUATE    Icon: Checkmark/chart    Code: <code>model.score(X, y)</code> or metrics    Hover text: \"Assess model quality\"    Color: Teal</p> </li> </ol> <p>Annotations: - Arrow from \"FIT\" to coefficients stored - Note: \"This pattern works for ALL sklearn models!\" - Common errors callout: \"Forgot to reshape X?\", \"Wrong array shape?\"</p> <p>Interactive elements: - Click each step to see full code example - Hover for detailed explanation - Toggle between LinearRegression and other model examples</p> <p>Implementation: SVG with JavaScript interactivity</p>"},{"location":"chapters/07-simple-linear-regression/#putting-it-all-together-a-complete-example","title":"Putting It All Together: A Complete Example","text":"<p>Let's work through a complete regression analysis from start to finish:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Generate realistic data: House size vs Price\nnp.random.seed(42)\nn = 50\n\n# True relationship: Price = 50000 + 200 * sqft + noise\nsqft = np.random.uniform(800, 2500, n)\nprice = 50000 + 200 * sqft + np.random.normal(0, 30000, n)\n\ndf = pd.DataFrame({'sqft': sqft, 'price': price})\n\n# 1. VISUALIZE THE DATA\nfig = px.scatter(df, x='sqft', y='price',\n                 title='House Size vs Price',\n                 labels={'sqft': 'Square Feet', 'price': 'Price ($)'})\nfig.show()\n\n# 2. FIT THE MODEL\nX = df[['sqft']]\ny = df['price']\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(\"=== MODEL RESULTS ===\")\nprint(f\"Intercept: ${model.intercept_:,.0f}\")\nprint(f\"Slope: ${model.coef_[0]:.2f} per sqft\")\nprint(f\"\\nEquation: Price = ${model.intercept_:,.0f} + ${model.coef_[0]:.2f} \u00d7 sqft\")\n\n# 3. INTERPRET COEFFICIENTS\nprint(\"\\n=== INTERPRETATION ===\")\nprint(f\"\u2022 Each additional square foot adds ${model.coef_[0]:.2f} to the price\")\nprint(f\"\u2022 A 100 sqft increase adds ${100 * model.coef_[0]:,.0f}\")\nprint(f\"\u2022 Base price (theoretical 0 sqft): ${model.intercept_:,.0f}\")\n\n# 4. MAKE PREDICTIONS\ny_pred = model.predict(X)\n\n# Predict for new houses\nnew_houses = pd.DataFrame({'sqft': [1000, 1500, 2000, 2500]})\nnew_predictions = model.predict(new_houses)\n\nprint(\"\\n=== PREDICTIONS ===\")\nfor sqft_val, pred in zip(new_houses['sqft'], new_predictions):\n    print(f\"  {sqft_val} sqft \u2192 ${pred:,.0f}\")\n\n# 5. CHECK MODEL FIT\nr2 = r2_score(y, y_pred)\nrmse = np.sqrt(mean_squared_error(y, y_pred))\n\nprint(f\"\\n=== MODEL QUALITY ===\")\nprint(f\"R\u00b2 Score: {r2:.3f} ({r2*100:.1f}% of variance explained)\")\nprint(f\"RMSE: ${rmse:,.0f} (typical prediction error)\")\n\n# 6. VISUALIZE RESULTS\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=['Regression Fit', 'Residual Plot'])\n\n# Scatter with regression line\nfig.add_trace(go.Scatter(x=df['sqft'], y=df['price'],\n                         mode='markers', name='Actual'), row=1, col=1)\nfig.add_trace(go.Scatter(x=df['sqft'], y=y_pred,\n                         mode='lines', name='Predicted', line=dict(color='red')),\n              row=1, col=1)\n\n# Residual plot\nresiduals = y - y_pred\nfig.add_trace(go.Scatter(x=y_pred, y=residuals,\n                         mode='markers', name='Residuals'), row=1, col=2)\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=2)\n\nfig.update_layout(title='Complete Regression Analysis',\n                  height=400, width=900)\nfig.show()\n\n# 7. CHECK ASSUMPTIONS\nprint(\"\\n=== ASSUMPTION CHECKS ===\")\nprint(\"\u2713 Linearity: Scatter plot shows linear pattern\")\nprint(\"\u2713 Homoscedasticity: Residuals have roughly constant spread\")\nprint(\"\u2713 Normality: Check residual histogram (code above)\")\n</code></pre>"},{"location":"chapters/07-simple-linear-regression/#diagram-interactive-regression-builder-microsim","title":"Diagram: Interactive Regression Builder MicroSim","text":"Build Your Own Regression Model <p>Type: microsim</p> <p>Bloom Taxonomy: Create (L6)</p> <p>Learning Objective: Let students build, visualize, and interpret their own regression models interactively</p> <p>Canvas layout (950x700px): - Left panel (600x700): Main visualization area   - Top (600x400): Scatter plot with regression line   - Bottom (600x300): Residual plot - Right panel (350x700): Controls, coefficients, interpretation</p> <p>Visual elements: - Interactive scatter plot - Regression line (updates with data) - Residual lines connecting points to line - Coefficient display - Equation display - R\u00b2 score gauge</p> <p>Data options: - Preset datasets:   - \"Study Hours vs Scores\" (positive, strong)   - \"House Size vs Price\" (positive, moderate)   - \"Car Age vs Value\" (negative)   - \"Random Data\" (no relationship) - Custom: Click to add points</p> <p>Interactive controls: - Dropdown: Select dataset - Button: \"Add Point\" (click on plot to add) - Button: \"Remove Point\" (click to remove) - Button: \"Fit Model\" - calculates regression - Button: \"Clear All\" - Slider: Noise level (for preset datasets) - Toggle: Show residuals - Toggle: Show confidence band</p> <p>Right panel displays: - Equation: \u0177 = \u03b2\u2080 + \u03b2\u2081x (with actual values) - Interpretation text:   - \"For each unit increase in X, Y changes by [slope]\"   - \"When X = 0, predicted Y = [intercept]\" - Model quality:   - R\u00b2 score with visual gauge   - RMSE value - Assumption indicators (traffic lights)</p> <p>Prediction tool: - Input field: \"Enter X value\" - Button: \"Predict\" - Output: Predicted Y with confidence interval - Visual: Point added to plot at prediction</p> <p>Behavior: - Adding/removing points triggers model refit - All statistics update in real-time - Interpretation text updates with coefficient values - Warning when extrapolating beyond data range</p> <p>Educational features: - \"What happens if you add an outlier?\" - \"Can you create data with R\u00b2 &gt; 0.9?\" - \"What does negative slope look like?\"</p> <p>Visual style: Professional dashboard with clean aesthetics</p> <p>Implementation: p5.js with real-time OLS calculations</p>"},{"location":"chapters/07-simple-linear-regression/#common-pitfalls-and-best-practices","title":"Common Pitfalls and Best Practices","text":""},{"location":"chapters/07-simple-linear-regression/#pitfall-1-confusing-correlation-with-causation","title":"Pitfall 1: Confusing Correlation with Causation","text":"<p>A strong relationship doesn't mean x CAUSES y. Ice cream sales predict drowning deaths (both increase in summer), but ice cream doesn't cause drowning!</p>"},{"location":"chapters/07-simple-linear-regression/#pitfall-2-extrapolating-too-far","title":"Pitfall 2: Extrapolating Too Far","text":"<p>Your model is only reliable within the range of your training data. Predicting house prices for 50,000 square feet when your data only goes to 3,000 is dangerous.</p>"},{"location":"chapters/07-simple-linear-regression/#pitfall-3-ignoring-assumptions","title":"Pitfall 3: Ignoring Assumptions","text":"<p>Always check your assumptions! A model fit on data with severe violations gives misleading results.</p>"},{"location":"chapters/07-simple-linear-regression/#pitfall-4-forgetting-to-reshape-x","title":"Pitfall 4: Forgetting to Reshape X","text":"<p>Scikit-learn needs X as a 2D array. The most common error:</p> <pre><code># WRONG - will cause error\nX = df['feature']\n\n# RIGHT - reshape to 2D\nX = df[['feature']]  # Double brackets = DataFrame (2D)\n# or\nX = df['feature'].values.reshape(-1, 1)  # Explicit reshape\n</code></pre> Chapter 7 Checkpoint: Test Your Understanding <p>Question 1: A model has equation: Price = 25000 + 150 \u00d7 sqft. Interpret the slope.</p> <p>Question 2: What's the predicted price for a 1,200 sqft house using this model?</p> <p>Question 3: In a residual plot, you see residuals fanning out (spreading wider) as fitted values increase. Which assumption is violated?</p> <p>Click to reveal answers:</p> <p>Answer 1: For each additional square foot, the predicted price increases by $150. A house that's 100 sqft larger is predicted to cost $15,000 more.</p> <p>Answer 2: Price = 25000 + 150 \u00d7 1200 = 25000 + 180000 = $205,000</p> <p>Answer 3: Homoscedasticity is violated. The residuals should have constant spread (homoscedastic), but fanning indicates the spread changes with fitted values (heteroscedastic).</p> <p>Achievement Unlocked: Prediction Pioneer</p> <p>You've built your first predictive model! You can now fit lines to data, interpret what those lines mean, make predictions, and check if your model is trustworthy. This is the foundation of all machine learning\u2014everything else builds on these concepts.</p>"},{"location":"chapters/07-simple-linear-regression/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Regression analysis models relationships between variables to make predictions.</p> </li> <li> <p>Simple linear regression uses one input (x) to predict one output (y) with a straight line.</p> </li> <li> <p>The regression equation is \\(\\hat{y} = \\beta_0 + \\beta_1 x\\), where \u03b2\u2080 is the intercept and \u03b2\u2081 is the slope.</p> </li> <li> <p>Slope tells you how much y changes per unit increase in x. Intercept is the predicted y when x = 0.</p> </li> <li> <p>Residuals are prediction errors: actual minus predicted. The least squares method finds the line minimizing the sum of squared errors.</p> </li> <li> <p>OLS (Ordinary Least Squares) is the standard algorithm that finds the optimal coefficients.</p> </li> <li> <p>Fitted values are predictions for training data; predictions can be made for any new x values.</p> </li> <li> <p>Assumptions: linearity, independence, homoscedasticity, and normality of residuals. Check them!</p> </li> <li> <p>Scikit-learn provides the professional way to do regression: create model \u2192 fit(X, y) \u2192 predict(X_new).</p> </li> <li> <p>The LinearRegression class implements OLS. Use <code>.fit()</code> to train and <code>.predict()</code> to generate predictions.</p> </li> </ol> <p>You've now mastered the fundamentals of predictive modeling. In the next chapter, you'll learn how to evaluate whether your model is actually good\u2014because fitting a line is easy, but knowing if it's useful is the real skill!</p>"},{"location":"chapters/08-model-evaluation/","title":"Model Evaluation and Validation","text":"<p>title: Model Evaluation and Validation description: Learn to measure your model's true powers and avoid self-deception generated_by: chapter-content-generator skill date: 2025-12-15 version: 0.03</p>"},{"location":"chapters/08-model-evaluation/#summary","title":"Summary","text":"<p>This chapter teaches students how to properly evaluate and validate machine learning models. Students will learn about training and testing data splits, key performance metrics (R-squared, MSE, RMSE, MAE), and residual analysis. The chapter covers the critical concepts of overfitting and underfitting, the bias-variance tradeoff, and various cross-validation techniques. By the end of this chapter, students will be able to assess model quality, compare different models, and select the best model for their data.</p>"},{"location":"chapters/08-model-evaluation/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 25 concepts from the learning graph:</p> <ol> <li>Model Performance</li> <li>Training Data</li> <li>Testing Data</li> <li>Train Test Split</li> <li>Validation Data</li> <li>R-Squared</li> <li>Adjusted R-Squared</li> <li>Mean Squared Error</li> <li>Root Mean Squared Error</li> <li>Mean Absolute Error</li> <li>Residual Analysis</li> <li>Residual Plot</li> <li>Overfitting</li> <li>Underfitting</li> <li>Bias</li> <li>Variance</li> <li>Bias-Variance Tradeoff</li> <li>Model Complexity</li> <li>Cross-Validation</li> <li>K-Fold Cross-Validation</li> <li>Leave One Out CV</li> <li>Holdout Method</li> <li>Model Selection</li> <li>Hyperparameters</li> <li>Model Comparison</li> </ol>"},{"location":"chapters/08-model-evaluation/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 6: Statistical Foundations</li> <li>Chapter 7: Simple Linear Regression</li> </ul>"},{"location":"chapters/08-model-evaluation/#introduction-the-reality-check-superpower","title":"Introduction: The Reality Check Superpower","text":"<p>Congratulations! You've built your first predictive model. It can draw a line through data and make predictions about the future. That's genuinely impressive. But here's a question that separates the data science amateurs from the professionals: How do you know if your model is actually any good?</p> <p>Think about it this way. Imagine you have a friend who claims they can predict tomorrow's weather perfectly\u2014because they just memorized all the weather from the past year. Ask them about last Tuesday's weather? Perfect answer. Ask them about next Tuesday? Complete disaster. They didn't learn weather patterns; they just memorized history.</p> <p>This chapter gives you the superpower to see through this kind of self-deception. You'll learn to honestly evaluate whether your model has discovered genuine patterns or just memorized your data. This skill is crucial because in the real world, a model that looks amazing in training but fails in production is worse than useless\u2014it gives you false confidence that leads to bad decisions.</p>"},{"location":"chapters/08-model-evaluation/#the-problem-with-trusting-your-own-grades","title":"The Problem with Trusting Your Own Grades","text":"<p>Let's start with a fundamental truth about model performance: you can't trust a model to grade its own homework. If you train a model on data and then test it on that same data, you're essentially asking, \"Hey model, how well did you memorize what I showed you?\" The answer will always be \"Pretty darn well!\" But memorization isn't learning.</p> <p>Here's why this matters:</p> <ul> <li>A model that memorizes will score 100% on data it has seen</li> <li>That same model might score 40% on new data</li> <li>You need to know the real performance before deploying your model</li> <li>Real-world predictions always involve data the model hasn't seen</li> </ul> <p>This is why we need to be clever about how we evaluate our models. We need to simulate the real world\u2014where predictions are made on never-before-seen data\u2014while still using the limited data we have.</p>"},{"location":"chapters/08-model-evaluation/#training-data-and-testing-data-dividing-your-data-kingdom","title":"Training Data and Testing Data: Dividing Your Data Kingdom","text":"<p>The solution to the self-grading problem is beautifully simple: split your data into two kingdoms. One kingdom is for training\u2014teaching the model. The other is for testing\u2014evaluating the model honestly.</p> <p>Training data is the portion of your dataset that your model gets to learn from. This is the data that the model uses to find patterns, calculate coefficients, and tune its parameters. Think of training data as the textbook the model studies from.</p> <p>Testing data is the portion you hide from the model during training. It's the \"final exam\" that the model has never seen before. When you evaluate your model on testing data, you get an honest estimate of how it will perform on new, real-world data.</p> <p>Here's the key insight: your testing data must remain completely invisible to the model until the very end. If even a hint of testing data influences your model's training, you've contaminated your experiment. It's like a student peeking at the exam questions before the test\u2014their grade no longer reflects their true knowledge.</p> Data Type Purpose When Used Model Sees During Training? Training Data Teach the model patterns During model fitting Yes Testing Data Evaluate final performance After training complete No Validation Data Tune settings and choose models During development Sometimes (indirectly)"},{"location":"chapters/08-model-evaluation/#the-train-test-split-your-first-defense-against-self-deception","title":"The Train-Test Split: Your First Defense Against Self-Deception","text":"<p>The train-test split is the procedure of dividing your data into training and testing portions. Typically, you'll use 70-80% of your data for training and hold back 20-30% for testing. This ratio balances two competing needs: you want enough training data for the model to learn well, but you also want enough testing data for a reliable performance estimate.</p> <p>Here's how to perform a train-test split with scikit-learn:</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load your data\ndf = pd.read_csv('housing_prices.csv')\nX = df[['square_feet', 'bedrooms', 'age']]\ny = df['price']\n\n# Split: 80% training, 20% testing\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42  # Makes the split reproducible\n)\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")\n</code></pre> <p>The <code>random_state</code> parameter is important\u2014it ensures that every time you run this code, you get the same split. This makes your experiments reproducible. Without it, you'd get different results each time, making it impossible to compare different models fairly.</p>"},{"location":"chapters/08-model-evaluation/#diagram-train-test-split-visualization","title":"Diagram: Train-Test Split Visualization","text":"Train-Test Split Visualization <p>Type: infographic</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Help students visualize how data is divided and why the testing portion must remain separate</p> <p>Layout: Horizontal bar representation of full dataset with animated split</p> <p>Visual Elements: - Full dataset shown as a horizontal bar with 100 small squares (each representing a data point) - Squares are randomly colored to show data variety - Animation shows 80 squares sliding left (training) and 20 sliding right (testing) - \"Wall\" appears between training and testing portions - Icons show model can \"see\" training data (eye icon) but testing data is \"hidden\" (blindfold icon)</p> <p>Interactive Elements: - Slider to adjust split ratio from 50/50 to 90/10 - As slider moves, squares animate between groups - Display updates showing \"Training: X samples, Testing: Y samples\" - Warning appears if split becomes too extreme (&lt; 60% or &gt; 90% training)</p> <p>Color Scheme: - Training data: Green shades - Testing data: Blue shades - Warning states: Orange/Red</p> <p>Implementation: p5.js with smooth animations</p>"},{"location":"chapters/08-model-evaluation/#validation-data-the-third-kingdom","title":"Validation Data: The Third Kingdom","text":"<p>Sometimes two kingdoms aren't enough. Validation data is a third portion of data, carved out from your training set, that you use to make decisions during model development. This is different from testing data, which you only touch at the very end.</p> <p>Why do we need validation data? Because as you develop your model, you make many choices:</p> <ul> <li>Should you include this feature or that feature?</li> <li>Should you use a simple linear model or a complex polynomial?</li> <li>What settings (hyperparameters) work best?</li> </ul> <p>Every time you make a choice based on performance, you're implicitly \"using\" that data to train your decisions. If you make these choices using your test data, you're cheating\u2014you're letting test data influence your model development.</p> <p>The validation set solves this. You train on training data, evaluate choices on validation data, and only at the very end\u2014when all decisions are final\u2014do you touch the test data for your honest final grade.</p> <pre><code># Three-way split: 60% train, 20% validation, 20% test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 of 0.8 = 0.2\n)\n\nprint(f\"Training: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n</code></pre>"},{"location":"chapters/08-model-evaluation/#measuring-model-performance-the-metrics-that-matter","title":"Measuring Model Performance: The Metrics That Matter","text":"<p>Now that you know how to split your data honestly, let's talk about what to measure. There are several key metrics for evaluating regression models, and each tells you something different about your model's performance.</p>"},{"location":"chapters/08-model-evaluation/#r-squared-the-explanation-score","title":"R-Squared: The Explanation Score","text":"<p>R-squared (\\(R^2\\)), also called the coefficient of determination, tells you what fraction of the variation in your target variable your model explains. It ranges from 0 to 1, where:</p> <ul> <li>\\(R^2 = 0\\) means your model explains nothing (just predicts the average)</li> <li>\\(R^2 = 1\\) means your model explains everything (perfect predictions)</li> <li>\\(R^2 = 0.7\\) means your model explains 70% of the variation</li> </ul> <p>The formula is:</p> \\[R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\] <p>In plain English: R-squared compares your model's errors to the errors you'd get by just guessing the average every time. If your model's errors are much smaller, R-squared is close to 1. If your model is barely better than guessing the average, R-squared is close to 0.</p> <p>Interpreting R-Squared</p> <p>An R\u00b2 of 0.8 sounds great, but context matters! For predicting lottery numbers, even 0.1 would be suspicious. For predicting height from age in growing children, 0.8 might be disappointing. Always consider what R\u00b2 is typical for your domain.</p>"},{"location":"chapters/08-model-evaluation/#adjusted-r-squared-the-honest-version","title":"Adjusted R-Squared: The Honest Version","text":"<p>There's a sneaky problem with regular R-squared: it always increases when you add more features to your model, even if those features are useless. Your model might not actually get better\u2014it just gets more complicated.</p> <p>Adjusted R-squared fixes this by penalizing model complexity:</p> \\[R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\\] <p>Where \\(n\\) is the number of samples and \\(p\\) is the number of features. Adjusted R-squared only increases if a new feature improves the model enough to justify its added complexity. This makes it a better metric for comparing models with different numbers of features.</p>"},{"location":"chapters/08-model-evaluation/#mean-squared-error-the-average-squared-miss","title":"Mean Squared Error: The Average Squared Miss","text":"<p>Mean Squared Error (MSE) is exactly what it sounds like: the average of your squared prediction errors.</p> \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\] <p>Why square the errors? Two reasons:</p> <ol> <li>It prevents positive and negative errors from canceling out</li> <li>It punishes big mistakes more than small ones (a prediction off by 10 is 100 times worse than one off by 1)</li> </ol> <p>The downside of MSE is that it's in squared units, which can be hard to interpret. If you're predicting prices in dollars, MSE is in \"dollars squared,\" which is weird.</p>"},{"location":"chapters/08-model-evaluation/#root-mean-squared-error-mse-you-can-understand","title":"Root Mean Squared Error: MSE You Can Understand","text":"<p>Root Mean Squared Error (RMSE) solves the squared units problem by taking the square root:</p> \\[RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\] <p>RMSE is in the same units as your target variable. If you're predicting house prices and your RMSE is $25,000, you can say \"on average, my predictions are off by about $25,000.\" That's much more interpretable!</p>"},{"location":"chapters/08-model-evaluation/#mean-absolute-error-the-simple-alternative","title":"Mean Absolute Error: The Simple Alternative","text":"<p>Mean Absolute Error (MAE) takes a different approach\u2014instead of squaring errors, it just uses absolute values:</p> \\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\] <p>MAE is also in the original units and is simpler to understand than RMSE. The key difference: MAE treats all errors equally, while RMSE punishes big errors more severely. Which should you use? It depends on whether large errors are especially bad for your application.</p> Metric Units Big Errors Interpretation R\u00b2 Unitless (0-1) Averaged Fraction of variance explained MSE Squared units Heavily penalized Average squared error RMSE Original units Moderately penalized Typical error magnitude MAE Original units Equal weight Average absolute error"},{"location":"chapters/08-model-evaluation/#diagram-metrics-comparison-microsim","title":"Diagram: Metrics Comparison MicroSim","text":"Metrics Comparison MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Analyze</p> <p>Learning Objective: Help students understand how different error metrics respond to the same prediction errors, especially the difference between MAE and RMSE when outliers are present</p> <p>Canvas Layout (800x500): - Left side (500x500): Scatter plot with regression line and interactive points - Right side (300x500): Real-time metrics display</p> <p>Visual Elements: - 10 data points that can be dragged - Regression line that updates in real-time - Vertical lines showing residuals (prediction errors) - Residuals colored by size (green = small, yellow = medium, red = large)</p> <p>Interactive Controls: - Draggable data points to create different error patterns - Button: \"Add Outlier\" - adds a point far from the line - Button: \"Reset to Default\" - returns to initial configuration - Checkbox: \"Show squared residuals\" - visualizes MSE calculation - Checkbox: \"Show absolute residuals\" - visualizes MAE calculation</p> <p>Metrics Display (updates in real-time): - R\u00b2: X.XXX - MSE: X.XX - RMSE: X.XX - MAE: X.XX - Bar chart comparing metrics (normalized for visualization)</p> <p>Key Learning Moments: - Drag one point far away and watch RMSE spike more than MAE - Create symmetrical errors and see they still contribute to metrics - Notice how R\u00b2 can decrease when predictions get worse</p> <p>Default Parameters: - 10 points roughly following y = 2x + 1 with small noise - Initial R\u00b2 around 0.85</p> <p>Implementation: p5.js with real-time regression recalculation</p>"},{"location":"chapters/08-model-evaluation/#calculating-metrics-in-python","title":"Calculating Metrics in Python","text":"<p>Here's how to calculate all these metrics using scikit-learn:</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport numpy as np\n\n# Train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions on test data\ny_pred = model.predict(X_test)\n\n# Calculate metrics\nr2 = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)  # or mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\n\nprint(f\"R-Squared: {r2:.4f}\")\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"MAE: {mae:.2f}\")\n</code></pre> <p>Let's visualize these predictions with Plotly to see how well our model performs:</p> <pre><code>import plotly.express as px\nimport plotly.graph_objects as go\n\n# Create a comparison dataframe\nresults = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred\n})\n\n# Scatter plot of actual vs predicted\nfig = px.scatter(\n    results,\n    x='Actual',\n    y='Predicted',\n    title='Actual vs Predicted Values',\n    labels={'Actual': 'Actual Values', 'Predicted': 'Predicted Values'}\n)\n\n# Add perfect prediction line\nfig.add_trace(\n    go.Scatter(\n        x=[results['Actual'].min(), results['Actual'].max()],\n        y=[results['Actual'].min(), results['Actual'].max()],\n        mode='lines',\n        name='Perfect Prediction',\n        line=dict(dash='dash', color='red')\n    )\n)\n\nfig.update_layout(height=500, width=600)\nfig.show()\n</code></pre>"},{"location":"chapters/08-model-evaluation/#residual-analysis-csi-data-science","title":"Residual Analysis: CSI Data Science","text":"<p>Residual analysis is like being a detective investigating your model's mistakes. A residual is simply the difference between the actual value and your predicted value:</p> \\[\\text{Residual} = y_{\\text{actual}} - y_{\\text{predicted}}\\] <p>Looking at individual residuals tells you where your model is struggling. But the real power comes from looking at patterns in your residuals. If your residuals are randomly scattered (no pattern), your model is working well. If there's a pattern, something is wrong.</p> <p>A residual plot shows residuals on the y-axis and either predicted values or a feature on the x-axis. Here's what to look for:</p> <ul> <li>Random scatter around zero: Good! Model assumptions are met.</li> <li>Curved pattern: Bad! Relationship might be non-linear.</li> <li>Funnel shape (spreads out): Bad! Variance isn't constant (heteroscedasticity).</li> <li>Clusters or groups: Bad! Missing categorical information.</li> </ul> <pre><code># Calculate residuals\nresiduals = y_test - y_pred\n\n# Create residual plot with Plotly\nfig = px.scatter(\n    x=y_pred,\n    y=residuals,\n    title='Residual Plot: Hunting for Patterns',\n    labels={'x': 'Predicted Values', 'y': 'Residuals'}\n)\n\n# Add horizontal line at y=0\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n\nfig.update_layout(height=400, width=600)\nfig.show()\n</code></pre>"},{"location":"chapters/08-model-evaluation/#diagram-residual-pattern-detective","title":"Diagram: Residual Pattern Detective","text":"Residual Pattern Detective <p>Type: infographic</p> <p>Bloom Taxonomy: Analyze, Evaluate</p> <p>Learning Objective: Train students to recognize common residual patterns and diagnose what's wrong with their model</p> <p>Layout: 2x2 grid of residual plot examples with diagnostic labels</p> <p>Panels: 1. Top-Left: \"Healthy Residuals\"    - Random scatter around horizontal line at 0    - Caption: \"Random pattern = model is working well\"    - Green checkmark icon    - Hover: \"No systematic bias, assumptions met\"</p> <ol> <li>Top-Right: \"Curved Pattern\"</li> <li>U-shaped or wave pattern in residuals</li> <li>Caption: \"Curved pattern = try polynomial features\"</li> <li>Yellow warning icon</li> <li> <p>Hover: \"Linear model missing non-linear relationship\"</p> </li> <li> <p>Bottom-Left: \"Funnel Shape\"</p> </li> <li>Residuals spread out as predictions increase</li> <li>Caption: \"Funnel shape = variance problems\"</li> <li>Orange warning icon</li> <li> <p>Hover: \"Consider log transformation of target\"</p> </li> <li> <p>Bottom-Right: \"Clustered Groups\"</p> </li> <li>Distinct groups of residuals at different levels</li> <li>Caption: \"Clusters = missing categorical variable\"</li> <li>Red warning icon</li> <li>Hover: \"Include the grouping variable as a feature\"</li> </ol> <p>Interactive Elements: - Click each panel for expanded explanation - Hover shows diagnostic advice - \"Quiz mode\" button randomly shows a pattern and asks for diagnosis</p> <p>Color Scheme: - Residual points in blue - Reference line in red (dashed) - Background panels in light gray</p> <p>Implementation: HTML/CSS/JavaScript with interactive panels</p>"},{"location":"chapters/08-model-evaluation/#the-perils-of-overfitting-when-your-model-studies-too-hard","title":"The Perils of Overfitting: When Your Model Studies Too Hard","text":"<p>Here's a paradox: a model can perform too well on training data. When this happens, we call it overfitting. An overfit model has essentially memorized the training data, including all its noise and random fluctuations. It achieves amazing training scores but fails miserably on new data.</p> <p>Think of a student who memorizes every practice test word-for-word instead of learning the underlying concepts. They'll ace practice tests but bomb the actual exam if the questions are phrased even slightly differently.</p> <p>Signs of overfitting:</p> <ul> <li>Training error is very low</li> <li>Test error is much higher than training error</li> <li>Model is complex (many features, high polynomial degree)</li> <li>Training data is limited</li> </ul> <p>An overfit model has low bias (its predictions aren't systematically wrong) but high variance (its predictions are very sensitive to which specific training data it saw).</p>"},{"location":"chapters/08-model-evaluation/#the-dangers-of-underfitting-when-your-model-doesnt-try-hard-enough","title":"The Dangers of Underfitting: When Your Model Doesn't Try Hard Enough","text":"<p>The opposite problem is underfitting. An underfit model is too simple to capture the patterns in the data. It performs poorly on both training and test data because it never learned the real relationship.</p> <p>Think of a student who only skims the textbook and tries to pass by guessing. They'll do poorly on everything.</p> <p>Signs of underfitting:</p> <ul> <li>Training error is high</li> <li>Test error is also high (and similar to training error)</li> <li>Model is very simple (few features, too restrictive)</li> <li>There's clearly more pattern in the data to capture</li> </ul> <p>An underfit model has high bias (it systematically misses the true pattern) but low variance (its predictions are consistent, just consistently wrong).</p> Condition Training Error Test Error Model Complexity Cure Underfitting High High Too low Add features, increase complexity Good Fit Low Low (similar) Just right Keep it! Overfitting Very Low High Too high Reduce complexity, get more data"},{"location":"chapters/08-model-evaluation/#bias-and-variance-the-fundamental-tradeoff","title":"Bias and Variance: The Fundamental Tradeoff","text":"<p>Bias and variance are two types of model errors that pull in opposite directions.</p> <p>Bias is the error from oversimplifying. A high-bias model makes strong assumptions about the data that might not be true. It will consistently miss the target in the same direction, like a dart thrower who always aims too far left.</p> <p>Variance is the error from being too sensitive to training data. A high-variance model changes dramatically depending on which specific samples it was trained on. It's like a dart thrower whose aim is all over the place\u2014sometimes left, sometimes right, sometimes high, sometimes low.</p> <p>The bias-variance tradeoff is the fundamental tension in machine learning:</p> <ul> <li>Simple models: High bias, low variance (consistent but often wrong)</li> <li>Complex models: Low bias, high variance (can be right but unstable)</li> </ul> <p>Your goal is to find the sweet spot\u2014a model complex enough to capture the real pattern but simple enough to not chase noise.</p>"},{"location":"chapters/08-model-evaluation/#diagram-bias-variance-dartboard","title":"Diagram: Bias-Variance Dartboard","text":"Bias-Variance Dartboard <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize bias and variance using the intuitive dartboard analogy, and understand how model complexity affects this tradeoff</p> <p>Canvas Layout (800x450): - Left side: Four dartboard panels (2x2 grid, each 180x180) - Right side: Interactive model complexity slider and explanation panel</p> <p>Dartboard Panels: 1. Top-left: \"Low Bias, Low Variance\" - Darts clustered at center (bullseye)    - Label: \"The Goal: Accurate and Consistent\" 2. Top-right: \"Low Bias, High Variance\" - Darts scattered but centered on bullseye    - Label: \"Accurate on Average, But Inconsistent\" 3. Bottom-left: \"High Bias, Low Variance\" - Darts clustered but off-center    - Label: \"Consistent but Systematically Wrong\" 4. Bottom-right: \"High Bias, High Variance\" - Darts scattered and off-center    - Label: \"The Worst: Wrong and Inconsistent\"</p> <p>Interactive Elements: - Slider: \"Model Complexity\" (1 to 10 scale) - As slider moves left (simpler): highlight high-bias panels - As slider moves right (complex): highlight high-variance panels - Button: \"Throw 10 Darts\" - animates darts landing based on current complexity setting - The fifth dartboard shows real-time results based on slider position</p> <p>Real-time Display: - Bias indicator bar - Variance indicator bar - Total Error = Bias\u00b2 + Variance (visualized as stacked bar)</p> <p>Animation: - Darts \"thrown\" one at a time with slight delay - Each dart leaves a mark on the board - After all darts, metrics calculate and display</p> <p>Implementation: p5.js with dart physics animation</p>"},{"location":"chapters/08-model-evaluation/#model-complexity-the-goldilocks-problem","title":"Model Complexity: The Goldilocks Problem","text":"<p>Model complexity refers to how flexible or expressive your model is. A simple linear model with one feature has low complexity. A polynomial model with degree 10 has high complexity. A neural network with millions of parameters has very high complexity.</p> <p>The Goldilocks principle applies: you want a model that's just right. Too simple, and you underfit. Too complex, and you overfit.</p> <p>Here's how complexity relates to polynomial regression:</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Compare different polynomial degrees\ndegrees = [1, 3, 5, 10, 15]\n\nfor degree in degrees:\n    model = make_pipeline(\n        PolynomialFeatures(degree),\n        LinearRegression()\n    )\n    model.fit(X_train, y_train)\n\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n\n    print(f\"Degree {degree:2d}: Train R\u00b2={train_score:.4f}, Test R\u00b2={test_score:.4f}\")\n</code></pre> <p>Typically, you'll see training R\u00b2 keep increasing with complexity, but test R\u00b2 will peak and then decrease as overfitting kicks in.</p>"},{"location":"chapters/08-model-evaluation/#diagram-complexity-curve-explorer","title":"Diagram: Complexity Curve Explorer","text":"Complexity Curve Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Evaluate</p> <p>Learning Objective: Visualize how training and test error change as model complexity increases, and identify the optimal complexity level</p> <p>Canvas Layout (800x500): - Top (800x350): Main visualization showing data points and fitted curve - Bottom (800x150): Error vs. Complexity chart</p> <p>Top Panel Elements: - 30 data points following a cubic relationship with noise - Polynomial curve that updates with complexity slider - Curve color indicates fit quality (green = good, red = overfit/underfit)</p> <p>Bottom Panel Elements: - X-axis: Model Complexity (polynomial degree 1-15) - Y-axis: Error (MSE) - Two lines: Training Error (blue) and Test Error (orange) - Vertical marker showing current complexity selection - Shaded regions: \"Underfitting Zone\" (left), \"Sweet Spot\" (middle), \"Overfitting Zone\" (right)</p> <p>Interactive Controls: - Slider: \"Polynomial Degree\" (1 to 15) - Checkbox: \"Show training error curve\" - Checkbox: \"Show test error curve\" - Button: \"Auto-find optimal\" - animates to minimum test error - Button: \"Reset data\" - generates new random dataset</p> <p>Real-time Metrics Display: - Current degree: X - Training MSE: X.XX - Test MSE: X.XX - Gap (Test - Train): X.XX (with color coding)</p> <p>Key Insights Highlighted: - When gap is large (&gt;threshold): \"Overfitting Warning!\" in red - When both errors are high: \"Underfitting Warning!\" in yellow - When gap is small and errors low: \"Good fit!\" in green</p> <p>Implementation: p5.js with polynomial regression calculation</p>"},{"location":"chapters/08-model-evaluation/#cross-validation-the-ultimate-fairness-test","title":"Cross-Validation: The Ultimate Fairness Test","text":"<p>The simple train-test split has a weakness: your results depend heavily on which specific data points ended up in training vs. testing. With a different random split, you might get very different results. Cross-validation solves this by testing on all of your data, just not all at once.</p>"},{"location":"chapters/08-model-evaluation/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<p>K-Fold Cross-Validation divides your data into K equal-sized chunks called \"folds.\" Then it trains K different models, each time using a different fold as the test set and the remaining K-1 folds for training. Finally, it averages the K test scores to get a more reliable performance estimate.</p> <p>The most common choice is K=5 or K=10. Here's how 5-fold cross-validation works:</p> <ol> <li>Split data into 5 folds</li> <li>Train on folds 1,2,3,4; test on fold 5 \u2192 Score 1</li> <li>Train on folds 1,2,3,5; test on fold 4 \u2192 Score 2</li> <li>Train on folds 1,2,4,5; test on fold 3 \u2192 Score 3</li> <li>Train on folds 1,3,4,5; test on fold 2 \u2192 Score 4</li> <li>Train on folds 2,3,4,5; test on fold 1 \u2192 Score 5</li> <li>Final score = Average of all 5 scores</li> </ol> <p>This gives you a much more reliable estimate because every data point gets to be in the test set exactly once.</p> <pre><code>from sklearn.model_selection import cross_val_score\n\n# 5-fold cross-validation\nmodel = LinearRegression()\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n\nprint(f\"CV Scores: {cv_scores}\")\nprint(f\"Mean CV Score: {cv_scores.mean():.4f}\")\nprint(f\"Standard Deviation: {cv_scores.std():.4f}\")\n</code></pre> <p>The standard deviation tells you how stable your model's performance is. A low standard deviation means your model performs consistently across different subsets of data.</p>"},{"location":"chapters/08-model-evaluation/#diagram-k-fold-cross-validation-animator","title":"Diagram: K-Fold Cross-Validation Animator","text":"K-Fold Cross-Validation Animator <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize how K-fold cross-validation rotates through the data and why it provides a more reliable performance estimate</p> <p>Canvas Layout (700x500): - Main area (700x350): Visual representation of data folds - Bottom area (700x150): Results table and summary statistics</p> <p>Visual Elements: - Data represented as 50 colored squares in a horizontal strip - Squares grouped into K folds with subtle borders between groups - Training folds colored green - Test fold colored blue - Animation shows the \"window\" of test data sliding across folds</p> <p>Interactive Controls: - Dropdown: \"Number of folds (K)\" - options: 3, 5, 10 - Button: \"Run Cross-Validation\" - starts animation - Button: \"Pause/Resume\" - Speed slider: controls animation speed - Button: \"Reset\"</p> <p>Animation Sequence: 1. Show all data as neutral color 2. Divide into K folds with visual separation 3. For each iteration:    - Highlight test fold in blue    - Highlight training folds in green    - Show mini-chart of model being \"trained\"    - Display score for this fold    - Pause briefly, then move to next fold 4. After all folds complete, show final averaged score</p> <p>Results Display: - Table showing each fold's score - Running average line chart - Final statistics: Mean, Std Dev, Min, Max - Comparison to simple train-test split result</p> <p>Educational Callouts: - \"Every data point tested exactly once!\" - \"Average gives more reliable estimate\" - When std is high: \"High variance in scores - model might be unstable\"</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/08-model-evaluation/#leave-one-out-cross-validation","title":"Leave-One-Out Cross-Validation","text":"<p>Leave-One-Out Cross-Validation (LOOCV) is the extreme version where K equals the number of data points. For each iteration, you train on all data except one point, then test on that single point. This is the most thorough form of cross-validation but can be computationally expensive for large datasets.</p> <pre><code>from sklearn.model_selection import LeaveOneOut, cross_val_score\n\nloo = LeaveOneOut()\ncv_scores = cross_val_score(model, X, y, cv=loo, scoring='r2')\n\nprint(f\"Number of splits: {len(cv_scores)}\")\nprint(f\"Mean Score: {cv_scores.mean():.4f}\")\n</code></pre> <p>LOOCV is mostly used when you have very limited data and need to squeeze every drop of information from it.</p>"},{"location":"chapters/08-model-evaluation/#the-holdout-method","title":"The Holdout Method","text":"<p>The holdout method is the simplest validation approach\u2014it's just the train-test split we learned earlier. While it's simple and fast, it's also the least reliable because your results depend on the random split. Cross-validation improves upon the holdout method by removing this randomness.</p>"},{"location":"chapters/08-model-evaluation/#hyperparameters-the-settings-you-choose","title":"Hyperparameters: The Settings You Choose","text":"<p>Hyperparameters are the settings you choose before training your model. They're different from regular parameters (like regression coefficients) which are learned during training.</p> <p>Examples of hyperparameters:</p> <ul> <li>The degree in polynomial regression</li> <li>The train-test split ratio</li> <li>The number of folds K in cross-validation</li> <li>(In future chapters) Learning rate, number of layers, regularization strength</li> </ul> <p>Hyperparameters are typically chosen by trying different values and seeing which performs best on validation data. This process is called hyperparameter tuning.</p> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Create a pipeline\npipe = Pipeline([\n    ('poly', PolynomialFeatures()),\n    ('linear', LinearRegression())\n])\n\n# Define hyperparameters to try\nparam_grid = {'poly__degree': [1, 2, 3, 4, 5]}\n\n# Search for best hyperparameters\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='r2')\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best degree: {grid_search.best_params_['poly__degree']}\")\nprint(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n</code></pre>"},{"location":"chapters/08-model-evaluation/#model-selection-and-comparison","title":"Model Selection and Comparison","text":"<p>Model selection is the process of choosing the best model from a set of candidates. This could mean choosing between:</p> <ul> <li>Different algorithms (linear vs. polynomial)</li> <li>Different feature sets (which columns to include)</li> <li>Different hyperparameter settings</li> </ul> <p>The key principle: always compare models using their test performance (or cross-validation score), never their training performance. A model that looks great on training data might be terrible in practice.</p> <p>Model comparison involves evaluating multiple models on the same data using the same metrics. Here's a systematic approach:</p> <pre><code>import plotly.graph_objects as go\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Compare multiple polynomial degrees\nresults = []\ndegrees = range(1, 11)\n\nfor degree in degrees:\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n\n    results.append({\n        'degree': degree,\n        'mean_score': cv_scores.mean(),\n        'std_score': cv_scores.std()\n    })\n\nresults_df = pd.DataFrame(results)\n\n# Visualize with Plotly\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=results_df['degree'],\n    y=results_df['mean_score'],\n    mode='lines+markers',\n    name='Mean CV Score',\n    error_y=dict(type='data', array=results_df['std_score'], visible=True)\n))\n\nfig.update_layout(\n    title='Model Comparison: Polynomial Degree vs. CV Score',\n    xaxis_title='Polynomial Degree',\n    yaxis_title='Cross-Validation R\u00b2',\n    height=400\n)\n\nfig.show()\n\n# Find the best model\nbest_idx = results_df['mean_score'].idxmax()\nprint(f\"Best model: degree {results_df.loc[best_idx, 'degree']}\")\nprint(f\"Score: {results_df.loc[best_idx, 'mean_score']:.4f}\")\n</code></pre>"},{"location":"chapters/08-model-evaluation/#diagram-model-selection-dashboard","title":"Diagram: Model Selection Dashboard","text":"Model Selection Dashboard <p>Type: microsim</p> <p>Bloom Taxonomy: Evaluate, Analyze</p> <p>Learning Objective: Practice the complete model selection workflow, from training multiple models to selecting the best one based on validation performance</p> <p>Canvas Layout (900x600): - Left panel (450x600): Model configuration and training - Right panel (450x600): Results comparison and visualization</p> <p>Left Panel Elements: - Dataset selector: \"Generate Data\" button with options (linear, quadratic, sine wave, noisy) - Model type selector: Linear, Polynomial (with degree slider 1-10) - Train-Test split slider (60-90%) - Cross-validation folds dropdown (3, 5, 10) - \"Train Model\" button - \"Add to Comparison\" button</p> <p>Right Panel Elements: - Table of trained models with columns: Model Name, Train R\u00b2, Test R\u00b2, CV Mean, CV Std - Bar chart comparing CV scores across models - Selected model's predictions vs actual scatter plot - \"Declare Winner\" button highlights best model - \"Clear All\" button resets comparison</p> <p>Interactive Workflow: 1. Generate or load data 2. Configure model settings 3. Click \"Train Model\" to see individual results 4. Click \"Add to Comparison\" to add to leaderboard 5. Repeat with different configurations 6. Compare all models in the results table 7. Click \"Declare Winner\" to highlight the best performer</p> <p>Visual Feedback: - Training progress animation when model trains - Color coding: green for best model, yellow for good, red for poor - Warning icons when overfitting detected (large train-test gap) - Trophy icon next to winning model</p> <p>Educational Hints: - Tooltip: \"Look for high CV score with low standard deviation\" - Warning when user tries to compare models on different data - Celebration animation when optimal model found</p> <p>Implementation: p5.js with integrated ML calculations</p>"},{"location":"chapters/08-model-evaluation/#putting-it-all-together-the-model-evaluation-workflow","title":"Putting It All Together: The Model Evaluation Workflow","text":"<p>Here's the complete workflow for evaluating models like a professional:</p> <ol> <li> <p>Split your data into training and test sets (or training, validation, and test)</p> </li> <li> <p>Train your model on the training data only</p> </li> <li> <p>Evaluate using cross-validation during model development</p> </li> <li> <p>Try different models/hyperparameters and compare using validation or CV scores</p> </li> <li> <p>Select the best model based on validation performance</p> </li> <li> <p>Final evaluation on test data only after all decisions are made</p> </li> <li> <p>Analyze residuals to check if model assumptions hold</p> </li> <li> <p>Report honest metrics including uncertainty (standard deviation)</p> </li> </ol> <pre><code># Complete evaluation workflow example\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Step 1: Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 2-5: Find best model using cross-validation on training data\npipe = Pipeline([('poly', PolynomialFeatures()), ('linear', LinearRegression())])\nparam_grid = {'poly__degree': [1, 2, 3, 4, 5]}\ngrid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='r2')\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best hyperparameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {grid_search.best_score_:.4f}\")\n\n# Step 6: Final evaluation on test data\nbest_model = grid_search.best_estimator_\ntest_score = best_model.score(X_test, y_test)\nprint(f\"Final test score: {test_score:.4f}\")\n\n# Step 7: Residual analysis\ny_pred = best_model.predict(X_test)\nresiduals = y_test - y_pred\nprint(f\"Mean residual: {residuals.mean():.4f}\")  # Should be close to 0\nprint(f\"Residual std: {residuals.std():.4f}\")\n</code></pre>"},{"location":"chapters/08-model-evaluation/#diagram-model-evaluation-workflow","title":"Diagram: Model Evaluation Workflow","text":"Model Evaluation Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Apply, Analyze</p> <p>Learning Objective: Understand the complete model evaluation pipeline and the order of operations to avoid data leakage</p> <p>Visual Style: Vertical flowchart with swimlanes for different data subsets</p> <p>Swimlanes: - Full Dataset - Training Data - Validation/CV - Test Data (final)</p> <p>Steps: 1. Start: \"Load Complete Dataset\"    Hover: \"All data before any splits\"    Lane: Full Dataset</p> <ol> <li> <p>Process: \"Split into Train and Test\"    Hover: \"Typically 80/20 split, test data is locked away\"    Lane: Full Dataset \u2192 Training Data + Test Data    Color: Blue</p> </li> <li> <p>Process: \"Train Initial Model\"    Hover: \"Fit model on training data only\"    Lane: Training Data    Color: Green</p> </li> <li> <p>Process: \"Cross-Validate\"    Hover: \"Get reliable performance estimate using K-fold CV\"    Lane: Training Data (with internal splits shown)    Color: Green</p> </li> <li> <p>Decision: \"Try Different Models?\"    Hover: \"Compare polynomial degrees, feature sets, algorithms\"    Lane: Validation/CV    Color: Yellow</p> </li> <li> <p>Process: \"Hyperparameter Tuning\"    Hover: \"Use GridSearchCV or similar to find best settings\"    Lane: Training Data    Color: Green</p> </li> <li> <p>Process: \"Select Best Model\"    Hover: \"Choose based on validation/CV performance, not training!\"    Lane: Validation/CV    Color: Yellow</p> </li> <li> <p>Process: \"Final Evaluation\"    Hover: \"ONLY NOW touch test data - this is your honest grade\"    Lane: Test Data    Color: Red</p> </li> <li> <p>Process: \"Residual Analysis\"    Hover: \"Check for patterns, validate assumptions\"    Lane: Test Data    Color: Red</p> </li> <li> <p>End: \"Report Results\"     Hover: \"Report test metrics with confidence intervals\"     Lane: All lanes     Color: Purple</p> </li> </ol> <p>Arrows and Flow: - Main flow goes top to bottom - Iteration loop from \"Try Different Models?\" back to \"Train Initial Model\" - Clear visual barrier before \"Final Evaluation\" indicating \"Point of No Return\"</p> <p>Key Visual Elements: - Lock icon on Test Data swimlane until step 8 - Warning symbol if any arrow tries to cross into Test Data early - Checkmarks appearing as each step completes</p> <p>Implementation: HTML/CSS/JavaScript with hover interactions</p>"},{"location":"chapters/08-model-evaluation/#common-pitfalls-and-how-to-avoid-them","title":"Common Pitfalls and How to Avoid Them","text":"<p>As you develop your model evaluation superpowers, watch out for these traps:</p> <p>Data Leakage: Information from test data influences training. This inflates your metrics and leads to disappointment in production. Always split data before any preprocessing that looks at target values.</p> <p>Overfitting to Validation Data: If you try too many models and always pick the best validation score, you can overfit to your validation set. Hold out a truly final test set and only use it once.</p> <p>Ignoring Variance: A single train-test split gives you one number. That number has uncertainty! Use cross-validation to estimate how stable your performance is.</p> <p>Wrong Metric for the Problem: R\u00b2 isn't always the right choice. For some problems, you might care more about avoiding big mistakes (use RMSE) or want robust performance (use MAE). Match your metric to your real-world goals.</p> <p>Not Checking Residuals: A model can have decent R\u00b2 but still have systematic problems visible in residual plots. Always look at your residuals!</p> <p>The Final Test Rule</p> <p>Once you evaluate on your test set, you're done. If you go back and tune your model based on test results, and then evaluate again, your test set has become a validation set. You've lost your honest evaluation. Some practitioners save a final \"holdout\" set that never gets touched until the very final model goes to production.</p>"},{"location":"chapters/08-model-evaluation/#summary-your-model-evaluation-toolkit","title":"Summary: Your Model Evaluation Toolkit","text":"<p>You now have a powerful toolkit for honest model evaluation:</p> <ul> <li>Train-test split separates learning from evaluation</li> <li>Validation data helps tune models without cheating</li> <li>R\u00b2, MSE, RMSE, MAE each tell different stories about performance</li> <li>Residual analysis reveals hidden problems</li> <li>Overfitting and underfitting are the twin dangers to avoid</li> <li>Bias-variance tradeoff explains why model complexity matters</li> <li>Cross-validation gives stable, reliable estimates</li> <li>Model comparison helps you choose the best approach</li> </ul> <p>Remember: the goal isn't just to build a model that looks good on paper. It's to build a model that will perform well on data it has never seen before\u2014because that's the only kind of data that matters in the real world.</p> <p>With these evaluation superpowers, you can confidently assess any model's true capabilities and avoid the trap of self-deception. You're no longer just building models; you're building models you can trust.</p>"},{"location":"chapters/08-model-evaluation/#looking-ahead","title":"Looking Ahead","text":"<p>In the next chapter, we'll extend our regression toolkit to handle multiple features simultaneously. Multiple linear regression will let you model more complex relationships\u2014but with great power comes great responsibility. Your new evaluation skills will be essential for navigating the increased complexity without falling into the overfitting trap.</p>"},{"location":"chapters/08-model-evaluation/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Never evaluate a model on the same data it was trained on\u2014that's just testing memorization</li> <li>The train-test split creates honest evaluation; cross-validation makes it reliable</li> <li>R\u00b2 tells you proportion of variance explained; RMSE tells you typical error size in original units</li> <li>Residual plots reveal patterns your metrics might miss</li> <li>Overfitting (high variance) and underfitting (high bias) are equally dangerous</li> <li>Cross-validation gives you both a performance estimate and uncertainty measure</li> <li>Model selection should be based on validation/CV performance, with final evaluation on held-out test data</li> <li>The simpler model that performs nearly as well is often the better choice</li> </ul>"},{"location":"chapters/09-multiple-linear-regression/","title":"Multiple Linear Regression","text":"<p>title: Multiple Linear Regression description: Unlock the power of multiple features to build more accurate predictions generated_by: chapter-content-generator skill date: 2025-12-15 version: 0.03</p>"},{"location":"chapters/09-multiple-linear-regression/#summary","title":"Summary","text":"<p>This chapter extends linear regression to handle multiple predictor variables. Students will learn to build models with multiple features, understand and diagnose multicollinearity, and apply various feature selection methods. The chapter covers handling categorical variables through dummy variables and one-hot encoding, creating interaction terms, and understanding feature importance. By the end of this chapter, students will be able to build and interpret multiple regression models with both numerical and categorical predictors.</p>"},{"location":"chapters/09-multiple-linear-regression/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>Multiple Linear Regression</li> <li>Multiple Predictors</li> <li>Multicollinearity</li> <li>Variance Inflation Factor</li> <li>Feature Selection</li> <li>Forward Selection</li> <li>Backward Elimination</li> <li>Stepwise Selection</li> <li>Categorical Variables</li> <li>Dummy Variables</li> <li>One-Hot Encoding</li> <li>Interaction Terms</li> <li>Polynomial Features</li> <li>Feature Engineering</li> <li>Feature Importance</li> </ol>"},{"location":"chapters/09-multiple-linear-regression/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 7: Simple Linear Regression</li> <li>Chapter 8: Model Evaluation and Validation</li> </ul>"},{"location":"chapters/09-multiple-linear-regression/#introduction-leveling-up-your-prediction-powers","title":"Introduction: Leveling Up Your Prediction Powers","text":"<p>In the last few chapters, you learned to predict outcomes using a single feature. That's like trying to predict someone's basketball skills by only looking at their height. Sure, height matters, but what about their practice hours, speed, and jumping ability? Real-world predictions almost always depend on multiple factors working together.</p> <p>Multiple linear regression is your superpower upgrade. Instead of drawing a line through 2D data, you're now fitting a hyperplane through multi-dimensional space. Don't worry if that sounds intimidating\u2014the math is surprisingly similar to what you already know, and scikit-learn handles the heavy lifting. Your job is to understand what the model is doing and how to use it wisely.</p> <p>By the end of this chapter, you'll be able to build models that consider dozens of features simultaneously, handle both numbers and categories, and identify which features actually matter. That's serious prediction power.</p>"},{"location":"chapters/09-multiple-linear-regression/#from-one-feature-to-many-multiple-predictors","title":"From One Feature to Many: Multiple Predictors","text":"<p>In simple linear regression, we had one predictor variable \\(x\\) and one target \\(y\\):</p> \\[y = \\beta_0 + \\beta_1 x\\] <p>With multiple linear regression, we have multiple predictors\u2014let's call them \\(x_1, x_2, x_3\\), and so on:</p> \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... + \\beta_p x_p\\] <p>Each \\(\\beta\\) coefficient tells you how much \\(y\\) changes when that specific \\(x\\) increases by one unit, holding all other variables constant. That last part is crucial\u2014it's what makes multiple regression so powerful. You can isolate the effect of each feature.</p> <p>Here's a concrete example. Suppose you're predicting house prices with three features:</p> <ul> <li>\\(x_1\\) = square footage</li> <li>\\(x_2\\) = number of bedrooms</li> <li>\\(x_3\\) = age of house (years)</li> </ul> <p>Your model might look like:</p> \\[\\text{Price} = 50000 + 150 \\times \\text{SqFt} + 10000 \\times \\text{Bedrooms} - 1000 \\times \\text{Age}\\] <p>This tells you:</p> <ul> <li>Base price is $50,000</li> <li>Each square foot adds $150</li> <li>Each bedroom adds $10,000</li> <li>Each year of age subtracts $1,000</li> </ul> <p>The negative coefficient for age makes sense\u2014older houses typically sell for less, all else being equal.</p>"},{"location":"chapters/09-multiple-linear-regression/#building-your-first-multiple-regression-model","title":"Building Your First Multiple Regression Model","text":"<p>Let's build a multiple regression model in Python. The process is almost identical to simple regression\u2014scikit-learn handles the complexity behind the scenes.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# Load housing data\ndf = pd.read_csv('housing.csv')\n\n# Select multiple features\nfeatures = ['square_feet', 'bedrooms', 'bathrooms', 'age', 'lot_size']\nX = df[features]\ny = df['price']\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# View the coefficients\nprint(\"Intercept:\", model.intercept_)\nfor feature, coef in zip(features, model.coef_):\n    print(f\"{feature}: {coef:.2f}\")\n</code></pre> <p>The output shows you how each feature contributes to the prediction. Positive coefficients increase the predicted price; negative ones decrease it.</p>"},{"location":"chapters/09-multiple-linear-regression/#diagram-multiple-regression-anatomy","title":"Diagram: Multiple Regression Anatomy","text":"Multiple Regression Anatomy <p>Type: infographic</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Help students visualize how multiple features combine to form a single prediction, understanding each coefficient's role</p> <p>Layout: Central equation with branching explanations for each component</p> <p>Visual Elements: - Large central equation: y = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + \u03b2\u2083x\u2083 - Each term has an arrow pointing to an explanation box - \u03b2\u2080 box: \"Starting point (intercept) - prediction when all features are zero\" - Each \u03b2\u1d62x\u1d62 box: shows feature name, coefficient value, and contribution - Final prediction shown as sum of all contributions with animated addition</p> <p>Interactive Elements: - Hover over each term to see its specific contribution - Slider for each feature value (x\u2081, x\u2082, x\u2083) - As sliders move, show each term's contribution updating - Final prediction updates in real-time as sum of all terms - Color coding: positive contributions in green, negative in red</p> <p>Example Data: - House price prediction with square_feet, bedrooms, age - Show specific numbers: 150 \u00d7 1500 sqft = $225,000 contribution</p> <p>Color Scheme: - Intercept: Blue - Positive coefficients: Green gradient - Negative coefficients: Red gradient - Final prediction: Gold</p> <p>Implementation: HTML/CSS/JavaScript with interactive sliders</p>"},{"location":"chapters/09-multiple-linear-regression/#interpreting-multiple-regression-coefficients","title":"Interpreting Multiple Regression Coefficients","text":"<p>Each coefficient in multiple regression has a specific interpretation: it tells you the expected change in \\(y\\) for a one-unit increase in that feature, while holding all other features constant. This \"all else being equal\" interpretation is what makes multiple regression so valuable.</p> <p>Let's examine our model's coefficients:</p> <pre><code>import plotly.express as px\n\n# Create coefficient visualization\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': model.coef_\n})\n\nfig = px.bar(\n    coef_df,\n    x='Feature',\n    y='Coefficient',\n    title='Feature Coefficients: Impact on House Price',\n    color='Coefficient',\n    color_continuous_scale='RdYlGn',\n    color_continuous_midpoint=0\n)\n\nfig.update_layout(height=400)\nfig.show()\n</code></pre> <p>A few important caveats about interpreting coefficients:</p> Consideration Why It Matters Scale differences A coefficient of 100 for square feet isn't comparable to 10,000 for bedrooms\u2014units differ Correlation between features If bedrooms and square feet are correlated, their individual effects are harder to isolate Non-linear relationships Coefficients assume linear effects; reality might be curved Categorical variables Need special handling (we'll cover this soon) <p>Standardizing for Fair Comparison</p> <p>To compare coefficient magnitudes fairly, standardize your features first (subtract mean, divide by standard deviation). Then coefficients represent \"effect of one standard deviation change\" and are directly comparable.</p>"},{"location":"chapters/09-multiple-linear-regression/#the-multicollinearity-problem","title":"The Multicollinearity Problem","text":"<p>Here's a tricky situation: what happens when your predictor variables are highly correlated with each other? This is called multicollinearity, and it can cause serious problems for your model.</p> <p>Imagine predicting house prices with both \"square feet\" and \"number of rooms.\" These features are strongly related\u2014bigger houses have more rooms. When features are correlated:</p> <ul> <li>Coefficients become unstable (small data changes cause big coefficient swings)</li> <li>Standard errors inflate, making significance tests unreliable</li> <li>Individual feature effects become hard to interpret</li> <li>The model might still predict well overall, but you can't trust individual coefficients</li> </ul> <p>Think of it like two people trying to push a car together at the exact same angle. You can see the car moved, but you can't tell who pushed harder\u2014their efforts are indistinguishable.</p> <pre><code># Check for correlations between features\ncorrelation_matrix = X.corr()\n\nimport plotly.figure_factory as ff\n\nfig = ff.create_annotated_heatmap(\n    z=correlation_matrix.values,\n    x=list(correlation_matrix.columns),\n    y=list(correlation_matrix.index),\n    colorscale='RdBu',\n    zmid=0\n)\n\nfig.update_layout(title='Feature Correlation Matrix', height=500)\nfig.show()\n</code></pre> <p>Look for correlations above 0.7 or below -0.7\u2014these pairs of features might cause multicollinearity issues.</p>"},{"location":"chapters/09-multiple-linear-regression/#variance-inflation-factor-quantifying-multicollinearity","title":"Variance Inflation Factor: Quantifying Multicollinearity","text":"<p>The Variance Inflation Factor (VIF) is a precise way to measure multicollinearity. It tells you how much the variance of a coefficient is inflated due to correlations with other predictors.</p> <ul> <li>VIF = 1: No correlation with other features (ideal)</li> <li>VIF = 1-5: Moderate correlation (usually acceptable)</li> <li>VIF &gt; 5: High correlation (concerning)</li> <li>VIF &gt; 10: Severe multicollinearity (definitely a problem)</li> </ul> <p>Here's how to calculate VIF for each feature:</p> <pre><code>from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['Feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\nprint(vif_data.sort_values('VIF', ascending=False))\n</code></pre> <p>If you find high VIF values, you have options:</p> <ul> <li>Remove one of the correlated features</li> <li>Combine correlated features into a single composite feature</li> <li>Use regularization techniques (covered in a later chapter)</li> <li>Accept that individual coefficients may be unreliable, but overall predictions are fine</li> </ul>"},{"location":"chapters/09-multiple-linear-regression/#diagram-multicollinearity-detector-microsim","title":"Diagram: Multicollinearity Detector MicroSim","text":"Multicollinearity Detector MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze, Evaluate</p> <p>Learning Objective: Help students understand how correlated features affect coefficient stability and learn to diagnose multicollinearity using VIF</p> <p>Canvas Layout (850x500): - Left panel (400x500): Scatter plot matrix showing feature correlations - Right panel (450x500): VIF display and coefficient stability visualization</p> <p>Left Panel Elements: - 3x3 scatter plot matrix for selected features - Correlation coefficients displayed on off-diagonal - Color intensity indicates correlation strength - Clickable to focus on any pair</p> <p>Right Panel Elements: - Bar chart of VIF values for all features - Color coding: Green (&lt;5), Yellow (5-10), Red (&gt;10) - Below: Coefficient confidence intervals that widen with higher VIF - Warning messages for problematic features</p> <p>Interactive Controls: - Dropdown: Select dataset (housing, cars, student performance) - Checkbox: Add highly correlated feature (to demonstrate VIF increase) - Button: \"Simulate 100 data samples\" - shows coefficient variation - Slider: Artificially adjust correlation between two features</p> <p>Key Demonstrations: - Watch VIF spike when adding a correlated feature - See coefficient confidence intervals widen with high VIF - Observe coefficient values fluctuate wildly when resampling with multicollinearity</p> <p>Implementation: p5.js with statistical calculations</p>"},{"location":"chapters/09-multiple-linear-regression/#feature-selection-choosing-the-right-variables","title":"Feature Selection: Choosing the Right Variables","text":"<p>Not every available feature belongs in your model. Feature selection is the art and science of choosing which variables to include. Too few features, and you underfit. Too many, and you risk overfitting and multicollinearity.</p> <p>There are three classic approaches to feature selection:</p>"},{"location":"chapters/09-multiple-linear-regression/#forward-selection","title":"Forward Selection","text":"<p>Forward selection starts with no features and adds them one at a time. At each step, you add the feature that most improves the model, until no remaining feature provides significant improvement.</p> <p>The process:</p> <ol> <li>Start with an empty model (intercept only)</li> <li>Try adding each remaining feature one at a time</li> <li>Keep the one that gives the biggest improvement (if significant)</li> <li>Repeat until no feature improves the model enough</li> </ol>"},{"location":"chapters/09-multiple-linear-regression/#backward-elimination","title":"Backward Elimination","text":"<p>Backward elimination works in reverse. Start with all features and remove the least useful ones:</p> <ol> <li>Start with all features in the model</li> <li>Find the feature with the smallest contribution (highest p-value or lowest impact)</li> <li>Remove it if it's below your threshold</li> <li>Repeat until all remaining features are significant</li> </ol>"},{"location":"chapters/09-multiple-linear-regression/#stepwise-selection","title":"Stepwise Selection","text":"<p>Stepwise selection combines both approaches. At each step, you can either add a feature or remove one, depending on which action most improves the model. This flexibility helps find combinations that neither forward nor backward selection would discover alone.</p> <pre><code># Simple implementation of forward selection using cross-validation\nfrom sklearn.model_selection import cross_val_score\n\ndef forward_selection(X, y, max_features=None):\n    remaining = list(X.columns)\n    selected = []\n    best_scores = []\n\n    if max_features is None:\n        max_features = len(remaining)\n\n    while remaining and len(selected) &lt; max_features:\n        best_score = -np.inf\n        best_feature = None\n\n        for feature in remaining:\n            current_features = selected + [feature]\n            X_subset = X[current_features]\n\n            # Use cross-validation to evaluate\n            score = cross_val_score(\n                LinearRegression(), X_subset, y, cv=5, scoring='r2'\n            ).mean()\n\n            if score &gt; best_score:\n                best_score = score\n                best_feature = feature\n\n        if best_feature:\n            selected.append(best_feature)\n            remaining.remove(best_feature)\n            best_scores.append(best_score)\n            print(f\"Added {best_feature}: CV R\u00b2 = {best_score:.4f}\")\n\n    return selected, best_scores\n\n# Run forward selection\nselected_features, scores = forward_selection(X_train, y_train)\n</code></pre> Method Starts With Action Best For Forward Selection No features Adds best one at a time Many features, few are relevant Backward Elimination All features Removes worst one at a time Fewer features, most are useful Stepwise Selection Any starting point Adds or removes each step Complex relationships"},{"location":"chapters/09-multiple-linear-regression/#diagram-feature-selection-race","title":"Diagram: Feature Selection Race","text":"Feature Selection Race <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Analyze</p> <p>Learning Objective: Visualize and compare different feature selection strategies, understanding how each method builds or prunes the feature set</p> <p>Canvas Layout (800x550): - Top area (800x400): Three parallel \"race tracks\" for each method - Bottom area (800x150): Results comparison table</p> <p>Race Track Elements: - Each track shows features as checkpoints - Forward: Start empty, light up features as added - Backward: Start full, dim features as removed - Stepwise: Show both add and remove actions - Current model score displayed at each step</p> <p>Interactive Controls: - Button: \"Start Race\" - animate all three methods simultaneously - Speed slider: Control animation speed - Dropdown: Select dataset - Checkbox: \"Show R\u00b2 at each step\" - Button: \"Compare Final Models\"</p> <p>Animation: - Features light up (added) or dim (removed) as methods progress - Score counter updates at each step - Pause at each step to show decision being made - Highlight which feature is being considered</p> <p>Results Comparison: - Table showing: Method, Features Selected, Final R\u00b2, Time - Visual indicator of which method \"won\" (best score) - Discussion of when each method excels</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/09-multiple-linear-regression/#handling-categorical-variables","title":"Handling Categorical Variables","text":"<p>So far, we've only used numerical features. But what about categorical variables like neighborhood, car brand, or education level? These don't have a natural numeric ordering, so we can't just plug them into the equation.</p> <p>The solution is to convert categories into numbers using dummy variables or one-hot encoding.</p>"},{"location":"chapters/09-multiple-linear-regression/#dummy-variables","title":"Dummy Variables","text":"<p>A dummy variable is a binary (0 or 1) variable that represents whether an observation belongs to a category. For a categorical variable with \\(k\\) categories, you create \\(k-1\\) dummy variables.</p> <p>Why \\(k-1\\) instead of \\(k\\)? Because the last category is implied when all dummies are 0. This avoids redundancy and multicollinearity.</p> <p>Example: For \"Neighborhood\" with three values (Downtown, Suburbs, Rural):</p> Observation Neighborhood Is_Downtown Is_Suburbs House 1 Downtown 1 0 House 2 Suburbs 0 1 House 3 Rural 0 0 House 4 Downtown 1 0 <p>Notice that Rural is the \"reference category\"\u2014it's represented by zeros in both columns.</p>"},{"location":"chapters/09-multiple-linear-regression/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>One-hot encoding creates \\(k\\) dummy variables (one for each category). While this seems simpler, it creates redundancy that must be handled. Most libraries automatically drop one category to prevent issues.</p> <pre><code>import pandas as pd\n\n# Method 1: pd.get_dummies (creates one-hot encoding)\ndf_encoded = pd.get_dummies(df, columns=['neighborhood'], drop_first=True)\n\n# Method 2: Using scikit-learn's OneHotEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Create preprocessor\ncategorical_features = ['neighborhood', 'style']\nnumerical_features = ['square_feet', 'bedrooms', 'age']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', 'passthrough', numerical_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ]\n)\n\n# Fit and transform\nX_processed = preprocessor.fit_transform(X)\n</code></pre>"},{"location":"chapters/09-multiple-linear-regression/#diagram-one-hot-encoding-visualizer","title":"Diagram: One-Hot Encoding Visualizer","text":"One-Hot Encoding Visualizer <p>Type: infographic</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Demonstrate how categorical variables are transformed into numerical format through one-hot encoding</p> <p>Layout: Before/After transformation with animated conversion</p> <p>Visual Elements: - Left side: Original categorical column with color-coded categories - Right side: Multiple binary columns (one per category) - Animated arrows showing the transformation - Each row clearly shows which column gets the \"1\"</p> <p>Example Data: - Categorical column: Color (Red, Blue, Green, Red, Blue) - Transforms to: Is_Red, Is_Blue, Is_Green columns - Shows both \"keep all\" and \"drop first\" options</p> <p>Interactive Elements: - Dropdown: Select different categorical variables to encode - Toggle: \"Drop first category\" vs \"Keep all categories\" - Hover: Highlight corresponding cells in original and encoded view - Button: \"Add new category\" - shows a new column appears - Slider: Adjust number of unique categories (2-8) to see encoding grow</p> <p>Educational Callouts: - Warning when all categories kept: \"This creates multicollinearity!\" - Explanation of reference category concept - Formula showing how original is reconstructed</p> <p>Color Scheme: - Each category has unique color - Same colors used in binary columns for matching</p> <p>Implementation: HTML/CSS/JavaScript with smooth animations</p>"},{"location":"chapters/09-multiple-linear-regression/#interaction-terms-when-features-work-together","title":"Interaction Terms: When Features Work Together","text":"<p>Sometimes the effect of one feature depends on the value of another. For example, the value of a swimming pool might depend on whether the house is in a warm or cold climate. A pool adds more value in Arizona than in Alaska!</p> <p>Interaction terms capture these combined effects by multiplying features together:</p> \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\times x_2)\\] <p>The interaction term \\(x_1 \\times x_2\\) allows the effect of \\(x_1\\) to change depending on the value of \\(x_2\\) (and vice versa).</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\n\n# Create interaction terms (degree=2, no squared terms)\ninteraction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_with_interactions = interaction.fit_transform(X)\n\n# See the new feature names\nfeature_names = interaction.get_feature_names_out(features)\nprint(\"Features with interactions:\", feature_names)\n</code></pre> <p>When to consider interactions:</p> <ul> <li>Domain knowledge suggests features work together</li> <li>Residual plots show patterns when you split by another variable</li> <li>Theory indicates multiplicative effects</li> <li>You have enough data to estimate additional parameters</li> </ul> <p>Interaction Explosion</p> <p>With many features, the number of possible interactions explodes. Five features have 10 pairwise interactions. Ten features have 45. Only include interactions you have good reason to suspect exist, or use regularization to prevent overfitting.</p>"},{"location":"chapters/09-multiple-linear-regression/#polynomial-features-capturing-curved-relationships","title":"Polynomial Features: Capturing Curved Relationships","text":"<p>Remember from simple regression that relationships aren't always linear? Polynomial features extend multiple regression to handle curved relationships by including squared, cubed, or higher-order terms.</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Create polynomial features of degree 2\npoly_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n    ('linear', LinearRegression())\n])\n\npoly_model.fit(X_train, y_train)\n\n# Evaluate\ntrain_score = poly_model.score(X_train, y_train)\ntest_score = poly_model.score(X_test, y_test)\n\nprint(f\"Train R\u00b2: {train_score:.4f}\")\nprint(f\"Test R\u00b2: {test_score:.4f}\")\n</code></pre> <p>With polynomial features of degree 2 on 5 original features, you get:</p> <ul> <li>5 original features</li> <li>5 squared terms (x\u2081\u00b2, x\u2082\u00b2, ...)</li> <li>10 interaction terms (x\u2081x\u2082, x\u2081x\u2083, ...)</li> <li>Total: 20 features!</li> </ul> <p>This is powerful but dangerous. Watch your test score carefully\u2014it's easy to overfit with high-degree polynomials.</p>"},{"location":"chapters/09-multiple-linear-regression/#feature-engineering-the-art-of-creating-better-features","title":"Feature Engineering: The Art of Creating Better Features","text":"<p>Feature engineering is the creative process of transforming raw data into features that better represent the underlying problem. This is often where data scientists add the most value\u2014domain knowledge transformed into predictive power.</p> <p>Common feature engineering techniques:</p> Technique Example Why It Helps Log transform log(income) Handles skewed distributions Binning Age groups (20s, 30s, 40s) Captures non-linear thresholds Date extraction Day of week from timestamp Captures cyclical patterns Ratios Price per square foot Normalizes for size Aggregations Average neighborhood price Incorporates context Domain calculations BMI from height and weight Captures known relationships <pre><code># Feature engineering examples\ndf['price_per_sqft'] = df['price'] / df['square_feet']\ndf['age_squared'] = df['age'] ** 2\ndf['log_lot_size'] = np.log(df['lot_size'] + 1)  # +1 to handle zeros\ndf['rooms_per_bathroom'] = df['bedrooms'] / df['bathrooms']\ndf['is_new'] = (df['age'] &lt; 5).astype(int)\n</code></pre> <p>Good feature engineering requires:</p> <ul> <li>Understanding your domain</li> <li>Exploring the data thoroughly</li> <li>Creativity and experimentation</li> <li>Validation to confirm new features actually help</li> </ul>"},{"location":"chapters/09-multiple-linear-regression/#diagram-feature-engineering-laboratory","title":"Diagram: Feature Engineering Laboratory","text":"Feature Engineering Laboratory <p>Type: microsim</p> <p>Bloom Taxonomy: Create, Apply</p> <p>Learning Objective: Practice creating new features and immediately see their impact on model performance</p> <p>Canvas Layout (900x550): - Left panel (350x550): Feature creation interface - Center panel (350x550): Data preview with new features - Right panel (200x550): Model performance metrics</p> <p>Left Panel - Feature Creation: - Dropdown: Select first variable - Dropdown: Select operation (+, -, *, /, log, square, bin) - Dropdown: Select second variable (if applicable) - Text input: New feature name - Button: \"Create Feature\" - List of created features with delete option</p> <p>Center Panel - Data Preview: - Table showing original and engineered features - First 10 rows of data - Histogram of new feature distribution - Correlation of new feature with target</p> <p>Right Panel - Performance: - R\u00b2 score (updates when features change) - Train vs Test comparison - Feature importance ranking - Delta from baseline (how much new features helped)</p> <p>Interactive Workflow: 1. View baseline model performance 2. Create a new feature 3. See immediate impact on R\u00b2 4. Try different transformations 5. Compare which features help most</p> <p>Preset Examples: - Button: \"Try log transform on skewed feature\" - Button: \"Create ratio feature\" - Button: \"Add polynomial term\"</p> <p>Implementation: p5.js with real-time model retraining</p>"},{"location":"chapters/09-multiple-linear-regression/#feature-importance-understanding-what-matters","title":"Feature Importance: Understanding What Matters","text":"<p>After building a model with many features, you'll want to know which ones are actually important. Feature importance measures how much each feature contributes to predictions.</p> <p>Several approaches to measure importance:</p>"},{"location":"chapters/09-multiple-linear-regression/#coefficient-magnitude-after-standardization","title":"Coefficient Magnitude (After Standardization)","text":"<p>When features are standardized, coefficient magnitude indicates relative importance:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# Fit model on standardized data\nmodel_scaled = LinearRegression()\nmodel_scaled.fit(X_scaled, y_train)\n\n# Compare coefficient magnitudes\nimportance_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': np.abs(model_scaled.coef_)\n}).sort_values('Coefficient', ascending=False)\n\nprint(importance_df)\n</code></pre>"},{"location":"chapters/09-multiple-linear-regression/#permutation-importance","title":"Permutation Importance","text":"<p>Permutation importance measures how much the model's performance drops when you randomly shuffle one feature's values. A big drop means the feature was important:</p> <pre><code>from sklearn.inspection import permutation_importance\n\n# Calculate permutation importance\nperm_importance = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Create importance dataframe\nimportance_df = pd.DataFrame({\n    'Feature': features,\n    'Importance': perm_importance.importances_mean,\n    'Std': perm_importance.importances_std\n}).sort_values('Importance', ascending=False)\n\n# Visualize with Plotly\nfig = px.bar(\n    importance_df,\n    x='Importance',\n    y='Feature',\n    orientation='h',\n    error_x='Std',\n    title='Permutation Feature Importance'\n)\n\nfig.update_layout(height=400, yaxis={'categoryorder': 'total ascending'})\nfig.show()\n</code></pre> <p>Permutation importance has advantages:</p> <ul> <li>Works for any model, not just linear regression</li> <li>Captures importance in context of other features</li> <li>Accounts for interactions</li> </ul>"},{"location":"chapters/09-multiple-linear-regression/#diagram-feature-importance-explorer","title":"Diagram: Feature Importance Explorer","text":"Feature Importance Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze, Evaluate</p> <p>Learning Objective: Compare different methods of measuring feature importance and understand their trade-offs</p> <p>Canvas Layout (800x500): - Left panel (400x500): Importance comparison chart - Right panel (400x500): Individual feature deep-dive</p> <p>Left Panel Elements: - Three parallel horizontal bar charts stacked vertically:   1. Coefficient magnitude (standardized)   2. Permutation importance   3. Drop-column importance (R\u00b2 drop when feature removed) - Features aligned across all three charts for easy comparison - Color coding shows agreement/disagreement between methods</p> <p>Right Panel - Feature Deep-Dive: - Select a feature to explore in detail - Scatter plot: feature vs target - Partial dependence plot - Distribution of feature values - Interaction effects with other top features</p> <p>Interactive Controls: - Dropdown: Select which importance method to highlight - Click on feature bar to see deep-dive in right panel - Toggle: Show error bars (std across iterations) - Button: \"Run permutation test\" (animated shuffling)</p> <p>Visual Insights: - Highlight when methods disagree about importance ranking - Show confidence intervals for permutation importance - Indicate which features might be redundant (similar importance patterns)</p> <p>Implementation: p5.js with multiple visualization modes</p>"},{"location":"chapters/09-multiple-linear-regression/#putting-it-all-together-a-complete-multiple-regression-workflow","title":"Putting It All Together: A Complete Multiple Regression Workflow","text":"<p>Here's a complete workflow for building a multiple regression model with all the techniques we've learned:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport plotly.express as px\n\n# 1. Load and explore data\ndf = pd.read_csv('housing.csv')\n\n# 2. Identify feature types\nnumerical_features = ['square_feet', 'bedrooms', 'bathrooms', 'age', 'lot_size']\ncategorical_features = ['neighborhood', 'style']\ntarget = 'price'\n\n# 3. Create feature engineering\ndf['price_per_sqft'] = df['price'] / df['square_feet']\ndf['age_squared'] = df['age'] ** 2\n\n# Update feature list\nnumerical_features.append('age_squared')\n\n# 4. Split data\nX = df[numerical_features + categorical_features]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 5. Create preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n    ]\n)\n\n# 6. Create full pipeline\nmodel_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# 7. Train and evaluate with cross-validation\ncv_scores = cross_val_score(model_pipeline, X_train, y_train, cv=5, scoring='r2')\nprint(f\"CV R\u00b2 scores: {cv_scores}\")\nprint(f\"Mean CV R\u00b2: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n\n# 8. Fit final model and evaluate on test set\nmodel_pipeline.fit(X_train, y_train)\ntest_score = model_pipeline.score(X_test, y_test)\nprint(f\"Test R\u00b2: {test_score:.4f}\")\n\n# 9. Check for multicollinearity in numerical features\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX_num = X_train[numerical_features]\nvif_data = pd.DataFrame({\n    'Feature': numerical_features,\n    'VIF': [variance_inflation_factor(X_num.values, i) for i in range(len(numerical_features))]\n})\nprint(\"\\nVIF values:\")\nprint(vif_data)\n\n# 10. Visualize predictions vs actuals\ny_pred = model_pipeline.predict(X_test)\n\nfig = px.scatter(\n    x=y_test,\n    y=y_pred,\n    labels={'x': 'Actual Price', 'y': 'Predicted Price'},\n    title='Multiple Regression: Actual vs Predicted'\n)\nfig.add_shape(type='line', x0=y_test.min(), x1=y_test.max(),\n              y0=y_test.min(), y1=y_test.max(),\n              line=dict(dash='dash', color='red'))\nfig.show()\n</code></pre>"},{"location":"chapters/09-multiple-linear-regression/#diagram-multiple-regression-pipeline","title":"Diagram: Multiple Regression Pipeline","text":"Multiple Regression Pipeline <p>Type: workflow</p> <p>Bloom Taxonomy: Apply, Analyze</p> <p>Learning Objective: Understand the complete workflow for building production-ready multiple regression models</p> <p>Visual Style: Horizontal flowchart with data transformation stages</p> <p>Stages: 1. \"Raw Data\"    Hover: \"Mixed types: numbers, categories, missing values\"    Icon: Database    Color: Gray</p> <ol> <li> <p>\"Feature Engineering\"    Hover: \"Create new features: ratios, transformations, domain knowledge\"    Icon: Wrench    Color: Blue    Sub-items: log transforms, ratios, polynomials</p> </li> <li> <p>\"Train/Test Split\"    Hover: \"80/20 split before any preprocessing\"    Icon: Scissors    Color: Purple</p> </li> <li> <p>\"Preprocessing\"    Hover: \"Scale numerics, encode categoricals\"    Icon: Filter    Color: Orange    Sub-items: StandardScaler, OneHotEncoder</p> </li> <li> <p>\"Check Multicollinearity\"    Hover: \"Calculate VIF, handle correlated features\"    Icon: Warning    Color: Yellow</p> </li> <li> <p>\"Feature Selection\"    Hover: \"Forward, backward, or stepwise selection\"    Icon: Checkboxes    Color: Teal</p> </li> <li> <p>\"Model Training\"    Hover: \"Fit LinearRegression on training data\"    Icon: Brain    Color: Green</p> </li> <li> <p>\"Cross-Validation\"    Hover: \"Get stable performance estimate\"    Icon: Loop    Color: Blue</p> </li> <li> <p>\"Final Evaluation\"    Hover: \"Test set performance, residual analysis\"    Icon: Chart    Color: Red</p> </li> <li> <p>\"Feature Importance\"     Hover: \"Understand what drives predictions\"     Icon: Bar Chart     Color: Gold</p> </li> </ol> <p>Data Flow Arrows: - Show data shape changing at each stage - Indicate sample counts at train/test split - Show feature counts growing (engineering) and shrinking (selection)</p> <p>Interactive Elements: - Click each stage for expanded view - Hover shows common pitfalls at each stage - Toggle to show \"what can go wrong\" warnings</p> <p>Implementation: HTML/CSS/JavaScript with click interactions</p>"},{"location":"chapters/09-multiple-linear-regression/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<p>As you build multiple regression models, watch out for these pitfalls:</p> <p>Including Too Many Features: More features don't always mean better models. Each feature adds complexity and potential for overfitting. Start simple and add features only when they demonstrably help.</p> <p>Ignoring Multicollinearity: High VIF values don't break your model, but they make coefficient interpretation unreliable. If you need to explain what each feature does, address multicollinearity first.</p> <p>Forgetting to Encode Categoricals: Passing string columns directly to scikit-learn causes errors. Always one-hot encode or use a proper preprocessor.</p> <p>Data Leakage in Preprocessing: Fit your scaler and encoder only on training data, then transform both train and test. Using information from test data during preprocessing inflates your performance estimates.</p> <p>Overfitting with Interactions and Polynomials: Each interaction or polynomial term is an additional feature. With the power to add quadratic terms and interactions, it's easy to create dozens of features that overfit your training data.</p> <p>The Simplicity Principle</p> <p>If a simpler model performs almost as well as a complex one, choose the simpler model. It will be easier to explain, more robust to new data, and less likely to fail in production.</p>"},{"location":"chapters/09-multiple-linear-regression/#summary-your-multiple-regression-toolkit","title":"Summary: Your Multiple Regression Toolkit","text":"<p>You now have a comprehensive toolkit for multiple regression:</p> <ul> <li>Multiple predictors let you model complex, multi-factor relationships</li> <li>Multicollinearity and VIF help you diagnose problematic feature correlations</li> <li>Feature selection methods (forward, backward, stepwise) find the best feature subsets</li> <li>Dummy variables and one-hot encoding handle categorical features</li> <li>Interaction terms capture features that work together</li> <li>Polynomial features model curved relationships</li> <li>Feature engineering creates new predictive variables from domain knowledge</li> <li>Feature importance reveals what's driving your predictions</li> </ul> <p>With these tools, you can build models that capture the true complexity of real-world problems while remaining interpretable and reliable.</p>"},{"location":"chapters/09-multiple-linear-regression/#looking-ahead","title":"Looking Ahead","text":"<p>In the next chapter, we'll explore NumPy in depth\u2014the numerical computing engine that powers all of these calculations. Understanding NumPy will help you work more efficiently with large datasets and understand what's happening under the hood of scikit-learn.</p>"},{"location":"chapters/09-multiple-linear-regression/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Multiple linear regression extends simple regression to handle any number of features</li> <li>Each coefficient represents the effect of that feature while holding others constant</li> <li>Multicollinearity occurs when features are correlated; use VIF to detect it</li> <li>Feature selection methods help identify the most useful features</li> <li>Categorical variables must be encoded as dummy variables before modeling</li> <li>Interaction terms capture features that work together in non-additive ways</li> <li>Feature engineering often provides more improvement than algorithm choice</li> <li>Always validate with cross-validation and check residuals for patterns</li> </ul>"},{"location":"chapters/10-numpy-computing/","title":"NumPy and Numerical Computing","text":"<p>title: NumPy and Numerical Computing description: Master the engine that powers all of data science generated_by: chapter-content-generator skill date: 2025-12-15 version: 0.03</p>"},{"location":"chapters/10-numpy-computing/#summary","title":"Summary","text":"<p>This chapter introduces NumPy, the fundamental library for numerical computing in Python. Students will learn to create and manipulate NumPy arrays, understand array shapes and indexing, and leverage broadcasting for efficient operations. The chapter covers vectorized operations, matrix mathematics, and linear algebra concepts essential for machine learning. By the end of this chapter, students will understand why NumPy is critical for computational efficiency and be able to perform fast numerical computations.</p>"},{"location":"chapters/10-numpy-computing/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>NumPy Library</li> <li>NumPy Array</li> <li>Array Creation</li> <li>Array Shape</li> <li>Array Indexing</li> <li>Array Slicing</li> <li>Broadcasting</li> <li>Vectorized Operations</li> <li>Element-wise Operations</li> <li>Matrix Operations</li> <li>Dot Product</li> <li>Matrix Multiplication</li> <li>Transpose</li> <li>Linear Algebra</li> <li>Computational Efficiency</li> </ol>"},{"location":"chapters/10-numpy-computing/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Python Environment and Setup</li> <li>Chapter 3: Python Data Structures</li> </ul>"},{"location":"chapters/10-numpy-computing/#introduction-the-speed-superpower","title":"Introduction: The Speed Superpower","text":"<p>Every superhero has an origin story, and NumPy is the origin story of fast data science. Without NumPy, all those fancy machine learning algorithms you've been using would take hours instead of seconds. NumPy is the invisible engine that makes everything else possible.</p> <p>Here's the deal: regular Python is fantastic for many things, but it's slow at math. Like, embarrassingly slow. When you need to multiply a million numbers, Python's built-in lists make you wait... and wait... and wait. NumPy solves this problem with arrays that are up to 100 times faster than regular Python lists.</p> <p>Why is NumPy so fast? Three secrets:</p> <ol> <li>Contiguous memory: NumPy stores numbers in a tight, organized row in your computer's memory, so the CPU can grab them quickly</li> <li>Compiled C code: The actual calculations happen in lightning-fast C, not interpreted Python</li> <li>Vectorization: Instead of looping through items one by one, NumPy processes entire arrays at once</li> </ol> <p>By the end of this chapter, you'll understand how to wield this speed superpower and why every data science library\u2014pandas, scikit-learn, PyTorch\u2014is built on NumPy's foundation.</p>"},{"location":"chapters/10-numpy-computing/#the-numpy-library-your-new-best-friend","title":"The NumPy Library: Your New Best Friend","text":"<p>The NumPy library is imported with the conventional alias <code>np</code>. This is so universal that if you see <code>np</code> in any data science code, you can be 99.9% certain it means NumPy.</p> <pre><code>import numpy as np\n\n# Check your version\nprint(np.__version__)\n</code></pre> <p>NumPy provides:</p> <ul> <li>Fast array operations for numerical data</li> <li>Mathematical functions (sin, cos, exp, log, etc.)</li> <li>Linear algebra operations</li> <li>Random number generation</li> <li>Tools for reading/writing array data</li> </ul> <p>Essentially, NumPy replaces Python's slow list operations with turbocharged alternatives. Once you start using NumPy, you'll wonder how you ever lived without it.</p>"},{"location":"chapters/10-numpy-computing/#numpy-arrays-the-core-data-structure","title":"NumPy Arrays: The Core Data Structure","text":"<p>The NumPy array (technically called <code>ndarray</code> for \"n-dimensional array\") is NumPy's main attraction. Unlike Python lists, NumPy arrays:</p> <ul> <li>Contain only one data type (all integers, all floats, etc.)</li> <li>Have a fixed size when created</li> <li>Support fast mathematical operations</li> <li>Can have multiple dimensions (1D, 2D, 3D, or more)</li> </ul> <p>Here's your first array:</p> <pre><code>import numpy as np\n\n# Create a simple 1D array\ntemperatures = np.array([72, 75, 79, 82, 85, 83, 80])\nprint(temperatures)\nprint(type(temperatures))  # &lt;class 'numpy.ndarray'&gt;\n</code></pre> <p>The difference between a NumPy array and a Python list might seem subtle at first, but watch what happens when we do math:</p> <pre><code># Python list: can't do this directly!\npython_list = [1, 2, 3, 4, 5]\n# python_list * 2  # This gives [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] - not what we want!\n\n# NumPy array: math just works!\nnumpy_array = np.array([1, 2, 3, 4, 5])\nprint(numpy_array * 2)  # [2, 4, 6, 8, 10] - exactly what we want!\n</code></pre> <p>This is the magic of NumPy: mathematical operations work element by element automatically.</p>"},{"location":"chapters/10-numpy-computing/#diagram-numpy-array-vs-python-list","title":"Diagram: NumPy Array vs Python List","text":"NumPy Array vs Python List <p>Type: infographic</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Help students visualize the structural differences between Python lists and NumPy arrays, and why those differences matter for performance</p> <p>Layout: Side-by-side comparison with memory visualization</p> <p>Left Side - Python List: - Show a list [1, 2, 3, 4, 5] as scattered boxes in memory - Each box contains a pointer to the actual number - Numbers stored in different memory locations - Label: \"Scattered in memory - slow to access\" - Show Python interpreter stepping through one at a time</p> <p>Right Side - NumPy Array: - Show array [1, 2, 3, 4, 5] as contiguous boxes - Numbers stored directly, side by side - Label: \"Contiguous in memory - fast bulk operations\" - Show CPU processing entire block at once</p> <p>Performance Comparison: - Speedometer graphics showing relative speeds - Python list: \"1x speed\" - NumPy array: \"50-100x speed\"</p> <p>Interactive Elements: - Slider: Array size (100 to 1,000,000) - Button: \"Run speed test\" - shows actual timing comparison - Animation: Watch memory access patterns for each type - Toggle: Show/hide memory addresses</p> <p>Color Scheme: - Python list elements: Various colors (scattered) - NumPy array elements: Uniform blue (organized) - Memory blocks: Gray background</p> <p>Implementation: p5.js with animated memory visualization</p>"},{"location":"chapters/10-numpy-computing/#array-creation-many-ways-to-build-arrays","title":"Array Creation: Many Ways to Build Arrays","text":"<p>Array creation in NumPy offers many convenient methods beyond <code>np.array()</code>. Depending on your needs, you can create arrays filled with specific values, sequences, or random numbers.</p>"},{"location":"chapters/10-numpy-computing/#from-python-lists","title":"From Python Lists","text":"<pre><code># 1D array from list\nscores = np.array([85, 92, 78, 96, 88])\n\n# 2D array from nested lists\nmatrix = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\nprint(matrix)\n</code></pre>"},{"location":"chapters/10-numpy-computing/#arrays-of-zeros-and-ones","title":"Arrays of Zeros and Ones","text":"<pre><code># 5 zeros\nzeros = np.zeros(5)\nprint(zeros)  # [0. 0. 0. 0. 0.]\n\n# 3x4 matrix of ones\nones = np.ones((3, 4))\nprint(ones)\n\n# 3x3 identity matrix (ones on diagonal)\nidentity = np.eye(3)\nprint(identity)\n</code></pre>"},{"location":"chapters/10-numpy-computing/#sequences-and-ranges","title":"Sequences and Ranges","text":"<pre><code># Like Python's range, but returns an array\nsequence = np.arange(0, 10, 2)  # start, stop, step\nprint(sequence)  # [0 2 4 6 8]\n\n# Evenly spaced numbers between two values\nsmooth = np.linspace(0, 1, 5)  # start, stop, num_points\nprint(smooth)  # [0.   0.25 0.5  0.75 1.  ]\n</code></pre>"},{"location":"chapters/10-numpy-computing/#random-arrays","title":"Random Arrays","text":"<pre><code># Random floats between 0 and 1\nrandom_floats = np.random.random(5)\nprint(random_floats)\n\n# Random integers in a range\nrandom_ints = np.random.randint(1, 100, size=10)  # 10 random ints from 1-99\nprint(random_ints)\n\n# Random numbers from normal distribution\nnormal = np.random.normal(loc=0, scale=1, size=1000)  # mean=0, std=1\nprint(f\"Mean: {normal.mean():.3f}, Std: {normal.std():.3f}\")\n</code></pre> Creation Method Use Case Example <code>np.array()</code> Convert existing data <code>np.array([1,2,3])</code> <code>np.zeros()</code> Initialize placeholders <code>np.zeros((3,3))</code> <code>np.ones()</code> Initialize to ones <code>np.ones(5)</code> <code>np.arange()</code> Integer sequences <code>np.arange(0,10,2)</code> <code>np.linspace()</code> Evenly spaced floats <code>np.linspace(0,1,100)</code> <code>np.random.random()</code> Random floats [0,1) <code>np.random.random(10)</code> <code>np.eye()</code> Identity matrix <code>np.eye(4)</code>"},{"location":"chapters/10-numpy-computing/#array-shape-understanding-dimensions","title":"Array Shape: Understanding Dimensions","text":"<p>The array shape tells you the size of each dimension of your array. This is crucial for understanding how your data is organized and for ensuring operations work correctly.</p> <pre><code># 1D array: shape is (n,)\nvector = np.array([1, 2, 3, 4, 5])\nprint(vector.shape)  # (5,)\n\n# 2D array: shape is (rows, columns)\nmatrix = np.array([[1, 2, 3], [4, 5, 6]])\nprint(matrix.shape)  # (2, 3) - 2 rows, 3 columns\n\n# 3D array: shape is (depth, rows, columns)\ncube = np.zeros((2, 3, 4))\nprint(cube.shape)  # (2, 3, 4)\n</code></pre> <p>Think of shape as describing the \"dimensions\" of your data:</p> <ul> <li>1D: A line of numbers (vector)</li> <li>2D: A table of numbers (matrix)</li> <li>3D: A stack of tables (tensor)</li> <li>nD: Higher dimensions follow the same pattern</li> </ul> <p>You can reshape arrays to change their dimensions (as long as the total number of elements stays the same):</p> <pre><code># Create 12 numbers\nnumbers = np.arange(12)\nprint(numbers)  # [ 0  1  2  3  4  5  6  7  8  9 10 11]\nprint(numbers.shape)  # (12,)\n\n# Reshape to 3x4 matrix\nmatrix = numbers.reshape(3, 4)\nprint(matrix)\nprint(matrix.shape)  # (3, 4)\n\n# Reshape to 2x2x3 cube\ncube = numbers.reshape(2, 2, 3)\nprint(cube.shape)  # (2, 2, 3)\n\n# Use -1 to let NumPy calculate one dimension\nauto_reshaped = numbers.reshape(4, -1)  # 4 rows, auto-calculate columns\nprint(auto_reshaped.shape)  # (4, 3)\n</code></pre> <p>The -1 Trick</p> <p>When reshaping, you can use -1 for one dimension, and NumPy will automatically calculate it. This is super handy when you know one dimension but not the other: <code>array.reshape(-1, 3)</code> gives you 3 columns with however many rows are needed.</p>"},{"location":"chapters/10-numpy-computing/#diagram-array-shape-visualizer","title":"Diagram: Array Shape Visualizer","text":"Array Shape Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Help students visualize how array shapes correspond to physical dimensions and how reshaping reorganizes data</p> <p>Canvas Layout (800x500): - Left panel (400x500): 3D visualization of array - Right panel (400x500): Shape controls and data view</p> <p>Left Panel - Visual Representation: - 1D: Horizontal row of numbered boxes - 2D: Grid of numbered boxes (rows \u00d7 columns) - 3D: Stack of 2D grids (depth \u00d7 rows \u00d7 columns) - Boxes contain actual values, colored by magnitude - Axes labeled with dimension sizes</p> <p>Right Panel - Controls: - Input fields for each dimension size - Dropdown: Quick presets (vector, matrix, cube) - Current shape display: (d1, d2, d3) - Total elements counter - Flattened view showing element order</p> <p>Interactive Elements: - Click and drag to rotate 3D view - Slider for each dimension (1-10) - Button: \"Reshape\" - animates transition between shapes - Button: \"Flatten\" - shows elements laid out in 1D - Toggle: \"Show indices\" - displays [i,j,k] for each cell - Highlight: Click an element to see its index in all views</p> <p>Reshape Animation: - Elements smoothly transition from old shape to new - Color trails show where each element moves - Error message if total elements don't match</p> <p>Educational Callouts: - \"Total elements must stay constant when reshaping\" - Show calculation: d1 \u00d7 d2 \u00d7 d3 = total</p> <p>Implementation: p5.js with 3D rendering (WEBGL mode)</p>"},{"location":"chapters/10-numpy-computing/#array-indexing-accessing-your-data","title":"Array Indexing: Accessing Your Data","text":"<p>Array indexing lets you access individual elements or groups of elements. NumPy indexing is similar to Python list indexing but more powerful.</p>"},{"location":"chapters/10-numpy-computing/#1d-indexing","title":"1D Indexing","text":"<pre><code>arr = np.array([10, 20, 30, 40, 50])\n\n# Single element (0-indexed)\nprint(arr[0])   # 10 (first element)\nprint(arr[2])   # 30 (third element)\nprint(arr[-1])  # 50 (last element)\n\n# Multiple elements with a list of indices\nprint(arr[[0, 2, 4]])  # [10 30 50]\n</code></pre>"},{"location":"chapters/10-numpy-computing/#2d-indexing","title":"2D Indexing","text":"<p>For 2D arrays, you provide two indices: <code>[row, column]</code>.</p> <pre><code>matrix = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\n# Single element\nprint(matrix[0, 0])  # 1 (top-left)\nprint(matrix[1, 2])  # 6 (row 1, column 2)\nprint(matrix[-1, -1])  # 9 (bottom-right)\n\n# Entire row\nprint(matrix[1])  # [4 5 6]\n\n# Entire column\nprint(matrix[:, 1])  # [2 5 8]\n</code></pre>"},{"location":"chapters/10-numpy-computing/#boolean-indexing-filtering","title":"Boolean Indexing (Filtering)","text":"<p>One of NumPy's most powerful features is boolean indexing\u2014using conditions to select elements:</p> <pre><code>scores = np.array([85, 92, 78, 96, 88, 73, 95])\n\n# Create boolean mask\npassing = scores &gt;= 80\nprint(passing)  # [ True  True False  True  True False  True]\n\n# Use mask to filter\nprint(scores[passing])  # [85 92 96 88 95]\n\n# Or do it in one step\nprint(scores[scores &gt;= 90])  # [92 96 95]\n\n# Combine conditions with &amp; (and) or | (or)\nprint(scores[(scores &gt;= 80) &amp; (scores &lt; 95)])  # [85 92 88]\n</code></pre>"},{"location":"chapters/10-numpy-computing/#array-slicing-grabbing-sections","title":"Array Slicing: Grabbing Sections","text":"<p>Array slicing extracts portions of arrays using the familiar <code>start:stop:step</code> notation. With multi-dimensional arrays, you can slice along each dimension.</p> <pre><code>arr = np.arange(10)  # [0 1 2 3 4 5 6 7 8 9]\n\n# Basic slicing\nprint(arr[2:7])    # [2 3 4 5 6]\nprint(arr[:5])     # [0 1 2 3 4]\nprint(arr[5:])     # [5 6 7 8 9]\nprint(arr[::2])    # [0 2 4 6 8] (every other element)\nprint(arr[::-1])   # [9 8 7 6 5 4 3 2 1 0] (reversed)\n</code></pre>"},{"location":"chapters/10-numpy-computing/#2d-slicing","title":"2D Slicing","text":"<pre><code>matrix = np.arange(20).reshape(4, 5)\nprint(matrix)\n# [[ 0  1  2  3  4]\n#  [ 5  6  7  8  9]\n#  [10 11 12 13 14]\n#  [15 16 17 18 19]]\n\n# First two rows, all columns\nprint(matrix[:2, :])\n\n# All rows, first three columns\nprint(matrix[:, :3])\n\n# Submatrix: rows 1-2, columns 2-4\nprint(matrix[1:3, 2:5])\n# [[ 7  8  9]\n#  [12 13 14]]\n\n# Every other row, every other column\nprint(matrix[::2, ::2])\n# [[ 0  2  4]\n#  [10 12 14]]\n</code></pre> <p>Views vs Copies</p> <p>Slicing in NumPy creates a view of the original array, not a copy. Changes to the slice affect the original! Use <code>.copy()</code> if you need an independent copy: <code>my_copy = arr[2:5].copy()</code>.</p>"},{"location":"chapters/10-numpy-computing/#diagram-slicing-playground","title":"Diagram: Slicing Playground","text":"Slicing Playground <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Analyze</p> <p>Learning Objective: Practice array slicing with immediate visual feedback, understanding how start:stop:step notation works in multiple dimensions</p> <p>Canvas Layout (850x500): - Left panel (500x500): Visual array representation - Right panel (350x500): Slicing controls and code</p> <p>Left Panel - Array View: - 2D grid showing array values (default 6x8) - Selected elements highlighted in blue - Unselected elements in gray - Row and column indices labeled - Animation when selection changes</p> <p>Right Panel - Slicing Controls: - Row slice inputs: start [ ] : stop [ ] : step [ ] - Column slice inputs: start [ ] : stop [ ] : step [ ] - Live code preview: <code>array[0:3, 1:5:2]</code> - Result preview showing selected values - Preset buttons: \"First 3 rows\", \"Last column\", \"Checkerboard\", \"Reverse\"</p> <p>Interactive Features: - Click and drag on grid to visually select region - Inputs update automatically from visual selection - Code updates in real-time as inputs change - \"Run\" button executes in console and shows result - Error messages for invalid slices</p> <p>Quick Challenges: - \"Select the corners\" - shows 4 corner elements - \"Select every other element\" - checkerboard pattern - \"Reverse the rows\" - shows negative step - Button: \"Check Answer\" for each challenge</p> <p>Visual Feedback: - Green flash when slice is valid - Red outline for invalid slice notation - Animation showing element selection order</p> <p>Implementation: p5.js with interactive grid</p>"},{"location":"chapters/10-numpy-computing/#vectorized-operations-the-speed-secret","title":"Vectorized Operations: The Speed Secret","text":"<p>Vectorized operations are operations that apply to entire arrays at once, without explicit Python loops. This is NumPy's superpower\u2014it's what makes NumPy fast.</p> <p>Compare these two approaches to squaring a million numbers:</p> <pre><code>import time\n\n# Create a million random numbers\ndata = np.random.random(1_000_000)\n\n# Slow way: Python loop\nstart = time.time()\nresult_slow = [x**2 for x in data]\nslow_time = time.time() - start\nprint(f\"Python loop: {slow_time:.4f} seconds\")\n\n# Fast way: NumPy vectorized\nstart = time.time()\nresult_fast = data ** 2\nfast_time = time.time() - start\nprint(f\"NumPy vectorized: {fast_time:.4f} seconds\")\n\nprint(f\"NumPy is {slow_time/fast_time:.0f}x faster!\")\n</code></pre> <p>Typical output: NumPy is 50-100x faster than the Python loop. That's not a typo\u2014NumPy really is that much faster.</p>"},{"location":"chapters/10-numpy-computing/#common-vectorized-operations","title":"Common Vectorized Operations","text":"<pre><code>arr = np.array([1, 4, 9, 16, 25])\n\n# Arithmetic\nprint(arr + 10)     # [11 14 19 26 35]\nprint(arr * 2)      # [ 2  8 18 32 50]\nprint(arr / 5)      # [0.2 0.8 1.8 3.2 5.0]\nprint(arr ** 0.5)   # [1. 2. 3. 4. 5.] (square roots)\n\n# Math functions\nprint(np.sqrt(arr))     # Square root\nprint(np.log(arr))      # Natural log\nprint(np.exp(arr))      # e^x\nprint(np.sin(arr))      # Sine\n\n# Aggregations\nprint(np.sum(arr))      # 55\nprint(np.mean(arr))     # 11.0\nprint(np.std(arr))      # 8.0\nprint(np.min(arr))      # 1\nprint(np.max(arr))      # 25\n</code></pre> <p>The key insight: whenever you're tempted to write a <code>for</code> loop over array elements, ask yourself \"Is there a NumPy function that does this?\" There usually is, and it's almost always faster.</p>"},{"location":"chapters/10-numpy-computing/#element-wise-operations-array-math","title":"Element-wise Operations: Array Math","text":"<p>Element-wise operations apply the same operation to each corresponding pair of elements in two arrays. The arrays must have compatible shapes (more on this in the Broadcasting section).</p> <pre><code>a = np.array([1, 2, 3, 4])\nb = np.array([10, 20, 30, 40])\n\n# Element-wise arithmetic\nprint(a + b)   # [11 22 33 44]\nprint(a - b)   # [-9 -18 -27 -36]\nprint(a * b)   # [10 40 90 160]\nprint(a / b)   # [0.1 0.1 0.1 0.1]\nprint(a ** b)  # Very big numbers!\n\n# Element-wise comparisons\nprint(a &lt; b)   # [ True  True  True  True]\nprint(a == 2)  # [False  True False False]\n</code></pre> <p>This is different from matrix multiplication (which we'll cover soon). Element-wise <code>*</code> multiplies position by position; matrix multiplication follows linear algebra rules.</p> <pre><code># 2D element-wise operations\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[10, 20], [30, 40]])\n\nprint(A * B)\n# [[ 10  40]\n#  [ 90 160]]\n\n# Each element multiplied by its counterpart\n</code></pre>"},{"location":"chapters/10-numpy-computing/#broadcasting-the-shape-matching-magic","title":"Broadcasting: The Shape-Matching Magic","text":"<p>Broadcasting is NumPy's clever way of handling operations between arrays of different shapes. Instead of requiring identical shapes, NumPy \"broadcasts\" smaller arrays to match larger ones.</p>"},{"location":"chapters/10-numpy-computing/#simple-broadcasting","title":"Simple Broadcasting","text":"<pre><code># Add a scalar to every element\narr = np.array([1, 2, 3, 4, 5])\nprint(arr + 10)  # [11 12 13 14 15]\n\n# The scalar 10 is \"broadcast\" to [10, 10, 10, 10, 10]\n</code></pre>"},{"location":"chapters/10-numpy-computing/#broadcasting-with-different-shapes","title":"Broadcasting with Different Shapes","text":"<pre><code># 3x3 matrix\nmatrix = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\n# 1D array with 3 elements\nrow = np.array([10, 20, 30])\n\n# Row is broadcast to each row of matrix\nprint(matrix + row)\n# [[11 22 33]\n#  [14 25 36]\n#  [17 28 39]]\n\n# Column vector (shape 3,1)\ncol = np.array([[100], [200], [300]])\n\n# Column is broadcast to each column\nprint(matrix + col)\n# [[101 102 103]\n#  [204 205 206]\n#  [307 308 309]]\n</code></pre>"},{"location":"chapters/10-numpy-computing/#broadcasting-rules","title":"Broadcasting Rules","text":"<p>Broadcasting works when comparing shapes from right to left:</p> <ol> <li>Dimensions are compatible if they're equal OR one of them is 1</li> <li>Missing dimensions are treated as 1</li> </ol> Shape A Shape B Result Shape Works? (3,) (3,) (3,) Yes - identical (3, 4) (4,) (3, 4) Yes - 4 matches 4 (3, 4) (3, 1) (3, 4) Yes - 1 broadcasts (3, 4) (2, 4) Error No - 3 \u2260 2"},{"location":"chapters/10-numpy-computing/#diagram-broadcasting-visualizer","title":"Diagram: Broadcasting Visualizer","text":"Broadcasting Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize how NumPy stretches smaller arrays to match larger ones during broadcasting</p> <p>Canvas Layout (850x550): - Top area (850x350): Visual array representations - Bottom area (850x200): Shape analysis and controls</p> <p>Top Area - Visual Representation: - Left: First array (A) with shape label - Center: Operation symbol (+, *, etc.) - Right: Second array (B) with shape label - Below: Result array showing combined operation - Animation: Smaller array \"stretches\" to match larger</p> <p>Broadcasting Animation: - Show original arrays - Animate smaller array duplicating to match dimensions - Show element-wise operation occurring - Display final result with highlighting</p> <p>Interactive Controls: - Dropdown: Select Array A shape (scalar, 1D, 2D options) - Dropdown: Select Array B shape - Dropdown: Select operation (+, -, *, /) - Input: Custom values for arrays - Button: \"Animate Broadcasting\"</p> <p>Shape Analysis Panel: - Show shapes aligned right-to-left - Color code: Green = compatible, Red = incompatible - Explain which dimension broadcasts to which - Error message for incompatible shapes</p> <p>Preset Examples: - \"Scalar + Matrix\" - simplest broadcast - \"Row + Matrix\" - row broadcasts down - \"Column + Matrix\" - column broadcasts across - \"Incompatible\" - shows error case</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/10-numpy-computing/#matrix-operations-linear-algebra-essentials","title":"Matrix Operations: Linear Algebra Essentials","text":"<p>Now we enter the realm of matrix operations\u2014the mathematical operations that power machine learning. These are different from element-wise operations and follow the rules of linear algebra.</p>"},{"location":"chapters/10-numpy-computing/#the-transpose","title":"The Transpose","text":"<p>The transpose of a matrix flips it over its diagonal\u2014rows become columns and columns become rows.</p> <pre><code>A = np.array([\n    [1, 2, 3],\n    [4, 5, 6]\n])\nprint(\"Original shape:\", A.shape)  # (2, 3)\n\nA_T = A.T  # Transpose\nprint(A_T)\nprint(\"Transposed shape:\", A_T.shape)  # (3, 2)\n\n# Original:\n# [[1 2 3]\n#  [4 5 6]]\n\n# Transposed:\n# [[1 4]\n#  [2 5]\n#  [3 6]]\n</code></pre> <p>Transpose is used constantly in machine learning, especially when you need to align matrix dimensions for multiplication.</p>"},{"location":"chapters/10-numpy-computing/#the-dot-product","title":"The Dot Product","text":"<p>The dot product of two vectors produces a single number (scalar). It multiplies corresponding elements and sums the results.</p> \\[\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i = a_1b_1 + a_2b_2 + ... + a_nb_n\\] <pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Dot product\ndot = np.dot(a, b)\nprint(dot)  # 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n\n# Alternative syntax\nprint(a @ b)  # 32 (@ is the matrix multiplication operator)\n</code></pre> <p>The dot product measures how \"aligned\" two vectors are. In machine learning, it's used for:</p> <ul> <li>Computing predictions (features \u00b7 weights)</li> <li>Measuring similarity between vectors</li> <li>Computing attention in transformers</li> </ul>"},{"location":"chapters/10-numpy-computing/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrix multiplication extends the dot product to entire matrices. For matrices A (m\u00d7n) and B (n\u00d7p), the result C is (m\u00d7p), where each element is a dot product of a row from A and a column from B.</p> \\[C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\\] <pre><code>A = np.array([\n    [1, 2],\n    [3, 4],\n    [5, 6]\n])  # 3x2 matrix\n\nB = np.array([\n    [7, 8, 9],\n    [10, 11, 12]\n])  # 2x3 matrix\n\n# Matrix multiplication\nC = A @ B  # or np.dot(A, B)\nprint(C)\nprint(C.shape)  # (3, 3)\n\n# [[1*7+2*10, 1*8+2*11, 1*9+2*12],    [[27, 30, 33],\n#  [3*7+4*10, 3*8+4*11, 3*9+4*12],  =  [61, 68, 75],\n#  [5*7+6*10, 5*8+6*11, 5*9+6*12]]     [95, 106, 117]]\n</code></pre> <p>Shape Requirements for Matrix Multiplication</p> <p>For A @ B to work, the number of columns in A must equal the number of rows in B. Shape (m, n) @ (n, p) = (m, p). If shapes don't match, you'll get an error.</p>"},{"location":"chapters/10-numpy-computing/#diagram-matrix-multiplication-visualizer","title":"Diagram: Matrix Multiplication Visualizer","text":"Matrix Multiplication Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize how matrix multiplication works by showing the dot products between rows and columns</p> <p>Canvas Layout (900x550): - Left area (350x400): Matrix A with row highlighting - Center area (200x400): Matrix B with column highlighting - Right area (350x400): Result matrix C with cell highlighting - Bottom area (900x150): Calculation display</p> <p>Visual Elements: - Matrix A displayed as grid (rows emphasized) - Matrix B displayed as grid (columns emphasized) - Result C displayed as grid - Currently computed cell highlighted - Arrows showing which row and column are being multiplied</p> <p>Animation Sequence: - Highlight row i of A in blue - Highlight column j of B in green - Show element-wise multiplication along the way - Sum appears in C[i,j] with flash - Move to next cell</p> <p>Interactive Controls: - Slider: Matrix A rows (1-5) - Slider: Matrix A columns / B rows (1-5) - Slider: Matrix B columns (1-5) - Speed control for animation - Button: \"Step through\" - advance one calculation - Button: \"Play all\" - animate entire multiplication - Button: \"Reset\"</p> <p>Calculation Panel: - Shows current calculation: a[i] \u00b7 b[j] = sum - Running formula with actual numbers - Highlight matching elements being multiplied</p> <p>Shape Validation: - Green indicator when shapes are compatible - Red error when inner dimensions don't match - Show shape calculation: (m,n) @ (n,p) = (m,p)</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/10-numpy-computing/#linear-algebra-with-numpy","title":"Linear Algebra with NumPy","text":"<p>Linear algebra is the mathematical framework for machine learning. NumPy provides essential linear algebra operations through <code>np.linalg</code>.</p> <pre><code>from numpy import linalg as la\n\n# Create a square matrix\nA = np.array([\n    [4, 2],\n    [1, 3]\n])\n\n# Determinant\ndet = la.det(A)\nprint(f\"Determinant: {det}\")  # 10.0\n\n# Inverse (A^-1 such that A @ A^-1 = I)\nA_inv = la.inv(A)\nprint(\"Inverse:\")\nprint(A_inv)\n\n# Verify: A @ A^-1 should equal identity\nprint(\"A @ A_inv:\")\nprint(A @ A_inv)  # [[1, 0], [0, 1]] (approximately)\n\n# Eigenvalues and eigenvectors\neigenvalues, eigenvectors = la.eig(A)\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors:\\n{eigenvectors}\")\n\n# Solve linear system Ax = b\nb = np.array([8, 5])\nx = la.solve(A, b)\nprint(f\"Solution to Ax = b: {x}\")\nprint(f\"Verification A @ x: {A @ x}\")  # Should equal b\n</code></pre> <p>These operations are foundational for:</p> <ul> <li>Inverse: Solving equations, understanding transformations</li> <li>Determinant: Checking if matrix is invertible, computing volumes</li> <li>Eigenvalues: Principal Component Analysis (PCA), understanding matrices</li> <li>Solve: Linear regression (normal equations), optimization</li> </ul> Operation Function Use Case Inverse <code>la.inv(A)</code> Solving equations Determinant <code>la.det(A)</code> Check invertibility Eigendecomposition <code>la.eig(A)</code> PCA, spectral analysis Solve Ax=b <code>la.solve(A, b)</code> Linear systems Matrix rank <code>la.matrix_rank(A)</code> Dimensionality Norm <code>la.norm(v)</code> Vector/matrix magnitude"},{"location":"chapters/10-numpy-computing/#computational-efficiency-why-this-all-matters","title":"Computational Efficiency: Why This All Matters","text":"<p>Let's bring it all together and talk about computational efficiency\u2014why NumPy's speed matters for real data science work.</p>"},{"location":"chapters/10-numpy-computing/#the-numbers-dont-lie","title":"The Numbers Don't Lie","text":"<pre><code>import numpy as np\nimport time\n\n# Compare approaches for a common operation:\n# Normalize a dataset (subtract mean, divide by std)\n\n# Create a million data points\ndata = np.random.randn(1_000_000)\n\n# Method 1: Pure Python\ndef normalize_python(data):\n    mean = sum(data) / len(data)\n    variance = sum((x - mean)**2 for x in data) / len(data)\n    std = variance ** 0.5\n    return [(x - mean) / std for x in data]\n\n# Method 2: NumPy\ndef normalize_numpy(data):\n    return (data - data.mean()) / data.std()\n\n# Time Python version\nstart = time.time()\nresult_python = normalize_python(list(data))\npython_time = time.time() - start\n\n# Time NumPy version\nstart = time.time()\nresult_numpy = normalize_numpy(data)\nnumpy_time = time.time() - start\n\nprint(f\"Python: {python_time:.3f} seconds\")\nprint(f\"NumPy: {numpy_time:.4f} seconds\")\nprint(f\"Speedup: {python_time/numpy_time:.0f}x\")\n</code></pre> <p>Typical result: NumPy is 100-500x faster for this operation!</p>"},{"location":"chapters/10-numpy-computing/#memory-efficiency","title":"Memory Efficiency","text":"<p>NumPy arrays also use less memory than Python lists:</p> <pre><code>import sys\n\n# Python list of 1 million integers\npython_list = list(range(1_000_000))\npython_size = sys.getsizeof(python_list) + sum(sys.getsizeof(x) for x in python_list)\n\n# NumPy array of 1 million integers\nnumpy_array = np.arange(1_000_000)\nnumpy_size = numpy_array.nbytes\n\nprint(f\"Python list: {python_size / 1e6:.1f} MB\")\nprint(f\"NumPy array: {numpy_size / 1e6:.1f} MB\")\nprint(f\"Memory savings: {python_size/numpy_size:.1f}x\")\n</code></pre> <p>NumPy typically uses 4-8x less memory than equivalent Python lists.</p>"},{"location":"chapters/10-numpy-computing/#why-scikit-learn-and-pandas-use-numpy","title":"Why Scikit-learn and Pandas Use NumPy","text":"<p>Every major data science library is built on NumPy:</p> <ul> <li>Pandas: DataFrames store data as NumPy arrays internally</li> <li>Scikit-learn: All models expect NumPy arrays as input</li> <li>PyTorch/TensorFlow: Tensors are compatible with NumPy arrays</li> <li>Matplotlib/Plotly: Plotting functions accept NumPy arrays</li> </ul> <p>When you call <code>df.values</code> on a pandas DataFrame, you get a NumPy array. When you call <code>model.fit(X, y)</code> in scikit-learn, X and y are NumPy arrays. Understanding NumPy means understanding the foundation of modern data science.</p>"},{"location":"chapters/10-numpy-computing/#diagram-numpy-ecosystem-map","title":"Diagram: NumPy Ecosystem Map","text":"NumPy Ecosystem Map <p>Type: infographic</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Show how NumPy serves as the foundation for the entire Python data science ecosystem</p> <p>Layout: Hub-and-spoke diagram with NumPy at center</p> <p>Center Hub - NumPy: - Large central circle labeled \"NumPy\" - Subtitle: \"The Foundation\" - Icon: Array grid symbol</p> <p>Spokes - Major Libraries: 1. Pandas spoke:    - \"DataFrames use NumPy arrays internally\"    - Icon: Table    - Arrow showing data flow to/from NumPy</p> <ol> <li>Scikit-learn spoke:</li> <li>\"All ML models expect NumPy arrays\"</li> <li>Icon: Brain/ML</li> <li> <p>Arrow showing fit/predict using arrays</p> </li> <li> <p>Matplotlib/Plotly spoke:</p> </li> <li>\"Plotting functions accept arrays\"</li> <li>Icon: Chart</li> <li> <p>Arrow showing visualization of arrays</p> </li> <li> <p>SciPy spoke:</p> </li> <li>\"Scientific computing extends NumPy\"</li> <li>Icon: Integration symbol</li> <li> <p>Arrow showing enhanced operations</p> </li> <li> <p>PyTorch/TensorFlow spoke:</p> </li> <li>\"Deep learning tensors interoperate with NumPy\"</li> <li>Icon: Neural network</li> <li>Arrow showing array\u2194tensor conversion</li> </ol> <p>Interactive Elements: - Hover over each spoke to see code example - Click to see conversion syntax (e.g., <code>df.values</code>, <code>torch.from_numpy()</code>) - Animation: Data flowing from NumPy to each library - Toggle: Show memory sharing between libraries</p> <p>Visual Style: - NumPy in blue (foundation color) - Each library in its brand color - Arrows showing bidirectional data flow - Sizes proportional to library importance</p> <p>Implementation: HTML/CSS/JavaScript with hover interactions</p>"},{"location":"chapters/10-numpy-computing/#practical-tips-and-best-practices","title":"Practical Tips and Best Practices","text":"<p>As you incorporate NumPy into your data science workflow, keep these tips in mind:</p> <p>Think in Arrays, Not Loops Whenever you write a <code>for</code> loop over array elements, stop and ask: \"Is there a vectorized way to do this?\" There usually is.</p> <pre><code># Bad: Loop\nresult = []\nfor x in data:\n    result.append(x ** 2 + 2 * x + 1)\n\n# Good: Vectorized\nresult = data ** 2 + 2 * data + 1\n</code></pre> <p>Use Broadcasting Intentionally Broadcasting is powerful but can be confusing. When shapes don't match as expected, print them:</p> <pre><code>print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n</code></pre> <p>Be Careful with Views Remember that slices create views, not copies. If you need an independent array, use <code>.copy()</code>:</p> <pre><code>original = np.array([1, 2, 3, 4, 5])\nview = original[1:4]      # Changes affect original\ncopy = original[1:4].copy()  # Independent\n</code></pre> <p>Check Data Types NumPy infers data types, but sometimes you need to be explicit:</p> <pre><code># Float array (default for decimals)\nfloats = np.array([1.0, 2.0, 3.0])\nprint(floats.dtype)  # float64\n\n# Integer array\nints = np.array([1, 2, 3])\nprint(ints.dtype)  # int64\n\n# Force a specific type\nforced = np.array([1, 2, 3], dtype=np.float32)\nprint(forced.dtype)  # float32\n</code></pre>"},{"location":"chapters/10-numpy-computing/#summary-your-numpy-toolkit","title":"Summary: Your NumPy Toolkit","text":"<p>You now have a solid foundation in NumPy:</p> <ul> <li>NumPy arrays are faster and more memory-efficient than Python lists</li> <li>Array creation offers many methods: <code>zeros</code>, <code>ones</code>, <code>arange</code>, <code>linspace</code>, <code>random</code></li> <li>Shape describes array dimensions; <code>reshape</code> reorganizes without copying</li> <li>Indexing and slicing access elements and subarrays powerfully</li> <li>Vectorized operations apply functions to entire arrays at once</li> <li>Broadcasting handles operations between different-shaped arrays</li> <li>Matrix operations (transpose, dot product, matrix multiplication) enable linear algebra</li> <li>Computational efficiency makes NumPy essential for real-world data science</li> </ul> <p>NumPy is the bedrock of scientific Python. Every time you use pandas, scikit-learn, or PyTorch, NumPy is working behind the scenes. Master NumPy, and you've mastered the foundation.</p>"},{"location":"chapters/10-numpy-computing/#looking-ahead","title":"Looking Ahead","text":"<p>In the next chapter, we'll explore non-linear models and regularization techniques. You'll see how polynomial features (built with NumPy!) can capture curved relationships, and how regularization prevents overfitting. The matrix operations you learned here will help you understand what's happening inside these more advanced models.</p>"},{"location":"chapters/10-numpy-computing/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>NumPy arrays are 50-100x faster than Python lists for numerical operations</li> <li>Arrays have shapes that describe their dimensions; reshape changes organization without copying data</li> <li>Indexing with brackets accesses elements; boolean indexing filters based on conditions</li> <li>Vectorized operations avoid loops and leverage compiled C code for speed</li> <li>Broadcasting stretches smaller arrays to match larger ones automatically</li> <li>Matrix multiplication (@) follows linear algebra rules; element-wise (*) operates position by position</li> <li>The <code>np.linalg</code> module provides essential linear algebra operations</li> <li>Every major data science library is built on NumPy\u2014it's the universal foundation</li> </ul>"},{"location":"chapters/11-nonlinear-models-regularization/","title":"Non-linear Models and Regularization","text":"<p>title: Non-linear Models and Regularization description: Bend the line and tame the beast - mastering curves and preventing overfitting generated_by: chapter-content-generator skill date: 2025-12-15 version: 0.03</p>"},{"location":"chapters/11-nonlinear-models-regularization/#summary","title":"Summary","text":"<p>This chapter expands modeling capabilities beyond linear relationships. Students will learn polynomial regression for capturing non-linear patterns, various transformation techniques, and the concept of model flexibility. The chapter introduces regularization as a technique for preventing overfitting, covering Ridge regression, Lasso regression, and Elastic Net. By the end of this chapter, students will understand how to balance model complexity with generalization and apply regularization to improve model performance.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>Non-linear Regression</li> <li>Polynomial Regression</li> <li>Degree of Polynomial</li> <li>Curve Fitting</li> <li>Transformation</li> <li>Log Transformation</li> <li>Feature Transformation</li> <li>Model Flexibility</li> <li>Regularization</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Elastic Net</li> <li>Regularization Parameter</li> <li>Lambda Parameter</li> <li>Shrinkage</li> </ol>"},{"location":"chapters/11-nonlinear-models-regularization/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 8: Model Evaluation and Validation</li> <li>Chapter 9: Multiple Linear Regression</li> </ul>"},{"location":"chapters/11-nonlinear-models-regularization/#introduction-beyond-the-straight-line","title":"Introduction: Beyond the Straight Line","text":"<p>You've mastered linear regression\u2014congratulations! But here's the truth: the real world doesn't always follow straight lines. House prices don't increase linearly with size forever. Learning curves flatten out. Population growth accelerates and then stabilizes. To model these patterns, you need curves.</p> <p>This chapter gives you two new superpowers:</p> <ol> <li>Bending the line: Using polynomial regression and transformations to capture curved relationships</li> <li>Taming the beast: Using regularization to prevent models from going wild with overfitting</li> </ol> <p>Together, these techniques let you build models that are flexible enough to capture complex patterns yet disciplined enough to generalize to new data. It's a delicate balance\u2014and by the end of this chapter, you'll be a master at finding it.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#non-linear-regression-when-lines-arent-enough","title":"Non-linear Regression: When Lines Aren't Enough","text":"<p>Non-linear regression refers to any regression approach where the relationship between features and target isn't a simple straight line. Look at this data:</p> <pre><code>import numpy as np\nimport plotly.express as px\n\n# Generate curved data\nnp.random.seed(42)\nx = np.linspace(0, 10, 100)\ny = 2 + 3*x - 0.5*x**2 + 0.05*x**3 + np.random.normal(0, 2, 100)\n\nfig = px.scatter(x=x, y=y, title=\"This Data Needs a Curve, Not a Line!\")\nfig.show()\n</code></pre> <p>If you fit a straight line to this data, you'll miss the curve entirely. The model will systematically underpredict in some regions and overpredict in others. That's not a random error\u2014it's a sign that your model isn't flexible enough.</p> <p>Non-linear regression captures these curved patterns by:</p> <ul> <li>Adding polynomial terms (x\u00b2, x\u00b3, etc.)</li> <li>Transforming features (log, square root, etc.)</li> <li>Using inherently non-linear models (which we'll cover in later chapters)</li> </ul> <p>The key insight: even though the relationship is curved, we can still use linear regression techniques! We just need to transform our features first.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#polynomial-regression-curves-through-linear-regression","title":"Polynomial Regression: Curves Through Linear Regression","text":"<p>Polynomial regression is a clever trick: we create new features by raising the original feature to different powers, then use regular linear regression on these expanded features.</p> <p>For a single feature \\(x\\), polynomial regression of degree 3 looks like:</p> \\[y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\] <p>This is still \"linear\" regression in the sense that it's linear in the coefficients (\u03b2 values). But the resulting curve can bend and twist to fit complex patterns.</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Create polynomial regression pipeline\npoly_model = Pipeline([\n    ('poly_features', PolynomialFeatures(degree=3)),\n    ('linear', LinearRegression())\n])\n\n# Reshape x for sklearn\nX = x.reshape(-1, 1)\n\n# Fit the model\npoly_model.fit(X, y)\n\n# Generate predictions for smooth curve\nX_plot = np.linspace(0, 10, 200).reshape(-1, 1)\ny_pred = poly_model.predict(X_plot)\n\n# Visualize\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Data'))\nfig.add_trace(go.Scatter(x=X_plot.flatten(), y=y_pred, mode='lines',\n                         name='Polynomial Fit', line=dict(color='red')))\nfig.update_layout(title='Polynomial Regression: Curves that Fit!')\nfig.show()\n</code></pre> <p>The magic happens in <code>PolynomialFeatures</code>\u2014it takes your original feature and creates new columns for each power up to the specified degree.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#degree-of-polynomial-how-much-flexibility","title":"Degree of Polynomial: How Much Flexibility?","text":"<p>The degree of polynomial controls how flexible your curve can be:</p> <ul> <li>Degree 1: Straight line (regular linear regression)</li> <li>Degree 2: Parabola (one bend)</li> <li>Degree 3: S-curve possible (two bends)</li> <li>Degree 4+: Increasingly complex curves</li> </ul> <p>Here's the critical trade-off:</p> Degree Flexibility Risk of Underfitting Risk of Overfitting 1 Low High Low 2-3 Medium Medium Medium 5-7 High Low Medium-High 10+ Very High Very Low Very High <pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Data', opacity=0.5))\n\n# Fit polynomials of different degrees\ndegrees = [1, 2, 3, 5, 10]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\nfor degree, color in zip(degrees, colors):\n    model = Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('linear', LinearRegression())\n    ])\n    model.fit(X, y)\n    y_pred = model.predict(X_plot)\n    fig.add_trace(go.Scatter(x=X_plot.flatten(), y=y_pred, mode='lines',\n                             name=f'Degree {degree}', line=dict(color=color)))\n\nfig.update_layout(title='Effect of Polynomial Degree on Fit')\nfig.show()\n</code></pre> <p>Notice how degree 10 goes wild, trying to pass through every data point? That's overfitting in action. The curve fits the training data perfectly but would fail miserably on new data.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#diagram-polynomial-degree-explorer","title":"Diagram: Polynomial Degree Explorer","text":"Polynomial Degree Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Evaluate</p> <p>Learning Objective: Interactively explore how polynomial degree affects curve flexibility and the bias-variance tradeoff</p> <p>Canvas Layout (850x550): - Main area (850x400): Scatter plot with polynomial curve - Bottom area (850x150): Controls and metrics</p> <p>Main Visualization: - Data points (20-50 points) with some noise - Polynomial curve that updates in real-time - Shaded confidence region showing uncertainty - Residual lines from points to curve (optional toggle)</p> <p>Interactive Controls: - Slider: Polynomial Degree (1 to 15) - Dropdown: Dataset type (linear, quadratic, cubic, sine wave, step function) - Slider: Noise level (0 to high) - Button: \"Generate New Data\" - Checkbox: \"Show Train/Test Split\"</p> <p>Metrics Panel: - Training R\u00b2: updates live - Test R\u00b2: updates live (when split enabled) - Number of coefficients: degree + 1 - Visual warning when overfitting detected (train &gt;&gt; test)</p> <p>Educational Overlays: - At degree 1: \"Underfitting: Missing the curve\" - At degree 2-4: \"Good fit for this data\" - At degree 10+: \"Overfitting: Chasing noise!\" - Arrow pointing to where train/test scores diverge</p> <p>Animation: - Smooth curve transition when degree changes - Coefficients displayed with size proportional to magnitude</p> <p>Implementation: p5.js with polynomial fitting</p>"},{"location":"chapters/11-nonlinear-models-regularization/#curve-fitting-the-art-and-science","title":"Curve Fitting: The Art and Science","text":"<p>Curve fitting is the process of finding the mathematical function that best describes your data. While polynomial regression is one approach, the broader goal is matching the right curve shape to your data's underlying pattern.</p> <p>Good curve fitting requires:</p> <ol> <li>Visual inspection: Plot your data first! What shape does it suggest?</li> <li>Domain knowledge: Does theory predict a certain relationship?</li> <li>Validation: Does the curve generalize to new data?</li> <li>Parsimony: Prefer simpler curves when they fit adequately</li> </ol> <pre><code>from sklearn.model_selection import cross_val_score\n\n# Find optimal degree using cross-validation\ndegrees = range(1, 15)\ncv_scores = []\n\nfor degree in degrees:\n    model = Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('linear', LinearRegression())\n    ])\n    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n    cv_scores.append(scores.mean())\n\n# Plot CV scores vs degree\nfig = px.line(x=list(degrees), y=cv_scores, markers=True,\n              title='Cross-Validation Score vs Polynomial Degree',\n              labels={'x': 'Polynomial Degree', 'y': 'CV R\u00b2 Score'})\nfig.show()\n\n# Find best degree\nbest_degree = degrees[np.argmax(cv_scores)]\nprint(f\"Optimal degree: {best_degree}\")\n</code></pre> <p>The CV score typically rises, peaks, then falls as degree increases. The peak is your sweet spot\u2014enough flexibility to capture the pattern, not so much that you're fitting noise.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#transformation-changing-the-shape-of-data","title":"Transformation: Changing the Shape of Data","text":"<p>Transformation is a broader technique for handling non-linear relationships. Instead of adding polynomial terms, we transform the original variables to make the relationship more linear.</p> <p>Common transformations include:</p> Transformation Formula Use Case Log \\(\\log(x)\\) Exponential growth, multiplicative effects Square root \\(\\sqrt{x}\\) Count data, variance stabilization Reciprocal \\(1/x\\) Inverse relationships Power \\(x^n\\) Accelerating/decelerating patterns Box-Cox \\((x^\\lambda - 1)/\\lambda\\) General normalization <p>The key insight: if your scatter plot curves, the right transformation can straighten it\u2014making linear regression appropriate again.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#log-transformation-the-exponential-tamer","title":"Log Transformation: The Exponential Tamer","text":"<p>The log transformation is probably the most useful transformation in data science. It's perfect when:</p> <ul> <li>Your data spans several orders of magnitude (1 to 1,000,000)</li> <li>The relationship looks exponential</li> <li>You want to interpret coefficients as percentage changes</li> <li>Residuals show increasing variance (heteroscedasticity)</li> </ul> <pre><code>import numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Generate exponential-ish data\nnp.random.seed(42)\nx_exp = np.linspace(1, 10, 100)\ny_exp = 5 * np.exp(0.4 * x_exp) + np.random.normal(0, 10, 100)\n\n# Create subplots\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=['Original Scale (Curved)', 'Log-Transformed (Linear!)'])\n\n# Original scale\nfig.add_trace(go.Scatter(x=x_exp, y=y_exp, mode='markers', name='Original'),\n              row=1, col=1)\n\n# Log-transformed\nfig.add_trace(go.Scatter(x=x_exp, y=np.log(y_exp), mode='markers', name='Log(y)'),\n              row=1, col=2)\n\nfig.update_layout(height=400, title='The Magic of Log Transformation')\nfig.show()\n</code></pre> <p>Notice how the curved relationship becomes nearly linear after log transformation? Now regular linear regression will work beautifully.</p> <pre><code># Log-linear regression\nfrom sklearn.linear_model import LinearRegression\n\n# Transform y\ny_log = np.log(y_exp)\n\n# Fit model on log scale\nmodel = LinearRegression()\nmodel.fit(x_exp.reshape(-1, 1), y_log)\n\nprint(f\"Coefficient: {model.coef_[0]:.4f}\")\nprint(f\"Interpretation: For each unit increase in x, y increases by {np.exp(model.coef_[0]):.2%}\")\n</code></pre> <p>Interpreting Log-Transformed Coefficients</p> <p>When your target is log-transformed, coefficients represent multiplicative effects. A coefficient of 0.4 means each unit of x multiplies y by e^0.4 \u2248 1.49, or a 49% increase.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#feature-transformation-engineering-better-inputs","title":"Feature Transformation: Engineering Better Inputs","text":"<p>Feature transformation is the deliberate modification of input features to improve model performance. This is closely related to the feature engineering we covered earlier, but with a specific focus on mathematical transformations.</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\n\n# Sample data\ndf = pd.DataFrame({\n    'income': [30000, 45000, 55000, 80000, 150000, 500000],\n    'age': [22, 28, 35, 42, 55, 65],\n    'experience_years': [0, 3, 8, 15, 25, 35]\n})\n\n# Log transform skewed income\ndf['log_income'] = np.log(df['income'])\n\n# Square root of experience (diminishing returns)\ndf['sqrt_experience'] = np.sqrt(df['experience_years'])\n\n# Age polynomial\ndf['age_squared'] = df['age'] ** 2\n\nprint(df)\n</code></pre> <p>Scikit-learn's <code>PowerTransformer</code> can automatically find good transformations:</p> <pre><code>from sklearn.preprocessing import PowerTransformer\n\n# Yeo-Johnson transformation (handles zeros and negatives)\npt = PowerTransformer(method='yeo-johnson')\ndf_transformed = pt.fit_transform(df[['income', 'age', 'experience_years']])\n\nprint(\"Transformation parameters:\", pt.lambdas_)\n</code></pre>"},{"location":"chapters/11-nonlinear-models-regularization/#diagram-transformation-gallery","title":"Diagram: Transformation Gallery","text":"Transformation Gallery <p>Type: infographic</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Show common transformations side-by-side with their effects on data distribution and relationships</p> <p>Layout: 2x3 grid of transformation examples</p> <p>Each Panel Contains: - Original data histogram/scatter (left mini-plot) - Transformed data histogram/scatter (right mini-plot) - Transformation formula - When to use it</p> <p>Panels: 1. Log Transformation    - Before: Right-skewed histogram    - After: Symmetric histogram    - Formula: y' = log(y)    - Use: Exponential relationships, multiplicative effects</p> <ol> <li>Square Root</li> <li>Before: Count data with variance proportional to mean</li> <li>After: Stabilized variance</li> <li>Formula: y' = \u221ay</li> <li> <p>Use: Count data, Poisson-like distributions</p> </li> <li> <p>Reciprocal (1/x)</p> </li> <li>Before: Hyperbolic scatter</li> <li>After: Linear scatter</li> <li>Formula: y' = 1/y</li> <li> <p>Use: Inverse relationships</p> </li> <li> <p>Square (x\u00b2)</p> </li> <li>Before: Decelerating curve</li> <li>After: Linear relationship</li> <li>Formula: y' = x\u00b2</li> <li> <p>Use: Accelerating patterns</p> </li> <li> <p>Box-Cox</p> </li> <li>Before: Arbitrary skewed data</li> <li>After: Approximately normal</li> <li>Formula: y' = (y^\u03bb - 1)/\u03bb</li> <li> <p>Use: General normalization</p> </li> <li> <p>Standardization</p> </li> <li>Before: Different scales</li> <li>After: Mean=0, SD=1</li> <li>Formula: z = (x - \u03bc)/\u03c3</li> <li>Use: Comparing features, regularization</li> </ol> <p>Interactive Elements: - Click panel to see full-size comparison - Slider to adjust transformation parameter - Button: \"Try on your data\" - upload CSV option</p> <p>Implementation: HTML/CSS/JavaScript with D3.js visualizations</p>"},{"location":"chapters/11-nonlinear-models-regularization/#model-flexibility-the-complexity-dial","title":"Model Flexibility: The Complexity Dial","text":"<p>Model flexibility refers to how adaptable a model is to different patterns in data. A highly flexible model can capture intricate patterns but risks overfitting. A rigid model may miss important patterns but generalizes better.</p> <p>Think of flexibility as a dial:</p> <ul> <li>Low flexibility (simple models): Few parameters, strong assumptions, high bias, low variance</li> <li>High flexibility (complex models): Many parameters, weak assumptions, low bias, high variance</li> </ul> <p>The relationship between flexibility and error follows a U-shaped curve:</p> <ul> <li>Training error always decreases with more flexibility</li> <li>Test error decreases initially, then increases (the overfitting zone)</li> <li>The optimal flexibility minimizes test error</li> </ul> <pre><code>from sklearn.model_selection import train_test_split\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Track errors across flexibility levels\ntrain_errors = []\ntest_errors = []\ndegrees = range(1, 20)\n\nfor degree in degrees:\n    model = Pipeline([\n        ('poly', PolynomialFeatures(degree=degree)),\n        ('linear', LinearRegression())\n    ])\n    model.fit(X_train, y_train)\n\n    train_errors.append(1 - model.score(X_train, y_train))\n    test_errors.append(1 - model.score(X_test, y_test))\n\n# Plot the flexibility curve\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(degrees), y=train_errors, mode='lines+markers',\n                         name='Training Error', line=dict(color='blue')))\nfig.add_trace(go.Scatter(x=list(degrees), y=test_errors, mode='lines+markers',\n                         name='Test Error', line=dict(color='red')))\nfig.update_layout(title='The Bias-Variance Tradeoff in Action',\n                  xaxis_title='Model Flexibility (Polynomial Degree)',\n                  yaxis_title='Error (1 - R\u00b2)')\nfig.show()\n</code></pre> <p>The gap between training and test error is your overfitting indicator. When it's large, your model has learned noise specific to the training data.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#regularization-taming-overfitting","title":"Regularization: Taming Overfitting","text":"<p>Here's the million-dollar question: if more flexibility leads to overfitting, but we need flexibility to capture complex patterns, what do we do?</p> <p>Enter regularization\u2014a technique that adds a penalty for model complexity. Instead of just minimizing prediction error, regularized models minimize:</p> \\[\\text{Loss} = \\text{Prediction Error} + \\lambda \\times \\text{Complexity Penalty}\\] <p>The complexity penalty discourages large coefficients, effectively simplifying the model. This creates a controlled trade-off between fitting the data and keeping the model simple.</p> <p>Regularization gives you the best of both worlds:</p> <ul> <li>Use a flexible model (high-degree polynomial)</li> <li>Let regularization automatically \"turn off\" unnecessary complexity</li> <li>Result: captures real patterns, ignores noise</li> </ul> <p>The regularization parameter (often called \\(\\lambda\\) or <code>alpha</code> in scikit-learn) controls this trade-off:</p> <ul> <li>\u03bb = 0: No regularization (standard linear regression)</li> <li>Small \u03bb: Light penalty, nearly flexible</li> <li>Large \u03bb: Heavy penalty, nearly rigid</li> <li>\u03bb \u2192 \u221e: All coefficients shrink to zero</li> </ul>"},{"location":"chapters/11-nonlinear-models-regularization/#ridge-regression-the-l2-penalty","title":"Ridge Regression: The L2 Penalty","text":"<p>Ridge regression (also called L2 regularization or Tikhonov regularization) adds a penalty proportional to the squared coefficients:</p> \\[\\text{Loss}_{\\text{Ridge}} = \\sum(y_i - \\hat{y}_i)^2 + \\lambda \\sum \\beta_j^2\\] <p>The squared penalty means:</p> <ul> <li>All coefficients are shrunk toward zero</li> <li>Large coefficients are penalized more heavily</li> <li>Coefficients never become exactly zero (just very small)</li> <li>Good for multicollinearity\u2014it stabilizes correlated features</li> </ul> <pre><code>from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\n# Create Ridge regression pipeline with polynomial features\nridge_pipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=10)),  # High degree\n    ('scaler', StandardScaler()),              # Essential for regularization!\n    ('ridge', Ridge(alpha=1.0))                # alpha is \u03bb\n])\n\nridge_pipeline.fit(X_train, y_train)\n\nprint(f\"Train R\u00b2: {ridge_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"Test R\u00b2: {ridge_pipeline.score(X_test, y_test):.4f}\")\n\n# Compare to unregularized\nunreg_pipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=10)),\n    ('linear', LinearRegression())\n])\nunreg_pipeline.fit(X_train, y_train)\n\nprint(f\"\\nUnregularized Train R\u00b2: {unreg_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"Unregularized Test R\u00b2: {unreg_pipeline.score(X_test, y_test):.4f}\")\n</code></pre> <p>Notice how Ridge maintains good test performance even with degree 10, while unregularized regression overfits!</p> <p>Scale Your Features for Regularization</p> <p>Regularization penalizes large coefficients. If features are on different scales, the penalty affects them unequally. Always standardize features before applying regularization.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#lasso-regression-the-l1-penalty","title":"Lasso Regression: The L1 Penalty","text":"<p>Lasso regression (Least Absolute Shrinkage and Selection Operator) uses the absolute value of coefficients as the penalty:</p> \\[\\text{Loss}_{\\text{Lasso}} = \\sum(y_i - \\hat{y}_i)^2 + \\lambda \\sum |\\beta_j|\\] <p>The L1 penalty has a special property: it can shrink coefficients all the way to exactly zero. This means Lasso performs automatic feature selection\u2014useless features get eliminated entirely.</p> <pre><code>from sklearn.linear_model import Lasso\n\n# Create Lasso pipeline\nlasso_pipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=10)),\n    ('scaler', StandardScaler()),\n    ('lasso', Lasso(alpha=0.1))\n])\n\nlasso_pipeline.fit(X_train, y_train)\n\n# Count non-zero coefficients\ncoefficients = lasso_pipeline.named_steps['lasso'].coef_\nn_features = len(coefficients)\nn_nonzero = np.sum(coefficients != 0)\n\nprint(f\"Total features: {n_features}\")\nprint(f\"Non-zero coefficients: {n_nonzero}\")\nprint(f\"Features eliminated: {n_features - n_nonzero}\")\n\nprint(f\"\\nTrain R\u00b2: {lasso_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"Test R\u00b2: {lasso_pipeline.score(X_test, y_test):.4f}\")\n</code></pre> Aspect Ridge (L2) Lasso (L1) Penalty Sum of squared coefficients Sum of absolute coefficients Coefficients Shrunk toward zero Can become exactly zero Feature selection No Yes (automatic) Multicollinearity Handles well Arbitrary selection Best for Many small effects Few important features"},{"location":"chapters/11-nonlinear-models-regularization/#diagram-ridge-vs-lasso-comparison","title":"Diagram: Ridge vs Lasso Comparison","text":"Ridge vs Lasso Comparison <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze, Evaluate</p> <p>Learning Objective: Visualize and compare how Ridge and Lasso penalties affect coefficient shrinkage and feature selection</p> <p>Canvas Layout (900x550): - Top left (400x250): Ridge coefficient path - Top right (400x250): Lasso coefficient path - Bottom (900x250): Side-by-side coefficient comparison and controls</p> <p>Coefficient Path Plots: - X-axis: Log(\u03bb) from small to large - Y-axis: Coefficient values - Each line represents one coefficient - Show how coefficients shrink as \u03bb increases - Ridge: All lines approach zero asymptotically - Lasso: Lines hit zero and stay there</p> <p>Comparison Panel: - Bar chart showing final coefficient values - Ridge bars (blue): All non-zero, varying heights - Lasso bars (orange): Some exactly zero - Highlight eliminated features in gray</p> <p>Interactive Controls: - Slider: \u03bb (regularization strength) - both plots update - Dropdown: Select dataset (housing, synthetic, medical) - Checkbox: \"Show cross-validation optimal \u03bb\" - Toggle: \"Show mathematical penalty visualization\"</p> <p>Penalty Visualization (optional): - 2D contour plot showing loss surface - Ridge: Circular penalty (L2 ball) - Lasso: Diamond penalty (L1 ball) - Optimal point where loss contours meet penalty boundary</p> <p>Key Insights Displayed: - \"Lasso zeros out X features\" - \"Ridge reduces largest coefficient by Y%\" - Optimal \u03bb marked on both paths</p> <p>Implementation: p5.js with interactive plots</p>"},{"location":"chapters/11-nonlinear-models-regularization/#elastic-net-the-best-of-both-worlds","title":"Elastic Net: The Best of Both Worlds","text":"<p>Elastic Net combines Ridge and Lasso penalties:</p> \\[\\text{Loss}_{\\text{ElasticNet}} = \\sum(y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum |\\beta_j| + \\lambda_2 \\sum \\beta_j^2\\] <p>Or equivalently, using a mixing parameter \\(\\rho\\) (called <code>l1_ratio</code> in scikit-learn):</p> \\[\\text{Loss} = \\text{MSE} + \\alpha \\left( \\rho \\sum |\\beta_j| + (1-\\rho) \\sum \\beta_j^2 \\right)\\] <p>When \\(\\rho = 1\\): Pure Lasso When \\(\\rho = 0\\): Pure Ridge When \\(0 &lt; \\rho &lt; 1\\): Combination</p> <pre><code>from sklearn.linear_model import ElasticNet\n\n# Elastic Net with equal mix of L1 and L2\nelastic_pipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=10)),\n    ('scaler', StandardScaler()),\n    ('elastic', ElasticNet(alpha=0.1, l1_ratio=0.5))  # 50% L1, 50% L2\n])\n\nelastic_pipeline.fit(X_train, y_train)\n\ncoefficients = elastic_pipeline.named_steps['elastic'].coef_\nprint(f\"Non-zero coefficients: {np.sum(coefficients != 0)}\")\nprint(f\"Test R\u00b2: {elastic_pipeline.score(X_test, y_test):.4f}\")\n</code></pre> <p>Elastic Net is particularly useful when:</p> <ul> <li>You have groups of correlated features (Lasso arbitrarily picks one; Elastic Net keeps related features together)</li> <li>You want some feature selection but not as aggressive as pure Lasso</li> <li>You're not sure whether Ridge or Lasso is better (try Elastic Net!)</li> </ul>"},{"location":"chapters/11-nonlinear-models-regularization/#lambdaalpha-parameter-finding-the-sweet-spot","title":"Lambda/Alpha Parameter: Finding the Sweet Spot","text":"<p>The lambda parameter (called <code>alpha</code> in scikit-learn) controls regularization strength. Too small and you overfit; too large and you underfit. Finding the optimal \u03bb is crucial.</p> <p>Use cross-validation to find the best \u03bb:</p> <pre><code>from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n\n# RidgeCV automatically finds optimal alpha\nalphas = np.logspace(-4, 4, 50)  # Range from 0.0001 to 10000\n\nridge_cv = Pipeline([\n    ('poly', PolynomialFeatures(degree=10)),\n    ('scaler', StandardScaler()),\n    ('ridge', RidgeCV(alphas=alphas, cv=5))\n])\n\nridge_cv.fit(X_train, y_train)\n\noptimal_alpha = ridge_cv.named_steps['ridge'].alpha_\nprint(f\"Optimal alpha: {optimal_alpha:.4f}\")\nprint(f\"Test R\u00b2: {ridge_cv.score(X_test, y_test):.4f}\")\n</code></pre> <p>Visualize the regularization path:</p> <pre><code>from sklearn.linear_model import ridge_regression\n\n# Calculate coefficients for each alpha\ncoef_path = []\nfor alpha in alphas:\n    model = Pipeline([\n        ('poly', PolynomialFeatures(degree=5)),\n        ('scaler', StandardScaler()),\n        ('ridge', Ridge(alpha=alpha))\n    ])\n    model.fit(X_train, y_train)\n    coef_path.append(model.named_steps['ridge'].coef_)\n\ncoef_path = np.array(coef_path)\n\n# Plot coefficient paths\nfig = go.Figure()\nfor i in range(coef_path.shape[1]):\n    fig.add_trace(go.Scatter(x=np.log10(alphas), y=coef_path[:, i],\n                             mode='lines', name=f'Coef {i}',\n                             showlegend=False))\n\nfig.update_layout(title='Ridge Coefficient Path: How \u03bb Affects Coefficients',\n                  xaxis_title='log\u2081\u2080(\u03bb)',\n                  yaxis_title='Coefficient Value')\nfig.show()\n</code></pre>"},{"location":"chapters/11-nonlinear-models-regularization/#diagram-lambda-tuning-playground","title":"Diagram: Lambda Tuning Playground","text":"Lambda Tuning Playground <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Evaluate</p> <p>Learning Objective: Practice finding optimal regularization strength through interactive experimentation</p> <p>Canvas Layout (850x600): - Top area (850x350): Data and model fit visualization - Bottom left (425x250): CV score vs lambda plot - Bottom right (425x250): Coefficient magnitudes</p> <p>Top Panel - Model Fit: - Scatter plot of data - Polynomial curve showing current fit - Toggle between Ridge/Lasso/Elastic Net - Curve updates as lambda changes</p> <p>Bottom Left - Cross-Validation: - X-axis: log(\u03bb) scale - Y-axis: CV Score (R\u00b2 or MSE) - Line showing CV performance across \u03bb values - Vertical marker at current \u03bb - Optimal \u03bb highlighted with star</p> <p>Bottom Right - Coefficients: - Bar chart of coefficient magnitudes - Updates in real-time as \u03bb changes - For Lasso: Gray out zero coefficients - Show total number of non-zero coefficients</p> <p>Interactive Controls: - Slider: Lambda value (log scale) - Dropdown: Regularization type (Ridge, Lasso, Elastic Net) - Slider: Polynomial degree (2-15) - Slider: l1_ratio (for Elastic Net, 0-1) - Button: \"Find Optimal \u03bb\" - animates search - Button: \"Generate New Data\"</p> <p>Metrics Display: - Current \u03bb value - Train R\u00b2 - Test R\u00b2 - Cross-Validation R\u00b2 - Number of non-zero coefficients</p> <p>Educational Callouts: - When \u03bb too small: \"Overfitting warning!\" - When \u03bb too large: \"Underfitting warning!\" - At optimal: \"Sweet spot found!\"</p> <p>Implementation: p5.js with real-time model fitting</p>"},{"location":"chapters/11-nonlinear-models-regularization/#shrinkage-what-regularization-actually-does","title":"Shrinkage: What Regularization Actually Does","text":"<p>Shrinkage is the technical term for what regularization does to coefficients\u2014it pulls them toward zero. But why does shrinking coefficients help prevent overfitting?</p> <p>Consider what happens when a model overfits:</p> <ol> <li>It finds patterns in noise</li> <li>These patterns require extreme coefficients</li> <li>Small changes in data cause large prediction changes</li> <li>High variance = poor generalization</li> </ol> <p>Shrinkage counters this by:</p> <ol> <li>Penalizing extreme coefficients</li> <li>Forcing the model to find simpler solutions</li> <li>Reducing sensitivity to noise</li> <li>Lower variance = better generalization</li> </ol> <pre><code># Demonstrate shrinkage effect\nfrom sklearn.linear_model import Ridge\n\nalphas = [0, 0.01, 0.1, 1, 10, 100]\ncoefficients_by_alpha = {}\n\nfor alpha in alphas:\n    if alpha == 0:\n        model = Pipeline([\n            ('poly', PolynomialFeatures(degree=8)),\n            ('scaler', StandardScaler()),\n            ('linear', LinearRegression())\n        ])\n    else:\n        model = Pipeline([\n            ('poly', PolynomialFeatures(degree=8)),\n            ('scaler', StandardScaler()),\n            ('ridge', Ridge(alpha=alpha))\n        ])\n\n    model.fit(X_train, y_train)\n\n    if alpha == 0:\n        coeffs = model.named_steps['linear'].coef_\n    else:\n        coeffs = model.named_steps['ridge'].coef_\n\n    coefficients_by_alpha[alpha] = coeffs\n\n    # Calculate coefficient magnitude\n    coef_magnitude = np.sqrt(np.sum(coeffs**2))\n    print(f\"\u03b1={alpha:6}: Coefficient L2 norm = {coef_magnitude:.2f}, Test R\u00b2 = {model.score(X_test, y_test):.4f}\")\n</code></pre> <p>As regularization increases:</p> <ul> <li>Coefficient magnitudes shrink</li> <li>Model becomes more stable</li> <li>Test performance often improves (up to a point)</li> </ul>"},{"location":"chapters/11-nonlinear-models-regularization/#putting-it-all-together-a-complete-workflow","title":"Putting It All Together: A Complete Workflow","text":"<p>Here's a complete workflow for building regularized non-linear models:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.pipeline import Pipeline\nimport plotly.express as px\n\n# 1. Load and split data\nnp.random.seed(42)\nX = np.linspace(0, 10, 200).reshape(-1, 1)\ny = 3 + 2*X.flatten() - 0.5*X.flatten()**2 + 0.05*X.flatten()**3 + np.random.normal(0, 1, 200)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Create pipeline with polynomial features and regularization\npipeline = Pipeline([\n    ('poly', PolynomialFeatures()),\n    ('scaler', StandardScaler()),\n    ('regressor', Ridge())\n])\n\n# 3. Define hyperparameter grid\nparam_grid = {\n    'poly__degree': [2, 3, 4, 5, 6, 7],\n    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n}\n\n# 4. Grid search with cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# 5. Results\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(f\"Best CV score: {grid_search.best_score_:.4f}\")\nprint(f\"Test score: {grid_search.score(X_test, y_test):.4f}\")\n\n# 6. Visualize the fit\nbest_model = grid_search.best_estimator_\nX_plot = np.linspace(0, 10, 200).reshape(-1, 1)\ny_plot = best_model.predict(X_plot)\n\nfig = px.scatter(x=X.flatten(), y=y, opacity=0.5, title='Regularized Polynomial Regression')\nfig.add_scatter(x=X_plot.flatten(), y=y_plot, mode='lines', name='Best Model',\n                line=dict(color='red', width=2))\nfig.show()\n</code></pre>"},{"location":"chapters/11-nonlinear-models-regularization/#diagram-regularization-decision-tree","title":"Diagram: Regularization Decision Tree","text":"Regularization Decision Tree <p>Type: workflow</p> <p>Bloom Taxonomy: Evaluate, Apply</p> <p>Learning Objective: Guide students through choosing the right regularization approach for their problem</p> <p>Visual Style: Flowchart with decision diamonds and outcome rectangles</p> <p>Start: \"Need to Prevent Overfitting?\"</p> <p>Decision 1: \"Linear relationship?\" - Yes \u2192 Consider if regularization is needed - No \u2192 Add polynomial features</p> <p>Decision 2: \"How many features vs samples?\" - Many features, few samples \u2192 Strong regularization needed - Balanced \u2192 Moderate regularization - Few features, many samples \u2192 Light regularization</p> <p>Decision 3: \"Do you want feature selection?\" - Yes, aggressive \u2192 Use Lasso - Yes, some \u2192 Use Elastic Net - No, keep all features \u2192 Use Ridge</p> <p>Decision 4: \"Highly correlated features?\" - Yes \u2192 Use Ridge or Elastic Net (Lasso is unstable) - No \u2192 Any method works</p> <p>Decision 5: \"Interpretability important?\" - Yes \u2192 Lasso (sparse solution) - No \u2192 Ridge (often better accuracy)</p> <p>Final Outcomes: - Ridge: \"Many small effects, correlated features\" - Lasso: \"Few important features, interpretability\" - Elastic Net: \"Best of both, groups of features\"</p> <p>Interactive Elements: - Click each decision to see explanation - Hover shows examples of each scenario - \"Take Quiz\" mode walks through with your data characteristics</p> <p>Implementation: HTML/CSS/JavaScript with interactive flowchart</p>"},{"location":"chapters/11-nonlinear-models-regularization/#common-pitfalls-and-best-practices","title":"Common Pitfalls and Best Practices","text":"<p>Always Scale Before Regularizing Regularization penalizes coefficient magnitude. If features aren't scaled, features with larger values will be unfairly penalized.</p> <pre><code># Good: Scale inside pipeline\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('ridge', Ridge())\n])\n\n# Bad: Features on different scales penalized unequally\nridge_bad = Ridge()\nridge_bad.fit(X_unscaled, y)\n</code></pre> <p>Don't Regularize the Intercept Scikit-learn doesn't regularize the intercept by default (which is correct). Be careful if using other implementations.</p> <p>Use Cross-Validation for Lambda Never set \u03bb by looking at test performance. Use cross-validation to find optimal \u03bb, then evaluate on test data.</p> <p>Consider the Problem Type - Prediction focus \u2192 Ridge often wins - Interpretation focus \u2192 Lasso for sparsity - Groups of related features \u2192 Elastic Net</p> <p>Watch for Warning Signs - Very large or very small \u03bb optimal \u2192 reconsider model specification - All coefficients near zero \u2192 \u03bb too large - Test performance much worse than CV \u2192 something's wrong</p>"},{"location":"chapters/11-nonlinear-models-regularization/#summary-your-regularization-toolkit","title":"Summary: Your Regularization Toolkit","text":"<p>You now have powerful tools for handling non-linear relationships and overfitting:</p> <ul> <li>Polynomial regression captures curved patterns using powers of features</li> <li>Degree selection balances flexibility with overfitting risk</li> <li>Transformations (log, sqrt, etc.) can linearize relationships</li> <li>Model flexibility is the dial between underfitting and overfitting</li> <li>Regularization adds complexity penalties to prevent overfitting</li> <li>Ridge (L2) shrinks all coefficients, handles multicollinearity</li> <li>Lasso (L1) performs automatic feature selection</li> <li>Elastic Net combines L1 and L2 penalties</li> <li>Lambda tuning via cross-validation finds the optimal penalty strength</li> </ul> <p>With these techniques, you can build models that are flexible enough to capture complex real-world patterns while remaining robust enough to generalize to new data.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#looking-ahead","title":"Looking Ahead","text":"<p>In the next chapter, we'll explore machine learning more broadly\u2014including classification problems where we predict categories instead of numbers. You'll see how the regularization concepts you learned here apply to new types of models.</p>"},{"location":"chapters/11-nonlinear-models-regularization/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Polynomial regression captures non-linear patterns while still using linear regression techniques</li> <li>Higher polynomial degrees increase flexibility but also overfitting risk</li> <li>Log and other transformations can linearize curved relationships</li> <li>Regularization adds a penalty for complexity, balancing fit with generalization</li> <li>Ridge (L2) shrinks coefficients; Lasso (L1) can zero them out entirely</li> <li>Elastic Net combines both penalties for flexible feature selection</li> <li>Always scale features before regularizing and use CV to find optimal \u03bb</li> <li>The goal is finding the sweet spot: flexible enough to learn patterns, constrained enough to ignore noise</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/","title":"Introduction to Machine Learning","text":"<p>title: Introduction to Machine Learning description: Teaching computers to learn from experience - the ultimate superpower generated_by: chapter-content-generator skill date: 2025-12-15 version: 0.03</p>"},{"location":"chapters/12-intro-to-machine-learning/#summary","title":"Summary","text":"<p>This chapter provides a conceptual foundation for machine learning. Students will learn the distinction between supervised and unsupervised learning, understand the training process, and explore key concepts like generalization and error types. The chapter covers loss and cost functions, optimization theory, and gradient descent as the fundamental algorithm for training models. By the end of this chapter, students will understand how machine learning models learn from data and be prepared for neural networks.</p>"},{"location":"chapters/12-intro-to-machine-learning/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Machine Learning</li> <li>Supervised Learning</li> <li>Unsupervised Learning</li> <li>Classification</li> <li>Clustering</li> <li>Training Process</li> <li>Learning Algorithm</li> <li>Model Training</li> <li>Generalization</li> <li>Training Error</li> <li>Test Error</li> <li>Prediction Error</li> <li>Loss Function</li> <li>Cost Function</li> <li>Optimization</li> <li>Gradient Descent</li> <li>Learning Rate</li> <li>Convergence</li> <li>Local Minimum</li> <li>Global Minimum</li> </ol>"},{"location":"chapters/12-intro-to-machine-learning/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 7: Simple Linear Regression</li> <li>Chapter 8: Model Evaluation and Validation</li> <li>Chapter 10: NumPy and Numerical Computing</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/#introduction-welcome-to-the-machine-learning-revolution","title":"Introduction: Welcome to the Machine Learning Revolution","text":"<p>Everything you've learned so far has been building to this moment. Linear regression? That was machine learning. Model evaluation? Essential for machine learning. NumPy? The engine that powers machine learning. You've been doing machine learning all along\u2014you just didn't know it yet.</p> <p>But now we're going to pull back the curtain and understand the why and how behind it all. How does a computer actually \"learn\"? What does training a model really mean? And how can a bunch of math magically give computers the ability to recognize faces, translate languages, and predict the future?</p> <p>This chapter answers these questions and gives you the conceptual foundation you need for the most powerful tools in data science. By the end, you'll understand not just how to use machine learning, but how it works. That's the difference between using a superpower and truly mastering it.</p>"},{"location":"chapters/12-intro-to-machine-learning/#what-is-machine-learning","title":"What Is Machine Learning?","text":"<p>Machine learning is a field of computer science where we build systems that learn from data rather than being explicitly programmed. Instead of writing rules like \"if email contains 'free money,' mark as spam,\" we show the computer thousands of spam and non-spam emails and let it figure out the patterns itself.</p> <p>Here's the key insight: traditional programming is about rules; machine learning is about patterns.</p> Traditional Programming Machine Learning Input: Data + Rules Input: Data + Answers Output: Answers Output: Rules (the model) Human writes the logic Computer discovers the logic Brittle to new situations Adapts to new patterns <p>A simple definition:</p> <p>Machine learning is the study of algorithms that improve their performance at some task through experience.</p> <p>That \"experience\" is data. The \"improvement\" is measured by some metric. And the \"task\" could be predicting house prices, recognizing cats in photos, or recommending movies you'll love.</p> <pre><code># Traditional programming approach\ndef is_spam_traditional(email):\n    spam_words = ['free', 'winner', 'click here', 'urgent']\n    for word in spam_words:\n        if word in email.lower():\n            return True\n    return False\n\n# Machine learning approach (conceptual)\n# We don't write rules - we train a model on examples\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB()\nmodel.fit(training_emails, training_labels)  # Learn from data\npredictions = model.predict(new_emails)       # Apply learned patterns\n</code></pre> <p>The magic is in <code>model.fit()</code>\u2014that's where the learning happens.</p>"},{"location":"chapters/12-intro-to-machine-learning/#supervised-learning-learning-with-a-teacher","title":"Supervised Learning: Learning with a Teacher","text":"<p>Supervised learning is the most common type of machine learning. It's called \"supervised\" because we provide the correct answers during training\u2014like a teacher grading homework. The model learns to map inputs to outputs by studying examples where we already know the answer.</p> <p>The setup:</p> <ul> <li>Features (X): The input information (house size, location, age)</li> <li>Labels (y): The correct answers (house price, spam/not-spam)</li> <li>Goal: Learn a function f(X) \u2192 y that works for new data</li> </ul> <p>All the regression you've learned is supervised learning! You provided house features and prices, and the model learned to predict prices from features.</p> <pre><code>from sklearn.linear_model import LinearRegression\n\n# Supervised learning: we provide both X (features) and y (labels)\nX = df[['square_feet', 'bedrooms', 'age']]  # Features\ny = df['price']                               # Labels (correct answers)\n\nmodel = LinearRegression()\nmodel.fit(X, y)  # \"Supervised\" by the labels\n\n# Now predict on new data where we don't know the answer\nnew_house = [[2000, 3, 10]]\npredicted_price = model.predict(new_house)\n</code></pre> <p>Supervised learning powers:</p> <ul> <li>Price prediction (regression)</li> <li>Email spam detection (classification)</li> <li>Medical diagnosis (classification)</li> <li>Weather forecasting (regression)</li> <li>Credit scoring (classification)</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/#unsupervised-learning-discovering-hidden-structure","title":"Unsupervised Learning: Discovering Hidden Structure","text":"<p>Unsupervised learning works without labels\u2014no correct answers are provided. Instead, the model discovers patterns and structure in the data on its own. It's like exploring a new city without a map; you find natural groupings and patterns through observation.</p> <p>The setup:</p> <ul> <li>Features (X): The input information</li> <li>No labels (y): We don't tell the model what to look for</li> <li>Goal: Discover interesting structure in the data</li> </ul> <pre><code>from sklearn.cluster import KMeans\n\n# Unsupervised learning: only X, no y!\nX = df[['spending', 'frequency', 'recency']]\n\n# Find natural groupings of customers\nmodel = KMeans(n_clusters=4)\nmodel.fit(X)  # No labels provided\n\n# Which cluster does each customer belong to?\nclusters = model.predict(X)\n</code></pre> <p>Unsupervised learning powers:</p> <ul> <li>Customer segmentation</li> <li>Anomaly detection</li> <li>Topic discovery in documents</li> <li>Dimensionality reduction</li> <li>Recommendation systems (partially)</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/#diagram-supervised-vs-unsupervised-learning","title":"Diagram: Supervised vs Unsupervised Learning","text":"Supervised vs Unsupervised Learning <p>Type: infographic</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Clearly distinguish between supervised and unsupervised learning paradigms through visual comparison</p> <p>Layout: Side-by-side comparison with examples</p> <p>Left Panel - Supervised Learning: - Visual: Training data with input features AND color-coded labels - Example: Photos of cats and dogs, each labeled - Arrow showing: Data + Labels \u2192 Model \u2192 Predictions - Use cases listed: Spam detection, Price prediction, Medical diagnosis - Key insight: \"Learning WITH a teacher\"</p> <p>Right Panel - Unsupervised Learning: - Visual: Training data with input features, NO labels - Example: Unlabeled customer data points - Arrow showing: Data \u2192 Model \u2192 Discovered Groups/Patterns - Use cases listed: Customer segments, Anomaly detection, Topic modeling - Key insight: \"Learning to find structure\"</p> <p>Center Comparison: - Table showing key differences - Input data visualization (labeled vs unlabeled) - Output type (predictions vs structure)</p> <p>Interactive Elements: - Click each panel for expanded examples - Hover over use cases for brief explanations - Toggle: \"Show math notation\" for formal definitions - Quiz mode: \"Which type is this?\" with scenarios</p> <p>Color Scheme: - Supervised: Green (has guidance) - Unsupervised: Blue (exploring) - Labels shown in distinct colors in supervised examples</p> <p>Implementation: HTML/CSS/JavaScript with click interactions</p>"},{"location":"chapters/12-intro-to-machine-learning/#classification-predicting-categories","title":"Classification: Predicting Categories","text":"<p>Classification is a type of supervised learning where the target variable is categorical (a class or category) rather than numerical. Instead of predicting a number, you're predicting which group something belongs to.</p> <p>Examples of classification:</p> Problem Input Features Output Classes Email spam Email text, sender, links Spam, Not Spam Disease diagnosis Symptoms, test results Disease A, B, C, Healthy Image recognition Pixel values Cat, Dog, Bird, ... Customer churn Usage patterns, demographics Will Leave, Will Stay Loan default Income, history, debt Default, No Default <p>Binary classification has two classes; multi-class classification has more than two.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Binary classification example\nX = df[['age', 'income', 'debt_ratio']]\ny = df['will_default']  # 0 or 1\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train classifier\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Evaluate\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\nprint(classification_report(y_test, y_pred))\n</code></pre> <p>Classification metrics differ from regression:</p> <ul> <li>Accuracy: Fraction of correct predictions</li> <li>Precision: Of predicted positives, how many are correct?</li> <li>Recall: Of actual positives, how many did we catch?</li> <li>F1 Score: Harmonic mean of precision and recall</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/#clustering-finding-natural-groups","title":"Clustering: Finding Natural Groups","text":"<p>Clustering is a type of unsupervised learning that groups similar data points together. The algorithm discovers natural groupings without being told how many groups exist or what they should look like.</p> <p>K-Means is the most common clustering algorithm:</p> <pre><code>from sklearn.cluster import KMeans\nimport plotly.express as px\n\n# Customer data (no labels!)\nX = df[['annual_spending', 'visit_frequency']]\n\n# Find 4 customer segments\nkmeans = KMeans(n_clusters=4, random_state=42)\ndf['cluster'] = kmeans.fit_predict(X)\n\n# Visualize clusters\nfig = px.scatter(df, x='annual_spending', y='visit_frequency',\n                 color='cluster', title='Customer Segments Discovered by K-Means')\nfig.show()\n\n# Examine cluster centers\nprint(\"Cluster Centers:\")\nprint(kmeans.cluster_centers_)\n</code></pre> <p>Clustering applications:</p> <ul> <li>Customer segmentation: Group customers by behavior for targeted marketing</li> <li>Document organization: Group similar articles or papers</li> <li>Image compression: Group similar colors to reduce file size</li> <li>Anomaly detection: Points far from any cluster may be anomalies</li> <li>Biology: Group genes with similar expression patterns</li> </ul> <p>The key challenge: choosing the right number of clusters. Too few, and you miss distinctions. Too many, and you're overfitting to noise.</p>"},{"location":"chapters/12-intro-to-machine-learning/#the-training-process-how-models-learn","title":"The Training Process: How Models Learn","text":"<p>Now let's understand what actually happens when you call <code>model.fit()</code>. The training process is the procedure by which a model adjusts its internal parameters to better match the training data.</p> <p>Here's the cycle:</p> <ol> <li>Initialize: Start with random (or default) parameter values</li> <li>Predict: Use current parameters to make predictions</li> <li>Measure error: Compare predictions to actual values</li> <li>Update parameters: Adjust to reduce the error</li> <li>Repeat: Go back to step 2 until error is small enough</li> </ol> <p>This is iterative learning\u2014the model gets a little better with each cycle.</p> <pre><code># Conceptual training loop (what happens inside model.fit())\ndef train_model(X, y, learning_rate=0.01, iterations=1000):\n    # Step 1: Initialize parameters randomly\n    weights = np.random.randn(X.shape[1])\n    bias = 0\n\n    for i in range(iterations):\n        # Step 2: Make predictions with current parameters\n        predictions = X @ weights + bias\n\n        # Step 3: Measure error (mean squared error)\n        error = predictions - y\n        mse = np.mean(error ** 2)\n\n        # Step 4: Calculate how to adjust parameters (gradients)\n        weight_gradients = (2/len(y)) * X.T @ error\n        bias_gradient = (2/len(y)) * np.sum(error)\n\n        # Step 5: Update parameters\n        weights = weights - learning_rate * weight_gradients\n        bias = bias - learning_rate * bias_gradient\n\n        if i % 100 == 0:\n            print(f\"Iteration {i}: MSE = {mse:.4f}\")\n\n    return weights, bias\n</code></pre> <p>This simple loop is the heart of nearly all machine learning!</p>"},{"location":"chapters/12-intro-to-machine-learning/#diagram-training-process-animator","title":"Diagram: Training Process Animator","text":"Training Process Animator <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize the iterative training process showing how parameters adjust over time to fit the data</p> <p>Canvas Layout (850x550): - Main area (850x400): Scatter plot with evolving regression line - Bottom area (850x150): Controls and metrics</p> <p>Main Visualization: - Data points (fixed throughout training) - Regression line that updates with each iteration - Residual lines from points to current line - Ghost trails of previous line positions (fading) - Current parameter values displayed: w = X.XX, b = X.XX</p> <p>Training Animation: - Step through iterations one at a time or auto-play - Line visibly adjusts toward better fit - Error metric (MSE) decreases over time - Color intensity of line changes (red = high error, green = low error)</p> <p>Metrics Panel: - Current iteration counter: 0 / 1000 - Mean Squared Error: updating value - Line chart showing MSE over iterations - \"Converged!\" message when improvement stops</p> <p>Interactive Controls: - Button: \"Step\" - advance one iteration - Button: \"Play/Pause\" - auto-advance - Speed slider: iterations per second - Button: \"Reset\" - restart training - Slider: Learning rate (0.001 to 1.0) - Dropdown: Different starting positions</p> <p>Educational Overlays: - First iteration: \"Starting with random parameters\" - Early iterations: \"Big adjustments to reduce error\" - Later iterations: \"Fine-tuning approaches optimal\" - Converged: \"Training complete!\"</p> <p>Implementation: p5.js with smooth animation</p>"},{"location":"chapters/12-intro-to-machine-learning/#learning-algorithm-and-model-training","title":"Learning Algorithm and Model Training","text":"<p>A learning algorithm is the specific procedure used to find good parameters. It defines how the model adjusts its weights based on the error. Different algorithms have different strategies:</p> <ul> <li>Ordinary Least Squares: Solve directly using linear algebra (fast, exact for linear regression)</li> <li>Gradient Descent: Iteratively follow the slope downhill (general, works for complex models)</li> <li>Stochastic Gradient Descent: Use random samples for faster updates (scales to big data)</li> </ul> <p>Model training is the execution of the learning algorithm on your data. It's the process of finding parameter values that minimize prediction error.</p> <pre><code># Different learning algorithms for the same problem\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\n\n# Method 1: Ordinary Least Squares (closed-form solution)\nols_model = LinearRegression()\nols_model.fit(X_train, y_train)\n\n# Method 2: Stochastic Gradient Descent (iterative)\nsgd_model = SGDRegressor(max_iter=1000, learning_rate='optimal')\nsgd_model.fit(X_train, y_train)\n\n# Both should give similar results!\nprint(f\"OLS coefficients: {ols_model.coef_}\")\nprint(f\"SGD coefficients: {sgd_model.coef_}\")\n</code></pre> <p>For linear regression, OLS is typically faster and more accurate. But gradient descent becomes essential for complex models like neural networks where closed-form solutions don't exist.</p>"},{"location":"chapters/12-intro-to-machine-learning/#generalization-the-ultimate-goal","title":"Generalization: The Ultimate Goal","text":"<p>Generalization is the ability of a trained model to perform well on new, unseen data. This is the whole point of machine learning! A model that only works on training data is useless\u2014we need it to work in the real world.</p> <p>Think about it:</p> <ul> <li>We train on past house sales, but want to predict future prices</li> <li>We train on known spam, but want to catch new spam</li> <li>We train on diagnosed patients, but want to diagnose new patients</li> </ul> <p>The challenge: training data is limited, but the real world is vast. A model must learn general patterns that transfer to new situations, not specific quirks of the training data.</p> <pre><code>from sklearn.model_selection import train_test_split\n\n# The generalization test: does it work on data it hasn't seen?\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel.fit(X_train, y_train)\n\ntrain_score = model.score(X_train, y_train)  # How well it memorized\ntest_score = model.score(X_test, y_test)      # How well it generalized\n\nprint(f\"Training R\u00b2: {train_score:.4f}\")\nprint(f\"Test R\u00b2: {test_score:.4f}\")\nprint(f\"Generalization gap: {train_score - test_score:.4f}\")\n</code></pre> <p>A small gap means good generalization. A large gap means the model memorized training data instead of learning patterns.</p>"},{"location":"chapters/12-intro-to-machine-learning/#training-error-test-error-and-prediction-error","title":"Training Error, Test Error, and Prediction Error","text":"<p>Understanding different types of error is crucial for diagnosing model problems.</p> <p>Training error (also called in-sample error) measures how well the model fits the training data. It's calculated using the same data used to train the model.</p> <p>Test error (also called out-of-sample error) measures how well the model performs on new data it hasn't seen. This is the true measure of model quality.</p> <p>Prediction error is the error on any specific prediction\u2014the difference between predicted and actual values.</p> <pre><code>from sklearn.metrics import mean_squared_error\n\n# Training error\ntrain_predictions = model.predict(X_train)\ntrain_mse = mean_squared_error(y_train, train_predictions)\n\n# Test error\ntest_predictions = model.predict(X_test)\ntest_mse = mean_squared_error(y_test, test_predictions)\n\nprint(f\"Training MSE: {train_mse:.2f}\")\nprint(f\"Test MSE: {test_mse:.2f}\")\n</code></pre> Scenario Training Error Test Error Diagnosis Both high High High Underfitting (model too simple) Train low, test high Low High Overfitting (model memorized) Both low Low Low Good fit! Train high, test low High Low Rare; check for data leakage <p>The pattern to watch: if training error is much lower than test error, you're overfitting.</p>"},{"location":"chapters/12-intro-to-machine-learning/#diagram-error-types-visualizer","title":"Diagram: Error Types Visualizer","text":"Error Types Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze, Evaluate</p> <p>Learning Objective: Understand the relationship between training and test error, and diagnose underfitting vs overfitting</p> <p>Canvas Layout (850x500): - Left panel (425x350): Training data and model fit - Right panel (425x350): Test data and model fit - Bottom area (850x150): Error metrics and diagnosis</p> <p>Left Panel - Training View: - Scatter plot of training data - Fitted model curve/line - Residual lines shown - Training MSE displayed - Color coding: blue for data, green for good fit</p> <p>Right Panel - Test View: - Scatter plot of test data (different points) - Same model from training overlaid - Residual lines to new points - Test MSE displayed - Color coding: orange for data, fit quality color-coded</p> <p>Bottom Panel - Diagnosis: - Bar chart comparing Training MSE vs Test MSE - Gap indicator with color coding - Diagnosis text: \"Underfitting\", \"Good Fit\", or \"Overfitting\" - Recommendations based on diagnosis</p> <p>Interactive Controls: - Slider: Model complexity (polynomial degree 1-15) - Button: \"Generate New Data\" - Slider: Noise level in data - Slider: Training set size - Checkbox: \"Show residuals\"</p> <p>Visual Feedback: - As complexity increases, show training error dropping - Show test error following U-shaped curve - Highlight the optimal complexity point - Animate the gap between train and test growing with overfitting</p> <p>Key Learning Moments: - Degree 1-2: \"Model too simple - both errors high\" - Degree 3-4: \"Sweet spot - errors low and similar\" - Degree 10+: \"Model too complex - train low, test high\"</p> <p>Implementation: p5.js with split-panel visualization</p>"},{"location":"chapters/12-intro-to-machine-learning/#loss-function-measuring-prediction-quality","title":"Loss Function: Measuring Prediction Quality","text":"<p>A loss function (also called error function or objective function) measures how wrong a single prediction is. It takes the predicted value and actual value, and returns a number indicating how bad the prediction was.</p> <p>Common loss functions for regression:</p> Loss Function Formula Properties Squared Error \\((y - \\hat{y})^2\\) Penalizes large errors heavily Absolute Error $ y - \\hat{y} Huber Loss Squared if small, absolute if large Best of both <p>For classification:</p> Loss Function Use Case Properties Binary Cross-Entropy Two classes Measures probability error Categorical Cross-Entropy Multiple classes Extension of binary Hinge Loss SVM classifiers Margin-based <pre><code>import numpy as np\n\ndef squared_error(y_true, y_pred):\n    \"\"\"Loss for a single prediction\"\"\"\n    return (y_true - y_pred) ** 2\n\ndef absolute_error(y_true, y_pred):\n    \"\"\"More robust to outliers\"\"\"\n    return np.abs(y_true - y_pred)\n\n# Example\ny_true = 100\ny_pred = 90\n\nprint(f\"Squared Error: {squared_error(y_true, y_pred)}\")  # 100\nprint(f\"Absolute Error: {absolute_error(y_true, y_pred)}\")  # 10\n</code></pre> <p>The choice of loss function affects what the model optimizes for. Squared error emphasizes getting big predictions right; absolute error treats all errors equally.</p>"},{"location":"chapters/12-intro-to-machine-learning/#cost-function-total-training-error","title":"Cost Function: Total Training Error","text":"<p>The cost function (also called objective function) aggregates the loss across all training examples. While loss measures error for one prediction, cost measures error for the entire training set.</p> \\[J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, \\hat{y}_i)\\] <p>Where:</p> <ul> <li>\\(J(\\theta)\\) is the cost as a function of parameters \\(\\theta\\)</li> <li>\\(L\\) is the loss function</li> <li>\\(n\\) is the number of training examples</li> </ul> <p>For Mean Squared Error:</p> \\[J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] <pre><code>def cost_function(X, y, weights, bias):\n    \"\"\"\n    Calculate total cost (MSE) for given parameters\n    \"\"\"\n    n = len(y)\n    predictions = X @ weights + bias\n    squared_errors = (y - predictions) ** 2\n    cost = np.mean(squared_errors)\n    return cost\n\n# Example: cost at different parameter values\nweights_bad = np.array([0.1, 0.1, 0.1])\nweights_good = np.array([150, 10000, -500])\n\ncost_bad = cost_function(X_train, y_train, weights_bad, 0)\ncost_good = cost_function(X_train, y_train, weights_good, 50000)\n\nprint(f\"Cost with bad weights: {cost_bad:,.0f}\")\nprint(f\"Cost with good weights: {cost_good:,.0f}\")\n</code></pre> <p>Training is all about minimizing this cost function. The model that minimizes cost is the best fit to the training data.</p>"},{"location":"chapters/12-intro-to-machine-learning/#optimization-finding-the-best-parameters","title":"Optimization: Finding the Best Parameters","text":"<p>Optimization is the mathematical process of finding parameter values that minimize (or maximize) some objective. In machine learning, we minimize the cost function.</p> <p>Imagine the cost function as a landscape:</p> <ul> <li>High points = bad parameters (high cost)</li> <li>Low points = good parameters (low cost)</li> <li>The goal = find the lowest point (global minimum)</li> </ul> <p>For simple linear regression, we can find the optimal parameters directly using calculus (the \"normal equations\"). But for complex models, we need iterative methods.</p> <pre><code># Closed-form solution (only works for linear regression)\n# The normal equations: \u03b8 = (X^T X)^(-1) X^T y\ndef solve_normal_equations(X, y):\n    X_with_bias = np.column_stack([np.ones(len(X)), X])\n    theta = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n    return theta[0], theta[1:]  # bias, weights\n\nbias, weights = solve_normal_equations(X_train, y_train)\nprint(f\"Optimal bias: {bias:.2f}\")\nprint(f\"Optimal weights: {weights}\")\n</code></pre> <p>This direct solution is fast and exact for linear regression. But what about models where no closed-form solution exists? That's where gradient descent comes in.</p>"},{"location":"chapters/12-intro-to-machine-learning/#gradient-descent-the-universal-optimizer","title":"Gradient Descent: The Universal Optimizer","text":"<p>Gradient descent is the workhorse algorithm of machine learning. It finds the minimum of a function by repeatedly taking steps in the direction of steepest descent.</p> <p>The intuition: Imagine you're blindfolded on a hilly landscape and want to find the lowest point. What would you do? Feel the slope under your feet and step downhill. Repeat until you can't go any lower.</p> <p>That's gradient descent:</p> <ol> <li>Calculate the gradient (slope) of the cost function at current position</li> <li>Take a step in the opposite direction (downhill)</li> <li>Repeat until you reach a minimum</li> </ol> \\[\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla J(\\theta)\\] <p>Where:</p> <ul> <li>\\(\\theta\\) is the parameter vector</li> <li>\\(\\alpha\\) is the learning rate (step size)</li> <li>\\(\\nabla J(\\theta)\\) is the gradient (direction of steepest ascent)</li> </ul> <pre><code>def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n    \"\"\"\n    Find optimal parameters using gradient descent\n    \"\"\"\n    n = len(y)\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    costs = []\n\n    for i in range(iterations):\n        # Predictions with current parameters\n        predictions = X @ weights + bias\n\n        # Gradients (direction of steepest ascent)\n        error = predictions - y\n        weight_gradient = (2/n) * X.T @ error\n        bias_gradient = (2/n) * np.sum(error)\n\n        # Update parameters (step downhill)\n        weights = weights - learning_rate * weight_gradient\n        bias = bias - learning_rate * bias_gradient\n\n        # Track cost\n        cost = np.mean(error ** 2)\n        costs.append(cost)\n\n    return weights, bias, costs\n\n# Train with gradient descent\nweights, bias, costs = gradient_descent(X_train, y_train)\n\n# Plot convergence\nimport plotly.express as px\nfig = px.line(y=costs, title='Gradient Descent Convergence',\n              labels={'x': 'Iteration', 'y': 'Cost (MSE)'})\nfig.show()\n</code></pre>"},{"location":"chapters/12-intro-to-machine-learning/#diagram-gradient-descent-visualizer","title":"Diagram: Gradient Descent Visualizer","text":"Gradient Descent Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize gradient descent as navigating a cost landscape to find the minimum</p> <p>Canvas Layout (850x600): - Main area (850x450): 3D surface or 2D contour plot of cost function - Bottom area (850x150): Controls and current state</p> <p>Main Visualization Options: Toggle between: 1. 3D Surface View:    - Cost function as a bowl-shaped surface    - Current position marked with a ball    - Path of descent shown as connected line    - Axes: weight1, weight2, cost</p> <ol> <li>2D Contour View:</li> <li>Top-down view with contour lines (like a topographic map)</li> <li>Current position marked with dot</li> <li>Gradient arrow showing direction of steepest descent</li> <li>Path traced as line with markers at each step</li> </ol> <p>Animation: - Ball/dot moves along gradient descent path - Arrow shows current gradient direction - Leave trail showing history of positions - Cost value updates in real-time</p> <p>Interactive Controls: - Button: \"Step\" - take one gradient step - Button: \"Run\" - animate continuous descent - Slider: Learning rate (0.001 to 2.0) - Dropdown: Starting position (corner, middle, near minimum) - Checkbox: \"Show gradient arrows\" - Checkbox: \"Show path history\"</p> <p>Learning Rate Effects: - Too small: Slow progress, many small steps - Just right: Steady progress to minimum - Too large: Overshooting, oscillation, or divergence</p> <p>Visual Feedback: - Speed indicator showing step sizes - Warning when oscillating (too high learning rate) - \"Converged!\" message when reaching minimum - Display current parameter values and cost</p> <p>Different Landscapes: - Dropdown: Simple bowl, Elongated valley, Multiple minima - Shows how gradient descent behaves differently</p> <p>Implementation: p5.js with WEBGL for 3D or 2D canvas</p>"},{"location":"chapters/12-intro-to-machine-learning/#learning-rate-the-step-size","title":"Learning Rate: The Step Size","text":"<p>The learning rate (often denoted \\(\\alpha\\) or \\(\\eta\\)) controls how big each step is during gradient descent. It's one of the most important hyperparameters in machine learning.</p> Learning Rate Behavior Risk Too small Very slow convergence May never finish Just right Steady progress to minimum Goldilocks zone Too large Overshoots minimum May diverge (explode) <pre><code>import plotly.graph_objects as go\n\n# Compare different learning rates\nlearning_rates = [0.001, 0.01, 0.1, 0.5]\nfig = go.Figure()\n\nfor lr in learning_rates:\n    _, _, costs = gradient_descent(X_train, y_train, learning_rate=lr, iterations=200)\n    fig.add_trace(go.Scatter(y=costs, mode='lines', name=f'LR = {lr}'))\n\nfig.update_layout(title='Effect of Learning Rate on Convergence',\n                  xaxis_title='Iteration', yaxis_title='Cost')\nfig.show()\n</code></pre> <p>Finding the right learning rate often requires experimentation. Some strategies:</p> <ul> <li>Start large, decay: Begin with a larger rate, reduce over time</li> <li>Grid search: Try several values, pick the best</li> <li>Adaptive methods: Algorithms like Adam adjust the rate automatically</li> </ul> <p>Learning Rate Rules of Thumb</p> <p>Start with 0.01 or 0.001 as a default. If training is too slow, increase it. If cost increases or oscillates wildly, decrease it. For neural networks, use adaptive optimizers like Adam that adjust automatically.</p>"},{"location":"chapters/12-intro-to-machine-learning/#convergence-knowing-when-to-stop","title":"Convergence: Knowing When to Stop","text":"<p>Convergence is when the optimization process has reached a stable solution\u2014the parameters stop changing significantly. At convergence, additional iterations don't improve the model.</p> <p>Signs of convergence:</p> <ul> <li>Cost function value stops decreasing</li> <li>Parameter changes become very small</li> <li>Gradient magnitudes approach zero</li> </ul> <pre><code>def gradient_descent_with_convergence(X, y, learning_rate=0.01, max_iterations=10000, tolerance=1e-6):\n    \"\"\"\n    Gradient descent that stops when converged\n    \"\"\"\n    weights = np.zeros(X.shape[1])\n    bias = 0\n    prev_cost = float('inf')\n\n    for i in range(max_iterations):\n        predictions = X @ weights + bias\n        error = predictions - y\n        cost = np.mean(error ** 2)\n\n        # Check for convergence\n        if abs(prev_cost - cost) &lt; tolerance:\n            print(f\"Converged after {i} iterations!\")\n            break\n\n        # Gradient update\n        weights = weights - learning_rate * (2/len(y)) * X.T @ error\n        bias = bias - learning_rate * (2/len(y)) * np.sum(error)\n\n        prev_cost = cost\n\n    return weights, bias, i\n\nweights, bias, iterations = gradient_descent_with_convergence(X_train, y_train)\nprint(f\"Training completed in {iterations} iterations\")\n</code></pre> <p>Common stopping criteria:</p> <ul> <li>Maximum iterations reached</li> <li>Cost improvement below threshold</li> <li>Gradient magnitude below threshold</li> <li>Validation performance stops improving (early stopping)</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/#local-minimum-vs-global-minimum","title":"Local Minimum vs Global Minimum","text":"<p>When optimizing, we want to find the global minimum\u2014the lowest point across the entire cost landscape. But gradient descent can get stuck in a local minimum\u2014a point that's lower than its neighbors but not the absolute lowest.</p> <p>Think of it like hiking in the mountains:</p> <ul> <li>Global minimum: The valley with the lowest elevation in the entire range</li> <li>Local minimum: A small valley that's lower than nearby areas but not the lowest overall</li> </ul> <pre><code># Illustration of local vs global minima\nimport numpy as np\nimport plotly.graph_objects as go\n\n# A function with multiple minima\nx = np.linspace(-3, 3, 100)\ny = x**4 - 3*x**2 + 0.5*x  # Has a local and global minimum\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='Cost Function'))\n\n# Mark the minima\nfig.add_annotation(x=-1.3, y=-2, text=\"Global Minimum\", showarrow=True, arrowhead=2)\nfig.add_annotation(x=1.1, y=-1, text=\"Local Minimum\", showarrow=True, arrowhead=2)\n\nfig.update_layout(title='Local vs Global Minima',\n                  xaxis_title='Parameter Value', yaxis_title='Cost')\nfig.show()\n</code></pre> <p>For linear regression, the cost function is convex (bowl-shaped), so any minimum is the global minimum. But for neural networks and other complex models, the landscape can have many local minima.</p> <p>Strategies to avoid local minima:</p> <ul> <li>Random restarts: Run optimization from different starting points</li> <li>Momentum: Add \"inertia\" to roll through small local minima</li> <li>Stochastic gradient descent: Random sampling adds noise that can escape local minima</li> <li>Learning rate schedules: Adjusting the rate during training</li> </ul>"},{"location":"chapters/12-intro-to-machine-learning/#diagram-optimization-landscape-explorer","title":"Diagram: Optimization Landscape Explorer","text":"Optimization Landscape Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze, Evaluate</p> <p>Learning Objective: Understand the difference between local and global minima and how optimization strategies affect which minimum is found</p> <p>Canvas Layout (850x550): - Main area (850x400): Interactive cost landscape with optimizer - Bottom area (850x150): Controls and explanation</p> <p>Main Visualization: - 2D function plot with multiple valleys (minima) - One global minimum (deepest valley) - Several local minima (shallower valleys) - Current optimizer position marked with ball - Gradient direction shown with arrow</p> <p>Optimization Journey: - Animate ball rolling down toward minimum - Show where it gets \"stuck\" in local minima - Display \"Stuck in local minimum!\" vs \"Found global minimum!\"</p> <p>Interactive Controls: - Click on landscape to set starting position - Button: \"Start Optimization\" - Slider: Learning rate (affects whether it escapes local minima) - Checkbox: \"Add Momentum\" (helps escape shallow minima) - Dropdown: Cost landscape type (convex bowl, multi-modal, complex) - Slider: Noise level (stochastic gradient descent effect)</p> <p>Landscape Types: 1. Convex (simple bowl): Always finds global minimum 2. Two minima: May get stuck depending on start 3. Many minima: Very sensitive to start and learning rate 4. Saddle points: Shows how gradient can slow at flat regions</p> <p>Educational Annotations: - Mark each minimum with its cost value - Highlight when optimizer escapes a local minimum - Show gradient magnitude decreasing near minima - Compare final cost to global minimum cost</p> <p>Statistics Panel: - Number of iterations - Final cost value - Distance from global minimum - Success rate across multiple random starts</p> <p>Implementation: p5.js with physics-based ball animation</p>"},{"location":"chapters/12-intro-to-machine-learning/#putting-it-all-together-the-ml-pipeline","title":"Putting It All Together: The ML Pipeline","text":"<p>Here's how all these concepts connect in a typical machine learning workflow:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport plotly.express as px\n\n# 1. Load and prepare data\nnp.random.seed(42)\nn_samples = 1000\nX = np.random.randn(n_samples, 3)\ny = 3*X[:, 0] + 2*X[:, 1] - X[:, 2] + np.random.randn(n_samples) * 0.5\n\n# 2. Split for generalization testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 3. Scale features (important for gradient descent!)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 4. Train using gradient descent\nmodel = SGDRegressor(\n    loss='squared_error',      # Loss function\n    learning_rate='optimal',    # Learning rate strategy\n    max_iter=1000,              # Maximum iterations\n    tol=1e-4,                   # Convergence tolerance\n    random_state=42\n)\nmodel.fit(X_train_scaled, y_train)\n\n# 5. Evaluate generalization\ntrain_pred = model.predict(X_train_scaled)\ntest_pred = model.predict(X_test_scaled)\n\nprint(\"=== Training Performance ===\")\nprint(f\"MSE: {mean_squared_error(y_train, train_pred):.4f}\")\nprint(f\"R\u00b2: {r2_score(y_train, train_pred):.4f}\")\n\nprint(\"\\n=== Test Performance (Generalization) ===\")\nprint(f\"MSE: {mean_squared_error(y_test, test_pred):.4f}\")\nprint(f\"R\u00b2: {r2_score(y_test, test_pred):.4f}\")\n\nprint(\"\\n=== Learned Parameters ===\")\nprint(f\"Coefficients: {model.coef_}\")\nprint(f\"Intercept: {model.intercept_}\")\n</code></pre> <p>The pipeline connects:</p> <ol> <li>Data \u2192 Training/test split for generalization testing</li> <li>Cost Function (loss) \u2192 Defines what \"good\" means</li> <li>Optimization (gradient descent) \u2192 Finds best parameters</li> <li>Learning Rate \u2192 Controls optimization speed</li> <li>Convergence \u2192 Knows when to stop</li> <li>Generalization \u2192 Tests on unseen data</li> </ol>"},{"location":"chapters/12-intro-to-machine-learning/#summary-the-machine-learning-mental-model","title":"Summary: The Machine Learning Mental Model","text":"<p>You now understand the core concepts that power all of machine learning:</p> <ul> <li>Machine learning teaches computers to learn patterns from data</li> <li>Supervised learning learns from labeled examples; unsupervised learning discovers structure without labels</li> <li>Classification predicts categories; clustering finds natural groups</li> <li>Training iteratively adjusts parameters to reduce error</li> <li>Generalization is the ability to perform well on new data</li> <li>Loss functions measure prediction error; cost functions aggregate over training data</li> <li>Gradient descent finds optimal parameters by following the slope downhill</li> <li>Learning rate controls step size; too small is slow, too large is unstable</li> <li>Convergence occurs when parameters stabilize</li> <li>Local minima can trap optimization; various strategies help escape them</li> </ul> <p>This foundation prepares you for the most exciting topic in modern AI: neural networks. Everything you've learned\u2014gradients, optimization, loss functions, generalization\u2014will apply directly. You're ready.</p>"},{"location":"chapters/12-intro-to-machine-learning/#looking-ahead","title":"Looking Ahead","text":"<p>In the next chapter, we'll build neural networks and use PyTorch. You'll see how the gradient descent and loss function concepts you learned here scale up to millions of parameters. The optimization principles are the same\u2014just with more powerful models that can learn incredibly complex patterns.</p>"},{"location":"chapters/12-intro-to-machine-learning/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Machine learning is about learning patterns from data rather than explicitly programming rules</li> <li>Supervised learning uses labeled data; unsupervised learning discovers structure without labels</li> <li>The training process iteratively adjusts parameters to minimize a cost function</li> <li>Generalization\u2014performance on unseen data\u2014is the true measure of model quality</li> <li>Loss functions measure individual prediction errors; cost functions aggregate over training data</li> <li>Gradient descent finds optimal parameters by repeatedly stepping in the direction of steepest descent</li> <li>Learning rate controls step size; finding the right rate requires experimentation</li> <li>Convergence occurs when optimization has reached a stable solution</li> <li>For complex models, local minima can trap optimization; strategies exist to escape them</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/","title":"Neural Networks and PyTorch","text":"<p>title: Neural Networks and PyTorch description: The ultimate data science superpower - teaching machines to think generated_by: chapter-content-generator skill date: 2025-12-15 version: 0.03</p>"},{"location":"chapters/13-neural-networks-pytorch/#summary","title":"Summary","text":"<p>This comprehensive chapter introduces neural networks and deep learning using PyTorch. Students will learn neural network architecture including neurons, layers, activation functions, and propagation algorithms. The chapter covers PyTorch fundamentals including tensors, autograd, and building neural network modules. Students will implement complete training loops with optimizers and loss functions. The chapter concludes with best practices for model interpretability, documentation, reproducibility, ethics, and capstone project development. By the end of this chapter, students will be able to build, train, and deploy neural network models while following professional best practices.</p>"},{"location":"chapters/13-neural-networks-pytorch/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 55 concepts from the learning graph:</p>"},{"location":"chapters/13-neural-networks-pytorch/#neural-networks-20-concepts","title":"Neural Networks (20 concepts)","text":"<ol> <li>Neural Networks</li> <li>Artificial Neuron</li> <li>Perceptron</li> <li>Activation Function</li> <li>Sigmoid Function</li> <li>ReLU Function</li> <li>Input Layer</li> <li>Hidden Layer</li> <li>Output Layer</li> <li>Weights</li> <li>Biases</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Deep Learning</li> <li>Network Architecture</li> <li>Epochs</li> <li>Batch Size</li> <li>Mini-batch</li> <li>Stochastic Gradient</li> <li>Vanishing Gradient</li> </ol>"},{"location":"chapters/13-neural-networks-pytorch/#pytorch-20-concepts","title":"PyTorch (20 concepts)","text":"<ol> <li>PyTorch Library</li> <li>Tensors</li> <li>Tensor Operations</li> <li>Autograd</li> <li>Automatic Differentiation</li> <li>Computational Graph</li> <li>Neural Network Module</li> <li>Sequential Model</li> <li>Linear Layer</li> <li>Loss Functions PyTorch</li> <li>Optimizer</li> <li>SGD Optimizer</li> <li>Adam Optimizer</li> <li>Training Loop</li> <li>Model Evaluation PyTorch</li> <li>GPU Computing</li> <li>CUDA</li> <li>Model Saving</li> <li>Model Loading</li> <li>Transfer Learning</li> </ol>"},{"location":"chapters/13-neural-networks-pytorch/#best-practices-10-concepts","title":"Best Practices (10 concepts)","text":"<ol> <li>Explainable AI</li> <li>Model Interpretability</li> <li>Feature Importance Analysis</li> <li>SHAP Values</li> <li>Model Documentation</li> <li>Reproducibility</li> <li>Random Seed</li> <li>Version Control</li> <li>Git</li> <li>Data Ethics</li> </ol>"},{"location":"chapters/13-neural-networks-pytorch/#projects-5-concepts","title":"Projects (5 concepts)","text":"<ol> <li>Capstone Project</li> <li>End-to-End Pipeline</li> <li>Model Deployment</li> <li>Results Communication</li> <li>Data-Driven Decisions</li> </ol>"},{"location":"chapters/13-neural-networks-pytorch/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 10: NumPy and Numerical Computing</li> <li>Chapter 11: Non-linear Models and Regularization</li> <li>Chapter 12: Introduction to Machine Learning</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#introduction-welcome-to-the-deep-end","title":"Introduction: Welcome to the Deep End","text":"<p>You've arrived at the most exciting chapter in this entire book. Everything you've learned\u2014data structures, visualization, statistics, regression, model evaluation, NumPy, optimization\u2014has been preparing you for this moment. Neural networks are the technology behind self-driving cars, language translation, image recognition, and AI assistants. And now you're going to build them yourself.</p> <p>Neural networks aren't magic, even though they sometimes feel like it. They're just the concepts you already know\u2014gradient descent, loss functions, matrix multiplication\u2014stacked together in clever ways. If you understood the last chapter, you have everything you need to understand neural networks.</p> <p>By the end of this chapter, you'll have:</p> <ul> <li>Built neural networks from scratch and with PyTorch</li> <li>Trained models using professional techniques</li> <li>Learned best practices for real-world deployment</li> <li>Prepared for your capstone project</li> </ul> <p>Let's unlock your ultimate data science superpower.</p>"},{"location":"chapters/13-neural-networks-pytorch/#part-1-neural-network-fundamentals","title":"Part 1: Neural Network Fundamentals","text":""},{"location":"chapters/13-neural-networks-pytorch/#what-are-neural-networks","title":"What Are Neural Networks?","text":"<p>Neural networks are computing systems loosely inspired by biological brains. They consist of interconnected nodes (neurons) organized in layers that learn to transform inputs into outputs through training.</p> <p>The key insight: neural networks are universal function approximators. Given enough neurons and data, they can learn virtually any pattern\u2014recognizing faces, translating languages, playing chess, or predicting stock prices.</p> <pre><code># A neural network is just layers of transformations\n# Input \u2192 Transform \u2192 Transform \u2192 ... \u2192 Output\n\n# Conceptually:\ndef neural_network(x):\n    h1 = activation(x @ W1 + b1)      # First hidden layer\n    h2 = activation(h1 @ W2 + b2)     # Second hidden layer\n    output = h2 @ W3 + b3              # Output layer\n    return output\n</code></pre> <p>What makes neural networks special:</p> Feature Traditional ML Neural Networks Feature engineering Manual, requires expertise Automatic, learned from data Complexity Limited by model type Unlimited (add more layers) Data requirements Works with less data Needs lots of data Interpretability Often clear Often opaque (black box)"},{"location":"chapters/13-neural-networks-pytorch/#the-artificial-neuron-the-basic-unit","title":"The Artificial Neuron: The Basic Unit","text":"<p>An artificial neuron (or node) is the fundamental building block of neural networks. It takes multiple inputs, multiplies each by a weight, adds them up with a bias, and passes the result through an activation function.</p> \\[y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = f(\\mathbf{w} \\cdot \\mathbf{x} + b)\\] <p>Where:</p> <ul> <li>\\(x_i\\) are the inputs</li> <li>\\(w_i\\) are the weights (learned parameters)</li> <li>\\(b\\) is the bias (learned parameter)</li> <li>\\(f\\) is the activation function</li> <li>\\(y\\) is the output</li> </ul> <pre><code>import numpy as np\n\ndef artificial_neuron(inputs, weights, bias, activation_fn):\n    \"\"\"\n    A single artificial neuron\n    \"\"\"\n    # Weighted sum\n    z = np.dot(inputs, weights) + bias\n\n    # Apply activation function\n    output = activation_fn(z)\n\n    return output\n\n# Example\ninputs = np.array([0.5, 0.3, 0.2])\nweights = np.array([0.4, 0.6, 0.8])\nbias = 0.1\n\noutput = artificial_neuron(inputs, weights, bias, lambda x: max(0, x))\nprint(f\"Neuron output: {output:.4f}\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#the-perceptron-the-simplest-neural-network","title":"The Perceptron: The Simplest Neural Network","text":"<p>The perceptron is the simplest neural network\u2014just a single neuron with a step function as its activation. It was invented in 1958 and could learn to classify linearly separable data.</p> <pre><code>def perceptron(x, weights, bias):\n    \"\"\"\n    Original perceptron: weighted sum + step function\n    \"\"\"\n    z = np.dot(x, weights) + bias\n    return 1 if z &gt; 0 else 0\n\n# A perceptron can learn simple patterns\n# But it can't learn XOR - this limitation led to the \"AI winter\"\n</code></pre> <p>The perceptron's limitations sparked the development of multi-layer networks with non-linear activation functions\u2014the neural networks we use today.</p>"},{"location":"chapters/13-neural-networks-pytorch/#activation-functions-adding-non-linearity","title":"Activation Functions: Adding Non-Linearity","text":"<p>Activation functions introduce non-linearity into neural networks. Without them, stacking layers would be pointless\u2014a sequence of linear transformations is just one linear transformation. Activation functions allow networks to learn complex, non-linear patterns.</p>"},{"location":"chapters/13-neural-networks-pytorch/#sigmoid-function","title":"Sigmoid Function","text":"<p>The sigmoid function squashes any input to a value between 0 and 1:</p> \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] <pre><code>def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Output range: (0, 1)\n# Good for: output layer in binary classification (probability)\n# Problem: vanishing gradients for very large/small inputs\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#relu-function","title":"ReLU Function","text":"<p>The ReLU (Rectified Linear Unit) function is the most popular activation in modern networks:</p> \\[\\text{ReLU}(x) = \\max(0, x)\\] <pre><code>def relu(x):\n    return np.maximum(0, x)\n\n# Output range: [0, \u221e)\n# Good for: hidden layers, fast to compute\n# Problem: \"dying ReLU\" - neurons can get stuck at 0\n</code></pre> Activation Formula Range Use Case Sigmoid \\(1/(1+e^{-x})\\) (0, 1) Binary classification output Tanh \\((e^x-e^{-x})/(e^x+e^{-x})\\) (-1, 1) Hidden layers (centered) ReLU \\(\\max(0, x)\\) [0, \u221e) Hidden layers (default choice) Softmax \\(e^{x_i}/\\sum e^{x_j}\\) (0, 1), sums to 1 Multi-class output"},{"location":"chapters/13-neural-networks-pytorch/#diagram-activation-function-explorer","title":"Diagram: Activation Function Explorer","text":"Activation Function Explorer <p>Type: microsim</p> <p>Bloom Taxonomy: Understand, Apply</p> <p>Learning Objective: Visualize different activation functions and understand why non-linearity is essential for neural networks</p> <p>Canvas Layout (850x550): - Main area (850x400): Graph showing activation function curves - Bottom area (850x150): Controls and information panel</p> <p>Main Visualization: - X-axis range: -5 to 5 - Y-axis range: -2 to 2 (adjustable) - Multiple activation functions plotted (selectable) - Derivative shown as dashed line (optional) - Current function highlighted prominently</p> <p>Activation Functions to Include: 1. Linear (y = x) - shows why this is useless 2. Step function - original perceptron 3. Sigmoid - smooth S-curve 4. Tanh - centered sigmoid 5. ReLU - simple but powerful 6. Leaky ReLU - fixes dying ReLU 7. Softmax - for probabilities (1D simplified)</p> <p>Interactive Controls: - Checkboxes: Select which functions to display - Toggle: \"Show derivatives\" - Input field: Enter x value, see f(x) for each function - Slider: Adjust x to see moving point on each curve</p> <p>Educational Annotations: - Point out vanishing gradient regions (sigmoid extremes) - Show where ReLU has zero gradient - Demonstrate why non-linearity enables complex patterns</p> <p>Demo: \"Why Non-linearity Matters\" - Button to show: linear combinations of linear = still linear - Animation showing stacked linear layers collapsing to one</p> <p>Implementation: p5.js with multiple function plots</p>"},{"location":"chapters/13-neural-networks-pytorch/#network-architecture-layers-and-depth","title":"Network Architecture: Layers and Depth","text":"<p>Network architecture describes how neurons are organized into layers and connected. The architecture determines what patterns the network can learn.</p>"},{"location":"chapters/13-neural-networks-pytorch/#input-layer","title":"Input Layer","text":"<p>The input layer receives the raw data. It has one neuron per feature\u2014no computation happens here, just data entry.</p>"},{"location":"chapters/13-neural-networks-pytorch/#hidden-layers","title":"Hidden Layers","text":"<p>Hidden layers perform the actual computation. They're \"hidden\" because we don't directly observe their outputs. More hidden layers = deeper network = more complex patterns.</p>"},{"location":"chapters/13-neural-networks-pytorch/#output-layer","title":"Output Layer","text":"<p>The output layer produces the final prediction. Its structure depends on the task:</p> <ul> <li>Regression: 1 neuron, no activation (or linear)</li> <li>Binary classification: 1 neuron, sigmoid activation</li> <li>Multi-class: N neurons, softmax activation</li> </ul> <pre><code># A simple architecture\n\"\"\"\nInput (4 features) \u2192 Hidden (8 neurons, ReLU) \u2192 Hidden (4 neurons, ReLU) \u2192 Output (1 neuron)\n\nLayer sizes: [4, 8, 4, 1]\nTotal parameters: (4\u00d78 + 8) + (8\u00d74 + 4) + (4\u00d71 + 1) = 40 + 36 + 5 = 81\n\"\"\"\n</code></pre> <p>Deep learning refers to neural networks with many hidden layers. Depth allows networks to learn hierarchical features\u2014simple patterns in early layers, complex patterns in later layers.</p>"},{"location":"chapters/13-neural-networks-pytorch/#diagram-neural-network-architecture-builder","title":"Diagram: Neural Network Architecture Builder","text":"Neural Network Architecture Builder <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Create</p> <p>Learning Objective: Build and visualize neural network architectures, understanding how layer sizes and depth affect the network</p> <p>Canvas Layout (900x600): - Main area (650x600): Network visualization - Right panel (250x600): Architecture controls</p> <p>Network Visualization: - Circles represent neurons arranged in vertical layers - Lines connect neurons between adjacent layers - Line thickness proportional to weight magnitude (after training) - Neurons colored by activation value during forward pass - Labels showing layer names and sizes</p> <p>Layer Representation: - Input layer on left (green circles) - Hidden layers in middle (blue circles) - Output layer on right (orange circles) - If too many neurons, show sample with \"...\" indicator</p> <p>Interactive Controls: - Slider: Number of hidden layers (1-5) - Slider for each hidden layer: Number of neurons (1-128) - Dropdown: Input size (preset options or custom) - Dropdown: Output size (1 for regression, N for classification) - Dropdown: Activation function per layer</p> <p>Parameter Counter: - Total weights: calculated live - Total biases: calculated live - Total parameters: sum</p> <p>Forward Pass Animation: - Button: \"Run Forward Pass\" - Watch activations flow through network - Color intensity shows activation magnitude - Step-by-step or continuous animation</p> <p>Preset Architectures: - \"Simple\" [4, 8, 1] - \"Deep\" [4, 32, 16, 8, 1] - \"Wide\" [4, 128, 1] - \"Classification\" [4, 16, 8, 3]</p> <p>Implementation: p5.js with animated data flow</p>"},{"location":"chapters/13-neural-networks-pytorch/#weights-and-biases-the-learnable-parameters","title":"Weights and Biases: The Learnable Parameters","text":"<p>Weights and biases are the parameters that the network learns during training.</p> <ul> <li>Weights determine how strongly each input affects the output. Large positive weights amplify the input; negative weights invert it.</li> <li>Biases allow neurons to activate even when inputs are zero. They shift the activation function left or right.</li> </ul> <pre><code># For a layer with 10 inputs and 5 outputs:\n# Weights: 10 \u00d7 5 = 50 parameters\n# Biases: 5 parameters\n# Total: 55 parameters\n\nimport numpy as np\n\n# Initialize weights (many strategies exist)\nn_in, n_out = 10, 5\n\n# Xavier/Glorot initialization (good for sigmoid/tanh)\nweights_xavier = np.random.randn(n_in, n_out) * np.sqrt(1 / n_in)\n\n# He initialization (good for ReLU)\nweights_he = np.random.randn(n_in, n_out) * np.sqrt(2 / n_in)\n\n# Biases usually start at zero\nbiases = np.zeros(n_out)\n</code></pre> <p>The number of parameters in a network:</p> \\[\\text{Parameters} = \\sum_{\\ell=1}^{L} (n_{\\ell-1} \\times n_\\ell + n_\\ell)\\] <p>Where \\(n_\\ell\\) is the number of neurons in layer \\(\\ell\\).</p>"},{"location":"chapters/13-neural-networks-pytorch/#forward-propagation-making-predictions","title":"Forward Propagation: Making Predictions","text":"<p>Forward propagation is the process of passing inputs through the network to get outputs. Data flows forward from input to output, layer by layer.</p> <pre><code>def forward_propagation(X, weights, biases, activations):\n    \"\"\"\n    Forward pass through a neural network\n    \"\"\"\n    a = X  # Input is the first \"activation\"\n\n    for W, b, activation in zip(weights, biases, activations):\n        z = a @ W + b           # Linear transformation\n        a = activation(z)        # Non-linear activation\n\n    return a  # Final output\n\n# Example: 3-layer network\nweights = [W1, W2, W3]\nbiases = [b1, b2, b3]\nactivations = [relu, relu, sigmoid]\n\npredictions = forward_propagation(X, weights, biases, activations)\n</code></pre> <p>Forward propagation is just matrix multiplications and function applications\u2014exactly what NumPy and PyTorch are optimized for.</p>"},{"location":"chapters/13-neural-networks-pytorch/#backpropagation-learning-from-errors","title":"Backpropagation: Learning from Errors","text":"<p>Backpropagation is the algorithm that computes gradients for training neural networks. It works backward from the output, propagating error signals to update all weights and biases.</p> <p>The key insight: use the chain rule from calculus. If error depends on output, and output depends on weights, we can compute how error depends on weights.</p> \\[\\frac{\\partial \\text{Loss}}{\\partial w} = \\frac{\\partial \\text{Loss}}{\\partial \\text{output}} \\times \\frac{\\partial \\text{output}}{\\partial w}\\] <pre><code># Backprop computes gradients layer by layer, from output to input\n# We won't implement it manually - PyTorch does it automatically!\n\n# But conceptually:\n# 1. Compute loss at output\n# 2. Compute gradient of loss w.r.t. output layer weights\n# 3. Propagate gradient backward to previous layer\n# 4. Repeat until input layer\n# 5. Update all weights using gradients\n</code></pre> <p>The good news: you rarely implement backpropagation manually. Modern frameworks like PyTorch compute gradients automatically.</p>"},{"location":"chapters/13-neural-networks-pytorch/#training-concepts-epochs-batches-and-stochastic-gradient-descent","title":"Training Concepts: Epochs, Batches, and Stochastic Gradient Descent","text":"<p>When training neural networks, we don't process the entire dataset at once. Instead, we use batches and multiple passes.</p> <p>Epoch: One complete pass through the entire training dataset.</p> <p>Batch size: Number of samples processed before updating weights.</p> <p>Mini-batch: A subset of the training data used for one gradient update.</p> <p>Stochastic Gradient Descent (SGD): Using random mini-batches instead of the full dataset for each update.</p> Approach Batch Size Pros Cons Batch GD Entire dataset Stable, accurate gradients Slow, memory intensive Stochastic GD 1 sample Fast updates, escapes local minima Noisy, unstable Mini-batch GD 32-256 samples Best of both worlds Sweet spot <pre><code># Typical training configuration\nepochs = 100          # 100 passes through the data\nbatch_size = 32       # Process 32 samples at a time\nn_samples = 10000     # Total training samples\n\nbatches_per_epoch = n_samples // batch_size  # 312 batches\ntotal_updates = epochs * batches_per_epoch    # 31,200 weight updates\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#the-vanishing-gradient-problem","title":"The Vanishing Gradient Problem","text":"<p>The vanishing gradient problem occurs when gradients become extremely small in deep networks, causing early layers to learn very slowly (or not at all).</p> <p>Why it happens: Sigmoid and tanh saturate for large inputs, producing gradients near zero. When you multiply many small gradients together through backpropagation, the result vanishes.</p> <p>Solutions:</p> <ul> <li>Use ReLU activation (gradients are 1 for positive inputs)</li> <li>Use batch normalization</li> <li>Use skip connections (ResNets)</li> <li>Careful weight initialization</li> </ul> <pre><code># Sigmoid gradient is at most 0.25\n# After 10 layers: 0.25^10 = 0.000001\n# Gradients essentially disappear!\n\n# ReLU gradient is 1 for positive inputs\n# Gradients flow through unchanged\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#part-2-pytorch-fundamentals","title":"Part 2: PyTorch Fundamentals","text":""},{"location":"chapters/13-neural-networks-pytorch/#the-pytorch-library","title":"The PyTorch Library","text":"<p>PyTorch is a deep learning framework created by Facebook's AI Research lab. It's the most popular framework for research and increasingly popular in industry.</p> <p>Why PyTorch:</p> <ul> <li>Pythonic: Feels like natural Python code</li> <li>Dynamic graphs: Build networks on-the-fly</li> <li>Easy debugging: Use standard Python debugger</li> <li>GPU acceleration: Automatic CUDA support</li> <li>Rich ecosystem: torchvision, torchaudio, transformers</li> </ul> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#tensors-pytorchs-data-structure","title":"Tensors: PyTorch's Data Structure","text":"<p>Tensors are PyTorch's core data structure\u2014like NumPy arrays but with GPU support and automatic differentiation.</p> <pre><code>import torch\n\n# Create tensors\na = torch.tensor([1, 2, 3, 4])                    # From list\nb = torch.zeros(3, 4)                              # 3x4 zeros\nc = torch.ones(2, 3)                               # 2x3 ones\nd = torch.randn(5, 5)                              # Random normal\ne = torch.arange(0, 10, 2)                         # Range\n\n# From NumPy\nimport numpy as np\nnumpy_array = np.array([1, 2, 3])\ntensor_from_numpy = torch.from_numpy(numpy_array)\n\n# Tensor properties\nprint(f\"Shape: {d.shape}\")\nprint(f\"Data type: {d.dtype}\")\nprint(f\"Device: {d.device}\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#tensor-operations","title":"Tensor Operations","text":"<p>Tensor operations work similarly to NumPy but run on GPU when available.</p> <pre><code># Basic operations\na = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([4.0, 5.0, 6.0])\n\n# Element-wise\nprint(a + b)           # Addition\nprint(a * b)           # Multiplication\nprint(a ** 2)          # Power\n\n# Matrix operations\nA = torch.randn(3, 4)\nB = torch.randn(4, 5)\nC = A @ B              # Matrix multiplication\nprint(f\"Result shape: {C.shape}\")  # (3, 5)\n\n# Aggregations\nprint(A.sum())         # Sum all elements\nprint(A.mean(dim=0))   # Mean along dimension 0\nprint(A.max())         # Maximum value\n\n# Reshaping\nD = torch.arange(12)\nprint(D.reshape(3, 4))\nprint(D.view(2, 6))    # view is like reshape but shares memory\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#autograd-automatic-differentiation","title":"Autograd: Automatic Differentiation","text":"<p>Autograd is PyTorch's automatic differentiation engine. It computes gradients automatically\u2014no manual backpropagation needed!</p> <pre><code># Enable gradient tracking\nx = torch.tensor([2.0, 3.0], requires_grad=True)\n\n# Perform operations\ny = x ** 2\nz = y.sum()  # z = x[0]\u00b2 + x[1]\u00b2 = 4 + 9 = 13\n\n# Compute gradients\nz.backward()\n\n# dz/dx = 2x\nprint(f\"x: {x}\")\nprint(f\"Gradient: {x.grad}\")  # [4.0, 6.0] = 2*[2, 3]\n</code></pre> <p>Automatic differentiation builds a computational graph as you perform operations. When you call <code>.backward()</code>, it traverses this graph in reverse to compute all gradients.</p> <pre><code># Every operation builds the graph\na = torch.tensor(2.0, requires_grad=True)\nb = a * 3        # Graph: a \u2192 multiply \u2192 b\nc = b + 5        # Graph: b \u2192 add \u2192 c\nd = c ** 2       # Graph: c \u2192 power \u2192 d\n\nd.backward()     # Traverse graph backward\nprint(f\"Gradient of d w.r.t. a: {a.grad}\")  # Chain rule: 2*c * 3 = 2*(a*3+5) * 3\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#neural-network-modules","title":"Neural Network Modules","text":"<p>PyTorch provides <code>nn.Module</code> as the base class for all neural networks. You define layers in <code>__init__</code> and the forward pass in <code>forward</code>.</p> <pre><code>import torch.nn as nn\n\nclass SimpleNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.layer2(x)\n        return x\n\n# Create model\nmodel = SimpleNetwork(input_size=10, hidden_size=32, output_size=1)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#sequential-models","title":"Sequential Models","text":"<p>For simple architectures, <code>nn.Sequential</code> provides a convenient shortcut:</p> <pre><code># Same network using Sequential\nmodel = nn.Sequential(\n    nn.Linear(10, 32),\n    nn.ReLU(),\n    nn.Linear(32, 16),\n    nn.ReLU(),\n    nn.Linear(16, 1)\n)\n\n# Make prediction\nx = torch.randn(5, 10)  # 5 samples, 10 features\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # (5, 1)\n</code></pre> <p>The Linear layer (<code>nn.Linear</code>) performs \\(y = xW^T + b\\)\u2014exactly the weighted sum we discussed earlier.</p>"},{"location":"chapters/13-neural-networks-pytorch/#loss-functions-in-pytorch","title":"Loss Functions in PyTorch","text":"<p>PyTorch provides common loss functions ready to use:</p> <pre><code># Regression\nmse_loss = nn.MSELoss()           # Mean Squared Error\nmae_loss = nn.L1Loss()            # Mean Absolute Error\nhuber_loss = nn.HuberLoss()       # Huber Loss\n\n# Classification\nbce_loss = nn.BCELoss()                    # Binary Cross-Entropy (after sigmoid)\nbce_logits = nn.BCEWithLogitsLoss()        # BCE with built-in sigmoid\nce_loss = nn.CrossEntropyLoss()            # Multi-class (includes softmax)\n\n# Example\npredictions = torch.tensor([0.8, 0.2, 0.9])\ntargets = torch.tensor([1.0, 0.0, 1.0])\n\nloss = mse_loss(predictions, targets)\nprint(f\"MSE Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#optimizers-sgd-and-adam","title":"Optimizers: SGD and Adam","text":"<p>Optimizers update model weights based on gradients. PyTorch provides many optimizers:</p> <pre><code># SGD Optimizer - simple, reliable\noptimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Adam Optimizer - adaptive learning rates, usually works well\noptimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer_adam, step_size=10, gamma=0.1)\n</code></pre> <p>SGD (Stochastic Gradient Descent) is the classic optimizer. Add momentum for smoother updates.</p> <p>Adam adapts the learning rate for each parameter. It's often the default choice for neural networks.</p>"},{"location":"chapters/13-neural-networks-pytorch/#the-training-loop","title":"The Training Loop","text":"<p>The training loop is where learning happens. It's the heart of neural network training:</p> <pre><code>def train_model(model, train_loader, criterion, optimizer, epochs):\n    model.train()  # Set to training mode\n\n    for epoch in range(epochs):\n        total_loss = 0\n\n        for batch_x, batch_y in train_loader:\n            # 1. Zero gradients from previous step\n            optimizer.zero_grad()\n\n            # 2. Forward pass\n            predictions = model(batch_x)\n\n            # 3. Compute loss\n            loss = criterion(predictions, batch_y)\n\n            # 4. Backward pass (compute gradients)\n            loss.backward()\n\n            # 5. Update weights\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n    return model\n</code></pre> <p>The five essential steps:</p> <ol> <li>Zero gradients: Clear gradients from the previous iteration</li> <li>Forward pass: Compute predictions</li> <li>Compute loss: Measure how wrong we are</li> <li>Backward pass: Compute gradients via backpropagation</li> <li>Update weights: Apply gradients using optimizer</li> </ol>"},{"location":"chapters/13-neural-networks-pytorch/#diagram-training-loop-visualizer","title":"Diagram: Training Loop Visualizer","text":"Training Loop Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy: Apply, Analyze</p> <p>Learning Objective: Understand the five steps of the training loop and see how weights update over iterations</p> <p>Canvas Layout (900x600): - Left panel (450x600): Training loop steps with code - Right panel (450x600): Loss curve and weight visualization</p> <p>Left Panel - Step-by-Step: - Five cards showing each training step - Current step highlighted - Code snippet for each step - Arrow showing data/gradient flow</p> <p>Steps Display: 1. \"Zero Gradients\" - optimizer.zero_grad() 2. \"Forward Pass\" - predictions = model(x) 3. \"Compute Loss\" - loss = criterion(pred, y) 4. \"Backward Pass\" - loss.backward() 5. \"Update Weights\" - optimizer.step()</p> <p>Right Panel - Visualizations: - Top: Live loss curve (updates each iteration) - Bottom: Weight histogram or specific weight values</p> <p>Animation: - Watch data flow forward through network - See loss computed at output - Watch gradient flow backward - See weights shift after update - Loss decreases over iterations</p> <p>Interactive Controls: - Button: \"Step\" - advance one step - Button: \"Complete Iteration\" - run all 5 steps - Button: \"Run Epoch\" - run full epoch - Slider: Learning rate - Slider: Animation speed</p> <p>Metrics Display: - Current iteration number - Current batch loss - Running average loss - Number of weight updates</p> <p>Implementation: p5.js with synchronized animation</p>"},{"location":"chapters/13-neural-networks-pytorch/#model-evaluation-in-pytorch","title":"Model Evaluation in PyTorch","text":"<p>Evaluating models requires disabling gradient computation and switching to eval mode:</p> <pre><code>def evaluate_model(model, test_loader, criterion):\n    model.eval()  # Set to evaluation mode\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():  # Disable gradient computation\n        for batch_x, batch_y in test_loader:\n            predictions = model(batch_x)\n            loss = criterion(predictions, batch_y)\n            total_loss += loss.item()\n\n            # For classification\n            predicted_class = (predictions &gt; 0.5).float()\n            correct += (predicted_class == batch_y).sum().item()\n            total += batch_y.size(0)\n\n    avg_loss = total_loss / len(test_loader)\n    accuracy = correct / total\n\n    return avg_loss, accuracy\n\n# Usage\ntest_loss, test_acc = evaluate_model(model, test_loader, criterion)\nprint(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.2%}\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#gpu-computing-with-cuda","title":"GPU Computing with CUDA","text":"<p>GPU computing accelerates training dramatically. PyTorch makes it easy with CUDA support:</p> <pre><code># Check GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Move model to GPU\nmodel = model.to(device)\n\n# Move data to GPU\nx = torch.randn(64, 10).to(device)\ny = torch.randn(64, 1).to(device)\n\n# Training works the same way\noutput = model(x)  # Computation happens on GPU\n</code></pre> <p>GPU speedups depend on:</p> <ul> <li>Network size (larger = more benefit)</li> <li>Batch size (larger = more benefit)</li> <li>Operation type (matrix operations benefit most)</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#saving-and-loading-models","title":"Saving and Loading Models","text":"<p>Model saving preserves your trained models for later use:</p> <pre><code># Save model weights only (recommended)\ntorch.save(model.state_dict(), 'model_weights.pth')\n\n# Save entire model (includes architecture)\ntorch.save(model, 'full_model.pth')\n\n# Save checkpoint (weights, optimizer, epoch, loss)\ncheckpoint = {\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n}\ntorch.save(checkpoint, 'checkpoint.pth')\n</code></pre> <p>Model loading restores saved models:</p> <pre><code># Load weights into existing model\nmodel = SimpleNetwork(10, 32, 1)\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()\n\n# Load from checkpoint\ncheckpoint = torch.load('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#transfer-learning","title":"Transfer Learning","text":"<p>Transfer learning uses a model trained on one task as the starting point for another task. This leverages knowledge learned from large datasets.</p> <pre><code>import torchvision.models as models\n\n# Load pre-trained model\nresnet = models.resnet18(pretrained=True)\n\n# Freeze early layers (don't update their weights)\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Replace final layer for your task\nnum_classes = 10\nresnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n\n# Only train the new layer\noptimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)\n</code></pre> <p>Transfer learning is powerful because:</p> <ul> <li>Pre-trained models learned general features from millions of images</li> <li>You only need a small dataset for your specific task</li> <li>Training is much faster</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#part-3-best-practices","title":"Part 3: Best Practices","text":""},{"location":"chapters/13-neural-networks-pytorch/#explainable-ai-and-model-interpretability","title":"Explainable AI and Model Interpretability","text":"<p>Explainable AI (XAI) and model interpretability help us understand why models make their predictions. This is crucial for trust, debugging, and ethics.</p> <p>Methods for interpretability:</p> <pre><code># Feature importance (for tree-based models)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nimportances = model.feature_importances_\n\n# For neural networks, use specialized libraries\n# SHAP (SHapley Additive exPlanations)\nimport shap\nexplainer = shap.Explainer(model)\nshap_values = explainer(X_test)\nshap.summary_plot(shap_values, X_test)\n</code></pre> <p>SHAP values attribute each feature's contribution to a prediction, based on game theory. They show:</p> <ul> <li>Which features pushed the prediction higher/lower</li> <li>Feature importance across the dataset</li> <li>Interaction effects between features</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#model-documentation","title":"Model Documentation","text":"<p>Model documentation records everything needed to understand, reproduce, and maintain your model:</p> <p>Essential documentation:</p> <ul> <li>Model card: Purpose, training data, performance, limitations</li> <li>Data documentation: Sources, preprocessing, quality issues</li> <li>Code documentation: Comments, docstrings, README</li> <li>Experiment logs: Hyperparameters, metrics, decisions</li> </ul> <pre><code># Model Card: House Price Predictor\n\n## Model Details\n- Architecture: 3-layer neural network [10, 64, 32, 1]\n- Training data: 10,000 house sales from 2020-2023\n- Validation R\u00b2: 0.87\n\n## Intended Use\n- Estimate house prices for real estate listings\n- NOT for mortgage underwriting decisions\n\n## Limitations\n- Trained only on suburban properties\n- May not generalize to urban/rural markets\n- Does not account for market volatility\n\n## Ethical Considerations\n- Remove protected attributes (race, religion)\n- Monitor for disparate impact\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#reproducibility","title":"Reproducibility","text":"<p>Reproducibility ensures others (including future you) can recreate your results exactly.</p> <p>Key practices:</p> <pre><code># 1. Set random seeds everywhere\nimport random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# 2. Record all hyperparameters\nconfig = {\n    'learning_rate': 0.001,\n    'batch_size': 32,\n    'epochs': 100,\n    'hidden_sizes': [64, 32],\n    'optimizer': 'Adam',\n    'seed': 42\n}\n\n# 3. Use version control for code (Git)\n# 4. Version your data\n# 5. Log all experiments\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#version-control-with-git","title":"Version Control with Git","text":"<p>Version control tracks changes to your code over time. Git is the industry standard:</p> <pre><code># Initialize a repository\ngit init\n\n# Add files to staging\ngit add model.py data_processing.py\n\n# Commit changes\ngit commit -m \"Add initial neural network model\"\n\n# Create a branch for experiments\ngit checkout -b experiment/larger-network\n\n# Push to remote (GitHub, GitLab)\ngit push origin main\n</code></pre> <p>Git benefits:</p> <ul> <li>Track all changes with history</li> <li>Collaborate with teammates</li> <li>Revert to previous versions</li> <li>Branch for experiments without breaking main code</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#data-ethics","title":"Data Ethics","text":"<p>Data ethics ensures your work respects privacy, fairness, and societal impact:</p> <p>Key principles:</p> Principle Description Example Privacy Protect personal information Anonymize before training Fairness Avoid bias against groups Test for disparate impact Transparency Explain how decisions are made Provide model cards Consent Use data as authorized Respect terms of service Accountability Take responsibility for outcomes Monitor deployed models <pre><code># Check for protected attribute correlation\nprotected_attrs = ['race', 'gender', 'age']\nfor attr in protected_attrs:\n    if attr in df.columns:\n        print(f\"Warning: {attr} in dataset - ensure it's not used improperly\")\n\n# Test for fairness\nfrom fairlearn.metrics import MetricFrame\nmetric_frame = MetricFrame(\n    metrics={'accuracy': accuracy_score},\n    y_true=y_test,\n    y_pred=predictions,\n    sensitive_features=sensitive_test\n)\nprint(metric_frame.by_group)\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#part-4-capstone-projects","title":"Part 4: Capstone Projects","text":""},{"location":"chapters/13-neural-networks-pytorch/#the-end-to-end-pipeline","title":"The End-to-End Pipeline","text":"<p>A capstone project demonstrates everything you've learned by building a complete end-to-end pipeline:</p> <pre><code># Complete data science pipeline\nclass DataSciencePipeline:\n    def __init__(self):\n        self.scaler = None\n        self.model = None\n\n    def load_data(self, path):\n        \"\"\"1. Data Collection\"\"\"\n        self.df = pd.read_csv(path)\n        return self\n\n    def clean_data(self):\n        \"\"\"2. Data Cleaning\"\"\"\n        self.df = self.df.dropna()\n        self.df = self.df.drop_duplicates()\n        return self\n\n    def engineer_features(self):\n        \"\"\"3. Feature Engineering\"\"\"\n        self.df['feature_ratio'] = self.df['a'] / self.df['b']\n        return self\n\n    def prepare_data(self):\n        \"\"\"4. Train/Test Split\"\"\"\n        X = self.df.drop('target', axis=1)\n        y = self.df['target']\n        self.X_train, self.X_test, self.y_train, self.y_test = \\\n            train_test_split(X, y, test_size=0.2)\n        return self\n\n    def train_model(self):\n        \"\"\"5. Model Training\"\"\"\n        self.model = NeuralNetwork()\n        self.model.fit(self.X_train, self.y_train)\n        return self\n\n    def evaluate(self):\n        \"\"\"6. Model Evaluation\"\"\"\n        predictions = self.model.predict(self.X_test)\n        print(f\"R\u00b2 Score: {r2_score(self.y_test, predictions):.4f}\")\n        return self\n\n    def save(self, path):\n        \"\"\"7. Model Deployment\"\"\"\n        torch.save(self.model.state_dict(), path)\n        return self\n\n# Run the complete pipeline\npipeline = DataSciencePipeline()\npipeline.load_data('data.csv') \\\n        .clean_data() \\\n        .engineer_features() \\\n        .prepare_data() \\\n        .train_model() \\\n        .evaluate() \\\n        .save('model.pth')\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#model-deployment","title":"Model Deployment","text":"<p>Model deployment makes your trained model available for real-world use:</p> <p>Deployment options:</p> Option Use Case Complexity Flask/FastAPI Simple web API Low Docker Containerized deployment Medium Cloud (AWS, GCP, Azure) Production scale Medium-High Edge devices Mobile, IoT High <pre><code># Simple Flask API for model serving\nfrom flask import Flask, request, jsonify\nimport torch\n\napp = Flask(__name__)\n\n# Load model once at startup\nmodel = torch.load('model.pth')\nmodel.eval()\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json\n    features = torch.tensor(data['features']).float()\n\n    with torch.no_grad():\n        prediction = model(features)\n\n    return jsonify({'prediction': prediction.tolist()})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#results-communication","title":"Results Communication","text":"<p>Results communication translates technical findings into insights that stakeholders can act on:</p> <pre><code># Create executive summary visualization\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(rows=2, cols=2,\n                    subplot_titles=['Model Performance', 'Feature Importance',\n                                   'Predictions vs Actual', 'Error Distribution'])\n\n# Add visualizations that tell the story\n# ... (detailed plotting code)\n\nfig.update_layout(title='House Price Prediction Model - Executive Summary')\nfig.write_html('results_dashboard.html')\n</code></pre> <p>Key communication principles:</p> <ul> <li>Lead with the business impact, not technical details</li> <li>Use visualizations over tables of numbers</li> <li>Quantify uncertainty (confidence intervals, error ranges)</li> <li>Provide actionable recommendations</li> <li>Be honest about limitations</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#data-driven-decisions","title":"Data-Driven Decisions","text":"<p>The ultimate goal of data science is data-driven decisions\u2014using evidence to guide action:</p> <pre><code># From prediction to decision\ndef recommend_action(prediction, threshold=0.7):\n    \"\"\"\n    Convert model output to business recommendation\n    \"\"\"\n    if prediction['churn_probability'] &gt; threshold:\n        return {\n            'action': 'HIGH PRIORITY: Retention intervention needed',\n            'confidence': prediction['churn_probability'],\n            'suggested_offers': get_retention_offers(prediction)\n        }\n    else:\n        return {\n            'action': 'Standard engagement',\n            'confidence': 1 - prediction['churn_probability'],\n            'suggested_offers': []\n        }\n\n# Example output\nrecommendation = recommend_action({'churn_probability': 0.85})\nprint(recommendation)\n# {'action': 'HIGH PRIORITY: Retention intervention needed',\n#  'confidence': 0.85,\n#  'suggested_offers': ['20% discount', 'Free month', 'Upgrade offer']}\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#complete-example-building-a-neural-network-in-pytorch","title":"Complete Example: Building a Neural Network in PyTorch","text":"<p>Here's a complete, working example that ties everything together:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport plotly.express as px\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# 1. Generate synthetic data\nn_samples = 1000\nX = np.random.randn(n_samples, 10)\ny = (3*X[:, 0] - 2*X[:, 1] + X[:, 2]**2 + np.random.randn(n_samples)*0.5).reshape(-1, 1)\n\n# 2. Split and scale\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\n\nX_train = scaler_X.fit_transform(X_train)\nX_test = scaler_X.transform(X_test)\ny_train = scaler_y.fit_transform(y_train)\ny_test = scaler_y.transform(y_test)\n\n# 3. Convert to PyTorch tensors\nX_train_t = torch.FloatTensor(X_train)\ny_train_t = torch.FloatTensor(y_train)\nX_test_t = torch.FloatTensor(X_test)\ny_test_t = torch.FloatTensor(y_test)\n\n# 4. Create DataLoader\ntrain_dataset = TensorDataset(X_train_t, y_train_t)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# 5. Define the neural network\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size, hidden_sizes, output_size):\n        super().__init__()\n        layers = []\n\n        # Input layer\n        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n        layers.append(nn.ReLU())\n\n        # Hidden layers\n        for i in range(len(hidden_sizes) - 1):\n            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n            layers.append(nn.ReLU())\n\n        # Output layer\n        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)\n\n# 6. Create model, loss, optimizer\nmodel = NeuralNetwork(input_size=10, hidden_sizes=[64, 32], output_size=1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 7. Training loop\nepochs = 100\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0\n\n    for batch_X, batch_y in train_loader:\n        # Forward pass\n        predictions = model(batch_X)\n        loss = criterion(predictions, batch_y)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_loss = epoch_loss / len(train_loader)\n    train_losses.append(avg_loss)\n\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n# 8. Evaluate\nmodel.eval()\nwith torch.no_grad():\n    test_pred = model(X_test_t)\n    test_loss = criterion(test_pred, y_test_t)\n    print(f\"\\nTest Loss: {test_loss.item():.4f}\")\n\n# 9. Visualize training\nfig = px.line(y=train_losses, title='Training Loss Over Time',\n              labels={'x': 'Epoch', 'y': 'Loss'})\nfig.show()\n\n# 10. Save model\ntorch.save(model.state_dict(), 'trained_model.pth')\nprint(\"Model saved!\")\n</code></pre>"},{"location":"chapters/13-neural-networks-pytorch/#summary-your-complete-data-science-toolkit","title":"Summary: Your Complete Data Science Toolkit","text":"<p>Congratulations! You've now learned the complete data science toolkit:</p> <p>Neural Network Fundamentals: - Artificial neurons, activation functions (ReLU, Sigmoid) - Network architecture (input, hidden, output layers) - Weights, biases, forward propagation, backpropagation - Training with epochs, batches, and gradient descent</p> <p>PyTorch Skills: - Tensors and tensor operations - Autograd for automatic differentiation - Building models with nn.Module and Sequential - Training loops with optimizers (SGD, Adam) - GPU acceleration with CUDA - Saving and loading models</p> <p>Professional Best Practices: - Model interpretability and SHAP values - Documentation and reproducibility - Version control with Git - Data ethics and fairness</p> <p>Project Skills: - End-to-end pipelines - Model deployment - Results communication - Data-driven decision making</p> <p>You now have every tool you need to tackle real-world data science problems.</p>"},{"location":"chapters/13-neural-networks-pytorch/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Neural networks are universal function approximators built from simple neurons</li> <li>Activation functions add non-linearity; ReLU is the default choice for hidden layers</li> <li>Backpropagation computes gradients; PyTorch handles this automatically</li> <li>The training loop: zero gradients \u2192 forward \u2192 loss \u2192 backward \u2192 update</li> <li>PyTorch tensors are like NumPy arrays with GPU support and autograd</li> <li>Always use train/eval modes and disable gradients during evaluation</li> <li>Document everything: code, data, decisions, limitations</li> <li>Set random seeds for reproducibility</li> <li>Consider ethics: privacy, fairness, transparency</li> <li>Deploy models to create real-world impact</li> </ul>"},{"location":"chapters/13-neural-networks-pytorch/#your-capstone-project-awaits","title":"Your Capstone Project Awaits","text":"<p>You've completed an incredible journey from Python basics to building neural networks. You've learned to:</p> <ul> <li>Clean and explore data</li> <li>Create stunning visualizations</li> <li>Apply statistical analysis</li> <li>Build regression and classification models</li> <li>Evaluate and validate models</li> <li>Construct and train neural networks</li> <li>Follow professional best practices</li> </ul> <p>Now it's time to apply everything you've learned.</p> <p>What Will Your Capstone Project Be?</p> <p>Think about a problem you care about solving. Consider:</p> <ul> <li>Personal interests: Sports analytics? Music recommendations? Climate data?</li> <li>Social impact: Healthcare predictions? Educational outcomes? Environmental monitoring?</li> <li>Career goals: Financial analysis? Customer behavior? Manufacturing optimization?</li> <li>Local community: Traffic patterns? Local business trends? Public health?</li> </ul> <p>Your capstone project is your chance to demonstrate your new superpowers. You'll build an end-to-end pipeline, from raw data to deployed model, solving a problem that matters to you.</p> <p>So, what will YOU build?</p>"},{"location":"chapters/14-capstone-project/","title":"Capstone Project","text":""},{"location":"chapters/14-capstone-project/#summary","title":"Summary","text":"<p>This chapter prepares student to create a capstone project for the course. The goal of a capstone project is to create a data science project from start to finish and present your project to your entire class.</p>"},{"location":"chapters/14-capstone-project/#concept-covered","title":"Concept Covered","text":"<ol> <li>Capstone Project</li> <li>Working in Teams</li> <li>Steps in Your Project</li> <li>Planning Your Project</li> <li>Sourcing Data</li> <li>Cleaning Your Data</li> <li>Building a Model</li> <li>Creating Predictions</li> <li>Testing Your Model</li> <li>Visualizing Data</li> <li>Presenting Your Project</li> <li>Giving Feedback</li> <li>Receiving Feedback</li> </ol>"},{"location":"chapters-v1/","title":"Course Chapters - Table of Contents","text":"<p>This course provides a comprehensive introduction to data science using Python, progressing from foundational concepts to advanced machine learning techniques. Each chapter includes interactive MicroSims to reinforce learning through hands-on experience.</p>"},{"location":"chapters-v1/#course-structure-overview","title":"Course Structure Overview","text":"<p>Duration: 10 weeks Target Audience: Advanced high school students and college freshmen Prerequisites: Basic algebra and introductory programming experience</p>"},{"location":"chapters-v1/#chapter-progression","title":"Chapter Progression","text":""},{"location":"chapters-v1/#foundation-phase-weeks-1-3","title":"Foundation Phase (Weeks 1-3)","text":""},{"location":"chapters-v1/#chapter-0-setup","title":"Chapter 0: Setup","text":"<ul> <li>Python environment and Jupyter notebooks setup</li> <li>Conda virtual environment configuration</li> <li>Required package installation</li> <li>Development environment best practices</li> </ul>"},{"location":"chapters-v1/#chapter-1-foundations-of-data-science","title":"Chapter 1: Foundations of Data Science","text":"<ul> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> <li>Understanding the data science workflow</li> <li>Data science roles and career paths</li> <li>Ethics and best practices in data science</li> </ul>"},{"location":"chapters-v1/#chapter-2-data-exploration-and-visualization","title":"Chapter 2: Data Exploration and Visualization","text":"<ul> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> <li>Data profiling and quality assessment</li> <li>Handling missing values and outliers</li> <li>Exploratory data analysis techniques</li> </ul>"},{"location":"chapters-v1/#chapter-3-data-visualization-techniques","title":"Chapter 3: Data Visualization Techniques","text":"<ul> <li>Principles of effective data visualization</li> <li>Matplotlib fundamentals and customization</li> <li>Plotly for interactive visualizations</li> <li>Statistical plots and distributions</li> <li>Time series visualization</li> <li>Multi-dimensional data representation</li> <li>MicroSim: Visualization parameter explorer</li> </ul>"},{"location":"chapters-v1/#statistical-foundation-phase-weeks-4-5","title":"Statistical Foundation Phase (Weeks 4-5)","text":""},{"location":"chapters-v1/#chapter-4-statistical-foundations","title":"Chapter 4: Statistical Foundations","text":"<ul> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> <li>Central limit theorem and sampling distributions</li> <li>Hypothesis testing fundamentals</li> <li>Correlation vs. causation</li> </ul>"},{"location":"chapters-v1/#chapter-5-simple-linear-regression","title":"Chapter 5: Simple Linear Regression","text":"<ul> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> <li>Assumptions of linear regression</li> <li>Residual analysis and diagnostics</li> <li>Making predictions with linear models</li> </ul>"},{"location":"chapters-v1/#model-development-phase-weeks-6-8","title":"Model Development Phase (Weeks 6-8)","text":""},{"location":"chapters-v1/#chapter-6-model-evaluation-and-validation","title":"Chapter 6: Model Evaluation and Validation","text":"<ul> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> <li>Bias-variance trade-off</li> <li>Model selection criteria</li> <li>Performance metrics for different problem types</li> </ul>"},{"location":"chapters-v1/#chapter-7-multiple-linear-regression","title":"Chapter 7: Multiple Linear Regression","text":"<ul> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> <li>Interaction effects and polynomial terms</li> <li>Multicollinearity detection and treatment</li> <li>Model interpretation in multiple dimensions</li> </ul>"},{"location":"chapters-v1/#chapter-8-introduction-to-numpy-and-advanced-computation","title":"Chapter 8: Introduction to NumPy and Advanced Computation","text":"<ul> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> <li>Broadcasting and array manipulation</li> <li>Mathematical functions and statistics</li> <li>Integration with pandas and matplotlib</li> </ul>"},{"location":"chapters-v1/#advanced-modeling-phase-weeks-9-10","title":"Advanced Modeling Phase (Weeks 9-10)","text":""},{"location":"chapters-v1/#chapter-9-non-linear-models-and-feature-engineering","title":"Chapter 9: Non-linear Models and Feature Engineering","text":"<ul> <li>Polynomial regression and feature transformation</li> <li>Understanding non-linear relationships</li> <li>MicroSim: Polynomial degree explorer</li> <li>Feature engineering techniques</li> <li>Basis functions and kernel methods</li> <li>Model complexity and interpretation trade-offs</li> <li>When to use non-linear approaches</li> </ul>"},{"location":"chapters-v1/#chapter-10-regularization-techniques","title":"Chapter 10: Regularization Techniques","text":"<ul> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> <li>Cross-validation for hyperparameter tuning</li> <li>Elastic Net and other regularization methods</li> <li>Feature selection through regularization</li> <li>Preventing overfitting in complex models</li> </ul>"},{"location":"chapters-v1/#machine-learning-phase-advanced-topics","title":"Machine Learning Phase (Advanced Topics)","text":""},{"location":"chapters-v1/#chapter-11-introduction-to-machine-learning","title":"Chapter 11: Introduction to Machine Learning","text":"<ul> <li>Supervised vs. unsupervised learning</li> <li>Classification and regression problems</li> <li>Decision trees and ensemble methods</li> <li>MicroSim: Algorithm comparison explorer</li> <li>Feature importance and selection</li> <li>Model interpretability techniques</li> <li>Introduction to scikit-learn</li> </ul>"},{"location":"chapters-v1/#chapter-12-neural-networks-and-deep-learning","title":"Chapter 12: Neural Networks and Deep Learning","text":"<ul> <li>Neural networks and deep learning concepts</li> <li>Perceptrons and multi-layer networks</li> <li>Activation functions and backpropagation</li> <li>MicroSim: Neural network playground</li> <li>Training neural networks</li> <li>Common architectures and applications</li> <li>When to use neural networks vs. traditional methods</li> </ul>"},{"location":"chapters-v1/#chapter-13-introduction-to-machine-learning-with-pytorch","title":"Chapter 13: Introduction to Machine Learning with PyTorch","text":"<ul> <li>Building simple networks with PyTorch</li> <li>Tensors and automatic differentiation</li> <li>Creating and training models</li> <li>MicroSim: PyTorch model builder</li> <li>Comparing traditional and deep learning approaches</li> <li>GPU acceleration and optimization</li> <li>Model saving and deployment</li> </ul>"},{"location":"chapters-v1/#chapter-14-advanced-model-evaluation","title":"Chapter 14: Advanced Model Evaluation","text":"<ul> <li>Comprehensive performance metrics</li> <li>ROC curves and AUC analysis</li> <li>Confusion matrices and classification reports</li> <li>MicroSim: Metric comparison explorer</li> <li>Statistical significance testing</li> <li>Model comparison techniques</li> <li>Reporting and communicating results</li> </ul>"},{"location":"chapters-v1/#chapter-15-capstone-project-and-model-deployment","title":"Chapter 15: Capstone Project and Model Deployment","text":"<ul> <li>End-to-end data science project planning</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> <li>Model deployment strategies</li> <li>Documentation and reproducibility</li> <li>Presenting data science findings</li> </ul>"},{"location":"chapters-v1/#special-topics","title":"Special Topics","text":""},{"location":"chapters-v1/#matplotlib-vs-plotly-comparison","title":"Matplotlib vs Plotly Comparison","text":"<p>Detailed comparison of visualization libraries for AI-generated plots and animations, including pros and cons for different use cases.</p>"},{"location":"chapters-v1/#learning-methodology","title":"Learning Methodology","text":"<p>Each chapter incorporates: - Interactive MicroSims for hands-on parameter exploration - Real-world datasets and practical applications - Progressive complexity building from simple to advanced concepts - Explainable AI focus emphasizing model interpretability - Code examples with complete implementations</p>"},{"location":"chapters-v1/#course-philosophy","title":"Course Philosophy","text":"<p>This course emphasizes the balance between model explainability and predictive accuracy, guiding students to identify the simplest effective solutions to data-driven problems. The integration of interactive simulations ensures abstract mathematical concepts become concrete and intuitive.</p>"},{"location":"chapters-v1/matplotlib-vs-plotly/","title":"Comparison of Matplotlib and Plotly for GenAI-Created Plots and Animations","text":"<p>The wonderful thing about using Python for data science is the vast selection of libraries that are available to use for free.  Unfortunately, this also comes with a tradeoff.  You have to have a deep understanding of each library, what it's original intended purpose was and what its limitations are.</p> <p>In this section we will look at two of the most popular Python libraries for generating charts and other visualizations: Mathplotlib and [Plotly](../glossary.md#pl</p>"},{"location":"chapters-v1/matplotlib-vs-plotly/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is a versatile and widely-used Python library for creating static, animated, and interactive visualizations.</p>"},{"location":"chapters-v1/matplotlib-vs-plotly/#pros","title":"Pros","text":"<ol> <li>Simplicity and Customization:</li> <li>Great for generating static, publication-quality plots with fine-grained control.</li> <li> <p>Straightforward API for generating standard visualizations like line plots, bar charts, and scatter plots.</p> </li> <li> <p>Broad Ecosystem Support:</p> </li> <li>Integrates well with NumPy, pandas, and other scientific libraries.</li> <li> <p>Supported by many GenAI frameworks for generating data visualizations.</p> </li> <li> <p>Animations:</p> </li> <li>Provides tools for creating animations (e.g., <code>FuncAnimation</code>).</li> <li> <p>Ideal for embedding animations in applications or saving them as GIFs or videos.</p> </li> <li> <p>Customization:</p> </li> <li>Allows deep customization of every aspect of the plot.</li> <li> <p>Perfect for generating AI-tailored visualizations that require specific formats or aesthetics.</p> </li> <li> <p>Lightweight and Efficient:</p> </li> <li>Better for lightweight tasks where interactivity is not required.</li> </ol>"},{"location":"chapters-v1/matplotlib-vs-plotly/#cons","title":"Cons","text":"<ol> <li>Steep Learning Curve:</li> <li>Customizing plots can become verbose and complicated, especially for advanced layouts.</li> <li> <p>Syntax can feel tedious for beginners or for generating multiple visualizations programmatically.</p> </li> <li> <p>Limited Interactivity:</p> </li> <li>Interactivity is minimal compared to modern libraries like Plotly.</li> <li> <p>Interactive plots require third-party tools (e.g., <code>mpld3</code>) and are often less intuitive.</p> </li> <li> <p>Outdated Animations:</p> </li> <li>Animations are not as smooth or interactive as those created by Plotly.</li> </ol>"},{"location":"chapters-v1/matplotlib-vs-plotly/#plotly","title":"Plotly","text":"<p>Plotly is a modern library for creating highly interactive and dynamic visualizations.</p>"},{"location":"chapters-v1/matplotlib-vs-plotly/#pros_1","title":"Pros","text":"<ol> <li>Interactivity:</li> <li>All visualizations are interactive by default (e.g., zooming, panning, tooltips).</li> <li> <p>Excellent for dashboards, presentations, and GenAI-generated insights.</p> </li> <li> <p>Ease of Use:</p> </li> <li>Simple syntax for creating complex plots, suitable for AI-generated visualizations.</li> <li> <p>Works seamlessly with data from pandas, NumPy, and other libraries.</p> </li> <li> <p>Browser-Based Rendering:</p> </li> <li>Plots render in web browsers using JavaScript, making them highly portable and shareable.</li> <li> <p>Supports embedding in Jupyter Notebooks, web apps, and standalone HTML files.</p> </li> <li> <p>Built-in Animations:</p> </li> <li>Easy to create smooth, interactive animations with native support.</li> <li> <p>Ideal for demonstrating dynamic AI models or processes.</p> </li> <li> <p>Wide Range of Charts:</p> </li> <li>Includes advanced visualizations like 3D plots, heatmaps, choropleth maps, and Sankey diagrams.</li> <li>Perfect for showing complex GenAI outputs in an intuitive way.</li> </ol>"},{"location":"chapters-v1/matplotlib-vs-plotly/#cons_1","title":"Cons","text":"<ol> <li>Heavier and Slower:</li> <li>More resource-intensive due to JavaScript-based interactivity.</li> <li> <p>Can slow down with large datasets or complex visualizations.</p> </li> <li> <p>Limited Fine-Grained Control:</p> </li> <li>While customizable, it lacks the deep control over plot details that Matplotlib offers.</li> <li> <p>Certain niche visualizations might be harder to implement.</p> </li> <li> <p>Dependency on JavaScript:</p> </li> <li> <p>Requires JavaScript runtime for rendering, which might not be ideal for all environments.</p> </li> <li> <p>Animation Complexity:</p> </li> <li>While animations are smoother, creating complex animations programmatically can be less intuitive than with Matplotlib.</li> </ol>"},{"location":"chapters-v1/matplotlib-vs-plotly/#which-to-choose-for-genai","title":"Which to Choose for GenAI?","text":"<ol> <li>Static Plots:</li> <li>Use Matplotlib for quick, customizable, high-quality static plots.</li> <li> <p>Example: Generating high-resolution images for reports or papers.</p> </li> <li> <p>Interactive and Animated Plots:</p> </li> <li>Use Plotly for dynamic and interactive plots.</li> <li> <p>Example: Interactive GenAI dashboards or showcasing dynamic AI model predictions.</p> </li> <li> <p>Collaborative and Web-Based Use:</p> </li> <li> <p>Use Plotly if your plots need to be shared on the web or included in interactive applications.</p> </li> <li> <p>Complex Animations:</p> </li> <li>Use Matplotlib for complex, programmatically controlled animations.</li> <li>Use Plotly for simple, interactive animations with smooth transitions.</li> </ol>"},{"location":"chapters-v1/matplotlib-vs-plotly/#conclusion","title":"Conclusion","text":"<p>Both libraries are valuable for different contexts: - Matplotlib shines in fine-grained control and efficient static or programmatic visualizations. - Plotly is the go-to for interactivity, browser-based plots, and user-friendly animations.</p> <p>For GenAI, combining both libraries strategically (e.g., static plots with Matplotlib and interactive plots with Plotly) can provide the best of both worlds.</p>"},{"location":"chapters-v1/00-setup/","title":"Setting up Python, Visual Studio Code and Jupyter Notebooks","text":"<p>In this course will will use, Python, Visual Studio Code and Jupyter Notebooks to do our classroom work. We selected these tools because they are the most popular development environments in data science today.  You are free to use other tools in this course, however you may need to use generative AI to translate our code into other systems. In addition, we strongly recommend each student create their own personal GitHub repository to store their work.  Because GitHub is backed up the students only need to check their code into GitHub using either a command line or Source Control section of Visual Studio Code.</p> <p>We will be using the following tools for our desktop setup:</p> <ol> <li>Visual Studio Code - integrated development environment</li> <li>Conda - virtual environment</li> <li>Pip - Python installer package</li> </ol>"},{"location":"chapters-v1/00-setup/#conda","title":"Conda","text":"<p>Create a \"Data Science\" conda environment.</p> <pre><code>conda create -n \"ds\" python=3\nconda activate ds\n</code></pre>"},{"location":"chapters-v1/00-setup/#installing-jupyter-notebooks","title":"Installing Jupyter Notebooks","text":"<p>Jupyter Notebooks are essential for this course as they provide an interactive environment for data science work. There are several ways to install and use Jupyter Notebooks:</p>"},{"location":"chapters-v1/00-setup/#method-1-install-via-conda-recommended","title":"Method 1: Install via Conda (Recommended)","text":"<p>After activating your conda environment, install Jupyter:</p> <pre><code>conda activate ds\nconda install jupyter\n</code></pre>"},{"location":"chapters-v1/00-setup/#method-2-install-via-pip","title":"Method 2: Install via Pip","text":"<p>Alternatively, you can install using pip:</p> <pre><code>pip install jupyter\n</code></pre>"},{"location":"chapters-v1/00-setup/#method-3-install-jupyterlab-enhanced-interface","title":"Method 3: Install JupyterLab (Enhanced Interface)","text":"<p>For a more modern interface, you can install JupyterLab:</p> <pre><code>pip install jupyterlab\n</code></pre>"},{"location":"chapters-v1/00-setup/#running-jupyter-notebooks","title":"Running Jupyter Notebooks","text":""},{"location":"chapters-v1/00-setup/#starting-jupyter-notebook-server","title":"Starting Jupyter Notebook Server","text":"<p>To start the Jupyter Notebook server:</p> <pre><code># Activate your environment first\nconda activate ds\n\n# Start Jupyter Notebook\njupyter notebook\n</code></pre> <p>This will: 1. Start a local server (usually at <code>http://localhost:8888</code>) 2. Automatically open your web browser to the Jupyter interface 3. Display your file system where you can create new notebooks or open existing ones</p>"},{"location":"chapters-v1/00-setup/#starting-jupyterlab","title":"Starting JupyterLab","text":"<p>If you installed JupyterLab:</p> <pre><code>jupyter lab\n</code></pre>"},{"location":"chapters-v1/00-setup/#using-jupyter-notebooks-in-visual-studio-code","title":"Using Jupyter Notebooks in Visual Studio Code","text":"<p>Visual Studio Code provides excellent built-in support for Jupyter Notebooks, offering a seamless development experience.</p>"},{"location":"chapters-v1/00-setup/#installing-the-jupyter-extension","title":"Installing the Jupyter Extension","text":"<ol> <li>Open Visual Studio Code</li> <li>Go to the Extensions view (<code>Ctrl+Shift+X</code> or <code>Cmd+Shift+X</code>)</li> <li>Search for \"Jupyter\" </li> <li>Install the official \"Jupyter\" extension by Microsoft</li> </ol>"},{"location":"chapters-v1/00-setup/#working-with-notebooks-in-vs-code","title":"Working with Notebooks in VS Code","text":"<p>Creating a New Notebook: 1. Open VS Code 2. Create a new file with <code>.ipynb</code> extension (e.g., <code>my_analysis.ipynb</code>) 3. VS Code will automatically recognize it as a Jupyter notebook</p> <p>Selecting Python Interpreter: 1. Open a notebook file 2. Click on \"Select Kernel\" in the top-right corner 3. Choose your conda environment (<code>ds</code>) as the Python interpreter</p> <p>Running Code Cells: - Run Single Cell: Click the play button next to a cell or press <code>Shift+Enter</code> - Run All Cells: Use <code>Ctrl+Shift+P</code> (or <code>Cmd+Shift+P</code>) and search \"Jupyter: Run All Cells\" - Add New Cell: Click the <code>+ Code</code> or <code>+ Markdown</code> buttons between cells</p>"},{"location":"chapters-v1/00-setup/#advantages-of-using-jupyter-in-vs-code","title":"Advantages of Using Jupyter in VS Code","text":"<ul> <li>Integrated Development: Code editing, debugging, and notebook execution in one interface</li> <li>Git Integration: Easy version control for notebook files</li> <li>IntelliSense: Code completion and syntax highlighting</li> <li>Variable Explorer: View variables and data structures in real-time</li> <li>Debugging Support: Set breakpoints and debug notebook code</li> <li>Extensions: Access to VS Code's rich extension ecosystem</li> </ul>"},{"location":"chapters-v1/00-setup/#installing-essential-data-science-packages","title":"Installing Essential Data Science Packages","text":"<p>Install the core packages you'll need for this course:</p> <pre><code># Activate your environment\nconda activate ds\n\n# Install essential data science packages\npip install pandas numpy matplotlib seaborn scikit-learn jupyter\n\n# Optional: Install additional useful packages\npip install plotly ipywidgets\n</code></pre>"},{"location":"chapters-v1/00-setup/#transcript","title":"Transcript","text":"<pre><code>Retrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 23.5.2\n  latest version: 24.9.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=24.9.2\n\n\n## Package Plan ##\n\n  environment location: /Users/danmccreary/miniconda3/envs/ds\n\n  added / updated specs:\n    - python=3\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2024.9.24  |       hca03da5_0         131 KB\n    expat-2.6.3                |       h313beb8_0         154 KB\n    libmpdec-4.0.0             |       h80987f9_0          69 KB\n    openssl-3.0.15             |       h80987f9_0         4.3 MB\n    pip-24.2                   |  py313hca03da5_0         2.4 MB\n    python-3.13.0              |h4862095_100_cp313        13.7 MB\n    python_abi-3.13            |          0_cp313           7 KB\n    setuptools-75.1.0          |  py313hca03da5_0         1.9 MB\n    tzdata-2024b               |       h04d1e81_0         115 KB\n    wheel-0.44.0               |  py313hca03da5_0         139 KB\n    ------------------------------------------------------------\n                                           Total:        23.0 MB\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_6 \n  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2024.9.24-hca03da5_0 \n  expat              pkgs/main/osx-arm64::expat-2.6.3-h313beb8_0 \n  libcxx             pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 \n  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \n  libmpdec           pkgs/main/osx-arm64::libmpdec-4.0.0-h80987f9_0 \n  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 \n  openssl            pkgs/main/osx-arm64::openssl-3.0.15-h80987f9_0 \n  pip                pkgs/main/osx-arm64::pip-24.2-py313hca03da5_0 \n  python             pkgs/main/osx-arm64::python-3.13.0-h4862095_100_cp313 \n  python_abi         pkgs/main/osx-arm64::python_abi-3.13-0_cp313 \n  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 \n  setuptools         pkgs/main/osx-arm64::setuptools-75.1.0-py313hca03da5_0 \n  sqlite             pkgs/main/osx-arm64::sqlite-3.45.3-h80987f9_0 \n  tk                 pkgs/main/osx-arm64::tk-8.6.14-h6ba3021_0 \n  tzdata             pkgs/main/noarch::tzdata-2024b-h04d1e81_0 \n  wheel              pkgs/main/osx-arm64::wheel-0.44.0-py313hca03da5_0 \n  xz                 pkgs/main/osx-arm64::xz-5.4.6-h80987f9_1 \n  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h18a0788_1 \n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\n\nPreparing transaction: done                                                                          \nVerifying transaction: done                                                                          \nExecuting transaction: done                                                                        \n#                                                                                                    \n# To activate this environment, use                                                                  \n#                                                                                                    \n#     $ conda activate ds                                                                            \n#                                                                                                    \n# To deactivate an active environment, use                                                           \n#\n#     $ conda deactivate\n</code></pre>"},{"location":"chapters-v1/00-setup/#pip-install-transcript","title":"Pip Install Transcript","text":"<p>(ds) src/line-plot $ pip install matplotlib</p> <pre><code>Collecting matplotlib\n  Downloading matplotlib-3.9.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\nCollecting contourpy&gt;=1.0.1 (from matplotlib)\n  Downloading contourpy-1.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.4 kB)\nCollecting cycler&gt;=0.10 (from matplotlib)\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools&gt;=4.22.0 (from matplotlib)\n  Downloading fonttools-4.55.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (164 kB)\nCollecting kiwisolver&gt;=1.3.1 (from matplotlib)\n  Downloading kiwisolver-1.4.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\nCollecting numpy&gt;=1.23 (from matplotlib)\n  Downloading numpy-2.1.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\nCollecting packaging&gt;=20.0 (from matplotlib)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pillow&gt;=8 (from matplotlib)\n  Downloading pillow-11.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.1 kB)\nCollecting pyparsing&gt;=2.3.1 (from matplotlib)\n  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\nCollecting python-dateutil&gt;=2.7 (from matplotlib)\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting six&gt;=1.5 (from python-dateutil&gt;=2.7-&gt;matplotlib)\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nDownloading matplotlib-3.9.2-cp313-cp313-macosx_11_0_arm64.whl (7.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.8/7.8 MB 35.2 MB/s eta 0:00:00\nDownloading contourpy-1.3.1-cp313-cp313-macosx_11_0_arm64.whl (255 kB)\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\nDownloading fonttools-4.55.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.8/2.8 MB 19.8 MB/s eta 0:00:00\nDownloading kiwisolver-1.4.7-cp313-cp313-macosx_11_0_arm64.whl (63 kB)\nDownloading numpy-2.1.3-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.1/5.1 MB 33.8 MB/s eta 0:00:00\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\nDownloading pillow-11.0.0-cp313-cp313-macosx_11_0_arm64.whl (3.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.0/3.0 MB 28.6 MB/s eta 0:00:00\nDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\nUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, python-dateutil, contourpy, matplotlib\nSuccessfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.0 kiwisolver-1.4.7 matplotlib-3.9.2 numpy-2.1.3 packaging-24.2 pillow-11.0.0 pyparsing-3.2.0 python-dateutil-2.9.0.post0 six-1.16.0\n(ds) src/line-plot $ python line-plot.py   \n2024-11-15 07:14:21.701 python[53332:2841291] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-11-15 07:14:21.701 python[53332:2841291] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n</code></pre>"},{"location":"chapters-v1/01-foundations/","title":"Foundations of Data Science","text":""},{"location":"chapters-v1/01-foundations/#introduction-to-data-science-and-its-applications","title":"Introduction to Data Science and Its Applications","text":"<p>Data science is the discipline of extracting meaningful insights from data by combining statistics, programming, and domain expertise. It powers many of the services and tools we use every day\u2014from recommendation engines on streaming platforms to real-time fraud detection in banking. Governments, businesses, and non-profits alike depend on data science to make evidence-based decisions and improve efficiency.</p> <p>Applications of data science span nearly every field:</p> <ul> <li>Healthcare: Predicting disease risks, optimizing treatment effectiveness, and analyzing healthcare costs.</li> <li>Finance: Credit scoring, algorithmic trading, and fraud detection.</li> <li>Retail: Personalized recommendations and demand forecasting.</li> <li>Transportation: Route optimization and autonomous vehicle navigation.</li> <li>Environmental Science: Climate modeling and resource management.</li> </ul>"},{"location":"chapters-v1/01-foundations/#first-lab-exploring-sample-datasets","title":"First Lab: Exploring Sample Datasets","text":"<p>To begin, we will explore real datasets. A simple but practical task is to generate a CSV file of per-capita annual healthcare costs for 2023 across the 100 largest countries. You will then use Python to:</p> <ol> <li>Load the data into a Pandas DataFrame.</li> <li>Compute summary statistics such as mean, median, and standard deviation.</li> <li>Create visualizations such as bar charts and scatter plots.</li> <li>Ask ChatGPT (or another LLM) to interpret the results and suggest insights.</li> </ol> <p>World Bank Per Capita Healthcare Costs</p> <p>Here is a sample of what this data looks like:</p> <pre><code>Country Name,Country Code,health_exp_pc_ppp_2022\nAfrica Eastern and Southern,AFE,228\nAfghanistan,AFG,383\nAfrica Western and Central,AFW,201\nAngola,AGO,217\nAlbania,ALB,1186\nAndorra,AND,5136\nArab World,ARB,776\nUnited Arab Emirates,ARE,3814\nArgentina,ARG,2664\nArmenia,ARM,1824\nAntigua and Barbuda,ATG,1436\n</code></pre> <p>Note that this list from the World Bank contains not only countries but also regions.</p> <p>This exercise introduces data cleaning, exploration, and visualization, which form the foundation of every data science project.</p> <p>Suggested MicroSim: Exploring Data Points (students add/remove points on a scatter plot to see how the distribution changes).</p>"},{"location":"chapters-v1/01-foundations/#why-python-for-data-science","title":"Why Python for Data Science?","text":"<p>Python is the most widely used programming language for data science. It is popular because of:</p> <ul> <li>A rich ecosystem of libraries (NumPy, Pandas, scikit-learn, Matplotlib, PyTorch).</li> <li>Readable, beginner-friendly syntax.</li> <li>Strong community support and open-source resources.</li> </ul> <p>Over the past 15 years, Python has steadily risen to become the dominant language in data science. Other languages such as R, Java, and Julia are used in specific contexts, but Python\u2019s versatility has made it the industry standard.</p> <p>Here is an interactive time-series chart showing the change in popularity of different languages used in data science.  You can hover over each year to see what percent each language had in data science that year.</p>"},{"location":"chapters-v1/01-foundations/#key-insights-from-chart","title":"Key Insights from Chart:","text":"<ol> <li>Python's Rise: Python showed steady growth from 2010-2018, then accelerated dramatically after 2018 due to AI/ML boom, reaching 25%+ market share by 2025.</li> <li>R's Stability: R maintained consistent popularity (3-5%) throughout the period, remaining strong in academic and statistical research domains.</li> <li>SQL's Persistence: SQL showed steady growth and remained essential for data manipulation, reaching ~8% by 2025.</li> <li>Java's Decline: Java's popularity in data science decreased from ~20% to ~7% as Python gained dominance in ML/AI applications.</li> <li>JavaScript's Growth: JavaScript emerged as a data visualization tool, growing from ~2% to ~6% by 2025.</li> </ol> <p>Note</p> <p>This chart was generated by Generative AI using Claude Sonnet 4.0 using the Chart.js JavaScript library.  You can view the Data Science Programming Language Trends MicroSim to learn more.</p>"},{"location":"chapters-v1/01-foundations/#understanding-the-data-science-workflow","title":"Understanding the Data Science Workflow","text":"<p>Every data science project follows a structured workflow:</p> <p>This workflow is iterative. A failed model often sends us back to collect new data or engineer better features.</p> <p>Above is an interactive infographic that allows you to explore the six steps in a typical data science workflow.  For each step, hover over the step and view the text description below the step.</p> <ol> <li>Define the problem \u2013 Clarify what question is being answered.</li> <li>Collect data \u2013 Gather raw data from reliable sources.</li> <li>Clean and preprocess data \u2013 Handle missing values, errors, and inconsistencies.</li> <li>Explore and visualize \u2013 Use plots and descriptive statistics to understand patterns.</li> <li>Modeling and Analysis \u2013 Build predictive or explanatory models.</li> <li>Deploy and communicate results \u2013 Share findings with stakeholders.</li> </ol>"},{"location":"chapters-v1/01-foundations/#four-types-of-data-representations","title":"Four Types of Data Representations","text":"<p>In this course we will be looking at many ways to represent both raw data as well as connected knowledge.  Here is an interactive illustration of these four ways we represent different types of information.</p> <p>These types are:</p> <ol> <li>Images - Visual data represented as pixel arrays with RGB color values. Each pixel contains red, green, and blue color components. Common in computer vision, medical imaging, satellite imagery, and photo recognition. Neural networks like CNNs are specifically designed to process this type of spatial data structure.</li> <li>Sequences - Sequences: Ordered data where position and timing matter critically. Examples include time series data, natural language text, DNA sequences, audio signals, and stock prices. RNNs, LSTMs, and Transformers are designed to capture temporal dependencies and patterns in sequential data.</li> <li>Tabular -  Structured data organized in rows and columns, similar to spreadsheets or databases. Each row represents an observation and each column represents a feature or variable. This is the most common data type in traditional machine learning, handled well by algorithms like random forests, SVM, and gradient boosting.  We can use Python data frames to manipulate tabular data.</li> <li>Graph - Graph: Network data representing relationships and connections between entities. Nodes represent objects (people, websites, molecules) while edges represent relationships (friendships, links, bonds). Used in social network analysis, recommendation systems, knowledge graphs, and molecular modeling. Graph Neural Networks (GNNs) are specialized for this data type.</li> </ol> <p>Future chapters will focus on different ways we represent this information and how our models vary based on the type of data we are working with.</p>"},{"location":"chapters-v1/01-foundations/#basic-atomic-data-types-and-structures-in-python","title":"Basic Atomic Data Types and Structures in Python","text":"<p>Before analyzing data, students must understand how Python stores fundamental atomic entities like numbers and string.  In this course we will look at some of the core Python atomic atomic data types include:</p> <ul> <li>Integers (whole numbers, e.g., <code>42</code>)</li> <li>Floats (decimal numbers, e.g., <code>3.14</code>)</li> <li>Strings (text, e.g., <code>\"data science\"</code>)</li> <li>Booleans (<code>True</code> or <code>False</code>)</li> </ul> <p>Core data structures include:</p> <ul> <li>Lists \u2013 ordered, mutable collections (e.g., <code>[1,2,3]</code>)</li> <li>Tuples \u2013 ordered, immutable collections (e.g., <code>(1,2,3)</code>)</li> <li>Dictionaries \u2013 key-value pairs (e.g., <code>{\"name\": \"Alice\", \"age\": 20}</code>)</li> <li>Sets \u2013 unordered, unique elements (e.g., <code>{1,2,3}</code>)</li> </ul> <p>Later in the course, we will rely heavily on NumPy arrays and Pandas DataFrames, which are optimized for data manipulation.</p>"},{"location":"chapters-v1/01-foundations/#head","title":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD","text":""},{"location":"chapters-v1/01-foundations/#understanding-the-data-science-workflow_1","title":"Understanding the Data Science Workflow","text":"<p>Every data science project follows a structured workflow:</p> <ol> <li>Define the problem \u2013 Clarify what question is being answered.</li> <li>Collect data \u2013 Gather raw data from reliable sources.</li> <li>Clean and preprocess data \u2013 Handle missing values, errors, and inconsistencies.</li> <li>Explore and visualize \u2013 Use plots and descriptive statistics to understand patterns.</li> <li>Modeling \u2013 Build predictive or explanatory models.</li> <li>Evaluate \u2013 Use metrics to test accuracy and generalizability.</li> <li>Deploy and communicate results \u2013 Share findings with stakeholders.</li> </ol> <p>This workflow is iterative. A failed model often sends us back to collect new data or engineer better features.</p> <p>7836e01 (updates)</p>"},{"location":"chapters-v1/01-foundations/#microsim-data-science-workflow-infographic","title":"MicroSim \u2013 Data Science Workflow Infographic","text":"<p>Students can explore an interactive infographic where clicking each stage of the workflow reveals its purpose, key tools, and example questions.</p>"},{"location":"chapters-v1/01-foundations/#head_1","title":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD","text":"<p>7836e01 (updates)</p>"},{"location":"chapters-v1/01-foundations/#data-science-roles-and-career-paths","title":"Data Science Roles and Career Paths","text":"<p>Data science is a team effort, involving many specialized roles:</p> <ul> <li>Data Scientist: Builds models, interprets results, and communicates insights.</li> <li>Data Engineer: Designs pipelines and storage systems for reliable data access.</li> <li>Machine Learning Engineer: Deploys and optimizes models in production systems.</li> <li>Business Analyst: Translates data insights into actionable strategies.</li> <li>Ethics &amp; Compliance Specialist: Ensures fairness, transparency, and privacy in projects.</li> </ul> <p>These roles often overlap, and many entry-level positions expect a blend of programming, statistics, and communication skills.</p>"},{"location":"chapters-v1/01-foundations/#head_2","title":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD","text":"<p>7836e01 (updates)</p>"},{"location":"chapters-v1/01-foundations/#ethics-and-best-practices-in-data-science","title":"Ethics and Best Practices in Data Science","text":"<p>Data science has great potential, but also significant risks. Poorly designed or biased models can reinforce inequalities or cause harm. To practice ethical data science, we must:</p> <ul> <li>Protect privacy: Respect data ownership and confidentiality.</li> <li>Avoid bias: Check datasets and models for fairness across subgroups.</li> <li>Be transparent: Document methods and assumptions clearly.</li> <li>Ensure reproducibility: Use version control and pipelines so results can be verified.</li> <li>Balance efficiency and responsibility: Consider environmental and social impacts.</li> </ul> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"chapters-v1/01-foundations/#sample-hands-on-lab-exploring-sample-datasets-with-python-dataframes","title":"Sample Hands-On Lab: Exploring Sample Datasets with Python Dataframes","text":"<p>To begin, we will explore a real-world datasets.  Let's say a friend of yours complained that the healthcare costs in the country that you live it were unreasonably high compared to other countries.  Can you provide evidence to prove or disprove this claim?</p> <p>A simple but practical task is to generate a CSV file of per-capita annual healthcare costs for 2023 across the 100 largest countries. </p> <p>You will then use Python to:</p> <ol> <li>Load the data into a Pandas DataFrame.</li> <li>Compute summary statistics such as mean, median, and standard deviation.</li> <li>Create visualizations such as bar charts and scatter plots.</li> <li>Ask ChatGPT (or another LLM) to interpret the results and suggest insights.</li> </ol> <p>Here is a sample of what this data looks like:</p> <pre><code>Country Name,Country Code,Health_Exp_Per_Capita_2022\nAfrica Eastern and Southern,AFE,228\nAfghanistan,AFG,383\nAfrica Western and Central,AFW,201\nAngola,AGO,217\nAlbania,ALB,1186\nAndorra,AND,5136\nArab World,ARB,776\nUnited Arab Emirates,ARE,3814\nArgentina,ARG,2664\nArmenia,ARM,1824\nAntigua and Barbuda,ATG,1436\n</code></pre> <p>You can download this sample data here: Worldwide Healthcare Costs Per Capita for 2022</p> <p>Note that this list from the World Bank includes not just countries, but also regions like \"Africa Wester and Central\". The third column is in US dollars</p> <p>This exercise introduces data cleaning, exploration, and visualization, which form the foundation of every data science project.</p> <p>\u2705 This completes the Foundations of Data Science chapter, preparing students for Week 1 of the course.</p> <p>=======</p> <p>\u2705 This completes the Foundations of Data Science chapter, preparing students for Week 1 of the course.</p> <p>7836e01 (updates)</p>"},{"location":"chapters-v1/02-data-exploration/","title":"Data Exploration and Visualization","text":"<ol> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> <li>Data profiling and quality assessment</li> <li>Handling missing values and outliers</li> <li>Exploratory data analysis techniques</li> </ol>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/","title":"YData Profiling Tutorial: Comprehensive Data Analysis","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#overview","title":"Overview","text":"<p>YData Profiling (formerly pandas-profiling) is a powerful Python library that generates comprehensive reports for exploratory data analysis. This tutorial demonstrates its capabilities using a synthetic e-commerce customer dataset designed to showcase various data quality issues and patterns that YData Profiling excels at detecting.</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#installation","title":"Installation","text":"<p>First, install YData Profiling:</p> <pre><code>pip install ydata-profiling\n</code></pre> <p>For Jupyter notebooks, you might also want:</p> <pre><code>pip install ydata-profiling[notebook]\n</code></pre>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#dataset-overview","title":"Dataset Overview","text":"<p>Our tutorial dataset contains 503 rows of e-commerce customer data with 16 columns featuring:</p> <ul> <li>Missing values in multiple columns (ages, income, phone numbers, etc.)</li> <li>Data quality issues (age outliers, inconsistent phone formatting)</li> <li>Various data types (numeric, categorical, datetime, boolean, text)</li> <li>High cardinality categorical variables (cities)</li> <li>Correlations between variables (age and product preferences)</li> <li>Duplicate rows for detection testing</li> <li>Skewed distributions (income follows log-normal distribution)</li> </ul>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#basic-usage","title":"Basic Usage","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#1-generate-a-simple-report","title":"1. Generate a Simple Report","text":"<pre><code>import pandas as pd\nfrom ydata_profiling import ProfileReport\n\n# Load the dataset\ndf = pd.read_csv('ecommerce_customer_data.csv')\n\n# Generate basic profile report\nprofile = ProfileReport(df, title=\"E-commerce Customer Data Analysis\")\n\n# Save to HTML file\nprofile.to_file(\"basic_report.html\")\n\n# Display in Jupyter notebook\n# profile.to_notebook_iframe()\n</code></pre>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#2-customized-report-with-advanced-configuration","title":"2. Customized Report with Advanced Configuration","text":"<pre><code># Advanced configuration for more detailed analysis\nprofile = ProfileReport(\n    df,\n    title=\"Advanced E-commerce Data Analysis\",\n    dataset={\n        \"description\": \"Synthetic e-commerce customer dataset for YData Profiling demonstration\",\n        \"creator\": \"Data Science Tutorial\",\n        \"author\": \"Tutorial Author\"\n    },\n    variables={\n        \"descriptions\": {\n            \"customer_id\": \"Unique identifier for each customer\",\n            \"age\": \"Customer age in years\",\n            \"annual_income\": \"Customer's annual income in USD\",\n            \"total_spent\": \"Total amount spent by customer\",\n            \"satisfaction_score\": \"Customer satisfaction rating (1-10)\"\n        }\n    },\n    correlations={\n        \"pearson\": {\"calculate\": True},\n        \"spearman\": {\"calculate\": True},\n        \"kendall\": {\"calculate\": False},  # Skip Kendall for performance\n        \"phi_k\": {\"calculate\": True},\n        \"cramers\": {\"calculate\": True},\n    },\n    missing_diagrams={\n        \"bar\": True,\n        \"matrix\": True,\n        \"heatmap\": True,\n        \"dendrogram\": True,\n    },\n    duplicates={\n        \"head\": 10,  # Show first 10 duplicate rows\n        \"key\": None  # Check entire row for duplicates\n    },\n    samples={\n        \"head\": 5,\n        \"tail\": 5,\n        \"random\": 10\n    },\n    reject_variables=False,  # Don't automatically reject any variables\n    infer_dtypes=True,      # Automatically infer better data types\n    interactions={\n        \"continuous\": True,   # Analyze continuous variable interactions\n        \"targets\": []        # Specify target variables if doing supervised learning\n    }\n)\n\nprofile.to_file(\"advanced_report.html\")\n</code></pre>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#key-features-demonstrated","title":"Key Features Demonstrated","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#1-data-types-and-overview","title":"1. Data Types and Overview","text":"<p>YData Profiling automatically detects and categorizes:</p> <ul> <li>Numerical variables: age, annual_income, total_spent, satisfaction_score</li> <li>Categorical variables: gender, education_level, favorite_category</li> <li>DateTime variables: registration_date</li> <li>Boolean variables: premium_member</li> <li>Text variables: last_review</li> <li>High cardinality categorical: city (60+ unique values)</li> </ul>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#2-missing-data-analysis","title":"2. Missing Data Analysis","text":"<p>The report provides multiple visualizations for missing data:</p> <ul> <li>Bar chart: Shows missing count per column</li> <li>Matrix plot: Visualizes missing data patterns</li> <li>Heatmap: Shows correlations in missingness</li> <li>Dendrogram: Clusters variables by missing patterns</li> </ul> <p>Our dataset includes strategic missing values: - 25 missing ages (5%) - 35 missing incomes (7%) - 40 missing credit scores (8%) - 25 missing phone numbers (5%)</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#3-data-quality-issues-detection","title":"3. Data Quality Issues Detection","text":"<p>YData Profiling automatically identifies:</p> <p>Outliers:  - Age outliers (150 and 5 years old) are flagged as extreme values - Income distribution shows high-value outliers</p> <p>Inconsistent Formatting: - Phone numbers in multiple formats: (xxx) xxx-xxxx, xxx-xxx-xxxx, xxx.xxx.xxxx, xxxxxxxxxx - Gender entries with inconsistent capitalization: Male, M, m, male</p> <p>Data Type Issues: - Suggests better data types for mixed-format columns</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#4-distribution-analysis","title":"4. Distribution Analysis","text":"<p>For each numerical variable, the report shows:</p> <ul> <li>Descriptive statistics: mean, median, std, quartiles</li> <li>Distribution plots: histograms with optional normal distribution overlay</li> <li>Skewness and kurtosis: measures of distribution shape</li> </ul> <p>Our dataset demonstrates: - Normal distribution: age (with outliers) - Log-normal distribution: annual_income (right-skewed) - Poisson-like distribution: total_purchases - Gamma distribution: total_spent</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#5-correlation-analysis","title":"5. Correlation Analysis","text":"<p>Multiple correlation methods reveal relationships:</p> <ul> <li>Pearson: Linear relationships between continuous variables</li> <li>Spearman: Monotonic relationships (rank-based)</li> <li>Phi-K: Correlation for categorical variables</li> <li>Cram\u00e9r's V: Association between categorical variables</li> </ul> <p>Expected correlations in our dataset: - Age and favorite product category - Income and total spent - Education level and income</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#6-categorical-variable-analysis","title":"6. Categorical Variable Analysis","text":"<p>For categorical variables, the report provides:</p> <ul> <li>Frequency tables: Count and percentage for each category</li> <li>Bar charts: Visual representation of category distributions</li> <li>Cardinality warnings: Flags for high-cardinality variables like 'city'</li> </ul>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#7-duplicate-detection","title":"7. Duplicate Detection","text":"<p>The report identifies: - 3 duplicate rows intentionally added to the dataset - Exact matches across all columns - Percentage of duplicates: Impact on dataset size</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#8-text-analysis","title":"8. Text Analysis","text":"<p>For text columns like 'last_review': - Length distribution: Character count statistics - Sample values: Examples of text entries - Completeness: Percentage of non-null text entries</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#advanced-configuration-options","title":"Advanced Configuration Options","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#minimal-configuration-for-large-datasets","title":"Minimal Configuration for Large Datasets","text":"<pre><code># For large datasets, use minimal configuration for faster processing\nprofile = ProfileReport(\n    df,\n    title=\"Quick Analysis\",\n    minimal=True,  # Faster processing, fewer features\n    correlations={\n        \"pearson\": {\"calculate\": True},\n        \"spearman\": {\"calculate\": False},\n        \"kendall\": {\"calculate\": False},\n        \"phi_k\": {\"calculate\": False},\n        \"cramers\": {\"calculate\": False},\n    },\n    missing_diagrams={\n        \"bar\": True,\n        \"matrix\": False,\n        \"heatmap\": False,\n        \"dendrogram\": False,\n    }\n)\n</code></pre>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#sensitive-data-configuration","title":"Sensitive Data Configuration","text":"<pre><code># For datasets with sensitive information\nprofile = ProfileReport(\n    df,\n    title=\"Sensitive Data Analysis\",\n    samples={\"head\": 0, \"tail\": 0, \"random\": 0},  # Don't show actual data\n    duplicates={\"head\": 0},  # Don't show duplicate examples\n    sensitive=True  # Additional privacy protections\n)\n</code></pre>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#interpreting-the-report","title":"Interpreting the Report","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#1-executive-summary","title":"1. Executive Summary","text":"<p>The report begins with high-level insights: - Dataset dimensions (503 rows \u00d7 16 columns) - Missing cells percentage - Duplicate rows count - Data types distribution</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#2-variable-analysis","title":"2. Variable Analysis","text":"<p>Each variable gets detailed analysis: - Distinct count: Unique values - Missing count: Null values - Memory usage: Storage requirements - Type-specific metrics: Based on variable type</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#3-warnings-and-alerts","title":"3. Warnings and Alerts","text":"<p>YData Profiling automatically flags: - High cardinality: Variables with many unique values - High correlation: Potentially redundant variables - Skewed distributions: Variables needing transformation - Constant values: Variables with no variation - Missing values: Variables with significant missingness</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#best-practices","title":"Best Practices","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#1-performance-optimization","title":"1. Performance Optimization","text":"<p>For large datasets: - Use <code>minimal=True</code> for quick overview - Disable expensive correlation calculations - Limit sample sizes - Use <code>lazy=False</code> for immediate computation</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#2-customization-tips","title":"2. Customization Tips","text":"<ul> <li>Add variable descriptions for better documentation</li> <li>Configure correlation methods based on data types</li> <li>Customize missing data visualizations</li> <li>Set appropriate duplicate detection keys</li> </ul>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#3-integration-workflow","title":"3. Integration Workflow","text":"<pre><code># Typical data science workflow integration\ndef analyze_dataset(df, output_path=\"profile_report.html\"):\n    \"\"\"Generate comprehensive data profile report.\"\"\"\n\n    # Basic data info\n    print(f\"Dataset shape: {df.shape}\")\n    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n    # Generate profile\n    profile = ProfileReport(\n        df,\n        title=f\"Data Analysis Report - {datetime.now().strftime('%Y-%m-%d')}\",\n        minimal=False,\n        correlations={\"phi_k\": {\"calculate\": True}},\n        missing_diagrams={\"matrix\": True, \"bar\": True}\n    )\n\n    # Save report\n    profile.to_file(output_path)\n    print(f\"Report saved to: {output_path}\")\n\n    return profile\n\n# Usage\nprofile = analyze_dataset(df, \"customer_analysis_report.html\")\n</code></pre>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#common-use-cases","title":"Common Use Cases","text":""},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#1-initial-data-exploration","title":"1. Initial Data Exploration","text":"<p>Perfect for understanding new datasets before analysis: - Data quality assessment - Variable relationship discovery - Missing data patterns - Distribution characteristics</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#2-data-quality-monitoring","title":"2. Data Quality Monitoring","text":"<p>Regular profiling for ongoing data pipelines: - Detect data drift - Monitor missing value trends - Track distribution changes - Identify new data quality issues</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#3-documentation-generation","title":"3. Documentation Generation","text":"<p>Automated documentation for datasets: - Share with stakeholders - Document data characteristics - Support reproducible research - Create data dictionaries</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#conclusion","title":"Conclusion","text":"<p>YData Profiling provides comprehensive automated exploratory data analysis that would take hours to perform manually. This tutorial dataset demonstrates its ability to:</p> <ul> <li>Automatically detect various data types and quality issues</li> <li>Generate publication-ready visualizations</li> <li>Identify patterns and relationships</li> <li>Provide actionable insights for data cleaning</li> <li>Create comprehensive documentation</li> </ul> <p>The tool is invaluable for data scientists, analysts, and anyone working with datasets who needs quick, thorough data understanding.</p>"},{"location":"chapters-v1/02-data-exploration/ydata-profiling/#next-steps","title":"Next Steps","text":"<p>After reviewing the YData Profiling report:</p> <ol> <li>Address data quality issues: Clean outliers, standardize formats</li> <li>Handle missing values: Decide on imputation or removal strategies  </li> <li>Feature engineering: Use correlation insights for feature selection</li> <li>Distribution analysis: Consider transformations for skewed variables</li> <li>Duplicate handling: Remove or investigate duplicate records</li> </ol> <p>YData Profiling provides the foundation for informed data preprocessing and analysis decisions.</p>"},{"location":"labs/","title":"List of Labs","text":""},{"location":"labs/#load-csv-data-into-a-python-dataframe","title":"Load CSV Data Into A Python Dataframe","text":"<p>Go to Lab 1</p>"},{"location":"labs/#explore-data","title":"Explore Data","text":"<p>Go to Lab 2</p>"},{"location":"labs/01-dataframe/","title":"Lab 1: Dataframes","text":""},{"location":"labs/01-dataframe/#steps","title":"Steps","text":"<p>These steps assume you have setup your desktop using an IDE such as Visual Studio Code. See the Setup Chapter for details.</p>"},{"location":"labs/01-dataframe/#step-1-load-the-following-dataset-using-your-ide-such-as-visual-studio-code","title":"Step 1: Load the following dataset using your IDE such as Visual Studio Code","text":""},{"location":"labs/01-dataframe/#step-2-load-the-rainbow-csv-extension-into-visual-studio-code","title":"Step 2: Load the Rainbow CSV Extension into Visual Studio Code","text":""},{"location":"labs/01-dataframe/#step-3-load-python-code","title":"Step 3: Load Python Code","text":"<pre><code># Import the pandas library for data manipulation\nimport pandas as pd\n\n# Load the CSV file into a pandas DataFrame\n# This assumes the CSV file is in the same directory as this script\ndf = pd.read_csv('healthcare-per-capita-2022.csv')\n\n# Count the number of rows (lines) in the DataFrame\nline_count = len(df)\n\n# Display the first few rows to verify the data loaded correctly\nprint(\"First 5 rows of the data:\")\nprint(df.head())\n\n# Print the total number of lines\nprint(f\"\\nTotal number of lines in the CSV file: {line_count}\")\n\n# Optional: Display basic information about the DataFrame\nprint(f\"\\nDataFrame shape (rows, columns): {df.shape}\")\nprint(f\"Column names: {list(df.columns)}\")\n</code></pre> <p>If you get the following <code>Module Not Found</code> error, you must make sure that you use pip to install the pandas library.</p> <pre><code>import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n</code></pre> <pre><code>(ds) $ pip install pandas\n</code></pre> <p>Note that the '(ds)' in the prompt means you are working with your ds (data science) virtual Python environment.</p>"},{"location":"labs/01-dataframe/#step-4-run-python-code","title":"Step 4: Run Python Code","text":"<pre><code>python count-lines.py\n</code></pre> <p>Result:</p> <pre><code>python count-lines.py \nFirst 5 rows of the data:\n                  Country_Name Country_Code  Health_Exp_PerCapita_2022\n0  Africa Eastern and Southern          AFE                        228\n1                  Afghanistan          AFG                        383\n2   Africa Western and Central          AFW                        201\n3                       Angola          AGO                        217\n4                      Albania          ALB                       1186\n\nTotal number of lines in the CSV file: 238\n\nDataFrame shape (rows, columns): (238, 3)\nColumn names: ['Country_Name', 'Country_Code', 'Health_Exp_PerCapita_2022']\n</code></pre>"},{"location":"labs/02-explore-data/","title":"Lab 2: Introduction to DataFrames - Loading and Exploring Data","text":""},{"location":"labs/02-explore-data/#objectives","title":"Objectives","text":"<p>Students will learn to: - Load CSV data into a pandas DataFrame - Explore basic DataFrame properties and methods - Display and examine data structure</p>"},{"location":"labs/02-explore-data/#lab-steps","title":"Lab Steps","text":""},{"location":"labs/02-explore-data/#step-1-load-the-data","title":"Step 1: Load the Data","text":"<pre><code>import pandas as pd\n\n# Load the healthcare data\ndf = pd.read_csv('healthcare-per-capita-2022.csv')\n\n# Display the DataFrame\nprint(\"Healthcare Per Capita Data:\")\nprint(df)\n</code></pre> <pre><code>                    Country_Name Country_Code  Health_Exp_PerCapita_2022\n0    Africa Eastern and Southern          AFE                        228\n1                    Afghanistan          AFG                        383\n2     Africa Western and Central          AFW                        201\n3                         Angola          AGO                        217\n4                        Albania          ALB                       1186\n..                           ...          ...                        ...\n233                        Samoa          WSM                        396\n234                  Yemen, Rep.          YEM                        109\n235                 South Africa          ZAF                       1341\n236                       Zambia          ZMB                        208\n237                     Zimbabwe          ZWE                         96\n</code></pre>"},{"location":"labs/02-explore-data/#step-2-explore-dataframe-shape-and-info","title":"Step 2: Explore DataFrame Shape and Info","text":"<pre><code># Check the shape (rows, columns)\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Number of rows: {df.shape[0]}\")\nprint(f\"Number of columns: {df.shape[1]}\")\n\n# Get basic information about the DataFrame\nprint(\"\\nDataFrame Info:\")\ndf.info()\n</code></pre>"},{"location":"labs/02-explore-data/#step-3-examine-column-names-and-data-types","title":"Step 3: Examine Column Names and Data Types","text":"<pre><code># Display column names\nprint(\"Column names:\")\nprint(df.columns.tolist())\n\n# Check data types\nprint(\"\\nData types:\")\nprint(df.dtypes)\n</code></pre> <pre><code>Dataset shape: (238, 3)\nNumber of rows: 238\nNumber of columns: 3\n\nDataFrame Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 238 entries, 0 to 237\nData columns (total 3 columns):\n #   Column                     Non-Null Count  Dtype \n---  ------                     --------------  ----- \n 0   Country_Name               238 non-null    object\n 1   Country_Code               238 non-null    object\n 2   Health_Exp_PerCapita_2022  238 non-null    int64 \ndtypes: int64(1), object(2)\nmemory usage: 5.7+ KB\n</code></pre>"},{"location":"labs/02-explore-data/#step-4-preview-the-data","title":"Step 4: Preview the Data","text":"<pre><code># Look at first 5 rows\nprint(\"First 5 rows:\")\nprint(df.head())\n\n# Look at last 5 rows\nprint(\"\\nLast 5 rows:\")\nprint(df.tail())\n</code></pre> <p>Results:</p> <pre><code>First 5 rows:\n                  Country_Name Country_Code  Health_Exp_PerCapita_2022\n0  Africa Eastern and Southern          AFE                        228\n1                  Afghanistan          AFG                        383\n2   Africa Western and Central          AFW                        201\n3                       Angola          AGO                        217\n4                      Albania          ALB                       1186\n\nLast 5 rows:\n     Country_Name Country_Code  Health_Exp_PerCapita_2022\n233         Samoa          WSM                        396\n234   Yemen, Rep.          YEM                        109\n235  South Africa          ZAF                       1341\n236        Zambia          ZMB                        208\n237      Zimbabwe          ZWE                         96\n</code></pre>"},{"location":"labs/02-explore-data/#step-5-get-basic-information-about-the-dataframe","title":"Step 5: Get basic information about the DataFrame","text":"<pre><code>print(\"\\nDataFrame Info:\")\ndf.info()\n</code></pre> <pre><code>DataFrame Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 238 entries, 0 to 237\nData columns (total 3 columns):\n #   Column                     Non-Null Count  Dtype \n---  ------                     --------------  ----- \n 0   Country_Name               238 non-null    object\n 1   Country_Code               238 non-null    object\n 2   Health_Exp_PerCapita_2022  238 non-null    int64 \ndtypes: int64(1), object(2)\n</code></pre>"},{"location":"labs/02-explore-data/#look-at-a-random-sample-of-5-rows","title":"Look at a random sample of 5 rows","text":"<pre><code>print(\"\\nRandom sample of 5 rows:\")\nprint(df.sample(5))\n</code></pre>"},{"location":"labs/02-explore-data/#step-5-basic-data-exploration","title":"Step 5: Basic Data Exploration","text":"<pre><code># Get basic statistics for numerical columns\nprint(\"Basic statistics:\")\nprint(df.describe())\n\n# Check for missing values\nprint(\"\\nMissing values per column:\")\nprint(df.isnull().sum())\n\n# Count unique values in each column\nprint(\"\\nUnique values per column:\")\nfor col in df.columns:\n    print(f\"{col}: {df[col].nunique()} unique values\")\n</code></pre>"},{"location":"labs/02-explore-data/#questions-for-students","title":"Questions for Students","text":"<ol> <li>How many countries are included in this dataset?</li> <li>What is the data type of each column?</li> <li>Are there any missing values in the dataset?</li> <li>What country has the highest healthcare expenditure per capita?</li> <li>What is the average healthcare expenditure per capita across all countries?</li> </ol>"},{"location":"labs/02-explore-data/#expected-output-discussion","title":"Expected Output Discussion","text":"<p>Students should observe:</p> <ul> <li>The dataset has 238 rows (countries) and 3 columns</li> <li>Country_Name and Country_Code are text (object) data types</li> <li>Health_Exp_PerCapita_2022 is numerical (integer)</li> <li>Whether there are any missing values to handle</li> </ul>"},{"location":"labs/02-explore-data/#extension-activities","title":"Extension Activities","text":"<p>For advanced students:</p> <ul> <li>Sort the data by healthcare expenditure</li> <li>Find countries with expenditure above/below certain thresholds</li> <li>Create simple filtering operations</li> </ul> <p>This lab builds naturally from counting rows to actually working with the data structure,  introducing essential pandas concepts while keeping the complexity manageable for beginners.</p>"},{"location":"labs/03-statistics/","title":"Lab 3 - Statistics","text":""},{"location":"labs/03-statistics/#lab-3-basic-statistical-analysis-with-dataframes","title":"Lab 3: Basic Statistical Analysis with DataFrames","text":""},{"location":"labs/03-statistics/#objectives","title":"Objectives","text":"<p>Students will learn to: - Find minimum and maximum values in a DataFrame - Calculate basic statistical measures (mean, median, standard deviation) - Identify specific rows based on conditions - Use pandas methods for statistical analysis</p>"},{"location":"labs/03-statistics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Lab 2 (Loading and exploring DataFrames)</li> <li>Understanding of basic statistical concepts</li> </ul>"},{"location":"labs/03-statistics/#lab-steps","title":"Lab Steps","text":""},{"location":"labs/03-statistics/#step-1-load-and-prepare-the-data","title":"Step 1: Load and Prepare the Data","text":"<pre><code>import pandas as pd\n\n# Load the healthcare data\ndf = pd.read_csv('healthcarepercapita2022.csv')\n\n# Display basic info to remind ourselves of the data structure\nprint(\"Dataset shape:\", df.shape)\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n</code></pre>"},{"location":"labs/03-statistics/#step-2-find-the-country-with-lowest-healthcare-expenditure","title":"Step 2: Find the Country with Lowest Healthcare Expenditure","text":"<pre><code># Method 1: Using min() and boolean indexing\nmin_expenditure = df['Health_Exp_PerCapita_2022'].min()\nlowest_country = df[df['Health_Exp_PerCapita_2022'] == min_expenditure]\n\nprint(\"Country with LOWEST healthcare expenditure per capita:\")\nprint(f\"Country: {lowest_country['Country_Name'].iloc[0]}\")\nprint(f\"Expenditure: ${min_expenditure:,.2f}\")\n\n# Alternative method using idxmin()\nmin_index = df['Health_Exp_PerCapita_2022'].idxmin()\nprint(f\"\\nAlternative method - Lowest: {df.loc[min_index, 'Country_Name']} (${min_expenditure:,.2f})\")\n</code></pre>"},{"location":"labs/03-statistics/#step-3-find-the-country-with-highest-healthcare-expenditure","title":"Step 3: Find the Country with Highest Healthcare Expenditure","text":"<pre><code># Method 1: Using max() and boolean indexing\nmax_expenditure = df['Health_Exp_PerCapita_2022'].max()\nhighest_country = df[df['Health_Exp_PerCapita_2022'] == max_expenditure]\n\nprint(\"\\nCountry with HIGHEST healthcare expenditure per capita:\")\nprint(f\"Country: {highest_country['Country_Name'].iloc[0]}\")\nprint(f\"Expenditure: ${max_expenditure:,.2f}\")\n\n# Alternative method using idxmax()\nmax_index = df['Health_Exp_PerCapita_2022'].idxmax()\nprint(f\"\\nAlternative method - Highest: {df.loc[max_index, 'Country_Name']} (${max_expenditure:,.2f})\")\n</code></pre>"},{"location":"labs/03-statistics/#step-4-calculate-mean-healthcare-expenditure","title":"Step 4: Calculate Mean Healthcare Expenditure","text":"<pre><code># Calculate the average (mean)\nmean_expenditure = df['Health_Exp_PerCapita_2022'].mean()\n\nprint(f\"\\nAverage (Mean) healthcare expenditure per capita:\")\nprint(f\"${mean_expenditure:,.2f}\")\n\n# Round to 2 decimal places for cleaner display\nprint(f\"Rounded: ${round(mean_expenditure, 2):,.2f}\")\n</code></pre>"},{"location":"labs/03-statistics/#step-5-calculate-median-healthcare-expenditure","title":"Step 5: Calculate Median Healthcare Expenditure","text":"<pre><code># Calculate the median (middle value)\nmedian_expenditure = df['Health_Exp_PerCapita_2022'].median()\n\nprint(f\"\\nMedian healthcare expenditure per capita:\")\nprint(f\"${median_expenditure:,.2f}\")\n\n# Compare mean vs median\nprint(f\"\\nComparison:\")\nprint(f\"Mean:   ${mean_expenditure:,.2f}\")\nprint(f\"Median: ${median_expenditure:,.2f}\")\nif mean_expenditure &gt; median_expenditure:\n    print(\"Mean &gt; Median: Data is likely right-skewed (few very high values)\")\nelif mean_expenditure &lt; median_expenditure:\n    print(\"Mean &lt; Median: Data is likely left-skewed (few very low values)\")\nelse:\n    print(\"Mean \u2248 Median: Data is likely normally distributed\")\n</code></pre>"},{"location":"labs/03-statistics/#step-6-calculate-standard-deviation","title":"Step 6: Calculate Standard Deviation","text":"<pre><code># Calculate standard deviation\nstd_expenditure = df['Health_Exp_PerCapita_2022'].std()\n\nprint(f\"\\nStandard deviation of healthcare expenditure:\")\nprint(f\"${std_expenditure:,.2f}\")\n\n# Interpret the standard deviation\nprint(f\"\\nInterpretation:\")\nprint(f\"About 68% of countries spend between ${mean_expenditure - std_expenditure:,.2f} and ${mean_expenditure + std_expenditure:,.2f} per capita\")\nprint(f\"About 95% of countries spend between ${mean_expenditure - 2*std_expenditure:,.2f} and ${mean_expenditure + 2*std_expenditure:,.2f} per capita\")\n</code></pre>"},{"location":"labs/03-statistics/#step-7-summary-statistics-all-at-once","title":"Step 7: Summary Statistics (All at Once)","text":"<pre><code># Get all basic statistics at once\nprint(\"\\nComplete Statistical Summary:\")\nprint(df['Health_Exp_PerCapita_2022'].describe())\n\n# Create a custom summary\nprint(f\"\\n{'='*50}\")\nprint(\"HEALTHCARE EXPENDITURE ANALYSIS SUMMARY\")\nprint(f\"{'='*50}\")\nprint(f\"Lowest spending country:  {df.loc[df['Health_Exp_PerCapita_2022'].idxmin(), 'Country_Name']}\")\nprint(f\"Highest spending country: {df.loc[df['Health_Exp_PerCapita_2022'].idxmax(), 'Country_Name']}\")\nprint(f\"Range: ${min_expenditure:,.2f} - ${max_expenditure:,.2f}\")\nprint(f\"Mean: ${mean_expenditure:,.2f}\")\nprint(f\"Median: ${median_expenditure:,.2f}\")\nprint(f\"Standard Deviation: ${std_expenditure:,.2f}\")\nprint(f\"Total countries analyzed: {len(df)}\")\n</code></pre> <pre><code>Complete Statistical Summary:\ncount      238.000000\nmean      1930.205882\nstd       2458.454731\nmin         39.000000\n25%        270.000000\n50%        930.500000\n75%       2467.500000\nmax      12434.000000\nName: Health_Exp_PerCapita_2022, dtype: float64\n\n==================================================\nHEALTHCARE EXPENDITURE ANALYSIS SUMMARY\n==================================================\nLowest spending country:  South Sudan\nHighest spending country: United States\nRange: $39.00 - $12,434.00\nMean: $1,930.21\nMedian: $930.50\nStandard Deviation: $2,458.45\nTotal countries analyzed: 238\n</code></pre>"},{"location":"labs/03-statistics/#discussion-questions","title":"Discussion Questions","text":"<ol> <li> <p>Which country spends the least on healthcare per capita? How much do they spend?</p> </li> <li> <p>Which country spends the most on healthcare per capita? How much do they spend?</p> </li> <li> <p>What is the difference between the highest and lowest spending countries?</p> </li> <li> <p>Is the mean higher or lower than the median? What does this tell us about the distribution of healthcare spending?</p> </li> <li> <p>How many countries fall within one standard deviation of the mean?</p> </li> <li> <p>If a country spends $2,000 per capita on healthcare, is this above or below average?</p> </li> </ol>"},{"location":"labs/03-statistics/#extension-activities","title":"Extension Activities","text":"<p>For Advanced Students: <pre><code># Find countries within certain ranges\nprint(\"\\nCountries spending more than $5,000 per capita:\")\nhigh_spenders = df[df['Health_Exp_PerCapita_2022'] &gt; 5000]\nprint(high_spenders[['Country_Name', 'Health_Exp_PerCapita_2022']].sort_values('Health_Exp_PerCapita_2022', ascending=False))\n\n# Calculate what percentage of countries spend above the mean\nabove_mean = len(df[df['Health_Exp_PerCapita_2022'] &gt; mean_expenditure])\npercentage_above_mean = (above_mean / len(df)) * 100\nprint(f\"\\n{above_mean} countries ({percentage_above_mean:.1f}%) spend above the global average\")\n</code></pre></p>"},{"location":"labs/03-statistics/#key-learning-outcomes","title":"Key Learning Outcomes","text":"<ul> <li>Understanding the difference between mean and median</li> <li>Learning to find extreme values in datasets</li> <li>Interpreting standard deviation as a measure of variability</li> <li>Using pandas methods for statistical analysis</li> <li>Connecting statistical concepts to real-world data</li> </ul> <p>This lab builds naturally from the previous exploration lab and introduces fundamental statistical analysis that students will use throughout their data science journey.</p>"},{"location":"learning-graph/","title":"Learning Graph for Introduction to Data Science with Python","text":"<p>This section contains the learning graph for this textbook. A learning graph is a graph of concepts used in this textbook. Each concept is represented by a node in a network graph. Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts. They have no outbound edges. They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts. At the far right we have the most advanced concepts in the course. To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG). DAGs do not have cycles where concepts depend on themselves. We provide the DAG in two formats. One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format. The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties. This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating 200 concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 2 entry points (Data Science, Python Programming)</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains (22 concepts)</li> <li>Connectivity: 100% of nodes connected to the main cluster</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type. We use generative AI to create about a dozen categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>A concept classifier taxonomy with 15 categories</li> <li>Category organization - foundational elements first, capstone project ideas last</li> <li>Balanced categories (1.7% - 10% each)</li> <li>All categories under 30% threshold</li> <li>Pedagogical flow recommendations</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This reports shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds an equal number of concepts. We also don't want any category to contain over 30% of our concepts.</p> <ul> <li>Statistical breakdown</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/concept-list/","title":"Concept List for Introduction to Data Science with Python","text":"<p>This document contains 200 concepts for the learning graph. Each concept is numbered with a unique ConceptID.</p>"},{"location":"learning-graph/concept-list/#foundational-concepts-1-20","title":"Foundational Concepts (1-20)","text":"<ol> <li>Data Science</li> <li>Python Programming</li> <li>Jupyter Notebooks</li> <li>Data</li> <li>Variables</li> <li>Data Types</li> <li>Numerical Data</li> <li>Categorical Data</li> <li>Ordinal Data</li> <li>Nominal Data</li> <li>Measurement Scales</li> <li>Independent Variable</li> <li>Dependent Variable</li> <li>Dataset</li> <li>Observation</li> <li>Feature</li> <li>Target Variable</li> <li>Data Science Workflow</li> <li>Problem Definition</li> <li>Data Collection</li> </ol>"},{"location":"learning-graph/concept-list/#python-environment-21-35","title":"Python Environment (21-35)","text":"<ol> <li>Python Installation</li> <li>Package Management</li> <li>Pip</li> <li>Conda Environment</li> <li>Virtual Environment</li> <li>IDE Setup</li> <li>VS Code</li> <li>Notebook Cells</li> <li>Code Cell</li> <li>Markdown Cell</li> <li>Cell Execution</li> <li>Kernel</li> <li>Import Statement</li> <li>Python Libraries</li> <li>Documentation</li> </ol>"},{"location":"learning-graph/concept-list/#data-structures-36-55","title":"Data Structures (36-55)","text":"<ol> <li>Lists</li> <li>Dictionaries</li> <li>Tuples</li> <li>Arrays</li> <li>Pandas Library</li> <li>DataFrame</li> <li>Series</li> <li>Index</li> <li>Column</li> <li>Row</li> <li>Data Loading</li> <li>CSV Files</li> <li>Read CSV</li> <li>Data Inspection</li> <li>Head Method</li> <li>Tail Method</li> <li>Shape Attribute</li> <li>Info Method</li> <li>Describe Method</li> <li>Data Selection</li> </ol>"},{"location":"learning-graph/concept-list/#data-cleaning-56-75","title":"Data Cleaning (56-75)","text":"<ol> <li>Missing Values</li> <li>NaN</li> <li>Null Detection</li> <li>Dropna Method</li> <li>Fillna Method</li> <li>Imputation</li> <li>Data Type Conversion</li> <li>Duplicate Detection</li> <li>Duplicate Removal</li> <li>Outliers</li> <li>Outlier Detection</li> <li>Data Validation</li> <li>String Cleaning</li> <li>Column Renaming</li> <li>Data Filtering</li> <li>Boolean Indexing</li> <li>Query Method</li> <li>Data Transformation</li> <li>Feature Scaling</li> <li>Normalization</li> </ol>"},{"location":"learning-graph/concept-list/#visualization-basics-76-100","title":"Visualization Basics (76-100)","text":"<ol> <li>Data Visualization</li> <li>Matplotlib Library</li> <li>Figure</li> <li>Axes</li> <li>Plot Function</li> <li>Line Plot</li> <li>Scatter Plot</li> <li>Bar Chart</li> <li>Histogram</li> <li>Box Plot</li> <li>Pie Chart</li> <li>Subplot</li> <li>Figure Size</li> <li>Title</li> <li>Axis Labels</li> <li>Legend</li> <li>Color</li> <li>Markers</li> <li>Line Styles</li> <li>Grid</li> <li>Annotations</li> <li>Save Figure</li> <li>Plot Customization</li> <li>Seaborn Library</li> <li>Statistical Plots</li> </ol>"},{"location":"learning-graph/concept-list/#statistics-foundations-101-130","title":"Statistics Foundations (101-130)","text":"<ol> <li>Descriptive Statistics</li> <li>Mean</li> <li>Median</li> <li>Mode</li> <li>Range</li> <li>Variance</li> <li>Standard Deviation</li> <li>Quartiles</li> <li>Percentiles</li> <li>Interquartile Range</li> <li>Skewness</li> <li>Kurtosis</li> <li>Distribution</li> <li>Normal Distribution</li> <li>Probability</li> <li>Random Variables</li> <li>Expected Value</li> <li>Sample</li> <li>Population</li> <li>Sampling</li> <li>Central Limit Theorem</li> <li>Confidence Interval</li> <li>Hypothesis Testing</li> <li>P-Value</li> <li>Statistical Significance</li> <li>Correlation</li> <li>Covariance</li> <li>Pearson Correlation</li> <li>Spearman Correlation</li> <li>Correlation Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#linear-regression-131-155","title":"Linear Regression (131-155)","text":"<ol> <li>Regression Analysis</li> <li>Linear Regression</li> <li>Simple Linear Regression</li> <li>Regression Line</li> <li>Slope</li> <li>Intercept</li> <li>Least Squares Method</li> <li>Residuals</li> <li>Sum of Squared Errors</li> <li>Ordinary Least Squares</li> <li>Regression Coefficients</li> <li>Coefficient Interpretation</li> <li>Prediction</li> <li>Fitted Values</li> <li>Regression Equation</li> <li>Line of Best Fit</li> <li>Assumptions of Regression</li> <li>Linearity Assumption</li> <li>Homoscedasticity</li> <li>Independence Assumption</li> <li>Normality of Residuals</li> <li>Scikit-learn Library</li> <li>LinearRegression Class</li> <li>Fit Method</li> <li>Predict Method</li> </ol>"},{"location":"learning-graph/concept-list/#model-evaluation-156-180","title":"Model Evaluation (156-180)","text":"<ol> <li>Model Performance</li> <li>Training Data</li> <li>Testing Data</li> <li>Train Test Split</li> <li>Validation Data</li> <li>R-Squared</li> <li>Adjusted R-Squared</li> <li>Mean Squared Error</li> <li>Root Mean Squared Error</li> <li>Mean Absolute Error</li> <li>Residual Analysis</li> <li>Residual Plot</li> <li>Overfitting</li> <li>Underfitting</li> <li>Bias</li> <li>Variance</li> <li>Bias-Variance Tradeoff</li> <li>Model Complexity</li> <li>Cross-Validation</li> <li>K-Fold Cross-Validation</li> <li>Leave One Out CV</li> <li>Holdout Method</li> <li>Model Selection</li> <li>Hyperparameters</li> <li>Model Comparison</li> </ol>"},{"location":"learning-graph/concept-list/#multiple-regression-181-195","title":"Multiple Regression (181-195)","text":"<ol> <li>Multiple Linear Regression</li> <li>Multiple Predictors</li> <li>Multicollinearity</li> <li>Variance Inflation Factor</li> <li>Feature Selection</li> <li>Forward Selection</li> <li>Backward Elimination</li> <li>Stepwise Selection</li> <li>Categorical Variables</li> <li>Dummy Variables</li> <li>One-Hot Encoding</li> <li>Interaction Terms</li> <li>Polynomial Features</li> <li>Feature Engineering</li> <li>Feature Importance</li> </ol>"},{"location":"learning-graph/concept-list/#numpy-196-210","title":"NumPy (196-210)","text":"<ol> <li>NumPy Library</li> <li>NumPy Array</li> <li>Array Creation</li> <li>Array Shape</li> <li>Array Indexing</li> <li>Array Slicing</li> <li>Broadcasting</li> <li>Vectorized Operations</li> <li>Element-wise Operations</li> <li>Matrix Operations</li> <li>Dot Product</li> <li>Matrix Multiplication</li> <li>Transpose</li> <li>Linear Algebra</li> <li>Computational Efficiency</li> </ol>"},{"location":"learning-graph/concept-list/#non-linear-models-211-225","title":"Non-linear Models (211-225)","text":"<ol> <li>Non-linear Regression</li> <li>Polynomial Regression</li> <li>Degree of Polynomial</li> <li>Curve Fitting</li> <li>Transformation</li> <li>Log Transformation</li> <li>Feature Transformation</li> <li>Model Flexibility</li> <li>Regularization</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Elastic Net</li> <li>Regularization Parameter</li> <li>Lambda Parameter</li> <li>Shrinkage</li> </ol>"},{"location":"learning-graph/concept-list/#machine-learning-intro-226-245","title":"Machine Learning Intro (226-245)","text":"<ol> <li>Machine Learning</li> <li>Supervised Learning</li> <li>Unsupervised Learning</li> <li>Classification</li> <li>Clustering</li> <li>Training Process</li> <li>Learning Algorithm</li> <li>Model Training</li> <li>Generalization</li> <li>Training Error</li> <li>Test Error</li> <li>Prediction Error</li> <li>Loss Function</li> <li>Cost Function</li> <li>Optimization</li> <li>Gradient Descent</li> <li>Learning Rate</li> <li>Convergence</li> <li>Local Minimum</li> <li>Global Minimum</li> </ol>"},{"location":"learning-graph/concept-list/#neural-networks-246-265","title":"Neural Networks (246-265)","text":"<ol> <li>Neural Networks</li> <li>Artificial Neuron</li> <li>Perceptron</li> <li>Activation Function</li> <li>Sigmoid Function</li> <li>ReLU Function</li> <li>Input Layer</li> <li>Hidden Layer</li> <li>Output Layer</li> <li>Weights</li> <li>Biases</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Deep Learning</li> <li>Network Architecture</li> <li>Epochs</li> <li>Batch Size</li> <li>Mini-batch</li> <li>Stochastic Gradient</li> <li>Vanishing Gradient</li> </ol>"},{"location":"learning-graph/concept-list/#pytorch-266-285","title":"PyTorch (266-285)","text":"<ol> <li>PyTorch Library</li> <li>Tensors</li> <li>Tensor Operations</li> <li>Autograd</li> <li>Automatic Differentiation</li> <li>Computational Graph</li> <li>Neural Network Module</li> <li>Sequential Model</li> <li>Linear Layer</li> <li>Loss Functions PyTorch</li> <li>Optimizer</li> <li>SGD Optimizer</li> <li>Adam Optimizer</li> <li>Training Loop</li> <li>Model Evaluation PyTorch</li> <li>GPU Computing</li> <li>CUDA</li> <li>Model Saving</li> <li>Model Loading</li> <li>Transfer Learning</li> </ol>"},{"location":"learning-graph/concept-list/#advanced-topics-286-295","title":"Advanced Topics (286-295)","text":"<ol> <li>Explainable AI</li> <li>Model Interpretability</li> <li>Feature Importance Analysis</li> <li>SHAP Values</li> <li>Model Documentation</li> <li>Reproducibility</li> <li>Random Seed</li> <li>Version Control</li> <li>Git</li> <li>Data Ethics</li> </ol>"},{"location":"learning-graph/concept-list/#projects-and-applications-296-300","title":"Projects and Applications (296-300)","text":"<ol> <li>Capstone Project</li> <li>End-to-End Pipeline</li> <li>Model Deployment</li> <li>Results Communication</li> <li>Data-Driven Decisions</li> </ol>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>This document defines the categorical taxonomy for organizing the 300 concepts in the learning graph.</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-categories","title":"Taxonomy Categories","text":"Category Name TaxonomyID Description Foundation Concepts FOUND Core data science concepts, definitions, and fundamental terminology Python Environment PYENV Python installation, setup, IDE, package management, and development tools Data Structures DSTRC Python and pandas data structures including DataFrames, lists, arrays Data Cleaning CLEAN Data preprocessing, handling missing values, validation, and transformation Visualization VIZ Data visualization concepts, matplotlib, plotting techniques Statistics STATS Statistical concepts, measures, distributions, and probability Regression REGR Linear regression, model fitting, coefficients, and assumptions Model Evaluation EVAL Model performance metrics, validation, overfitting, cross-validation Advanced Regression ADVR Multiple regression, feature selection, regularization techniques NumPy Computing NUMPY NumPy arrays, matrix operations, linear algebra, vectorization Machine Learning ML Machine learning concepts, training, optimization, gradient descent Neural Networks NN Neural network architecture, layers, activation functions, deep learning PyTorch TORCH PyTorch library, tensors, autograd, training loops Best Practices BEST Explainability, reproducibility, ethics, documentation, version control Projects PROJ Capstone projects, end-to-end pipelines, deployment, communication"},{"location":"learning-graph/concept-taxonomy/#category-descriptions","title":"Category Descriptions","text":""},{"location":"learning-graph/concept-taxonomy/#found-foundation-concepts","title":"FOUND - Foundation Concepts","text":"<p>Core concepts that introduce data science terminology and fundamental ideas. These are typically the first concepts students encounter and form the basis for all other learning.</p>"},{"location":"learning-graph/concept-taxonomy/#pyenv-python-environment","title":"PYENV - Python Environment","text":"<p>Concepts related to setting up and managing the Python development environment, including installation, package managers, virtual environments, and IDEs.</p>"},{"location":"learning-graph/concept-taxonomy/#dstrc-data-structures","title":"DSTRC - Data Structures","text":"<p>Python native data structures (lists, dictionaries, tuples) and pandas structures (DataFrame, Series). These are essential for data manipulation.</p>"},{"location":"learning-graph/concept-taxonomy/#clean-data-cleaning","title":"CLEAN - Data Cleaning","text":"<p>Techniques for preparing data for analysis including handling missing values, detecting outliers, removing duplicates, and transforming data.</p>"},{"location":"learning-graph/concept-taxonomy/#viz-visualization","title":"VIZ - Visualization","text":"<p>Data visualization concepts using matplotlib and seaborn, including various plot types, customization, and best practices for visual communication.</p>"},{"location":"learning-graph/concept-taxonomy/#stats-statistics","title":"STATS - Statistics","text":"<p>Statistical foundations including descriptive statistics, probability, distributions, correlation, and hypothesis testing.</p>"},{"location":"learning-graph/concept-taxonomy/#regr-regression","title":"REGR - Regression","text":"<p>Linear regression concepts from simple to multiple regression, including model fitting, interpretation, and assumptions.</p>"},{"location":"learning-graph/concept-taxonomy/#eval-model-evaluation","title":"EVAL - Model Evaluation","text":"<p>Techniques for assessing model performance, including metrics, train/test splits, cross-validation, and understanding overfitting/underfitting.</p>"},{"location":"learning-graph/concept-taxonomy/#advr-advanced-regression","title":"ADVR - Advanced Regression","text":"<p>Advanced modeling techniques including multiple regression, feature engineering, regularization (Ridge, Lasso), and non-linear models.</p>"},{"location":"learning-graph/concept-taxonomy/#numpy-numpy-computing","title":"NUMPY - NumPy Computing","text":"<p>NumPy library concepts including array operations, broadcasting, vectorization, and linear algebra for efficient computation.</p>"},{"location":"learning-graph/concept-taxonomy/#ml-machine-learning","title":"ML - Machine Learning","text":"<p>Introduction to machine learning paradigms, supervised/unsupervised learning, optimization algorithms, and gradient descent.</p>"},{"location":"learning-graph/concept-taxonomy/#nn-neural-networks","title":"NN - Neural Networks","text":"<p>Artificial neural network concepts including architecture, activation functions, forward/backward propagation, and deep learning.</p>"},{"location":"learning-graph/concept-taxonomy/#torch-pytorch","title":"TORCH - PyTorch","text":"<p>PyTorch-specific concepts including tensors, autograd, neural network modules, optimizers, and training workflows.</p>"},{"location":"learning-graph/concept-taxonomy/#best-best-practices","title":"BEST - Best Practices","text":"<p>Professional practices including explainability, reproducibility, documentation, version control, and ethical considerations.</p>"},{"location":"learning-graph/concept-taxonomy/#proj-projects","title":"PROJ - Projects","text":"<p>Applied concepts for real-world projects including end-to-end pipelines, model deployment, and communicating results.</p>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":"<p>Assessment Date: 2025-12-12 Skill Version: Learning Graph Generator v0.03</p>"},{"location":"learning-graph/course-description-assessment/#scoring-summary","title":"Scoring Summary","text":"Element Status Points Notes Title Present 5/5 \"Introduction to Data Science with Python\" Target Audience Present 5/5 \"Advanced high school students and college freshmen\" Prerequisites Present 5/5 \"Basic algebra and introductory programming experience recommended\" Main Topics Covered Present 10/10 10-week schedule with comprehensive topics Topics Excluded Missing 0/5 No explicit boundaries on what's NOT covered Learning Outcomes Header Present 5/5 \"By the end of this course, students will be able to:\" Remember Level Strong 10/10 4 specific outcomes Understand Level Strong 10/10 4 specific outcomes Apply Level Strong 10/10 4 specific outcomes Analyze Level Strong 10/10 4 specific outcomes Evaluate Level Strong 10/10 4 specific outcomes Create Level Strong 10/10 4 specific outcomes including capstone Descriptive Context Present 5/5 Course philosophy and key learning principles included"},{"location":"learning-graph/course-description-assessment/#overall-quality-score-95100","title":"Overall Quality Score: 95/100","text":""},{"location":"learning-graph/course-description-assessment/#strengths","title":"Strengths","text":"<ul> <li>Excellent Bloom's Taxonomy Coverage: All six cognitive levels are represented with 4+ specific, actionable outcomes at each level</li> <li>Comprehensive Weekly Schedule: 10-week curriculum provides clear learning progression</li> <li>Clear Target Audience: Specifically identifies advanced high school and college freshmen</li> <li>Strong Prerequisites: Appropriately scoped for the target audience</li> <li>Pedagogical Philosophy: Emphasizes explainable AI and model interpretability</li> <li>Assessment Methods: Clear breakdown of formative (60%) and summative (40%) assessment</li> <li>MicroSim Integration: Interactive simulations embedded throughout each week</li> </ul>"},{"location":"learning-graph/course-description-assessment/#areas-for-minor-improvement","title":"Areas for Minor Improvement","text":"<ul> <li>Could add explicit \"Topics NOT Covered\" section to set clear boundaries</li> <li>Example: \"This course does not cover deep learning architectures beyond basic neural networks, time series analysis, natural language processing, or big data technologies like Spark\"</li> </ul>"},{"location":"learning-graph/course-description-assessment/#concept-generation-estimate","title":"Concept Generation Estimate","text":"<p>Based on this course description, approximately 200+ distinct concepts can be derived:</p> Topic Area Estimated Concepts Python fundamentals and environment setup ~15 Data structures and types ~20 Visualization techniques ~25 Statistical foundations ~30 Linear regression ~25 Model evaluation ~25 Advanced regression techniques ~20 NumPy and computation ~15 Machine learning and neural networks ~25 Total ~200"},{"location":"learning-graph/course-description-assessment/#comparison-with-similar-courses","title":"Comparison with Similar Courses","text":"<p>This course description is comparable to or exceeds the quality of similar introductory data science courses in terms of:</p> <ul> <li>Scope and depth of content</li> <li>Clarity of learning objectives</li> <li>Structured progression from simple to complex topics</li> <li>Integration of practical exercises (MicroSims)</li> </ul>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>Proceed with learning graph generation. The quality score of 95/100 exceeds the recommended threshold of 70.</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Foundational Concepts (no dependencies): 2</li> <li>Concepts with Dependencies: 298</li> <li>Average Dependencies per Concept: 1.47</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>1: Data Science</li> <li>2: Python Programming</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 22</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>Data Science (ID: 1)</li> <li>Data (ID: 4)</li> <li>Data Types (ID: 6)</li> <li>Categorical Data (ID: 8)</li> <li>Ordinal Data (ID: 9)</li> <li>Measurement Scales (ID: 11)</li> <li>Descriptive Statistics (ID: 101)</li> <li>Correlation (ID: 126)</li> <li>Regression Analysis (ID: 131)</li> <li>Linear Regression (ID: 132)</li> <li>Simple Linear Regression (ID: 133)</li> <li>Prediction (ID: 143)</li> <li>Residuals (ID: 138)</li> <li>Sum of Squared Errors (ID: 139)</li> <li>Least Squares Method (ID: 137)</li> <li>Cost Function (ID: 239)</li> <li>Loss Function (ID: 238)</li> <li>Loss Functions PyTorch (ID: 275)</li> <li>Training Loop (ID: 279)</li> <li>Model Saving (ID: 283)</li> <li>Model Loading (ID: 284)</li> <li>Transfer Learning (ID: 285)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 121</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>15: Observation</li> <li>20: Data Collection</li> <li>23: Pip</li> <li>24: Conda Environment</li> <li>25: Virtual Environment</li> <li>27: VS Code</li> <li>30: Markdown Cell</li> <li>32: Kernel</li> <li>37: Dictionaries</li> <li>38: Tuples</li> <li>42: Series</li> <li>45: Row</li> <li>48: Read CSV</li> <li>50: Head Method</li> <li>51: Tail Method</li> <li>52: Shape Attribute</li> <li>53: Info Method</li> <li>54: Describe Method</li> <li>59: Dropna Method</li> <li>61: Imputation</li> </ul> <p>...and 101 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 41 DataFrame 14 2 101 Descriptive Statistics 10 3 156 Model Performance 9 4 4 Data 8 5 132 Linear Regression 8 6 5 Variables 7 7 14 Dataset 7 8 79 Axes 7 9 138 Residuals 7 10 181 Multiple Linear Regression 7"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 2 1 169 2 119 3 8 4 2"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (121): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Long dependency chains (22): Ensure students can follow extended learning paths</li> <li>\u2139\ufe0f Consider adding cross-dependencies: More connections could create richer learning pathways</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Number of Taxonomies: 15</li> <li>Average Concepts per Taxonomy: 20.0</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status Statistics STATS 30 10.0% OK Advanced Regression ADVR 30 10.0% OK Visualization VIZ 25 8.3% OK Regression REGR 25 8.3% OK Model Evaluation EVAL 25 8.3% OK Data Structures DSTRC 20 6.7% OK Data Cleaning CLEAN 20 6.7% OK Machine Learning ML 20 6.7% OK Neural Networks NN 20 6.7% OK PyTorch TORCH 20 6.7% OK Foundation Concepts FOUND 19 6.3% OK Python Environment PYENV 15 5.0% OK NumPy Computing NUMPY 15 5.0% OK Best Practices BEST 11 3.7% OK Projects PROJ 5 1.7% Under"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>STATS  #####  30 ( 10.0%)\nADVR   #####  30 ( 10.0%)\nVIZ    ####  25 (  8.3%)\nREGR   ####  25 (  8.3%)\nEVAL   ####  25 (  8.3%)\nDSTRC  ###  20 (  6.7%)\nCLEAN  ###  20 (  6.7%)\nML     ###  20 (  6.7%)\nNN     ###  20 (  6.7%)\nTORCH  ###  20 (  6.7%)\nFOUND  ###  19 (  6.3%)\nPYENV  ##  15 (  5.0%)\nNUMPY  ##  15 (  5.0%)\nBEST   #  11 (  3.7%)\nPROJ      5 (  1.7%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#under-represented-categories-3","title":"Under-Represented Categories (&lt;3%)","text":"<ul> <li>Projects (PROJ): 5 concepts (1.7%)</li> <li>Note: Small categories are acceptable for specialized topics</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#statistics-stats","title":"Statistics (STATS)","text":"<p>Count: 30 concepts (10.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Descriptive Statistics</li> </ol> </li> <li> <ol> <li>Mean</li> </ol> </li> <li> <ol> <li>Median</li> </ol> </li> <li> <ol> <li>Mode</li> </ol> </li> <li> <ol> <li>Range</li> </ol> </li> <li> <ol> <li>Variance</li> </ol> </li> <li> <ol> <li>Standard Deviation</li> </ol> </li> <li> <ol> <li>Quartiles</li> </ol> </li> <li> <ol> <li>Percentiles</li> </ol> </li> <li> <ol> <li>Interquartile Range</li> </ol> </li> <li> <ol> <li>Skewness</li> </ol> </li> <li> <ol> <li>Kurtosis</li> </ol> </li> <li> <ol> <li>Distribution</li> </ol> </li> <li> <ol> <li>Normal Distribution</li> </ol> </li> <li> <ol> <li>Probability</li> </ol> </li> <li>...and 15 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#advanced-regression-advr","title":"Advanced Regression (ADVR)","text":"<p>Count: 30 concepts (10.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Multiple Linear Regression</li> </ol> </li> <li> <ol> <li>Multiple Predictors</li> </ol> </li> <li> <ol> <li>Multicollinearity</li> </ol> </li> <li> <ol> <li>Variance Inflation Factor</li> </ol> </li> <li> <ol> <li>Feature Selection</li> </ol> </li> <li> <ol> <li>Forward Selection</li> </ol> </li> <li> <ol> <li>Backward Elimination</li> </ol> </li> <li> <ol> <li>Stepwise Selection</li> </ol> </li> <li> <ol> <li>Categorical Variables</li> </ol> </li> <li> <ol> <li>Dummy Variables</li> </ol> </li> <li> <ol> <li>One-Hot Encoding</li> </ol> </li> <li> <ol> <li>Interaction Terms</li> </ol> </li> <li> <ol> <li>Polynomial Features</li> </ol> </li> <li> <ol> <li>Feature Engineering</li> </ol> </li> <li> <ol> <li>Feature Importance</li> </ol> </li> <li>...and 15 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#visualization-viz","title":"Visualization (VIZ)","text":"<p>Count: 25 concepts (8.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Data Visualization</li> </ol> </li> <li> <ol> <li>Matplotlib Library</li> </ol> </li> <li> <ol> <li>Figure</li> </ol> </li> <li> <ol> <li>Axes</li> </ol> </li> <li> <ol> <li>Plot Function</li> </ol> </li> <li> <ol> <li>Line Plot</li> </ol> </li> <li> <ol> <li>Scatter Plot</li> </ol> </li> <li> <ol> <li>Bar Chart</li> </ol> </li> <li> <ol> <li>Histogram</li> </ol> </li> <li> <ol> <li>Box Plot</li> </ol> </li> <li> <ol> <li>Pie Chart</li> </ol> </li> <li> <ol> <li>Subplot</li> </ol> </li> <li> <ol> <li>Figure Size</li> </ol> </li> <li> <ol> <li>Title</li> </ol> </li> <li> <ol> <li>Axis Labels</li> </ol> </li> <li>...and 10 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#regression-regr","title":"Regression (REGR)","text":"<p>Count: 25 concepts (8.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Regression Analysis</li> </ol> </li> <li> <ol> <li>Linear Regression</li> </ol> </li> <li> <ol> <li>Simple Linear Regression</li> </ol> </li> <li> <ol> <li>Regression Line</li> </ol> </li> <li> <ol> <li>Slope</li> </ol> </li> <li> <ol> <li>Intercept</li> </ol> </li> <li> <ol> <li>Least Squares Method</li> </ol> </li> <li> <ol> <li>Residuals</li> </ol> </li> <li> <ol> <li>Sum of Squared Errors</li> </ol> </li> <li> <ol> <li>Ordinary Least Squares</li> </ol> </li> <li> <ol> <li>Regression Coefficients</li> </ol> </li> <li> <ol> <li>Coefficient Interpretation</li> </ol> </li> <li> <ol> <li>Prediction</li> </ol> </li> <li> <ol> <li>Fitted Values</li> </ol> </li> <li> <ol> <li>Regression Equation</li> </ol> </li> <li>...and 10 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#model-evaluation-eval","title":"Model Evaluation (EVAL)","text":"<p>Count: 25 concepts (8.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Model Performance</li> </ol> </li> <li> <ol> <li>Training Data</li> </ol> </li> <li> <ol> <li>Testing Data</li> </ol> </li> <li> <ol> <li>Train Test Split</li> </ol> </li> <li> <ol> <li>Validation Data</li> </ol> </li> <li> <ol> <li>R-Squared</li> </ol> </li> <li> <ol> <li>Adjusted R-Squared</li> </ol> </li> <li> <ol> <li>Mean Squared Error</li> </ol> </li> <li> <ol> <li>Root Mean Squared Error</li> </ol> </li> <li> <ol> <li>Mean Absolute Error</li> </ol> </li> <li> <ol> <li>Residual Analysis</li> </ol> </li> <li> <ol> <li>Residual Plot</li> </ol> </li> <li> <ol> <li>Overfitting</li> </ol> </li> <li> <ol> <li>Underfitting</li> </ol> </li> <li> <ol> <li>Bias</li> </ol> </li> <li>...and 10 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#data-structures-dstrc","title":"Data Structures (DSTRC)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Lists</li> </ol> </li> <li> <ol> <li>Dictionaries</li> </ol> </li> <li> <ol> <li>Tuples</li> </ol> </li> <li> <ol> <li>Arrays</li> </ol> </li> <li> <ol> <li>Pandas Library</li> </ol> </li> <li> <ol> <li>DataFrame</li> </ol> </li> <li> <ol> <li>Series</li> </ol> </li> <li> <ol> <li>Index</li> </ol> </li> <li> <ol> <li>Column</li> </ol> </li> <li> <ol> <li>Row</li> </ol> </li> <li> <ol> <li>Data Loading</li> </ol> </li> <li> <ol> <li>CSV Files</li> </ol> </li> <li> <ol> <li>Read CSV</li> </ol> </li> <li> <ol> <li>Data Inspection</li> </ol> </li> <li> <ol> <li>Head Method</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#data-cleaning-clean","title":"Data Cleaning (CLEAN)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Missing Values</li> </ol> </li> <li> <ol> <li>NaN</li> </ol> </li> <li> <ol> <li>Null Detection</li> </ol> </li> <li> <ol> <li>Dropna Method</li> </ol> </li> <li> <ol> <li>Fillna Method</li> </ol> </li> <li> <ol> <li>Imputation</li> </ol> </li> <li> <ol> <li>Data Type Conversion</li> </ol> </li> <li> <ol> <li>Duplicate Detection</li> </ol> </li> <li> <ol> <li>Duplicate Removal</li> </ol> </li> <li> <ol> <li>Outliers</li> </ol> </li> <li> <ol> <li>Outlier Detection</li> </ol> </li> <li> <ol> <li>Data Validation</li> </ol> </li> <li> <ol> <li>String Cleaning</li> </ol> </li> <li> <ol> <li>Column Renaming</li> </ol> </li> <li> <ol> <li>Data Filtering</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#machine-learning-ml","title":"Machine Learning (ML)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Machine Learning</li> </ol> </li> <li> <ol> <li>Supervised Learning</li> </ol> </li> <li> <ol> <li>Unsupervised Learning</li> </ol> </li> <li> <ol> <li>Classification</li> </ol> </li> <li> <ol> <li>Clustering</li> </ol> </li> <li> <ol> <li>Training Process</li> </ol> </li> <li> <ol> <li>Learning Algorithm</li> </ol> </li> <li> <ol> <li>Model Training</li> </ol> </li> <li> <ol> <li>Generalization</li> </ol> </li> <li> <ol> <li>Training Error</li> </ol> </li> <li> <ol> <li>Test Error</li> </ol> </li> <li> <ol> <li>Prediction Error</li> </ol> </li> <li> <ol> <li>Loss Function</li> </ol> </li> <li> <ol> <li>Cost Function</li> </ol> </li> <li> <ol> <li>Optimization</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#neural-networks-nn","title":"Neural Networks (NN)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Neural Networks</li> </ol> </li> <li> <ol> <li>Artificial Neuron</li> </ol> </li> <li> <ol> <li>Perceptron</li> </ol> </li> <li> <ol> <li>Activation Function</li> </ol> </li> <li> <ol> <li>Sigmoid Function</li> </ol> </li> <li> <ol> <li>ReLU Function</li> </ol> </li> <li> <ol> <li>Input Layer</li> </ol> </li> <li> <ol> <li>Hidden Layer</li> </ol> </li> <li> <ol> <li>Output Layer</li> </ol> </li> <li> <ol> <li>Weights</li> </ol> </li> <li> <ol> <li>Biases</li> </ol> </li> <li> <ol> <li>Forward Propagation</li> </ol> </li> <li> <ol> <li>Backpropagation</li> </ol> </li> <li> <ol> <li>Deep Learning</li> </ol> </li> <li> <ol> <li>Network Architecture</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#pytorch-torch","title":"PyTorch (TORCH)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>PyTorch Library</li> </ol> </li> <li> <ol> <li>Tensors</li> </ol> </li> <li> <ol> <li>Tensor Operations</li> </ol> </li> <li> <ol> <li>Autograd</li> </ol> </li> <li> <ol> <li>Automatic Differentiation</li> </ol> </li> <li> <ol> <li>Computational Graph</li> </ol> </li> <li> <ol> <li>Neural Network Module</li> </ol> </li> <li> <ol> <li>Sequential Model</li> </ol> </li> <li> <ol> <li>Linear Layer</li> </ol> </li> <li> <ol> <li>Loss Functions PyTorch</li> </ol> </li> <li> <ol> <li>Optimizer</li> </ol> </li> <li> <ol> <li>SGD Optimizer</li> </ol> </li> <li> <ol> <li>Adam Optimizer</li> </ol> </li> <li> <ol> <li>Training Loop</li> </ol> </li> <li> <ol> <li>Model Evaluation PyTorch</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-found","title":"Foundation Concepts (FOUND)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Data Science</li> </ol> </li> <li> <ol> <li>Python Programming</li> </ol> </li> <li> <ol> <li>Data</li> </ol> </li> <li> <ol> <li>Variables</li> </ol> </li> <li> <ol> <li>Data Types</li> </ol> </li> <li> <ol> <li>Numerical Data</li> </ol> </li> <li> <ol> <li>Categorical Data</li> </ol> </li> <li> <ol> <li>Ordinal Data</li> </ol> </li> <li> <ol> <li>Nominal Data</li> </ol> </li> <li> <ol> <li>Measurement Scales</li> </ol> </li> <li> <ol> <li>Independent Variable</li> </ol> </li> <li> <ol> <li>Dependent Variable</li> </ol> </li> <li> <ol> <li>Dataset</li> </ol> </li> <li> <ol> <li>Observation</li> </ol> </li> <li> <ol> <li>Feature</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#python-environment-pyenv","title":"Python Environment (PYENV)","text":"<p>Count: 15 concepts (5.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Jupyter Notebooks</li> </ol> </li> <li> <ol> <li>Python Installation</li> </ol> </li> <li> <ol> <li>Package Management</li> </ol> </li> <li> <ol> <li>Pip</li> </ol> </li> <li> <ol> <li>Conda Environment</li> </ol> </li> <li> <ol> <li>Virtual Environment</li> </ol> </li> <li> <ol> <li>IDE Setup</li> </ol> </li> <li> <ol> <li>VS Code</li> </ol> </li> <li> <ol> <li>Notebook Cells</li> </ol> </li> <li> <ol> <li>Code Cell</li> </ol> </li> <li> <ol> <li>Markdown Cell</li> </ol> </li> <li> <ol> <li>Cell Execution</li> </ol> </li> <li> <ol> <li>Kernel</li> </ol> </li> <li> <ol> <li>Import Statement</li> </ol> </li> <li> <ol> <li>Python Libraries</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#numpy-computing-numpy","title":"NumPy Computing (NUMPY)","text":"<p>Count: 15 concepts (5.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>NumPy Library</li> </ol> </li> <li> <ol> <li>NumPy Array</li> </ol> </li> <li> <ol> <li>Array Creation</li> </ol> </li> <li> <ol> <li>Array Shape</li> </ol> </li> <li> <ol> <li>Array Indexing</li> </ol> </li> <li> <ol> <li>Array Slicing</li> </ol> </li> <li> <ol> <li>Broadcasting</li> </ol> </li> <li> <ol> <li>Vectorized Operations</li> </ol> </li> <li> <ol> <li>Element-wise Operations</li> </ol> </li> <li> <ol> <li>Matrix Operations</li> </ol> </li> <li> <ol> <li>Dot Product</li> </ol> </li> <li> <ol> <li>Matrix Multiplication</li> </ol> </li> <li> <ol> <li>Transpose</li> </ol> </li> <li> <ol> <li>Linear Algebra</li> </ol> </li> <li> <ol> <li>Computational Efficiency</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#best-practices-best","title":"Best Practices (BEST)","text":"<p>Count: 11 concepts (3.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Documentation</li> </ol> </li> <li> <ol> <li>Explainable AI</li> </ol> </li> <li> <ol> <li>Model Interpretability</li> </ol> </li> <li> <ol> <li>Feature Importance Analysis</li> </ol> </li> <li> <ol> <li>SHAP Values</li> </ol> </li> <li> <ol> <li>Model Documentation</li> </ol> </li> <li> <ol> <li>Reproducibility</li> </ol> </li> <li> <ol> <li>Random Seed</li> </ol> </li> <li> <ol> <li>Version Control</li> </ol> </li> <li> <ol> <li>Git</li> </ol> </li> <li> <ol> <li>Data Ethics</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#projects-proj","title":"Projects (PROJ)","text":"<p>Count: 5 concepts (1.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Capstone Project</li> </ol> </li> <li> <ol> <li>End-to-End Pipeline</li> </ol> </li> <li> <ol> <li>Model Deployment</li> </ol> </li> <li> <ol> <li>Results Communication</li> </ol> </li> <li> <ol> <li>Data-Driven Decisions</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>Excellent balance: Categories are evenly distributed (spread: 8.3%)</li> <li>MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by taxonomy-distribution.py</p>"},{"location":"prompts/","title":"Sample Generative AI Prompts","text":"<p>The following are a list of sample prompts you can use to generate your own Data Science textbook.  Note that these prompts are grouped into three areas:</p> <ol> <li>Level 1 Prompts: Generating a Leaning Graph from a Course Description</li> <li>Level 2 PromptsGenerating an list of chapters and other book-wide content such as glossaries,  FAQs, and book summaries</li> <li>Generating the chapter content and interactive content within a chapter</li> </ol>"},{"location":"prompts/#level-1-prompts","title":"Level 1 Prompts","text":""},{"location":"prompts/#course-description","title":"Course Description","text":"<ul> <li>Course Description</li> </ul>"},{"location":"prompts/#concept-dependencies","title":"Concept Dependencies","text":"<ul> <li>Concept Dependencies</li> </ul>"},{"location":"prompts/#level-2-prompts","title":"Level 2 Prompts","text":"<ul> <li>Chapter Content Prompt</li> <li>FAQs Prompt</li> <li>Glossary Prompt</li> </ul>"},{"location":"prompts/#level-3-prompts","title":"Level 3 Prompts","text":"<ul> <li>MicroSim Prompt</li> </ul>"},{"location":"prompts/chapter-content/","title":"Chapter Content Prompt","text":"<p>See the prompt here</p> <p>run the chapter-content-generator skill on @docs/chapters/12-intro-to-machine-learning/index.md Make the reading   level a   high-school senior.  Make the tone fun and humorous with the theme that this course will give them a data science   superpower.   Make it positive and optimistic.  Make it compelling so the readers are motivated to read the entire book.   When creating the text within a  element, do not indent the text. <p>run the chapter-content-generator skill on @docs/chapters/13-neural-networks/index.md Make the reading level a high-school senior.  Make the tone fun and humorous with the theme that this course will give them a data science  superpower. Make it positive and optimistic.   When creating the text within a  element, do not indent the text.   End with a question on what their capstone data science project will be?"},{"location":"prompts/concept-dependencies/","title":"Concept Dependencies","text":""},{"location":"prompts/concept-dependencies/#prompt","title":"Prompt","text":"<pre><code>That was perfect.  Thank you very much.\nThe next step is to create a dependency graph\nof all the 150 concepts on the list. \nFor each concept, think about other concepts\nthat this concept depends upon for understanding.\n\nFor each of the 150 concepts return a single row in CSV format.\n\nColumn 1 is the ConceptID (the integer 1 to 150)\nColumn 2 is the ConceptLabel\nColumn 3 is the Dependencies in the form of pipe-delimited list of ConceptIDs\n\nMake sure that Foundation Concepts (prerequisites) like knowledge of Python don't have any dependencies.\nMake sure that every concept except Foundation Concepts have at least one dependency.\n</code></pre>"},{"location":"prompts/concept-enumeration/","title":"Concept Enumeration","text":""},{"location":"prompts/concept-enumeration/#prompt","title":"Prompt","text":"<p>``` prompt     I would like to develop a new course called      \"Introduction to Data Science with Python using AI\". </p> <pre><code>I would like you to generate a list of all the core concepts\nthat we would cover in this course and the dependency\nrelationships between these concepts so we could order\nthem and create an learning graph. This course would assume\nthat students have basic Python programming experience. \nThis course would be a class that covers the following topics:\n\n1. What is data science? \n2. Summary of business needs for employees with data science skills. \n3. Review of Python concepts \n4. How generative AI can help you learn Python \n5. Basic Python libraries for handling data such as NumPy \n6. Python statistical libraries \n7. Python libraries for machine learning \n8. Python libraries for working with graph data \n9. Python libraries for visualization with a focus on Plotly \n10. Python libraries for NLP \n11. Python libraries for vector stores \n12. Python libraries for GenAI including LangChain and LlamaIndex\n\nPlease create a numbered list of the 250 most important topics \nthat would be taught in this course. Begin with the prerequisite\nconcepts and then proceed to defining key terms. Then include\nthe core concepts used in Python data analysis.\nFinally, close with some potential final project concepts \nthat students could create. Think carefully about the ordering \nof the concepts from simple to more abstract. For each concept, \nreturn a conceptID followed by short name which would be the\nlabel for the concept in a learning graph (concept dependency).\nFeel free to use short names and abbreviations.\n</code></pre> <p>ChatGPT</p> <p>Thought for 1m 34s</p> <ol> <li>DS Definition - Definition of Data Science</li> <li>DS Importance - Importance of Data Science</li> <li>DS vs DA - Data Science vs. Data Analysis</li> <li>DS Workflow - Data Science Workflow</li> <li>DS Roles - Roles in Data Science</li> <li>DS DataCollection - Data Collection Methods</li> <li>DS DataCleaning - Data Cleaning Processes</li> <li>DS DataExploration - Exploratory Data Analysis</li> <li>DS FeatureEngineering - Feature Engineering</li> <li>DS Modeling - Modeling and Algorithms</li> <li>DS Evaluation - Model Evaluation Techniques</li> <li>DS Deployment - Model Deployment Strategies</li> <li>Business DS Needs - Business Needs for Data Science Skills</li> <li>Business DS Impact - Impact on Decision Making</li> <li>Business DS Examples - Business Applications of Data Science</li> <li>Python DataTypes - Data Types in Python</li> <li>Python DataStructures - Data Structures (List, Dict, Set)</li> <li>Python Control - Control Structures (Loops, Conditionals)</li> <li>Python Functions - Functions and Modules</li> <li>Python OOP - Object-Oriented Programming</li> <li>Python FileHandling - File Handling</li> <li>Python Exceptions - Exception Handling</li> <li>Python Debugging - Debugging Techniques</li> <li>Python LibrariesIntro - Introduction to Python Libraries</li> <li>AI Intro - Introduction to Generative AI</li> <li>AI LearnPython - AI Tools for Learning Python</li> <li>AI CodeAssistants - Using AI Assistants for Coding</li> <li>AI LearningResources - AI-Generated Learning Resources</li> <li>AI Ethics - Ethical Considerations of AI</li> <li>NumPy Intro - Introduction to NumPy</li> <li>NumPy Arrays - NumPy Arrays and Operations</li> <li>NumPy Broadcasting - Broadcasting in NumPy</li> <li>NumPy AdvancedIndexing - Advanced Indexing</li> <li>NumPy MathFunctions - Mathematical Functions</li> <li>Pandas Intro - Introduction to Pandas</li> <li>Pandas DataFrames - DataFrames and Series</li> <li>Pandas IO - Reading and Writing Data</li> <li>Pandas Selection - Data Selection and Filtering</li> <li>Pandas Cleaning - Data Cleaning in Pandas</li> <li>Pandas MissingData - Handling Missing Data</li> <li>Pandas Merging - Merging and Joining DataFrames</li> <li>Pandas Grouping - Data Aggregation and Grouping</li> <li>Pandas Pivot - Pivot Tables</li> <li>Pandas TimeSeries - Time Series Data</li> <li>Stats Intro - Introduction to Statistics</li> <li>Stats Descriptive - Descriptive Statistics</li> <li>Stats Inferential - Inferential Statistics</li> <li>Stats Distributions - Probability Distributions</li> <li>SciPy Intro - SciPy for Statistical Analysis</li> <li>Statsmodels Intro - Statsmodels Library</li> <li>Stats Hypothesis - Hypothesis Testing</li> <li>Stats Regression - Regression Analysis</li> <li>ML Intro - Introduction to Machine Learning</li> <li>ML Supervised - Supervised Learning</li> <li>ML Unsupervised - Unsupervised Learning</li> <li>Sklearn Intro - scikit-learn Library</li> <li>ML Preprocessing - Data Preprocessing</li> <li>ML FeatureSelection - Feature Selection Techniques</li> <li>ML CrossValidation - Cross-Validation Methods</li> <li>ML Classification - Classification Algorithms</li> <li>ML Regression - Regression Algorithms</li> <li>ML Clustering - Clustering Algorithms</li> <li>ML Evaluation - Model Evaluation and Validation</li> <li>NetworkX Intro - NetworkX Library</li> <li>NetworkX Create - Creating Graphs</li> <li>NetworkX Algorithms - Graph Algorithms</li> <li>NetworkX Visualize - Graph Visualization</li> <li>Viz Importance - Importance of Data Visualization</li> <li>Matplotlib Intro - Introduction to Matplotlib</li> <li>Seaborn Intro - Introduction to Seaborn</li> <li>Plotly Intro - Introduction to Plotly</li> <li>Plotly Basic - Basic Plots with Plotly</li> <li>Plotly Interactive - Interactive Visualizations</li> <li>Plotly Dash - Plotly Dash for Web Apps</li> <li>NLP Intro - Introduction to NLP</li> <li>NLP TextPreprocessing - Text Preprocessing Techniques</li> <li>NLP Tokenization - Tokenization Methods</li> <li>NLP WordEmbeddings - Word Embeddings</li> <li>NLTK Intro - NLTK Library</li> <li>Spacy Intro - spaCy Library</li> <li>NLP Sentiment - Sentiment Analysis</li> <li>NLP TextClassify - Text Classification</li> <li>VectorStores Intro - Introduction to Vector Stores</li> <li>Faiss Intro - FAISS Library</li> <li>Annoy Intro - Annoy Library</li> <li>VectorStores Apps - Vector Store Applications</li> <li>GenAI Intro - Introduction to Generative AI</li> <li>LangChain Intro - LangChain Library</li> <li>LangChain Pipelines - Creating Pipelines</li> <li>LangChain COT - Chain-of-thought Prompting</li> <li>LlamaIndex Intro - LlamaIndex Library</li> <li>LlamaIndex Indexing - Indexing Data</li> <li>LlamaIndex Querying - Querying Data</li> <li>Project Ideas - Potential Final Projects Overview</li> <li>Project DataAnalysis - Data Analysis Project</li> <li>Project MLModel - Machine Learning Model Project</li> <li>Project NLP - NLP Application Project</li> <li>Project Visualization - Data Visualization Dashboard</li> <li>Project GraphAnalysis - Graph Analysis Project</li> <li>Project AIChatbot - AI-Powered Chatbot Project</li> <li>Project Planning - Planning a Data Science Project</li> <li>Project Dataset - Choosing Datasets</li> <li>Project Teams - Team Collaboration</li> <li>Project Presentation - Presenting Findings</li> <li>Project Documentation - Project Documentation</li> <li>Project Deployment - Deploying Projects</li> <li>ML DeepLearning - Introduction to Deep Learning</li> <li>TensorFlow Intro - TensorFlow Library</li> <li>PyTorch Intro - PyTorch Library</li> <li>DL NeuralNetworks - Neural Network Fundamentals</li> <li>DL CNNs - Convolutional Neural Networks</li> <li>DL RNNs - Recurrent Neural Networks</li> <li>DL TransferLearning - Transfer Learning</li> <li>DL NLP Transformers - Transformers Architecture</li> <li>DL NLP BERT - BERT Model</li> <li>TimeSeries Intro - Time Series Analysis</li> <li>TimeSeries Decomposition - Time Series Decomposition</li> <li>TimeSeries ARIMA - ARIMA Models</li> <li>TimeSeries Forecasting - Forecasting Techniques</li> <li>Recommender Intro - Recommender Systems</li> <li>Recommender CollabFilter - Collaborative Filtering</li> <li>Recommender ContentBased - Content-Based Filtering</li> <li>Recommender Hybrid - Hybrid Recommenders</li> <li>DataEng Intro - Introduction to Data Engineering</li> <li>DataEng ETL - ETL Processes</li> <li>DataEng DataWarehouses - Data Warehouses</li> <li>DataEng DataLakes - Data Lakes</li> <li>ML Reinforcement - Reinforcement Learning Basics</li> <li>ML GANs - Generative Adversarial Networks</li> <li>ML AutoML - Automated Machine Learning</li> <li>ML Explainability - Model Explainability</li> <li>BigData Intro - Introduction to Big Data</li> <li>PySpark Intro - Introduction to PySpark</li> <li>Cloud Intro - Cloud Computing Basics</li> <li>AWS Intro - AWS Overview</li> <li>Docker Intro - Introduction to Docker</li> <li>Ethics AI - Ethical Considerations in AI</li> <li>Privacy Data - Data Privacy and Security</li> <li>Bias Fairness - Bias and Fairness in ML</li> <li>Git Intro - Introduction to Git</li> <li>GitHub Intro - Using GitHub</li> <li>Viz BestPractices - Visualization Best Practices</li> <li>Dask Intro - Dask for Parallel Computing</li> <li>CaseStudies Intro - Case Studies Overview</li> <li>Communication Skills - Communication Skills</li> <li>CriticalThinking Skills - Critical Thinking</li> <li>Leadership Skills - Leadership in Data Science</li> <li>Final Review - Course Summary and Review</li> <li>Next Steps - Next Steps in Learning</li> <li>Course Evaluation - Course Feedback and Evaluation</li> </ol>"},{"location":"prompts/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>Certainly! Based on the course content you've provided, here is a 10-category taxonomy of the types of concepts in this course:</p> <ol> <li>Foundation Concepts (Prerequisites)</li> <li>Key Terms</li> <li>Python Programming Concepts</li> <li>Python Libraries</li> <li>Data Manipulation and Analysis</li> <li>Statistical Analysis</li> <li>Machine Learning Concepts</li> <li>Data Visualization</li> <li>Natural Language Processing (NLP)</li> <li>Generative AI and Advanced Topics</li> </ol>"},{"location":"prompts/concept-taxonomy/#detailed-taxonomy","title":"Detailed Taxonomy:","text":"<ol> <li> <p>Foundation Concepts (Prerequisites)     Fundamental skills and knowledge required before diving into data science topics, including basic programming concepts and understanding of data types.</p> </li> <li> <p>Key Terms     Essential terminology and definitions used in data science, machine learning, AI, and related fields to build a strong conceptual foundation.</p> </li> <li> <p>Python Programming Concepts     Core Python programming skills necessary for data science, such as control structures, functions, object-oriented programming, file handling, and debugging.</p> </li> <li> <p>Python Libraries     Introduction to important Python libraries used in data science, including NumPy, Pandas, SciPy, Statsmodels, scikit-learn, NetworkX, and more.</p> </li> <li> <p>Data Manipulation and Analysis     Techniques for collecting, cleaning, exploring, and transforming data to prepare it for analysis, including data handling with Pandas and NumPy.</p> </li> <li> <p>Statistical Analysis     Concepts and methods in statistics necessary for analyzing data and making inferences, including descriptive and inferential statistics, probability distributions, and hypothesis testing.</p> </li> <li> <p>Machine Learning Concepts     Understanding of machine learning algorithms, model building, evaluation techniques, and deployment strategies using libraries like scikit-learn.</p> </li> <li> <p>Data Visualization     Tools and techniques for visualizing data to effectively communicate insights, focusing on libraries like Matplotlib, Seaborn, and Plotly.</p> </li> <li> <p>Natural Language Processing (NLP)     Concepts and tools for processing and analyzing textual data, including text preprocessing, tokenization, sentiment analysis, and using libraries like NLTK and spaCy.</p> </li> <li> <p>Generative AI and Advanced Topics     Advanced topics including generative AI, vector stores, deep learning frameworks, and the use of libraries like LangChain and LlamaIndex for building AI applications.</p> </li> </ol> <p>This taxonomy covers the breadth of topics in your course, organizing them into coherent categories that reflect their roles in the learning pathway. Each category groups related concepts together, making it easier to structure the course content and understand the dependencies between topics.</p> <p>o1</p>"},{"location":"prompts/course-description/","title":"Course Description for Introduction to Data Science with Python","text":"<p>Prompt</p> <p>Please generate a detailed course description for a 10-week class on the subject of introduction to data science with Python. The course description should be written in Markdown.  Follow Bloom's 2001 taxonomy.</p> <p>The course description follows an  online textbook about data science using the Python programming language.  The audience is advanced high-school students or freshmen in college.  The focus of the textbook is a series of interactive simulations (MicroSims) that allow the students to interact with data.  We start with simple linear regression and then build increasingly complicated predictive models with Python libraries like NumPy and PyTorch.  The focus is to find the simplest model that combines expandability with prediction fidelity.</p>"},{"location":"prompts/course-description/#response-from-claude-sonnet-4","title":"Response from Claude Sonnet-4","text":"<p>Course Duration: 10 weeks Target Audience: Advanced high school students and college freshmen Prerequisites: Basic algebra and introductory programming experience recommended</p>"},{"location":"prompts/course-description/#course-overview","title":"Course Overview","text":"<p>This introductory course provides students with foundational knowledge and practical skills in data science using Python. Through hands-on experience with interactive simulations (MicroSims) and real-world datasets, students will develop competency in data analysis, visualization, and predictive modeling. The course emphasizes the critical balance between model explainability and predictive accuracy, guiding students to identify the simplest effective solutions to data-driven problems.</p>"},{"location":"prompts/course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <p>Remember (Knowledge) - Recall fundamental data science terminology and concepts - Identify key Python libraries for data science (NumPy, pandas, matplotlib, PyTorch) - Recognize different types of data and measurement scales - List the steps in the data science workflow</p> <p>Understand (Comprehension) - Explain the relationship between independent and dependent variables - Describe how linear regression models make predictions - Interpret basic statistical measures and visualizations - Summarize the trade-offs between model complexity and interpretability</p> <p>Apply (Application) - Implement basic data cleaning and preprocessing techniques - Create visualizations using Python libraries - Build simple linear regression models - Execute standard data science workflows on new datasets</p> <p>Analyze (Analysis) - Examine datasets to identify patterns and relationships - Compare different modeling approaches for the same problem - Distinguish between correlation and causation in data relationships - Evaluate model performance using appropriate metrics</p> <p>Evaluate (Evaluation) - Assess the quality and reliability of data sources - Critique model assumptions and limitations - Judge the appropriateness of different models for specific problems - Validate model performance and identify potential overfitting</p> <p>Create (Synthesis) - Design experiments to test hypotheses using data - Construct predictive models for real-world scenarios - Develop data-driven solutions to complex problems - Generate original insights from exploratory data analysis</p>"},{"location":"prompts/course-description/#weekly-schedule","title":"Weekly Schedule","text":""},{"location":"prompts/course-description/#week-1-foundations-of-data-science","title":"Week 1: Foundations of Data Science","text":"<ul> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> </ul>"},{"location":"prompts/course-description/#week-2-data-exploration-and-visualization","title":"Week 2: Data Exploration and Visualization","text":"<ul> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> </ul>"},{"location":"prompts/course-description/#week-3-statistical-foundations","title":"Week 3: Statistical Foundations","text":"<ul> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> </ul>"},{"location":"prompts/course-description/#week-4-simple-linear-regression","title":"Week 4: Simple Linear Regression","text":"<ul> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> </ul>"},{"location":"prompts/course-description/#week-5-model-evaluation-and-validation","title":"Week 5: Model Evaluation and Validation","text":"<ul> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> </ul>"},{"location":"prompts/course-description/#week-6-multiple-linear-regression","title":"Week 6: Multiple Linear Regression","text":"<ul> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> </ul>"},{"location":"prompts/course-description/#week-7-introduction-to-numpy-and-advanced-computation","title":"Week 7: Introduction to NumPy and Advanced Computation","text":"<ul> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> </ul>"},{"location":"prompts/course-description/#week-8-non-linear-models-and-regularization","title":"Week 8: Non-linear Models and Regularization","text":"<ul> <li>Polynomial regression and feature transformation</li> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> </ul>"},{"location":"prompts/course-description/#week-9-introduction-to-machine-learning-with-pytorch","title":"Week 9: Introduction to Machine Learning with PyTorch","text":"<ul> <li>Neural networks and deep learning concepts</li> <li>Building simple networks with PyTorch</li> <li>MicroSim: Neural network playground</li> <li>Comparing traditional and deep learning approaches</li> </ul>"},{"location":"prompts/course-description/#week-10-capstone-project-and-model-deployment","title":"Week 10: Capstone Project and Model Deployment","text":"<ul> <li>End-to-end data science project</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> </ul>"},{"location":"prompts/course-description/#assessment-methods","title":"Assessment Methods","text":"<p>Formative Assessment (60%) - Weekly MicroSim exercises and reflections (30%) - Homework assignments applying concepts to new datasets (20%) - Peer review activities and collaborative problem-solving (10%)</p> <p>Summative Assessment (40%) - Midterm project: Complete data analysis report (15%) - Final capstone project: Original predictive modeling solution (20%) - Final examination covering theoretical concepts (5%)</p>"},{"location":"prompts/course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Computer with Python 3.8+ installed</li> <li>Access to interactive online textbook with MicroSims</li> <li>Jupyter Notebook environment</li> <li>Required Python packages: pandas, NumPy, matplotlib, scikit-learn, PyTorch</li> </ul>"},{"location":"prompts/course-description/#key-learning-principles","title":"Key Learning Principles","text":"<p>Interactive Learning: Each week features hands-on MicroSims that allow students to manipulate parameters and observe results in real-time, reinforcing theoretical concepts through experiential learning.</p> <p>Scaffolded Complexity: The course progresses systematically from simple linear relationships to complex neural networks, ensuring students build confidence before tackling advanced topics.</p> <p>Explainable AI Focus: Throughout the course, emphasis is placed on understanding and interpreting models rather than simply achieving high accuracy, preparing students for ethical and transparent data science practice.</p> <p>Real-world Applications: All examples and projects use authentic datasets and scenarios, helping students connect academic learning to practical problem-solving.</p>"},{"location":"prompts/course-description/#course-philosophy","title":"Course Philosophy","text":"<p>This course is built on the principle that effective data science requires both technical competence and critical thinking. Students will learn not just how to build predictive models, but when to use them, how to interpret their results, and how to communicate findings to diverse audiences. The integration of interactive simulations ensures that abstract mathematical concepts become concrete and intuitive, while the progression from simple to complex models helps students appreciate the value of parsimony in modeling.</p> <p>By the end of this course, students will have developed both the technical skills and analytical mindset necessary for success in advanced data science coursework or entry-level positions in data-driven fields.</p>"},{"location":"prompts/data-science-language-popularity/","title":"Data Science Popularity","text":"<p>Prompt</p> <p>Please generate a line plot that shows the popularity of different programming languages for use in data science over the last 15 years.  Make sure to include both Python and R.  Document the source of the dataset.  Make time be the horizontal axis and start at 2010.</p>"},{"location":"prompts/data-science-workflow/","title":"Data Science Workflow","text":"<p>Prompt</p> <p>Please generate a new p5.js program for showing a simple linear step-by-step workflow with no feedback.  Use the first horiziontal row in book-gen-workflow.js as your template.</p> <p>Change the steps in the workflow to be the following</p> <ol> <li>Identify Problem</li> <li>Source Data</li> <li>Prepare Data</li> <li>Analyze Data</li> <li>Visualize Data</li> <li>Present Findings</li> </ol> <p>For each of these steps, generate a detailed description of the processes within this step to display in the infobox below the workflow drawing.</p>"},{"location":"prompts/data-scient-heros-journey/","title":"Data Science Quest Image","text":"<p>Given to Google Nano Banana</p> <p>Please create highly detailed wide-landscape image of an old map.  The width:height ratio is 2:1.</p> <p>Use a style of a very old circular journey map with curved dashed paths between locations circumventing mountains, lakes, swamps, ravines and deep dark forests.  Place \"Here there be dragons\" at the edges of the map. The circular journey map used old treasure-map aesthetic with stages arranged in circle with forward arrows on winding paths. </p> <p>Present data science workflow as an engaging hero's journey (like Joseph Campbell books) with circular/iterative flow.</p> <p>Place a title in the center of the circle. \"Data Science Quest\"</p> <ol> <li> <p>Top Element. Label: \"The Call to Adventure\" - Text: \"Where does your curiosity burn?\" Icon: flames Color: Red</p> </li> <li> <p>Label: \"Gathering Allies\" - Treasure chest open with data gold coins with \"1\" and \"0\"s: Text: \"Seek out the data you need\" Color: Gold </p> </li> <li> <p>Right Element: Label: \"Trials and Tribulations\" - Broom sweeping bugs and trash: Text: \"Dirty Data! Tame the mess\" Color: Orange </p> </li> <li> <p>Label: \"The Revelation\" - Magnifying glass with sparkles \"Visualize and explore\" Color: Blue Magnifying glass</p> </li> <li> <p>Bottom Element: Label: \"Forging the Weapon\" - Anvil with hammer text: \"Build your model\" Color: Gray Anvil</p> </li> <li> <p>Label: \"The Ultimate Test\" - Target with arrow \"Does your model work?\": Color: Red/White Target</p> </li> <li> <p>Label: \"Return with the Elixir\" - Scroll with bar graph: \"Share your story\" </p> </li> </ol> <p>Dashed path from 6 to 2 \"Need more data\" Dashed line from 7 to 4 \"No one understands\"</p>"},{"location":"prompts/faqs/","title":"FAQs","text":""},{"location":"prompts/faqs/#prompt","title":"Prompt","text":"<pre><code>I am preparing to teach a new college-level course called \"Introduction to Data Science with Python\".  The prerequisites to this course are Beginning Python,  Algebra and simple statistics.  What are the 100 most common questions that will come up in this course?\n\nPlease return the questions and their answer as a in Markdown format. \nPlace the question text in a level-4 header (####) and the response in the body text.\n</code></pre>"},{"location":"prompts/faqs/#chatgpt-o1-preview-response","title":"ChatGPT o1-preview Response","text":""},{"location":"prompts/faqs/#what-is-data-science-and-why-is-it-important","title":"What is data science, and why is it important?","text":"<p>Data science is an interdisciplinary field that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data. It is important because it enables organizations to make informed decisions, predict trends, and solve complex problems by analyzing large datasets.</p>"},{"location":"prompts/faqs/#how-is-python-used-in-data-science","title":"How is Python used in data science?","text":"<p>Python is widely used in data science due to its simplicity and versatility. It offers a vast ecosystem of libraries and frameworks like NumPy, Pandas, Matplotlib, and scikit-learn, which facilitate data manipulation, analysis, visualization, and machine learning tasks.</p>"},{"location":"prompts/faqs/#what-are-the-key-python-libraries-for-data-analysis","title":"What are the key Python libraries for data analysis?","text":"<p>The key Python libraries for data analysis include:</p> <ul> <li>NumPy: For numerical computing and array operations.</li> <li>Pandas: For data manipulation and analysis using data structures like DataFrames.</li> <li>Matplotlib: For creating static, animated, and interactive visualizations.</li> <li>Seaborn: For statistical data visualization built on top of Matplotlib.</li> <li>scikit-learn: For machine learning algorithms and predictive data analysis.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-import-a-csv-file-into-a-pandas-dataframe","title":"How do you import a CSV file into a Pandas DataFrame?","text":"<p>You can import a CSV file using the <code>read_csv()</code> function from Pandas:</p> <pre><code>Copy code\n\n`import pandas as pd\n\ndf = pd.read_csv('file_name.csv')\n`\n</code></pre> <p>Replace <code>'file_name.csv'</code> with the path to your CSV file.</p>"},{"location":"prompts/faqs/#what-is-a-dataframe-in-pandas","title":"What is a DataFrame in Pandas?","text":"<p>A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). It is similar to a spreadsheet or SQL table and is the primary data structure used in Pandas for data manipulation.</p>"},{"location":"prompts/faqs/#how-do-you-handle-missing-data-in-a-dataset","title":"How do you handle missing data in a dataset?","text":"<p>Missing data can be handled by:</p> <ul> <li>Removing missing values: Using <code>dropna()</code> to remove rows or columns with missing values.</li> <li>Imputing missing values: Using <code>fillna()</code> to replace missing values with a specific value, mean, median, or mode.</li> <li>Interpolate missing values: Using <code>interpolate()</code> to estimate missing values based on other data points.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-difference-between-numpy-arrays-and-python-lists","title":"What is the difference between NumPy arrays and Python lists?","text":"<p>NumPy arrays are fixed-size, homogeneous collections of elements (all of the same data type) optimized for numerical computations. Python lists are dynamic, heterogeneous collections that can contain elements of different data types. NumPy arrays offer better performance for mathematical operations.</p>"},{"location":"prompts/faqs/#how-do-you-select-a-subset-of-data-from-a-dataframe","title":"How do you select a subset of data from a DataFrame?","text":"<p>You can select subsets using:</p> <ul> <li> <p>Label-based indexing with <code>.loc</code>:</p> <pre><code>python\nCopy code\n\n`df_subset = df.loc[row_labels, column_labels]\n`\n</code></pre> </li> <li> <p>Integer-based indexing with <code>.iloc</code>:</p> <pre><code>python\nCopy code\n\n`df_subset = df.iloc[row_indices, column_indices]\n`\n</code></pre> </li> </ul>"},{"location":"prompts/faqs/#what-is-data-visualization-and-why-is-it-important","title":"What is data visualization, and why is it important?","text":"<p>Data visualization is the graphical representation of data to communicate information clearly and efficiently. It is important because it helps identify patterns, trends, and outliers in data, making complex data more accessible and understandable.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-simple-line-plot-using-matplotlib","title":"How do you create a simple line plot using Matplotlib?","text":"<pre><code>python\nCopy code\n\n`import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny = [10, 20, 25, 30]\n\nplt.plot(x, y)\nplt.xlabel('X-axis Label')\nplt.ylabel('Y-axis Label')\nplt.title('Simple Line Plot')\nplt.show()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-groupby-function-in-pandas","title":"What is the purpose of the <code>groupby()</code> function in Pandas?","text":"<p>The <code>groupby()</code> function is used to split data into groups based on some criteria, perform operations on each group independently, and then combine the results. It is useful for aggregation, transformation, and filtration of data.</p>"},{"location":"prompts/faqs/#how-do-you-merge-two-dataframes-in-pandas","title":"How do you merge two DataFrames in Pandas?","text":"<p>You can merge two DataFrames using the <code>merge()</code> function:</p> <pre><code>python\nCopy code\n\n`merged_df = pd.merge(df1, df2, on='common_column')\n`\n</code></pre> <p>Replace <code>'common_column'</code> with the column name that is common to both DataFrames.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-merge-and-concat-in-pandas","title":"What is the difference between <code>merge()</code> and <code>concat()</code> in Pandas?","text":"<ul> <li><code>merge()</code>: Combines two DataFrames based on the values of common columns (similar to SQL joins).</li> <li><code>concat()</code>: Concatenates DataFrames either vertically or horizontally, stacking them along an axis.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-calculate-basic-statistical-measures-like-mean-and-median-in-pandas","title":"How do you calculate basic statistical measures like mean and median in Pandas?","text":"<p>You can use built-in functions:</p> <ul> <li> <p>Mean:</p> <pre><code>python\nCopy code\n\n`mean_value = df['column_name'].mean()\n`\n</code></pre> </li> <li> <p>Median:</p> <pre><code>python\nCopy code\n\n`median_value = df['column_name'].median()\n`\n</code></pre> </li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-apply-function-in-pandas","title":"What is the purpose of the <code>apply()</code> function in Pandas?","text":"<p>The <code>apply()</code> function allows you to apply a function along an axis of the DataFrame (either rows or columns). It is useful for performing complex operations on DataFrame elements.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-pivot-table-in-pandas","title":"How do you create a pivot table in Pandas?","text":"<p>You can create a pivot table using the <code>pivot_table()</code> function:</p> <pre><code>python\nCopy code\n\n`pivot = pd.pivot_table(df, values='value_column', index='index_column', columns='columns_column', aggfunc='mean')\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-supervised-and-unsupervised-learning","title":"What is the difference between supervised and unsupervised learning?","text":"<ul> <li>Supervised Learning: Involves training a model on labeled data, where the target outcome is known. Examples include regression and classification.</li> <li>Unsupervised Learning: Involves finding patterns in unlabeled data without predefined outcomes. Examples include clustering and dimensionality reduction.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-perform-linear-regression-using-scikit-learn","title":"How do you perform linear regression using scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-overfitting-in-machine-learning","title":"What is overfitting in machine learning?","text":"<p>Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor generalization to new, unseen data. It performs well on training data but poorly on test data.</p>"},{"location":"prompts/faqs/#how-can-you-prevent-overfitting","title":"How can you prevent overfitting?","text":"<p>Overfitting can be prevented by:</p> <ul> <li>Cross-validation: Splitting data into training and validation sets.</li> <li>Regularization: Adding penalties for complex models (e.g., Lasso, Ridge).</li> <li>Simplifying the model: Reducing the number of features.</li> <li>Early stopping: Halting training when performance on a validation set starts to degrade.</li> </ul>"},{"location":"prompts/faqs/#what-is-cross-validation","title":"What is cross-validation?","text":"<p>Cross-validation is a technique for assessing how a model will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets, and validating it on the remaining ones.</p>"},{"location":"prompts/faqs/#how-do-you-evaluate-the-performance-of-a-regression-model","title":"How do you evaluate the performance of a regression model?","text":"<p>Common metrics include:</p> <ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-squared (Coefficient of Determination)</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-feature-scaling","title":"What is the purpose of feature scaling?","text":"<p>Feature scaling standardizes the range of independent variables, improving the performance and convergence speed of some machine learning algorithms that are sensitive to the scale of data, such as gradient descent optimization.</p>"},{"location":"prompts/faqs/#how-do-you-perform-feature-scaling-in-python","title":"How do you perform feature scaling in Python?","text":"<p>Using scikit-learn's <code>StandardScaler</code>:</p> <pre><code>python\nCopy code\n\n`from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-one-hot-encoding","title":"What is one-hot encoding?","text":"<p>One-hot encoding is a process of converting categorical variables into a binary (0 or 1) representation. Each category becomes a new column, and a value of 1 indicates the presence of that category.</p>"},{"location":"prompts/faqs/#how-do-you-perform-one-hot-encoding-in-pandas","title":"How do you perform one-hot encoding in Pandas?","text":"<p>Using the <code>get_dummies()</code> function:</p> <pre><code>python\nCopy code\n\n`encoded_df = pd.get_dummies(df, columns=['categorical_column'])\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-confusion-matrix","title":"What is a confusion matrix?","text":"<p>A confusion matrix is a table used to evaluate the performance of a classification model. It displays the true positives, true negatives, false positives, and false negatives, providing insight into the types of errors made by the model.</p>"},{"location":"prompts/faqs/#how-do-you-calculate-accuracy-precision-and-recall-from-a-confusion-matrix","title":"How do you calculate accuracy, precision, and recall from a confusion matrix?","text":"<ul> <li>Accuracy: (TP + TN) / (TP + TN + FP + FN)</li> <li>Precision: TP / (TP + FP)</li> <li>Recall: TP / (TP + FN)</li> </ul> <p>Where TP=True Positives, TN=True Negatives, FP=False Positives, FN=False Negatives.</p>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-train_test_split-function","title":"What is the purpose of the <code>train_test_split</code> function?","text":"<p>The <code>train_test_split</code> function splits a dataset into training and testing sets, allowing you to train the model on one subset and evaluate its performance on another to prevent overfitting.</p>"},{"location":"prompts/faqs/#how-do-you-split-data-into-training-and-testing-sets-in-scikit-learn","title":"How do you split data into training and testing sets in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-classification-and-regression","title":"What is the difference between classification and regression?","text":"<ul> <li>Classification: Predicts categorical labels or classes.</li> <li>Regression: Predicts continuous numerical values.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-categorical-variables-in-machine-learning-models","title":"How do you handle categorical variables in machine learning models?","text":"<p>Categorical variables can be handled by:</p> <ul> <li>Label Encoding: Assigning a unique integer to each category.</li> <li>One-Hot Encoding: Creating binary columns for each category.</li> </ul>"},{"location":"prompts/faqs/#what-is-k-means-clustering","title":"What is k-means clustering?","text":"<p>K-means clustering is an unsupervised learning algorithm that partitions data into k clusters, where each data point belongs to the cluster with the nearest mean. It aims to minimize the within-cluster sum of squares.</p>"},{"location":"prompts/faqs/#how-do-you-determine-the-optimal-number-of-clusters-in-k-means","title":"How do you determine the optimal number of clusters in k-means?","text":"<p>Common methods include:</p> <ul> <li>Elbow Method: Plotting the explained variance as a function of the number of clusters and looking for an \"elbow\" point.</li> <li>Silhouette Score: Measuring how similar a data point is to its own cluster compared to other clusters.</li> </ul>"},{"location":"prompts/faqs/#what-is-principal-component-analysis-pca","title":"What is principal component analysis (PCA)?","text":"<p>PCA is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information by identifying the principal components (directions of maximum variance).</p>"},{"location":"prompts/faqs/#how-do-you-perform-pca-in-scikit-learn","title":"How do you perform PCA in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-bias-variance-tradeoff","title":"What is the bias-variance tradeoff?","text":"<p>The bias-variance tradeoff is the balance between a model's ability to generalize to new data (low variance) and its accuracy on training data (low bias). High bias can lead to underfitting, while high variance can lead to overfitting.</p>"},{"location":"prompts/faqs/#what-is-regularization-in-machine-learning","title":"What is regularization in machine learning?","text":"<p>Regularization involves adding a penalty term to the loss function to prevent overfitting by discouraging complex models. Common regularization techniques include Lasso (L1) and Ridge (L2) regression.</p>"},{"location":"prompts/faqs/#how-do-you-implement-ridge-regression-in-scikit-learn","title":"How do you implement Ridge regression in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-logistic-regression","title":"What is logistic regression?","text":"<p>Logistic regression is a classification algorithm used to predict binary outcomes (0 or 1) by modeling the probability of a certain class using a logistic function.</p>"},{"location":"prompts/faqs/#how-do-you-evaluate-the-performance-of-a-classification-model","title":"How do you evaluate the performance of a classification model?","text":"<p>Common metrics include:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1 Score</li> <li>ROC AUC Score</li> </ul>"},{"location":"prompts/faqs/#what-is-the-receiver-operating-characteristic-roc-curve","title":"What is the Receiver Operating Characteristic (ROC) curve?","text":"<p>The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It illustrates the diagnostic ability of a binary classifier.</p>"},{"location":"prompts/faqs/#how-do-you-handle-imbalanced-datasets","title":"How do you handle imbalanced datasets?","text":"<p>Techniques include:</p> <ul> <li>Resampling Methods: Oversampling the minority class or undersampling the majority class.</li> <li>Synthetic Data Generation: Using methods like SMOTE to generate synthetic examples.</li> <li>Using Appropriate Evaluation Metrics: Focusing on precision, recall, or F1 score instead of accuracy.</li> </ul>"},{"location":"prompts/faqs/#what-is-time-series-analysis","title":"What is time series analysis?","text":"<p>Time series analysis involves analyzing data points collected or recorded at specific time intervals to identify trends, cycles, and seasonal variations for forecasting and other purposes.</p>"},{"location":"prompts/faqs/#how-do-you-deal-with-date-and-time-data-in-pandas","title":"How do you deal with date and time data in Pandas?","text":"<p>Pandas provides the <code>to_datetime()</code> function to convert strings to datetime objects, and you can use datetime properties and methods to manipulate date and time data.</p> <pre><code>python\nCopy code\n\n`df['date_column'] = pd.to_datetime(df['date_column'])\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-autocorrelation-in-time-series-data","title":"What is autocorrelation in time series data?","text":"<p>Autocorrelation is the correlation of a signal with a delayed copy of itself. In time series data, it measures the relationship between a variable's current value and its past values.</p>"},{"location":"prompts/faqs/#how-do-you-perform-forecasting-using-arima-models","title":"How do you perform forecasting using ARIMA models?","text":"<p>Using the <code>statsmodels</code> library:</p> <pre><code>python\nCopy code\n\n`from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(time_series_data, order=(p, d, q))\nmodel_fit = model.fit()\nforecast = model_fit.forecast(steps=10)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-natural-language-processing-nlp","title":"What is natural language processing (NLP)?","text":"<p>NLP is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language.</p>"},{"location":"prompts/faqs/#how-do-you-perform-text-preprocessing-in-nlp","title":"How do you perform text preprocessing in NLP?","text":"<p>Common steps include:</p> <ul> <li>Tokenization: Splitting text into words or sentences.</li> <li>Stop Word Removal: Removing common words that add little meaning.</li> <li>Stemming/Lemmatization: Reducing words to their base or root form.</li> <li>Encoding: Converting text to numerical representation using methods like TF-IDF or word embeddings.</li> </ul>"},{"location":"prompts/faqs/#what-is-tf-idf","title":"What is TF-IDF?","text":"<p>Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects how important a word is to a document in a collection. It balances the frequency of a word in a document with how common the word is across all documents.</p>"},{"location":"prompts/faqs/#how-do-you-handle-large-datasets-that-dont-fit-into-memory","title":"How do you handle large datasets that don't fit into memory?","text":"<p>Techniques include:</p> <ul> <li>Data Sampling: Working with a subset of the data.</li> <li>Batch Processing: Processing data in chunks.</li> <li>Distributed Computing: Using tools like Apache Spark.</li> <li>Out-of-core Learning: Using algorithms that can learn from data incrementally.</li> </ul>"},{"location":"prompts/faqs/#what-is-a-pipeline-in-scikit-learn","title":"What is a pipeline in scikit-learn?","text":"<p>A pipeline is a sequence of data processing steps assembled into a single object. It ensures that all steps are applied consistently during training and testing, simplifying the workflow.</p> <pre><code>python\nCopy code\n\n`from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-save-and-load-trained-models-in-scikit-learn","title":"How do you save and load trained models in scikit-learn?","text":"<p>Using the <code>joblib</code> library:</p> <pre><code>python\nCopy code\n\n`import joblib\n\n# Save model\njoblib.dump(model, 'model.pkl')\n\n# Load model\nmodel = joblib.load('model.pkl')\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-gradient-descent","title":"What is gradient descent?","text":"<p>Gradient descent is an optimization algorithm used to minimize the cost function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-batch-stochastic-and-mini-batch-gradient-descent","title":"What is the difference between batch, stochastic, and mini-batch gradient descent?","text":"<ul> <li>Batch Gradient Descent: Uses the entire dataset to compute gradients.</li> <li>Stochastic Gradient Descent (SGD): Uses one sample at a time.</li> <li>Mini-Batch Gradient Descent: Uses a small subset (batch) of the data.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-multicollinearity-in-regression-analysis","title":"How do you handle multicollinearity in regression analysis?","text":"<p>Techniques include:</p> <ul> <li>Removing correlated features.</li> <li>Principal Component Analysis (PCA) to reduce dimensionality.</li> <li>Regularization methods like Ridge regression.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-central-limit-theorem","title":"What is the Central Limit Theorem?","text":"<p>The Central Limit Theorem states that the sampling distribution of the sample means approaches a normal distribution as the sample size becomes large, regardless of the population's distribution.</p>"},{"location":"prompts/faqs/#what-is-hypothesis-testing","title":"What is hypothesis testing?","text":"<p>Hypothesis testing is a statistical method used to decide whether there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis based on sample data.</p>"},{"location":"prompts/faqs/#what-is-p-value","title":"What is p-value?","text":"<p>A p-value is the probability of observing results at least as extreme as those measured when the null hypothesis is true. A low p-value indicates that the observed data is unlikely under the null hypothesis.</p>"},{"location":"prompts/faqs/#how-do-you-perform-a-t-test-in-python","title":"How do you perform a t-test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>python\nCopy code\n\n`from scipy import stats\n\nt_statistic, p_value = stats.ttest_ind(sample1, sample2)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-type-i-and-type-ii-errors","title":"What is the difference between Type I and Type II errors?","text":"<ul> <li>Type I Error: Rejecting a true null hypothesis (false positive).</li> <li>Type II Error: Failing to reject a false null hypothesis (false negative).</li> </ul>"},{"location":"prompts/faqs/#what-is-an-anova-test","title":"What is an ANOVA test?","text":"<p>Analysis of Variance (ANOVA) is a statistical method used to compare means across three or more groups to see if at least one mean is different from the others.</p>"},{"location":"prompts/faqs/#how-do-you-perform-an-anova-test-in-python","title":"How do you perform an ANOVA test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>python\nCopy code\n\n`from scipy import stats\n\nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-bootstrapping-in-statistics","title":"What is bootstrapping in statistics?","text":"<p>Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It allows estimation of the sampling distribution of almost any statistic.</p>"},{"location":"prompts/faqs/#what-is-the-law-of-large-numbers","title":"What is the law of large numbers?","text":"<p>The law of large numbers states that as the number of trials increases, the sample mean will converge to the expected value (population mean).</p>"},{"location":"prompts/faqs/#what-is-a-probability-distribution","title":"What is a probability distribution?","text":"<p>A probability distribution describes how the values of a random variable are distributed. It defines the probabilities of different outcomes.</p>"},{"location":"prompts/faqs/#what-are-common-probability-distributions-used-in-data-science","title":"What are common probability distributions used in data science?","text":"<ul> <li>Normal Distribution</li> <li>Binomial Distribution</li> <li>Poisson Distribution</li> <li>Exponential Distribution</li> </ul>"},{"location":"prompts/faqs/#how-do-you-generate-random-numbers-following-a-normal-distribution-in-numpy","title":"How do you generate random numbers following a normal distribution in NumPy?","text":"<pre><code>python\nCopy code\n\n`import numpy as np\n\nrandom_numbers = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-curse-of-dimensionality","title":"What is the curse of dimensionality?","text":"<p>The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces, often leading to challenges like overfitting and increased computational cost.</p>"},{"location":"prompts/faqs/#how-do-you-reduce-dimensionality","title":"How do you reduce dimensionality?","text":"<p>Techniques include:</p> <ul> <li>Feature Selection: Choosing a subset of relevant features.</li> <li>Feature Extraction: Transforming data into a lower-dimensional space (e.g., PCA).</li> </ul>"},{"location":"prompts/faqs/#what-is-the-difference-between-bagging-and-boosting","title":"What is the difference between bagging and boosting?","text":"<ul> <li>Bagging: Combines multiple models (usually of the same type) trained on different subsets of the data to reduce variance.</li> <li>Boosting: Sequentially trains models, where each new model focuses on correcting errors made by previous ones, reducing bias.</li> </ul>"},{"location":"prompts/faqs/#what-is-a-decision-tree","title":"What is a decision tree?","text":"<p>A decision tree is a flowchart-like structure used for classification and regression that splits data into branches based on feature values to make predictions.</p>"},{"location":"prompts/faqs/#how-do-you-prevent-a-decision-tree-from-overfitting","title":"How do you prevent a decision tree from overfitting?","text":"<p>By:</p> <ul> <li>Pruning: Removing branches that have little power in predicting target variables.</li> <li>Setting a maximum depth: Limiting the depth of the tree.</li> <li>Setting a minimum number of samples per leaf.</li> </ul>"},{"location":"prompts/faqs/#what-is-random-forest","title":"What is random forest?","text":"<p>Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.</p>"},{"location":"prompts/faqs/#how-do-you-implement-a-random-forest-classifier-in-scikit-learn","title":"How do you implement a random forest classifier in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-ensemble-learning","title":"What is ensemble learning?","text":"<p>Ensemble learning combines predictions from multiple machine learning algorithms to produce a more accurate prediction than any individual model.</p>"},{"location":"prompts/faqs/#what-is-a-neural-network","title":"What is a neural network?","text":"<p>A neural network is a computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons) that can learn complex patterns through training.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-deep-learning-and-machine-learning","title":"What is the difference between deep learning and machine learning?","text":"<ul> <li>Machine Learning: Involves algorithms that parse data, learn from it, and make decisions.</li> <li>Deep Learning: A subset of machine learning using neural networks with multiple layers to model complex patterns.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-class-imbalance-in-classification-problems","title":"How do you handle class imbalance in classification problems?","text":"<p>By:</p> <ul> <li>Resampling the dataset: Oversampling the minority class or undersampling the majority class.</li> <li>Using appropriate evaluation metrics: Such as ROC AUC, precision-recall curve.</li> <li>Using algorithms that handle imbalance: Like XGBoost.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-map-function-in-pandas","title":"What is the purpose of the <code>map()</code> function in Pandas?","text":"<p>The <code>map()</code> function is used to map values of a Series according to an input mapping or function, useful for substituting values or applying a function element-wise.</p>"},{"location":"prompts/faqs/#how-do-you-sort-a-dataframe-in-pandas","title":"How do you sort a DataFrame in Pandas?","text":"<p>Using the <code>sort_values()</code> function:</p> <pre><code>python\nCopy code\n\n`sorted_df = df.sort_values(by='column_name', ascending=True)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-apply-and-applymap-in-pandas","title":"What is the difference between <code>apply()</code> and <code>applymap()</code> in Pandas?","text":"<ul> <li><code>apply()</code>: Applies a function along an axis of the DataFrame (rows or columns).</li> <li><code>applymap()</code>: Applies a function element-wise to the entire DataFrame.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-remove-duplicates-from-a-dataframe","title":"How do you remove duplicates from a DataFrame?","text":"<p>Using the <code>drop_duplicates()</code> function:</p> <pre><code>python\nCopy code\n\n`df_unique = df.drop_duplicates()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-an-outlier-and-how-do-you-detect-them","title":"What is an outlier, and how do you detect them?","text":"<p>An outlier is a data point significantly different from others. Detection methods include:</p> <ul> <li>Statistical methods: Using Z-scores or IQR.</li> <li>Visualization: Box plots or scatter plots.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-outliers-in-data","title":"How do you handle outliers in data?","text":"<p>By:</p> <ul> <li>Removing them: If they are errors.</li> <li>Transforming data: Using log or square root transformations.</li> <li>Using robust algorithms: That are less sensitive to outliers.</li> </ul>"},{"location":"prompts/faqs/#what-is-data-normalization","title":"What is data normalization?","text":"<p>Data normalization scales numerical data into a specific range, typically [0,1], ensuring that each feature contributes equally to the analysis.</p>"},{"location":"prompts/faqs/#how-do-you-perform-data-normalization-in-scikit-learn","title":"How do you perform data normalization in scikit-learn?","text":"<p>Using <code>MinMaxScaler</code>:</p> <pre><code>python\nCopy code\n\n`from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(data)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-heatmap-and-when-do-you-use-it","title":"What is a heatmap, and when do you use it?","text":"<p>A heatmap is a graphical representation of data where individual values are represented as colors. It is used to visualize correlation matrices or to display patterns in data.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-heatmap-in-seaborn","title":"How do you create a heatmap in Seaborn?","text":"<pre><code>python\nCopy code\n\n`import seaborn as sns\n\nsns.heatmap(data, annot=True)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-pairplot-in-seaborn","title":"What is a pairplot in Seaborn?","text":"<p>A pairplot creates a grid of Axes such that each variable in the data is shared across the y-axes across a single row and the x-axes across a single column, visualizing pairwise relationships.</p> <pre><code>python\nCopy code\n\n`sns.pairplot(df)\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-interpret-a-correlation-coefficient","title":"How do you interpret a correlation coefficient?","text":"<p>A correlation coefficient measures the strength and direction of a linear relationship between two variables. Values range from -1 to 1:</p> <ul> <li>-1: Perfect negative correlation.</li> <li>0: No correlation.</li> <li>1: Perfect positive correlation.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-describe-function-in-pandas","title":"What is the purpose of the <code>describe()</code> function in Pandas?","text":"<p>The <code>describe()</code> function generates descriptive statistics, including count, mean, standard deviation, min, max, and quartile values for numerical columns.</p>"},{"location":"prompts/faqs/#how-do-you-handle-datetime-features-for-machine-learning-models","title":"How do you handle date/time features for machine learning models?","text":"<p>By extracting meaningful components:</p> <ul> <li>Year, Month, Day</li> <li>Weekday</li> <li>Hour, Minute, Second</li> <li>Time since a specific date</li> </ul>"},{"location":"prompts/faqs/#what-is-the-difference-between-loc-and-iloc-in-pandas","title":"What is the difference between <code>.loc</code> and <code>.iloc</code> in Pandas?","text":"<ul> <li><code>.loc</code>: Label-based indexing to select data by row and column labels.</li> <li><code>.iloc</code>: Integer-based indexing to select data by row and column positions.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-rename-columns-in-a-dataframe","title":"How do you rename columns in a DataFrame?","text":"<p>Using the <code>rename()</code> function:</p> <pre><code>python\nCopy code\n\n`df.rename(columns={'old_name': 'new_name'}, inplace=True)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-astype-function-in-pandas","title":"What is the purpose of the <code>astype()</code> function in Pandas?","text":"<p>The <code>astype()</code> function is used to change the data type of a Series or DataFrame columns.</p> <pre><code>python\nCopy code\n\n`df['column_name'] = df['column_name'].astype('float')\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-detect-missing-values-in-a-dataframe","title":"How do you detect missing values in a DataFrame?","text":"<p>Using <code>isnull()</code> or <code>isna()</code> functions:</p> <pre><code>python\nCopy code\n\n`missing_values = df.isnull().sum()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-an-ensemble-method-in-machine-learning","title":"What is an ensemble method in machine learning?","text":"<p>An ensemble method combines predictions from multiple machine learning models to improve performance over a single model. Examples include Random Forest, Gradient Boosting.</p>"},{"location":"prompts/faqs/#how-do-you-implement-gradient-boosting-in-scikit-learn","title":"How do you implement Gradient Boosting in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-xgboost","title":"What is XGBoost?","text":"<p>XGBoost (Extreme Gradient Boosting) is an optimized gradient boosting library designed to be highly efficient, flexible, and portable, widely used for its performance and speed.</p>"},{"location":"prompts/faqs/#how-do-you-install-and-use-xgboost-in-python","title":"How do you install and use XGBoost in Python?","text":"<p>Install using pip:</p> <pre><code>bash\nCopy code\n\n`pip install xgboost\n`\n</code></pre> <p>Use in code:</p> <pre><code>python\nCopy code\n\n`import xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-cross-entropy-loss","title":"What is cross-entropy loss?","text":"<p>Cross-entropy loss measures the performance of a classification model whose output is a probability between 0 and 1. It increases as the predicted probability diverges from the actual label.</p>"},{"location":"prompts/faqs/#how-do-you-calculate-the-learning-rate-in-gradient-descent","title":"How do you calculate the learning rate in gradient descent?","text":"<p>The learning rate is a hyperparameter that you set manually. It determines the step size at each iteration while moving toward a minimum of a loss function.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-epochs-batches-and-iterations","title":"What is the difference between epochs, batches, and iterations?","text":"<ul> <li>Epoch: One complete pass through the entire training dataset.</li> <li>Batch: A subset of the training data used in one iteration.</li> <li>Iteration: One update of the model's parameters.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-perform-hyperparameter-tuning","title":"How do you perform hyperparameter tuning?","text":"<p>By:</p> <ul> <li>Grid Search: Exhaustively searching through a specified subset of hyperparameters.</li> <li>Random Search: Randomly sampling hyperparameter combinations.</li> <li>Bayesian Optimization: Using probabilistic models to select hyperparameters.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-pipeline-module-in-scikit-learn","title":"What is the purpose of the <code>pipeline</code> module in scikit-learn?","text":"<p>It allows you to chain preprocessing steps and estimators in a sequential manner, ensuring consistent application of transformations during training and testing.</p>"},{"location":"prompts/faqs/#how-do-you-evaluate-a-clustering-algorithm","title":"How do you evaluate a clustering algorithm?","text":"<p>Using metrics like:</p> <ul> <li>Silhouette Score</li> <li>Calinski-Harabasz Index</li> <li>Davies-Bouldin Index</li> </ul>"},{"location":"prompts/faqs/#what-is-a-dummy-variable-trap","title":"What is a dummy variable trap?","text":"<p>The dummy variable trap occurs when multicollinearity is introduced in a regression model due to the inclusion of dummy variables that are linearly dependent. It can be avoided by dropping one dummy variable.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-correlation-matrix-in-pandas","title":"How do you create a correlation matrix in Pandas?","text":"<pre><code>python\nCopy code\n\n`corr_matrix = df.corr()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-an-roc-curve-and-how-do-you-plot-it","title":"What is an ROC curve, and how do you plot it?","text":"<p>An ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate at various threshold settings. You can plot it using scikit-learn:</p> <pre><code>python\nCopy code\n\n`from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nplt.plot(fpr, tpr)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-type-i-error","title":"What is a type I error?","text":"<p>A type I error occurs when the null hypothesis is true, but we incorrectly reject it (false positive).</p>"},{"location":"prompts/faqs/#what-is-a-type-ii-error","title":"What is a type II error?","text":"<p>A type II error occurs when the null hypothesis is false, but we fail to reject it (false negative).</p>"},{"location":"prompts/faqs/#how-do-you-calculate-the-p-value-in-a-hypothesis-test","title":"How do you calculate the p-value in a hypothesis test?","text":"<p>Using statistical tests from libraries like <code>scipy.stats</code>, which return the p-value as part of the output.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-parametric-and-non-parametric-tests","title":"What is the difference between parametric and non-parametric tests?","text":"<ul> <li>Parametric Tests: Assume underlying statistical distributions (e.g., t-test).</li> <li>Non-Parametric Tests: Do not assume any specific distribution (e.g., Mann-Whitney U test).</li> </ul>"},{"location":"prompts/faqs/#how-do-you-perform-a-chi-squared-test-in-python","title":"How do you perform a chi-squared test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>python\nCopy code\n\n`from scipy.stats import chi2_contingency\n\nchi2, p, dof, expected = chi2_contingency(observed_values)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-seaborn-library","title":"What is the purpose of the <code>seaborn</code> library?","text":"<p>Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-box-plot-in-seaborn","title":"How do you create a box plot in Seaborn?","text":"<pre><code>python\nCopy code\n\n`import seaborn as sns\n\nsns.boxplot(x='categorical_column', y='numerical_column', data=df)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-violin-plot","title":"What is a violin plot?","text":"<p>A violin plot combines a box plot and a kernel density plot to provide a richer depiction of the data distribution.</p>"},{"location":"prompts/faqs/#how-do-you-handle-multivariate-time-series-data","title":"How do you handle multivariate time series data?","text":"<p>By:</p> <ul> <li>Using models designed for multivariate data: Like VAR (Vector Autoregression).</li> <li>Feature engineering: Creating lag features for each variable.</li> </ul>"},{"location":"prompts/faqs/#what-is-an-ab-test","title":"What is an A/B test?","text":"<p>An A/B test is an experiment comparing two variants (A and B) to determine which one performs better regarding a specific metric.</p>"},{"location":"prompts/faqs/#how-do-you-analyze-ab-test-results","title":"How do you analyze A/B test results?","text":"<p>By:</p> <ul> <li>Calculating the difference in metrics between groups.</li> <li>Performing statistical tests: Like t-tests or chi-squared tests.</li> <li>Checking for statistical significance: Using p-values and confidence intervals.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-bonferroni-correction","title":"What is the Bonferroni correction?","text":"<p>A method to adjust p-values when multiple comparisons are made to reduce the chances of obtaining false-positive results (Type I errors).</p>"},{"location":"prompts/faqs/#what-is-survivorship-bias","title":"What is survivorship bias?","text":"<p>Survivorship bias occurs when analyses are conducted only on surviving subjects, leading to skewed results due to the overlooking of those that did not survive or were not included.</p>"},{"location":"prompts/faqs/#what-is-data-leakage","title":"What is data leakage?","text":"<p>Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates that won't generalize to new data.</p>"},{"location":"prompts/faqs/#how-do-you-prevent-data-leakage","title":"How do you prevent data leakage?","text":"<p>By:</p> <ul> <li>Separating training and testing data properly.</li> <li>Performing data preprocessing steps within cross-validation.</li> <li>Avoiding using future data in model training.</li> </ul>"},{"location":"prompts/faqs/#what-is-a-hash-table","title":"What is a hash table?","text":"<p>A hash table is a data structure that implements an associative array, mapping keys to values using a hash function to compute an index into an array of buckets.</p>"},{"location":"prompts/faqs/#what-is-memoization","title":"What is memoization?","text":"<p>Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.</p>"},{"location":"prompts/faqs/#how-do-you-reverse-a-linked-list","title":"How do you reverse a linked list?","text":"<p>By iterating through the list and reversing the pointers of each node to point to the previous node.</p>"},{"location":"prompts/faqs/#what-is-the-time-complexity-of-quicksort","title":"What is the time complexity of quicksort?","text":"<p>The average time complexity is O(n log n), while the worst-case is O(n^2).</p>"},{"location":"prompts/faqs/#what-is-a-generator-in-python","title":"What is a generator in Python?","text":"<p>A generator is a special type of function that returns an iterator object which can iterate over a sequence of values. It uses the <code>yield</code> keyword.</p> <pre><code>python\nCopy code\n\n`def my_generator():\n    yield value\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-handle-exceptions-in-python","title":"How do you handle exceptions in Python?","text":"<p>Using try-except blocks:</p> <pre><code>python\nCopy code\n\n`try:\n    # Code that may raise an exception\nexcept ExceptionType as e:\n    # Code to handle the exception\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-decorator-in-python","title":"What is a decorator in Python?","text":"<p>A decorator is a function that modifies the behavior of another function or method. It allows for the addition of functionality to existing code in a modular way.</p> <pre><code>python\nCopy code\n\n`def decorator_function(func):\n    def wrapper():\n        # Code before function call\n        func()\n        # Code after function call\n    return wrapper\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-read-and-write-json-files-in-python","title":"How do you read and write JSON files in Python?","text":"<p>Using the <code>json</code> module:</p> <pre><code>python\nCopy code\n\n`import json\n\n# Read JSON\nwith open('file.json', 'r') as f:\n    data = json.load(f)\n\n# Write JSON\nwith open('file.json', 'w') as f:\n    json.dump(data, f)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-multithreading-and-how-do-you-implement-it-in-python","title":"What is multithreading, and how do you implement it in Python?","text":"<p>Multithreading allows concurrent execution of threads (lightweight processes) to improve performance. In Python, you can use the <code>threading</code> module:</p> <pre><code>python\nCopy code\n\n`import threading\n\ndef function_to_run():\n    pass\n\nthread = threading.Thread(target=function_to_run)\nthread.start()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-global-interpreter-lock-gil-in-python","title":"What is the Global Interpreter Lock (GIL) in Python?","text":"<p>The GIL is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. It simplifies memory management but can limit performance in CPU-bound multi-threaded programs.</p>"},{"location":"prompts/faqs/#how-do-you-handle-file-operations-in-python","title":"How do you handle file operations in Python?","text":"<p>Using <code>open()</code>:</p> <pre><code>python\nCopy code\n\n`with open('file.txt', 'r') as file:\n    content = file.read()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-are-lambda-functions-in-python","title":"What are lambda functions in Python?","text":"<p>Lambda functions are anonymous functions defined using the <code>lambda</code> keyword, useful for short, simple functions.</p> <pre><code>python\nCopy code\n\n`add = lambda x, y: x + y\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-work-with-databases-in-python","title":"How do you work with databases in Python?","text":"<p>By using database connectors and ORMs like:</p> <ul> <li>SQLite: Using <code>sqlite3</code> module.</li> <li>MySQL: Using <code>mysql-connector-python</code>.</li> <li>ORMs: Using libraries like SQLAlchemy.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-virtual-environments-in-python","title":"What is the purpose of virtual environments in Python?","text":"<p>Virtual environments allow you to create isolated Python environments with specific packages and dependencies, preventing conflicts between projects.</p> <pre><code>bash\nCopy code\n\n`python -m venv myenv\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-install-packages-in-python","title":"How do you install packages in Python?","text":"<p>Using <code>pip</code>:</p> <pre><code>bash\nCopy code\n\n`pip install package_name\n`\n</code></pre>"},{"location":"prompts/faqs/#what-are-the-common-data-types-in-python","title":"What are the common data types in Python?","text":"<ul> <li>Numeric Types: int, float, complex</li> <li>Sequence Types: list, tuple, range</li> <li>Text Type: str</li> <li>Mapping Type: dict</li> <li>Set Types: set, frozenset</li> <li>Boolean Type: bool</li> </ul>"},{"location":"prompts/faqs/#how-do-you-create-a-class-in-python","title":"How do you create a class in Python?","text":"<pre><code>python\nCopy code\n\n`class MyClass:\n    def __init__(self, attribute):\n        self.attribute = attribute\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-inheritance-in-python","title":"What is inheritance in Python?","text":"<p>Inheritance allows a class (child) to inherit attributes and methods from another class (parent), promoting code reusability.</p> <pre><code>python\nCopy code\n\n`class ChildClass(ParentClass):\n    pass\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-polymorphism-in-python","title":"What is polymorphism in Python?","text":"<p>Polymorphism allows methods to have the same name but behave differently in different classes. It enables methods to be used interchangeably.</p> <p>o1</p>"},{"location":"prompts/generate-microsims/","title":"Generate List of MicroSims for Data Science","text":"<p>Prompt</p> <p>You are in expert at creating great instructional designs using interactive simulations.</p> <p>Please create a list of the 100 most difficult concepts to learn a course called \"Introduction to Data Science with Python\" and suggest a detailed list of MicroSims that could be used to explain these concepts to students.</p> <p>MicroSims are interactive in-browser simulations and animations that allow students to click on the screen and learn data science concepts.  They are often written in JavaScript libraries like p5.js.  MicroSims have input controls like buttons and sliders that students can click.  We can add logging tools to these controls to see what events the student clicked on.</p> <p>An example of a data-science MicroSim is a \"least square\" MicroSim that allows the student to change the slop and intercept which will minimize the area of squares on a graph. https://dmccreary.github.io/microsims/sims/least-squares/main.html</p> <p>For each MicroSim generate:</p> <ol> <li>The Name of the MicroSim in Level 2 Markdown Header</li> <li>A short description of the MicroSim in level 3 markdown</li> <li>The learning goals of the MicroSim in level 3 markdown</li> <li>A list of the input controls that the student could change in a numbered list</li> </ol> <p>Focus on good instructional design and step-by-step learning from simple to complex.  Begin with simple MicroSims and move on to more complex MicroSims. </p>"},{"location":"prompts/generate-microsims/#results-in-chatgpt-using-gpt-5","title":"Results in ChatGPT using GPT-5","text":"<p>100 data science microsims</p>"},{"location":"prompts/glossary/","title":"Glossary Generation","text":""},{"location":"prompts/glossary/#prompt","title":"Prompt","text":"<p>Prompt</p> <p>Please generate an glossary of terms for the 250 most frequently used terms used in an introductory course  in introduction to data science with Python. Make sure to include the names of Python libraries used in data science. The target audience for this glossary is college freshmen. Each term should have a term name in a level-4 markdown (####) and the definition placed in the body text. Do not use the term in the definition of the term. The definition should be precise, concise and distinct.</p> <p>If appropriate, create an Example: of how that term is used in the Introduction to Data Science with Python course.  Do not place a newline after the Example: Return the glossary of terms in alphabetical order.</p> <p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"prompts/glossary/#accuracy","title":"Accuracy","text":"<p>A metric used to evaluate classification models, representing the proportion of correct predictions over the total number of predictions.</p> <p>Example: In a spam email classifier, if the model correctly identifies 90 out of 100 emails, the accuracy is 90%.</p>"},{"location":"prompts/glossary/#algorithm","title":"Algorithm","text":"<p>A step-by-step procedure or set of rules designed to perform a specific task or solve a problem.</p> <p>Example: Implementing the k-means clustering algorithm to group similar data points in an unsupervised learning task.</p>"},{"location":"prompts/glossary/#anaconda","title":"Anaconda","text":"<p>A free and open-source distribution of Python and R programming languages for scientific computing and data science.</p> <p>Example: Using Anaconda to manage Python packages and environments for data analysis projects in the course.</p>"},{"location":"prompts/glossary/#analytics","title":"Analytics","text":"<p>The examining data sets to draw conclusions about the information they contain, often using specialized software and statistical techniques.</p> <p>Example: Performing customer behavior analytics using Pandas and Matplotlib to improve marketing strategies.</p> <p>```</p>"},{"location":"prompts/glossary/#chatgpt-o1-preview-response","title":"ChatGPT o1-preview Response","text":""},{"location":"prompts/glossary/#accuracy_1","title":"Accuracy","text":"<p>A metric used to evaluate classification models, representing the proportion of correct predictions over the total number of predictions.</p> <p>Example: In a spam email classifier, if the model correctly identifies 90 out of 100 emails, the accuracy is 90%.</p>"},{"location":"prompts/glossary/#algorithm_1","title":"Algorithm","text":"<p>A step-by-step procedure or set of rules designed to perform a specific task or solve a problem.</p> <p>Example: Implementing the k-means clustering algorithm to group similar data points in an unsupervised learning task.</p>"},{"location":"prompts/glossary/#anaconda_1","title":"Anaconda","text":"<p>A free and open-source distribution of Python and R programming languages for scientific computing and data science.</p> <p>Example: Using Anaconda to manage Python packages and environments for data analysis projects in the course.</p>"},{"location":"prompts/glossary/#analytics_1","title":"Analytics","text":"<p>The examining data sets to draw conclusions about the information they contain, often using specialized software and statistical techniques.</p> <p>Example: Performing customer behavior analytics using Pandas and Matplotlib to improve marketing strategies.</p>"},{"location":"prompts/glossary/#api-application-programming-interface","title":"API (Application Programming Interface)","text":"<p>An API is a set of protocols and tools that allow different software applications to communicate with each other.</p> <p>Example: Utilizing the Twitter API to collect real-time tweets for sentiment analysis in Python.</p>"},{"location":"prompts/glossary/#array","title":"Array","text":"<p>An array is a data structure that stores a collection of items at contiguous memory locations, allowing for efficient indexing.</p> <p>Example: Using NumPy arrays to perform vectorized operations for faster numerical computations.</p>"},{"location":"prompts/glossary/#artificial-intelligence-ai","title":"Artificial Intelligence (AI)","text":"<p>AI is the simulation of human intelligence processes by machines, especially computer systems, enabling them to perform tasks that typically require human intelligence.</p> <p>Example: Exploring AI concepts by implementing machine learning models that can recognize images or understand natural language.</p>"},{"location":"prompts/glossary/#attribute","title":"Attribute","text":"<p>An attribute refers to a variable or feature in a dataset that represents a characteristic of the data points.</p> <p>Example: In a dataset of cars, attributes might include horsepower, weight, and fuel efficiency.</p>"},{"location":"prompts/glossary/#auc-area-under-the-curve","title":"AUC (Area Under the Curve)","text":"<p>AUC is a performance metric for classification models, representing the area under the Receiver Operating Characteristic (ROC) curve.</p> <p>Example: Comparing models by evaluating their AUC scores to determine which has better classification performance.</p>"},{"location":"prompts/glossary/#bagging","title":"Bagging","text":"<p>Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that improves model stability and accuracy by combining predictions from multiple models trained on random subsets of the data.</p> <p>Example: Implementing bagging with decision trees to reduce variance and prevent overfitting in the course project.</p>"},{"location":"prompts/glossary/#bar-chart","title":"Bar Chart","text":"<p>A bar chart is a graphical representation of data using rectangular bars to show the frequency or value of different categories.</p> <p>Example: Creating a bar chart with Matplotlib to visualize the count of different species in an ecological dataset.</p>"},{"location":"prompts/glossary/#bias","title":"Bias","text":"<p>Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a much simpler model.</p> <p>Example: Recognizing high bias in a linear model that underfits the data during regression analysis.</p>"},{"location":"prompts/glossary/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The bias-variance tradeoff is the balance between a model's ability to generalize to new data (variance) and its accuracy on the training data (bias).</p> <p>Example: Adjusting the complexity of a model to find the optimal point where both bias and variance are minimized.</p>"},{"location":"prompts/glossary/#big-data","title":"Big Data","text":"<p>Big Data refers to datasets that are too large or complex for traditional data-processing software to handle efficiently.</p> <p>Example: Discussing how tools like Hadoop or Spark can process big data in the context of data science.</p>"},{"location":"prompts/glossary/#box-plot","title":"Box Plot","text":"<p>A box plot is a graphical representation of data that displays the distribution's quartiles and averages, highlighting the median and outliers.</p> <p>Example: Using Seaborn to create box plots for visualizing the distribution of test scores across different classrooms.</p>"},{"location":"prompts/glossary/#bootstrapping","title":"Bootstrapping","text":"<p>Bootstrapping is a statistical resampling technique that involves repeatedly drawing samples from a dataset with replacement to estimate a population parameter.</p> <p>Example: Applying bootstrapping methods to estimate confidence intervals for a sample mean in a data analysis assignment.</p>"},{"location":"prompts/glossary/#classification","title":"Classification","text":"<p>Classification is a supervised learning task where the goal is to predict discrete labels or categories for given input data.</p> <p>Example: Building a logistic regression model to classify emails as spam or not spam.</p>"},{"location":"prompts/glossary/#clustering","title":"Clustering","text":"<p>Clustering is an unsupervised learning technique that groups similar data points together based on their features.</p> <p>Example: Using k-means clustering to segment customers into different groups based on purchasing behavior.</p>"},{"location":"prompts/glossary/#confusion-matrix","title":"Confusion Matrix","text":"<p>A confusion matrix is a table used to evaluate the performance of a classification model by comparing predicted and actual labels.</p> <p>Example: Analyzing a confusion matrix to calculate precision and recall for a disease diagnosis model.</p>"},{"location":"prompts/glossary/#correlation","title":"Correlation","text":"<p>Correlation measures the statistical relationship between two variables, indicating how one may predict the other.</p> <p>Example: Calculating the correlation coefficient between hours studied and exam scores to determine their relationship.</p>"},{"location":"prompts/glossary/#cross-validation","title":"Cross-Validation","text":"<p>Cross-validation is a technique for assessing how a predictive model will perform on an independent dataset by partitioning the data into complementary subsets for training and validation.</p> <p>Example: Using k-fold cross-validation to evaluate the generalization performance of a machine learning model.</p>"},{"location":"prompts/glossary/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>CSV is a file format that uses commas to separate values, commonly used for storing tabular data.</p> <p>Example: Importing a CSV file into a Pandas DataFrame to begin data analysis.</p>"},{"location":"prompts/glossary/#dataframe","title":"DataFrame","text":"<p>A DataFrame is a two-dimensional labeled data structure in Pandas, similar to a spreadsheet or SQL table.</p> <p>Example: Manipulating data stored in a DataFrame to clean and prepare it for analysis.</p>"},{"location":"prompts/glossary/#data-mining","title":"Data Mining","text":"<p>Data mining is the process of discovering patterns and knowledge from large amounts of data.</p> <p>Example: Extracting useful information from a large customer database to identify purchasing trends.</p>"},{"location":"prompts/glossary/#data-science","title":"Data Science","text":"<p>Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights from structured and unstructured data.</p> <p>Example: Applying data science techniques to analyze social media data for sentiment analysis.</p>"},{"location":"prompts/glossary/#data-visualization","title":"Data Visualization","text":"<p>Data visualization is the graphical representation of data to help people understand complex data easily.</p> <p>Example: Creating interactive dashboards using Matplotlib or Seaborn to present findings.</p>"},{"location":"prompts/glossary/#decision-tree","title":"Decision Tree","text":"<p>A decision tree is a flowchart-like structure used for making decisions or predictions based on input features.</p> <p>Example: Building a decision tree classifier to predict whether a loan application should be approved.</p>"},{"location":"prompts/glossary/#deep-learning","title":"Deep Learning","text":"<p>Deep learning is a subset of machine learning involving neural networks with multiple layers that can learn representations from data.</p> <p>Example: Exploring deep learning concepts by creating a neural network for image recognition tasks.</p>"},{"location":"prompts/glossary/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction involves reducing the number of input variables in a dataset while retaining as much information as possible.</p> <p>Example: Using Principal Component Analysis (PCA) to reduce features before training a model.</p>"},{"location":"prompts/glossary/#distribution","title":"Distribution","text":"<p>A distribution describes how values of a variable are spread or dispersed.</p> <p>Example: Plotting the normal distribution of test scores to analyze class performance.</p>"},{"location":"prompts/glossary/#dummy-variable","title":"Dummy Variable","text":"<p>A dummy variable is a binary variable created to include categorical data in regression models.</p> <p>Example: Converting categorical variables like 'Gender' into dummy variables for a regression analysis.</p>"},{"location":"prompts/glossary/#encoding","title":"Encoding","text":"<p>Encoding transforms data into a different format using a specific scheme.</p> <p>Example: Applying one-hot encoding to convert categorical variables into numerical format for machine learning models.</p>"},{"location":"prompts/glossary/#ensemble-learning","title":"Ensemble Learning","text":"<p>Ensemble learning combines predictions from multiple machine learning models to improve overall performance.</p> <p>Example: Using a random forest, which is an ensemble of decision trees, to enhance prediction accuracy.</p>"},{"location":"prompts/glossary/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<p>EDA is an approach to analyzing data sets to summarize their main characteristics, often using visual methods.</p> <p>Example: Performing EDA to detect anomalies and patterns before building predictive models.</p>"},{"location":"prompts/glossary/#feature-engineering","title":"Feature Engineering","text":"<p>Feature engineering involves creating new input features from existing ones to improve model performance.</p> <p>Example: Combining 'Date of Birth' and 'Current Date' to create a new feature 'Age' for a predictive model.</p>"},{"location":"prompts/glossary/#feature-scaling","title":"Feature Scaling","text":"<p>Feature scaling adjusts the range of features in the data to ensure they contribute equally to the model.</p> <p>Example: Applying standardization to features before using gradient descent algorithms.</p>"},{"location":"prompts/glossary/#feature-selection","title":"Feature Selection","text":"<p>Feature selection is the process of selecting a subset of relevant features for model construction.</p> <p>Example: Using correlation analysis to remove redundant features that do not improve the model.</p>"},{"location":"prompts/glossary/#function","title":"Function","text":"<p>In programming, a function is a block of organized, reusable code that performs a single action.</p> <p>Example: Defining a Python function to calculate the mean of a list of numbers in data analysis.</p>"},{"location":"prompts/glossary/#f1-score","title":"F1 Score","text":"<p>The F1 score is the harmonic mean of precision and recall, used as a measure of a test's accuracy.</p> <p>Example: Evaluating a classification model with imbalanced classes using the F1 score.</p>"},{"location":"prompts/glossary/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting is an ensemble technique that builds models sequentially, each correcting the errors of the previous one.</p> <p>Example: Implementing Gradient Boosting Machines (GBM) to improve prediction accuracy on complex datasets.</p>"},{"location":"prompts/glossary/#gradient-descent","title":"Gradient Descent","text":"<p>Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.</p> <p>Example: Using gradient descent to find the optimal weights in a linear regression model.</p>"},{"location":"prompts/glossary/#grid-search","title":"Grid Search","text":"<p>Grid search is a hyperparameter optimization technique that exhaustively searches through a specified subset of hyperparameters.</p> <p>Example: Applying grid search to find the best combination of parameters for a support vector machine classifier.</p>"},{"location":"prompts/glossary/#histogram","title":"Histogram","text":"<p>A histogram is a graphical representation showing the distribution of numerical data by depicting the number of data points that fall within specified ranges.</p> <p>Example: Creating a histogram to visualize the frequency distribution of ages in a dataset.</p>"},{"location":"prompts/glossary/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Hyperparameter tuning involves adjusting the parameters that govern the training process of a model to improve performance.</p> <p>Example: Tuning the number of trees and depth in a random forest model to achieve better accuracy.</p>"},{"location":"prompts/glossary/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Hypothesis testing is a statistical method used to make decisions about the properties of a population based on sample data.</p> <p>Example: Conducting a t-test to determine if there is a significant difference between two groups' means.</p>"},{"location":"prompts/glossary/#imputation","title":"Imputation","text":"<p>Imputation is the process of replacing missing data with substituted values.</p> <p>Example: Filling missing values in a dataset with the mean or median of the column.</p>"},{"location":"prompts/glossary/#inferential-statistics","title":"Inferential Statistics","text":"<p>Inferential statistics use a random sample of data taken from a population to describe and make inferences about the population.</p> <p>Example: Estimating the average height of all students in a university by sampling a subset.</p>"},{"location":"prompts/glossary/#interpolation","title":"Interpolation","text":"<p>Interpolation is a method of estimating unknown values that fall between known data points.</p> <p>Example: Using interpolation to estimate missing temperature readings in a time series dataset.</p>"},{"location":"prompts/glossary/#json-javascript-object-notation","title":"JSON (JavaScript Object Notation)","text":"<p>JSON is a lightweight data-interchange format that is easy for humans to read and write and for machines to parse and generate.</p> <p>Example: Reading data from a JSON file into a Pandas DataFrame for analysis.</p>"},{"location":"prompts/glossary/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>Jupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text.</p> <p>Example: Using Jupyter Notebook to write Python code and document the data analysis process.</p>"},{"location":"prompts/glossary/#k-means-clustering","title":"k-means Clustering","text":"<p>k-means clustering is an unsupervised learning algorithm that partitions data into k distinct clusters based on feature similarity.</p> <p>Example: Segmenting customers into groups based on purchasing behavior using k-means clustering.</p>"},{"location":"prompts/glossary/#k-nearest-neighbors-knn","title":"K-Nearest Neighbors (KNN)","text":"<p>KNN is a simple, supervised machine learning algorithm that classifies new cases based on the majority class of their k nearest neighbors.</p> <p>Example: Implementing KNN to predict whether a patient has a certain disease based on symptoms.</p>"},{"location":"prompts/glossary/#label-encoding","title":"Label Encoding","text":"<p>Label encoding converts categorical text data into numerical values by assigning a unique integer to each category.</p> <p>Example: Transforming the 'Color' feature into numerical labels before model training.</p>"},{"location":"prompts/glossary/#learning-rate","title":"Learning Rate","text":"<p>The learning rate is a hyperparameter that controls how much we adjust the model weights with respect to the loss gradient.</p> <p>Example: Setting an appropriate learning rate in gradient descent to ensure the model converges.</p>"},{"location":"prompts/glossary/#linear-regression","title":"Linear Regression","text":"<p>Linear regression is a supervised learning algorithm that models the relationship between a dependent variable and one or more independent variables.</p> <p>Example: Predicting house prices based on features like size and location using linear regression.</p>"},{"location":"prompts/glossary/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic regression is a classification algorithm used to predict the probability of a categorical dependent variable.</p> <p>Example: Using logistic regression to determine the likelihood of a customer churning.</p>"},{"location":"prompts/glossary/#machine-learning","title":"Machine Learning","text":"<p>Machine learning is a subset of AI that focuses on building systems that learn from and make decisions based on data.</p> <p>Example: Implementing various machine learning algorithms to solve classification and regression problems in the course.</p>"},{"location":"prompts/glossary/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is a Python library used for creating static, animated, and interactive visualizations.</p> <p>Example: Plotting data trends using Matplotlib to support data analysis conclusions.</p>"},{"location":"prompts/glossary/#missing-data","title":"Missing Data","text":"<p>Missing data occurs when no value is stored for a variable in an observation, which can impact data analysis.</p> <p>Example: Identifying and handling missing data in a dataset before model training.</p>"},{"location":"prompts/glossary/#model-selection","title":"Model Selection","text":"<p>Model selection involves choosing the best model from a set of candidates based on their predictive performance.</p> <p>Example: Comparing different algorithms like decision trees and logistic regression to select the best model for a classification task.</p>"},{"location":"prompts/glossary/#multicollinearity","title":"Multicollinearity","text":"<p>Multicollinearity occurs when independent variables in a regression model are highly correlated, which can affect the model's stability.</p> <p>Example: Detecting multicollinearity using the Variance Inflation Factor (VIF) and addressing it in the dataset.</p>"},{"location":"prompts/glossary/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>NLP is a field of AI that gives computers the ability to understand, interpret, and generate human language.</p> <p>Example: Analyzing text data for sentiment analysis using NLP techniques.</p>"},{"location":"prompts/glossary/#neural-network","title":"Neural Network","text":"<p>A neural network is a series of algorithms that mimic the operations of a human brain to recognize patterns and solve complex problems.</p> <p>Example: Building a simple neural network to classify images of handwritten digits.</p>"},{"location":"prompts/glossary/#normal-distribution","title":"Normal Distribution","text":"<p>The normal distribution is a continuous probability distribution characterized by a symmetrical, bell-shaped curve.</p> <p>Example: Assuming normal distribution of residuals in linear regression models.</p>"},{"location":"prompts/glossary/#normalization","title":"Normalization","text":"<p>Normalization scales data to fit within a specific range, often between 0 and 1, to ensure all features contribute equally.</p> <p>Example: Applying Min-Max normalization to features before training a neural network.</p>"},{"location":"prompts/glossary/#numpy","title":"NumPy","text":"<p>NumPy is a Python library used for working with arrays and providing functions for mathematical operations on large, multi-dimensional arrays and matrices.</p> <p>Example: Using NumPy arrays for efficient numerical computations in data science projects.</p>"},{"location":"prompts/glossary/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>One-hot encoding converts categorical variables into a binary matrix representation.</p> <p>Example: Transforming the 'Country' feature into multiple binary columns representing each country.</p>"},{"location":"prompts/glossary/#optimization","title":"Optimization","text":"<p>Optimization involves adjusting the inputs or parameters of a model to minimize or maximize some objective function.</p> <p>Example: Optimizing the weights in a neural network to reduce the loss function during training.</p>"},{"location":"prompts/glossary/#outlier","title":"Outlier","text":"<p>An outlier is a data point that differs significantly from other observations, potentially indicating variability in measurement or experimental errors.</p> <p>Example: Identifying outliers in a dataset using box plots and deciding whether to remove or transform them.</p>"},{"location":"prompts/glossary/#overfitting","title":"Overfitting","text":"<p>Overfitting occurs when a model learns the training data too well, capturing noise and details that negatively impact its performance on new data.</p> <p>Example: Preventing overfitting by using regularization techniques and cross-validation.</p>"},{"location":"prompts/glossary/#pandas","title":"Pandas","text":"<p>Pandas is a Python library providing high-performance, easy-to-use data structures and data analysis tools.</p> <p>Example: Using Pandas DataFrames to manipulate and analyze tabular data in the course.</p>"},{"location":"prompts/glossary/#parameter","title":"Parameter","text":"<p>A parameter is a configuration variable that is internal to the model and estimated from data.</p> <p>Example: The coefficients in a linear regression model are parameters learned during training.</p>"},{"location":"prompts/glossary/#pca-principal-component-analysis","title":"PCA (Principal Component Analysis)","text":"<p>PCA is a dimensionality reduction technique that transforms data into a new coordinate system, reducing the number of variables while retaining most information.</p> <p>Example: Applying PCA to reduce the dimensionality of a dataset before clustering.</p>"},{"location":"prompts/glossary/#pipeline","title":"Pipeline","text":"<p>A pipeline is a sequence of data processing components or steps, where the output of one component is the input to the next.</p> <p>Example: Creating a scikit-learn pipeline to standardize data and train a model in a single workflow.</p>"},{"location":"prompts/glossary/#precision","title":"Precision","text":"<p>Precision is a metric that measures the proportion of true positives among all positive predictions.</p> <p>Example: Calculating precision to evaluate a model where false positives are costly, such as in fraud detection.</p>"},{"location":"prompts/glossary/#predictive-modeling","title":"Predictive Modeling","text":"<p>Predictive modeling uses statistics and data to predict outcomes with data models.</p> <p>Example: Building a predictive model to forecast sales based on historical data.</p>"},{"location":"prompts/glossary/#probability-distribution","title":"Probability Distribution","text":"<p>A probability distribution describes how the values of a random variable are distributed.</p> <p>Example: Using the normal distribution to model the heights of individuals in a population.</p>"},{"location":"prompts/glossary/#python","title":"Python","text":"<p>Python is a high-level, interpreted programming language known for its readability and versatility in data science.</p> <p>Example: Writing Python scripts to automate data cleaning and analysis tasks.</p>"},{"location":"prompts/glossary/#random-forest","title":"Random Forest","text":"<p>Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes for classification tasks.</p> <p>Example: Implementing a random forest classifier to improve accuracy over a single decision tree.</p>"},{"location":"prompts/glossary/#regression","title":"Regression","text":"<p>Regression is a set of statistical processes for estimating the relationships among variables.</p> <p>Example: Performing linear regression to understand how the price of a house varies with its size.</p>"},{"location":"prompts/glossary/#regularization","title":"Regularization","text":"<p>Regularization adds a penalty to the loss function to prevent overfitting by discouraging complex models.</p> <p>Example: Applying Lasso regularization to reduce overfitting in a regression model.</p>"},{"location":"prompts/glossary/#recall","title":"Recall","text":"<p>Recall is a metric that measures the proportion of actual positives correctly identified.</p> <p>Example: Evaluating recall in a medical diagnosis model where missing a positive case is critical.</p>"},{"location":"prompts/glossary/#roc-curve-receiver-operating-characteristic-curve","title":"ROC Curve (Receiver Operating Characteristic Curve)","text":"<p>An ROC curve is a graphical plot illustrating the diagnostic ability of a binary classifier as its discrimination threshold is varied.</p> <p>Example: Plotting the ROC curve to select the optimal threshold for a classification model.</p>"},{"location":"prompts/glossary/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>RMSE is a metric used to measure the difference between values predicted by a model and the actual values.</p> <p>Example: Using RMSE to assess the performance of a regression model predicting housing prices.</p>"},{"location":"prompts/glossary/#sampling","title":"Sampling","text":"<p>Sampling involves selecting a subset of data from a larger dataset to estimate characteristics of the whole population.</p> <p>Example: Drawing a random sample from a large dataset to make computations more manageable.</p>"},{"location":"prompts/glossary/#scikit-learn","title":"Scikit-learn","text":"<p>Scikit-learn is a Python library for machine learning that provides simple and efficient tools for data analysis and modeling.</p> <p>Example: Using scikit-learn to implement machine learning algorithms like SVMs and random forests.</p>"},{"location":"prompts/glossary/#seaborn","title":"Seaborn","text":"<p>Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics.</p> <p>Example: Creating complex visualizations like heatmaps and violin plots using Seaborn.</p>"},{"location":"prompts/glossary/#smote-synthetic-minority-over-sampling-technique","title":"SMOTE (Synthetic Minority Over-sampling Technique)","text":"<p>SMOTE is a technique used to address class imbalance by generating synthetic samples of the minority class.</p> <p>Example: Applying SMOTE to balance the dataset before training a classifier on imbalanced data.</p>"},{"location":"prompts/glossary/#standard-deviation","title":"Standard Deviation","text":"<p>Standard deviation measures the amount of variation or dispersion in a set of values.</p> <p>Example: Calculating the standard deviation to understand the spread of exam scores in a class.</p>"},{"location":"prompts/glossary/#standardscaler","title":"StandardScaler","text":"<p>StandardScaler is a scikit-learn tool that standardizes features by removing the mean and scaling to unit variance.</p> <p>Example: Using StandardScaler to preprocess data before feeding it into a machine learning algorithm.</p>"},{"location":"prompts/glossary/#statistical-significance","title":"Statistical Significance","text":"<p>Statistical significance indicates that the result of a test is unlikely to have occurred by chance alone.</p> <p>Example: Interpreting p-values to determine if the difference between two groups is statistically significant.</p>"},{"location":"prompts/glossary/#supervised-learning","title":"Supervised Learning","text":"<p>Supervised learning is a type of machine learning where models are trained using labeled data.</p> <p>Example: Training a supervised learning model to predict house prices based on historical data.</p>"},{"location":"prompts/glossary/#time-series","title":"Time Series","text":"<p>Time series data is a sequence of data points collected or recorded at time intervals.</p> <p>Example: Analyzing stock prices over time to forecast future market trends.</p>"},{"location":"prompts/glossary/#tokenization","title":"Tokenization","text":"<p>Tokenization is the process of breaking text into smaller units called tokens, often words or phrases.</p> <p>Example: Tokenizing text data for input into a natural language processing model.</p>"},{"location":"prompts/glossary/#training-set","title":"Training Set","text":"<p>A training set is a subset of the dataset used to train machine learning models.</p> <p>Example: Splitting data into training and test sets to build and evaluate a model.</p>"},{"location":"prompts/glossary/#t-test","title":"T-test","text":"<p>A t-test is a statistical test used to compare the means of two groups.</p> <p>Example: Performing a t-test to determine if there is a significant difference in test scores between two classes.</p>"},{"location":"prompts/glossary/#underfitting","title":"Underfitting","text":"<p>Underfitting occurs when a model is too simple and fails to capture the underlying pattern of the data.</p> <p>Example: Addressing underfitting by increasing the complexity of the model or adding more features.</p>"},{"location":"prompts/glossary/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Unsupervised learning involves training models on data without labeled responses, aiming to find hidden patterns.</p> <p>Example: Using unsupervised learning techniques like clustering to segment customers.</p>"},{"location":"prompts/glossary/#validation-set","title":"Validation Set","text":"<p>A validation set is a subset of the dataset used to tune hyperparameters and prevent overfitting during model training.</p> <p>Example: Using a validation set to adjust the learning rate and number of layers in a neural network.</p>"},{"location":"prompts/glossary/#variance","title":"Variance","text":"<p>Variance measures how far a set of numbers is spread out from their average value.</p> <p>Example: Calculating the variance to understand the variability in a dataset.</p>"},{"location":"prompts/glossary/#visualization","title":"Visualization","text":"<p>Visualization refers to the graphical representation of information and data.</p> <p>Example: Creating line charts and scatter plots to visualize trends and relationships in the data.</p>"},{"location":"prompts/glossary/#weight","title":"Weight","text":"<p>In machine learning models, weights are parameters that are learned during training to map input features to outputs.</p> <p>Example: Adjusting weights in a neural network during training to minimize the loss function.</p>"},{"location":"prompts/glossary/#z-score","title":"Z-score","text":"<p>A z-score indicates how many standard deviations an element is from the mean.</p> <p>Example: Calculating z-scores to identify outliers in a dataset.</p>"},{"location":"prompts/glossary/#xgboost","title":"XGBoost","text":"<p>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient and flexible.</p> <p>Example: Implementing XGBoost to improve model performance on a classification task.</p>"},{"location":"prompts/glossary/#confusion-matrix_1","title":"Confusion Matrix","text":"<p>A confusion matrix is a table used to describe the performance of a classification model.</p> <p>Example: Using a confusion matrix to calculate precision, recall, and F1 score for a classifier.</p>"},{"location":"prompts/glossary/#hyperparameter","title":"Hyperparameter","text":"<p>A hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.</p> <p>Example: Setting the number of neighbors in a KNN algorithm as a hyperparameter to tune.</p>"},{"location":"prompts/glossary/#kernel","title":"Kernel","text":"<p>In machine learning, a kernel is a function used in algorithms like SVM to transform data into a higher-dimensional space.</p> <p>Example: Choosing a radial basis function (RBF) kernel for an SVM to handle non-linear data.</p>"},{"location":"prompts/glossary/#lasso-regression","title":"Lasso Regression","text":"<p>Lasso regression is a type of linear regression that uses L1 regularization to reduce overfitting and perform feature selection.</p> <p>Example: Applying lasso regression to identify the most important features in a dataset.</p>"},{"location":"prompts/glossary/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>MAE is a measure of errors between paired observations expressing the same phenomenon.</p> <p>Example: Evaluating a regression model by calculating the MAE between predicted and actual values.</p>"},{"location":"prompts/glossary/#overfitting_1","title":"Overfitting","text":"<p>Overfitting occurs when a model learns the training data too well, capturing noise and details that negatively impact performance on new data.</p> <p>Example: Observing overfitting in a model that performs well on training data but poorly on test data.</p>"},{"location":"prompts/glossary/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":"<p>The Pearson correlation coefficient measures the linear correlation between two variables.</p> <p>Example: Calculating the Pearson coefficient to assess the strength of the relationship between two features.</p>"},{"location":"prompts/glossary/#r-squared-coefficient-of-determination","title":"R-Squared (Coefficient of Determination)","text":"<p>R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by independent variables.</p> <p>Example: Interpreting an R-squared value of 0.85 to mean that 85% of the variance in the dependent variable is predictable.</p>"},{"location":"prompts/glossary/#sampling-bias","title":"Sampling Bias","text":"<p>Sampling bias occurs when some members of a population are systematically more likely to be selected in a sample than others.</p> <p>Example: Ensuring random sampling in data collection to avoid sampling bias.</p>"},{"location":"prompts/glossary/#tokenization_1","title":"Tokenization","text":"<p>Tokenization is the process of splitting text into individual units (tokens), such as words or phrases.</p> <p>Example: Tokenizing customer reviews to prepare text data for sentiment analysis.</p>"},{"location":"prompts/glossary/#univariate-analysis","title":"Univariate Analysis","text":"<p>Univariate analysis examines each variable individually to summarize and find patterns.</p> <p>Example: Performing univariate analysis on the 'Age' feature to understand its distribution.</p>"},{"location":"prompts/glossary/#variance-inflation-factor-vif","title":"Variance Inflation Factor (VIF)","text":"<p>VIF quantifies the severity of multicollinearity in regression analysis.</p> <p>Example: Calculating VIF to detect multicollinearity and decide whether to remove correlated features.</p>"},{"location":"prompts/glossary/#white-noise","title":"White Noise","text":"<p>White noise refers to a time series of random data points that have a constant mean and variance.</p> <p>Example: Checking residuals for white noise to validate the assumptions of a time series model.</p>"},{"location":"prompts/glossary/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>Cross-entropy loss measures the performance of a classification model whose output is a probability between 0 and 1.</p> <p>Example: Using cross-entropy loss as the loss function in a logistic regression model.</p>"},{"location":"prompts/glossary/#epoch","title":"Epoch","text":"<p>An epoch refers to one complete pass through the entire training dataset.</p> <p>Example: Training a neural network for 10 epochs to optimize the weights.</p>"},{"location":"prompts/glossary/#fitting","title":"Fitting","text":"<p>Fitting a model involves adjusting its parameters to best match the data.</p> <p>Example: Fitting a linear regression model to the training data by minimizing the cost function.</p>"},{"location":"prompts/glossary/#hyperplane","title":"Hyperplane","text":"<p>A hyperplane is a flat affine subspace of one dimension less than its ambient space, used in SVMs to separate classes.</p> <p>Example: Understanding how an SVM finds the optimal hyperplane to classify data points.</p>"},{"location":"prompts/glossary/#iteration","title":"Iteration","text":"<p>An iteration refers to one update of the model's parameters during training.</p> <p>Example: Observing loss reduction after each iteration in gradient descent optimization.</p>"},{"location":"prompts/glossary/#learning-curve","title":"Learning Curve","text":"<p>A learning curve plots the model's performance on the training and validation sets over time or as the training set size increases.</p> <p>Example: Analyzing the learning curve to diagnose if a model is overfitting or underfitting.</p>"},{"location":"prompts/glossary/#loss-function","title":"Loss Function","text":"<p>A loss function measures how well a machine learning model performs, guiding the optimization process.</p> <p>Example: Using Mean Squared Error (MSE) as the loss function in a regression model.</p>"},{"location":"prompts/glossary/#mini-batch-gradient-descent","title":"Mini-Batch Gradient Descent","text":"<p>Mini-batch gradient descent is an optimization algorithm that updates the model parameters using small batches of data.</p> <p>Example: Accelerating training by using mini-batches instead of the entire dataset in each iteration.</p>"},{"location":"prompts/glossary/#multivariate-analysis","title":"Multivariate Analysis","text":"<p>Multivariate analysis examines the relationship between multiple variables simultaneously.</p> <p>Example: Performing multivariate regression to understand how multiple features affect the target variable.</p>"},{"location":"prompts/glossary/#natural-language-processing-nlp_1","title":"Natural Language Processing (NLP)","text":"<p>NLP focuses on the interaction between computers and human language.</p> <p>Example: Using NLP techniques to analyze customer feedback and extract key themes.</p>"},{"location":"prompts/glossary/#optimization-algorithm","title":"Optimization Algorithm","text":"<p>An optimization algorithm adjusts the parameters of a model to minimize the loss function.</p> <p>Example: Choosing Adam optimizer for faster convergence in training a neural network.</p>"},{"location":"prompts/glossary/#precision-recall-curve","title":"Precision-Recall Curve","text":"<p>A precision-recall curve plots the trade-off between precision and recall for different threshold settings.</p> <p>Example: Using the precision-recall curve to select the threshold that balances precision and recall.</p>"},{"location":"prompts/glossary/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions and receiving rewards.</p> <p>Example: Discussing reinforcement learning concepts as an advanced topic in the course.</p>"},{"location":"prompts/glossary/#stratified-sampling","title":"Stratified Sampling","text":"<p>Stratified sampling involves dividing the population into subgroups and sampling from each to ensure representation.</p> <p>Example: Using stratified sampling to maintain the class distribution in training and test sets.</p>"},{"location":"prompts/glossary/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<p>SVM is a supervised learning algorithm that finds the hyperplane that best separates classes.</p> <p>Example: Implementing an SVM classifier for a binary classification problem in the course.</p>"},{"location":"prompts/glossary/#synthetic-data","title":"Synthetic Data","text":"<p>Synthetic data is artificially generated data that mimics the properties of real data.</p> <p>Example: Generating synthetic data to augment the dataset and improve model training.</p>"},{"location":"prompts/glossary/#training-loss","title":"Training Loss","text":"<p>Training loss measures the error on the training dataset during model training.</p> <p>Example: Monitoring training loss to assess how well the model is learning from the training data.</p>"},{"location":"prompts/glossary/#type-i-error","title":"Type I Error","text":"<p>A Type I error occurs when the null hypothesis is true but is incorrectly rejected.</p> <p>Example: Understanding Type I errors when interpreting p-values in hypothesis testing.</p>"},{"location":"prompts/glossary/#type-ii-error","title":"Type II Error","text":"<p>A Type II error occurs when the null hypothesis is false but erroneously fails to be rejected.</p> <p>Example: Recognizing the implications of Type II errors in statistical testing.</p>"},{"location":"prompts/glossary/#validation-loss","title":"Validation Loss","text":"<p>Validation loss measures the error on the validation dataset, used to tune model hyperparameters.</p> <p>Example: Observing validation loss to detect overfitting during model training.</p>"},{"location":"prompts/glossary/#weight-initialization","title":"Weight Initialization","text":"<p>Weight initialization is the process of setting the initial values of the weights before training a neural network.</p> <p>Example: Using random initialization methods to start training a deep learning model.</p>"},{"location":"prompts/glossary/#word-embedding","title":"Word Embedding","text":"<p>Word embedding is a representation of text where words with similar meaning have similar vector representations.</p> <p>Example: Implementing word embeddings like Word2Vec in NLP tasks.</p>"},{"location":"prompts/glossary/#xgboost_1","title":"XGBoost","text":"<p>XGBoost is an optimized gradient boosting library designed for performance and speed.</p> <p>Example: Using XGBoost to improve model accuracy in classification problems.</p>"},{"location":"prompts/glossary/#z-score-normalization","title":"Z-score Normalization","text":"<p>Z-score normalization scales data based on mean and standard deviation.</p> <p>Example: Applying z-score normalization to standardize features before training a model.</p> <p>o1</p>"},{"location":"prompts/library-comparison-prompt/","title":"Comparison of Matplotlib and Plotly for GenAI-Created Plots and Animations","text":""},{"location":"prompts/library-comparison-prompt/#prompt","title":"Prompt","text":"<pre><code>Compare the Python libraries matplotlib with plotly.\nWhat are the pros and cons for GenAI created plots and animations?\n\nWhen choosing between **Matplotlib** and **Plotly** for creating\nplots and animations with generative AI, here are their pros and cons?\n</code></pre>"},{"location":"prompts/library-comparison-prompt/#chatgpt-response","title":"ChatGPT Response","text":""},{"location":"prompts/library-comparison-prompt/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is a versatile and widely-used Python library for creating static, animated, and interactive visualizations.</p>"},{"location":"prompts/library-comparison-prompt/#pros","title":"Pros","text":"<ol> <li>Simplicity and Customization:</li> <li>Great for generating static, publication-quality plots with fine-grained control.</li> <li> <p>Straightforward API for generating standard visualizations like line plots, bar charts, and scatter plots.</p> </li> <li> <p>Broad Ecosystem Support:</p> </li> <li>Integrates well with NumPy, pandas, and other scientific libraries.</li> <li> <p>Supported by many GenAI frameworks for generating data visualizations.</p> </li> <li> <p>Animations:</p> </li> <li>Provides tools for creating animations (e.g., <code>FuncAnimation</code>).</li> <li> <p>Ideal for embedding animations in applications or saving them as GIFs or videos.</p> </li> <li> <p>Customization:</p> </li> <li>Allows deep customization of every aspect of the plot.</li> <li> <p>Perfect for generating AI-tailored visualizations that require specific formats or aesthetics.</p> </li> <li> <p>Lightweight and Efficient:</p> </li> <li>Better for lightweight tasks where interactivity is not required.</li> </ol>"},{"location":"prompts/library-comparison-prompt/#cons","title":"Cons","text":"<ol> <li>Steep Learning Curve:</li> <li>Customizing plots can become verbose and complicated, especially for advanced layouts.</li> <li> <p>Syntax can feel tedious for beginners or for generating multiple visualizations programmatically.</p> </li> <li> <p>Limited Interactivity:</p> </li> <li>Interactivity is minimal compared to modern libraries like Plotly.</li> <li> <p>Interactive plots require third-party tools (e.g., <code>mpld3</code>) and are often less intuitive.</p> </li> <li> <p>Outdated Animations:</p> </li> <li>Animations are not as smooth or interactive as those created by Plotly.</li> </ol>"},{"location":"prompts/library-comparison-prompt/#plotly","title":"Plotly","text":"<p>Plotly is a modern library for creating highly interactive and dynamic visualizations.</p>"},{"location":"prompts/library-comparison-prompt/#pros_1","title":"Pros","text":"<ol> <li>Interactivity:</li> <li>All visualizations are interactive by default (e.g., zooming, panning, tooltips).</li> <li> <p>Excellent for dashboards, presentations, and GenAI-generated insights.</p> </li> <li> <p>Ease of Use:</p> </li> <li>Simple syntax for creating complex plots, suitable for AI-generated visualizations.</li> <li> <p>Works seamlessly with data from pandas, NumPy, and other libraries.</p> </li> <li> <p>Browser-Based Rendering:</p> </li> <li>Plots render in web browsers using JavaScript, making them highly portable and shareable.</li> <li> <p>Supports embedding in Jupyter Notebooks, web apps, and standalone HTML files.</p> </li> <li> <p>Built-in Animations:</p> </li> <li>Easy to create smooth, interactive animations with native support.</li> <li> <p>Ideal for demonstrating dynamic AI models or processes.</p> </li> <li> <p>Wide Range of Charts:</p> </li> <li>Includes advanced visualizations like 3D plots, heatmaps, choropleth maps, and Sankey diagrams.</li> <li>Perfect for showing complex GenAI outputs in an intuitive way.</li> </ol>"},{"location":"prompts/library-comparison-prompt/#cons_1","title":"Cons","text":"<ol> <li>Heavier and Slower:</li> <li>More resource-intensive due to JavaScript-based interactivity.</li> <li> <p>Can slow down with large datasets or complex visualizations.</p> </li> <li> <p>Limited Fine-Grained Control:</p> </li> <li>While customizable, it lacks the deep control over plot details that Matplotlib offers.</li> <li> <p>Certain niche visualizations might be harder to implement.</p> </li> <li> <p>Dependency on JavaScript:</p> </li> <li> <p>Requires JavaScript runtime for rendering, which might not be ideal for all environments.</p> </li> <li> <p>Animation Complexity:</p> </li> <li>While animations are smoother, creating complex animations programmatically can be less intuitive than with Matplotlib.</li> </ol>"},{"location":"prompts/library-comparison-prompt/#which-to-choose-for-genai","title":"Which to Choose for GenAI?","text":"<ol> <li>Static Plots:</li> <li>Use Matplotlib for quick, customizable, high-quality static plots.</li> <li> <p>Example: Generating high-resolution images for reports or papers.</p> </li> <li> <p>Interactive and Animated Plots:</p> </li> <li>Use Plotly for dynamic and interactive plots.</li> <li> <p>Example: Interactive GenAI dashboards or showcasing dynamic AI model predictions.</p> </li> <li> <p>Collaborative and Web-Based Use:</p> </li> <li> <p>Use Plotly if your plots need to be shared on the web or included in interactive applications.</p> </li> <li> <p>Complex Animations:</p> </li> <li>Use Matplotlib for complex, programmatically controlled animations.</li> <li>Use Plotly for simple, interactive animations with smooth transitions.</li> </ol>"},{"location":"prompts/library-comparison-prompt/#conclusion","title":"Conclusion","text":"<p>Both libraries are valuable for different contexts: - Matplotlib shines in fine-grained control and efficient static or programmatic visualizations. - Plotly is the go-to for interactivity, browser-based plots, and user-friendly animations.</p> <p>For GenAI, combining both libraries strategically (e.g., static plots with Matplotlib and interactive plots with Plotly) can provide the best of both worlds.</p>"},{"location":"prompts/microsim/","title":"Generating a MicroSim Prompt","text":"<p>Prompt</p> <p>Please generate a p5.js Javascript file that assumes that the canvas is placed in a <code>&lt;main&gt;</code> HTML element. Follow the rules from the file width-adaptive-microsim-rules.md. The title of the MicroSim is [TITLE]. The purpose of the MicroSim is [PURPOSE]. The description of the MicroSim is [DESCRIPTION]. The input controls are [CONTROLS].</p>"},{"location":"prompts/quiz/","title":"Data Science Quiz","text":""},{"location":"prompts/quiz/#sample-prompt","title":"Sample Prompt","text":"<pre><code>I am preparing a review quiz for a freshman course called \"Introduction to Data Science with Python\".\nFor the concept of the NumPy Python library, create a four-answer\nmultiple choice question\nabout what it is primarily used for in Data Science.  Create three\nplausible destructor answers that are not correct but have a\nsimilar form to the correct answer.  Use the following format:\n\n#### What is the primary purpose of the NumPy?\n\nA. Bla Bla Bla\nB. Bla Bla Bla\nC. Bla Bla Bla\nD. Bla Bla Bla\n\n??? note \"Show Answer\"\n\n    The correct answer is **LETTER**.  Explanation of answer text.\n</code></pre>"},{"location":"prompts/quiz/#response","title":"Response","text":""},{"location":"prompts/quiz/#what-is-the-primary-purpose-of-numpy","title":"What is the primary purpose of NumPy?","text":"<p>A. Data visualization and creating complex plots for reports B. Machine learning model training and hyperparameter tuning C. Performing numerical computations and handling multi-dimensional arrays D. Creating interactive web-based dashboards for data exploration</p> Show Answer <p>The correct answer is C. NumPy is primarily used for numerical computations, including working with multi-dimensional arrays and matrices, and providing mathematical functions to perform operations on these arrays efficiently.</p>"},{"location":"sims/","title":"List of MicroSims for Data Science","text":"<p>Learning Graph</p>"},{"location":"sims/bell-curve/","title":"Bell Curve Emergence","text":""},{"location":"sims/bell-curve/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/bell-curve/main.html\" height=\"450px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p> <p>Topics: Normal distribution, Central Limit Theorem (CLT), sampling distribution of the mean, law of large numbers, z\u2011scores</p> <p>Description: Students watch a true bell curve emerge in real time by repeatedly sampling from a non\u2011normal base distribution (choose Uniform, Skewed, Exponential, or Bernoulli). Each iteration draws <code>n</code> observations, computes their sample mean, and drops a small dot into a running histogram of sample means. As more means accumulate and/or as <code>n</code> increases, the histogram approaches a normal (Gaussian) curve. Overlay a theoretical normal with mean <code>\u03bc</code> (of the base distribution) and standard deviation <code>\u03c3/\u221an</code> to make the convergence explicit. Optional shading shows central probability regions (e.g., \u00b11\u03c3, \u00b12\u03c3).</p> <p>Input Controls (in controls region):</p> <ol> <li> <p>Base Distribution (dropdown: Uniform[0,1], Skewed (Beta), Exponential(\u03bb), Bernoulli(p))</p> </li> <li> <p>Sample Size (n) (slider: 1 \u2192 100, default 10)</p> </li> <li> <p>Samples per Tick (slider: 1 \u2192 200, default 50) --- controls animation speed</p> </li> <li> <p>Bins (slider: 10 \u2192 80, default 51)</p> </li> <li> <p>Show Theoretical Normal (checkbox)</p> </li> <li> <p>Shade \u00b1\u03c3, \u00b12\u03c3 (checkbox)</p> </li> <li> <p>Start/Pause (button)</p> </li> <li> <p>Reset (button)</p> </li> </ol> <p>On\u2011Canvas Readouts:</p> <ul> <li> <p>Base <code>\u03bc</code>, <code>\u03c3</code>; current <code>n</code>; theoretical sampling SD <code>\u03c3/\u221an</code></p> </li> <li> <p>Running count of sample means collected</p> </li> <li> <p>Optional z\u2011score under cursor (hover on histogram to show area left of x)</p> </li> </ul> <p>Learning Objectives:</p> <ul> <li> <p>See why the sampling distribution of the mean approaches normality regardless of the base distribution.</p> </li> <li> <p>Connect sample size to the spread of the sampling distribution (<code>\u03c3/\u221an</code>).</p> </li> <li> <p>Interpret bell curve parameters visually and relate to empirical rules (68--95--99.7%).</p> </li> <li> <p>Distinguish between a population distribution (often non\u2011normal) and the sampling distribution of the mean (approximately normal for large <code>n</code>).</p> </li> </ul> <p>Difficulty: Intermediate</p> <p>Implementation Notes (p5.js, responsive width per standard rules):</p> <ul> <li> <p>Two\u2011column layout: left = controls + small \"base distribution preview\"; right = main histogram of sample means.</p> </li> <li> <p>Maintain an array <code>means[]</code>; on each tick, generate <code>k = Samples per Tick</code> sample means by drawing <code>n</code> IID values from the chosen base distribution, pushing their averages.</p> </li> <li> <p>Histogram x\u2011axis spans a sensible range around the base mean (e.g., <code>\u03bc \u00b1 4\u03c3</code>); update bin counts incrementally for performance.</p> </li> <li> <p>When \"Show Theoretical Normal\" is checked, draw <code>N(\u03bc, \u03c3\u00b2/n)</code> scaled to histogram area.</p> </li> <li> <p>For shading, fill under the curve between <code>\u03bc\u00b1\u03c3</code> and <code>\u03bc\u00b12\u03c3</code>.</p> </li> <li> <p>Accessibility: <code>describe(description, p5.LABEL)</code> with a clear string like:     \"Sampling distribution simulator that shows a bell curve emerging from sample means drawn from a chosen non\u2011normal population.\"</p> </li> </ul> <p>Nice Extras (optional):</p> <ul> <li> <p>Toggle between \"Sample Means\" vs \"Sample Sums\" (sums also go normal as <code>n</code> grows).</p> </li> <li> <p>Snapshot button to freeze and annotate the current histogram with mean/SD labels.</p> </li> <li> <p>A tiny QQ\u2011plot inset comparing sample means to a perfect normal line.</p> </li> </ul>"},{"location":"sims/bell-curve/#references","title":"References","text":"<ol> <li>Central Limit Theorem - Wikipedia - Mathematical foundation for why the bell curve emerges</li> <li>Normal Distribution - Wikipedia - Properties of the Gaussian distribution</li> <li>Khan Academy: Central Limit Theorem - Khan Academy - Video explanation of the CLT</li> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/book-gen-workflow/","title":"Book Generation Workflow","text":"<p>This MicroSim illustrates the workflow for creating intelligent textbooks using generative AI tools.</p>"},{"location":"sims/book-gen-workflow/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/book-gen-workflow/main.html\" height=\"610px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p> <p>Edit this MicroSim in the p5.js Editor</p>"},{"location":"sims/book-gen-workflow/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive infographic shows the step-by-step process for generating intelligent textbooks, from initial concept development through final publication.</p>"},{"location":"sims/book-gen-workflow/#references","title":"References","text":"<ol> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/chapter-1-concept-map/","title":"Chapter 1 Concept Map","text":"<p>An interactive visualization showing how the concepts from Chapter 1 (Introduction to Data Science) connect and relate to each other.</p> <p>View Chapter 1 Concept Map in Full Screen</p>"},{"location":"sims/chapter-1-concept-map/#how-to-use-this-map","title":"How to Use This Map","text":"<ul> <li>Hover over any concept to see its definition</li> <li>Click a concept to highlight all its connections</li> <li>Double-click a concept to see examples</li> <li>Filter buttons show/hide concept categories</li> </ul>"},{"location":"sims/chapter-1-concept-map/#concept-categories","title":"Concept Categories","text":"Category Shape Color Concepts Core Concepts Circle Gold Data Science, Data, Variables, Dataset Data Types Square Blue Numerical, Categorical, Ordinal, Nominal Variable Roles Diamond Green Independent, Dependent, Feature, Target Workflow Hexagon Purple Data Science Workflow, Problem Definition, Data Collection Tools &amp; Practices Rectangle Orange Python Programming, Documentation Structure Triangle Teal Measurement Scales, Observation"},{"location":"sims/chapter-1-concept-map/#relationship-types","title":"Relationship Types","text":"Edge Style Meaning Example Solid gray \"is a type of\" Numerical \u2192 Data Types Dashed blue \"contains\" Dataset \u2192 Observation Dotted green \"uses\" Data Science \u2192 Python Programming Thick red \"same as\" Feature \u2194 Independent Variable"},{"location":"sims/chapter-1-concept-map/#key-relationships-to-understand","title":"Key Relationships to Understand","text":"<ol> <li>Data Science works with Data - The foundation of the field</li> <li>Data is organized into Variables - How we structure information</li> <li>Variables are classified as Numerical or Categorical - The fundamental split</li> <li>Categorical splits into Ordinal (ordered) and Nominal (unordered)</li> <li>Variables can play different roles: Independent (input) vs Dependent (output)</li> <li>In ML context: Feature = Independent Variable, Target = Dependent Variable</li> <li>The Data Science Workflow guides the process from Problem Definition through Data Collection</li> </ol>"},{"location":"sims/chapter-1-concept-map/#learning-objectives","title":"Learning Objectives","text":"<p>After exploring this concept map, you should be able to:</p> <ol> <li>Identify and define all 20 core concepts from Chapter 1</li> <li>Explain how concepts relate to each other</li> <li>Classify any variable by its type and role</li> <li>Describe the data science workflow</li> <li>Understand the equivalence between traditional statistics and ML terminology</li> </ol>"},{"location":"sims/chapter-1-concept-map/#related-resources","title":"Related Resources","text":"<ul> <li>Chapter 1: Introduction to Data Science</li> <li>Variable Types Decision Tree</li> <li>Variable Type Sorter Quiz</li> </ul>"},{"location":"sims/data-science-heros-journey/","title":"Data Science Hero's Journey","text":"<p>Run the Data Science Hero's Journey Fullscreen Edit the Data Science Hero's Journey Using the p5.js Editor</p>"},{"location":"sims/data-science-heros-journey/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization presents the data science workflow as a hero's journey, inspired by Joseph Campbell's monomyth structure. Just as every great hero follows a journey from the ordinary world through trials and transformation, data scientists follow a similar path from initial questions to actionable insights.</p>"},{"location":"sims/data-science-heros-journey/#the-seven-stages","title":"The Seven Stages","text":"<ol> <li>Problem Definition - \"The Call to Adventure\"</li> <li>Every data science project begins with a question</li> <li> <p>Vague questions lead to vague answers</p> </li> <li> <p>Data Collection - \"Gathering Allies\"</p> </li> <li>Seek data from databases, surveys, APIs, and experiments</li> <li> <p>Often the most challenging part of the journey</p> </li> <li> <p>Data Cleaning - \"Trials and Tribulations\"</p> </li> <li>Fix errors, handle missing values, tame the chaos</li> <li> <p>The hero must face challenges before reaching the goal</p> </li> <li> <p>Exploratory Analysis - \"The Revelation\"</p> </li> <li>Visualize and explore to find patterns</li> <li> <p>The fog begins to clear as insights emerge</p> </li> <li> <p>Modeling - \"Forging the Weapon\"</p> </li> <li>Build your predictive model</li> <li> <p>Create the tool that will help you conquer uncertainty</p> </li> <li> <p>Evaluation - \"The Ultimate Test\"</p> </li> <li>Does your model actually work?</li> <li> <p>Test your creation against reality</p> </li> <li> <p>Communication - \"Return with the Elixir\"</p> </li> <li>Share your discoveries with the world</li> <li>The journey is complete when knowledge is shared</li> </ol>"},{"location":"sims/data-science-heros-journey/#interactive-features","title":"Interactive Features","text":"<ul> <li>Hover over any stage to see a detailed description</li> <li>Click any stage to view real-world examples</li> <li>Auto-animate toggle cycles through stages with a glowing effect</li> <li>Return arrows show the iterative nature of data science (dotted lines)</li> </ul>"},{"location":"sims/data-science-heros-journey/#learning-objective","title":"Learning Objective","text":"<p>Help students understand the iterative nature of the data science workflow and see it as an adventure rather than a checklist. The circular layout emphasizes that data science is not a linear process - you often need to return to earlier stages as you learn more.</p>"},{"location":"sims/data-science-heros-journey/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/data-science-heros-journey/main.html\" height=\"602px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/data-science-heros-journey/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/data-science-heros-journey/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<ul> <li>Discuss the concept of the hero's journey in storytelling</li> <li>Ask students: \"What challenges do heroes face on their journeys?\"</li> </ul>"},{"location":"sims/data-science-heros-journey/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ul> <li>Have students explore the MicroSim, hovering over each stage</li> <li>Ask them to click on stages and read the real-world examples</li> <li>Discuss: \"Which stage do you think is most challenging? Why?\"</li> </ul>"},{"location":"sims/data-science-heros-journey/#discussion-10-minutes","title":"Discussion (10 minutes)","text":"<ul> <li>Focus on the dotted \"return\" arrows</li> <li>Ask: \"Why would a data scientist need to go back to earlier stages?\"</li> <li>Examples: Model not working might mean bad data, new questions emerge from findings</li> </ul>"},{"location":"sims/data-science-heros-journey/#activity-15-minutes","title":"Activity (15 minutes)","text":"<ul> <li>Give students a scenario (e.g., \"Predict which students might need tutoring\")</li> <li>Have them walk through all 7 stages, describing what they would do at each</li> <li>Emphasize that it's okay to go back - that's part of the process!</li> </ul>"},{"location":"sims/data-science-heros-journey/#references","title":"References","text":"<ul> <li>Campbell, Joseph. \"The Hero with a Thousand Faces\" (1949)</li> <li>CRISP-DM: Cross-Industry Standard Process for Data Mining</li> <li>Wickham, Hadley. \"R for Data Science\" - Data Science Workflow chapter</li> </ul>"},{"location":"sims/data-science-superpower/","title":"Data Science Superpower Concept Map","text":"<p>Run the Data Science Superpower MicroSim Fullscreen</p>"},{"location":"sims/data-science-superpower/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization presents data science as a central power source with six \"superpowers\" radiating outward to real-world applications. The comic book aesthetic emphasizes that data science skills are truly transformative capabilities.</p>"},{"location":"sims/data-science-superpower/#the-six-superpowers","title":"The Six Superpowers","text":""},{"location":"sims/data-science-superpower/#1-pattern-detection","title":"1. Pattern Detection","text":"<p>Find hidden patterns and anomalies in complex data.</p> <ul> <li>Fraud Detection - PayPal, Stripe, Visa use ML to identify suspicious transactions</li> <li>Disease Diagnosis - IBM Watson and Google Health analyze medical images and patient data</li> </ul>"},{"location":"sims/data-science-superpower/#2-prediction","title":"2. Prediction","text":"<p>Forecast future events and trends from historical data.</p> <ul> <li>Weather Forecasting - The Weather Company and AccuWeather use massive datasets</li> <li>Stock Markets - Bloomberg and quantitative hedge funds predict market movements</li> </ul>"},{"location":"sims/data-science-superpower/#3-optimization","title":"3. Optimization","text":"<p>Find the best solution among many possibilities.</p> <ul> <li>Route Planning - Google Maps, UPS, and FedEx optimize delivery routes</li> <li>Resource Allocation - Amazon, Uber, and Airbnb match supply with demand</li> </ul>"},{"location":"sims/data-science-superpower/#4-automation","title":"4. Automation","text":"<p>Enable machines to perform tasks intelligently.</p> <ul> <li>Self-Driving Cars - Tesla, Waymo, and Cruise process sensor data in real-time</li> <li>Smart Assistants - Amazon Alexa and Google Assistant understand natural language</li> </ul>"},{"location":"sims/data-science-superpower/#5-insight-discovery","title":"5. Insight Discovery","text":"<p>Uncover meaningful insights from raw information.</p> <ul> <li>Customer Behavior - Netflix, Spotify, and Amazon personalize recommendations</li> <li>Scientific Research - DeepMind and OpenAI accelerate discovery</li> </ul>"},{"location":"sims/data-science-superpower/#6-decision-support","title":"6. Decision Support","text":"<p>Provide evidence-based recommendations for choices.</p> <ul> <li>Business Strategy - McKinsey and Palantir analyze complex business scenarios</li> <li>Policy Making - World Bank and CDC use data for public policy decisions</li> </ul>"},{"location":"sims/data-science-superpower/#interactive-features","title":"Interactive Features","text":"<ul> <li>Hover over any yellow superpower node to see its description</li> <li>Hover over any green application node to see company examples</li> <li>Pulsing animation on the central core draws attention</li> <li>Comic book action lines create an energetic visual style</li> </ul>"},{"location":"sims/data-science-superpower/#learning-objective","title":"Learning Objective","text":"<p>Help students visualize how data science skills connect to real-world impact and career opportunities. By seeing the breadth of applications, students understand that data science is not just about algorithms\u2014it's about solving real problems across every industry.</p>"},{"location":"sims/data-science-superpower/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/data-science-superpower/main.html\" height=\"602px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/data-science-superpower/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/data-science-superpower/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<ul> <li>Ask students: \"If you could have any superpower, what would it be?\"</li> <li>Introduce the idea that data science gives us real-world \"superpowers\"</li> </ul>"},{"location":"sims/data-science-superpower/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ul> <li>Have students explore the MicroSim, hovering over each superpower</li> <li>Ask them to pick the superpower they find most interesting</li> <li>Have them hover over the applications to see real company examples</li> </ul>"},{"location":"sims/data-science-superpower/#discussion-10-minutes","title":"Discussion (10 minutes)","text":"<ul> <li>Which companies surprised you with their use of data science?</li> <li>Can you think of other applications for each superpower?</li> <li>Which superpower do you think will be most important in 10 years?</li> </ul>"},{"location":"sims/data-science-superpower/#activity-15-minutes","title":"Activity (15 minutes)","text":"<ul> <li>Divide class into 6 groups, one per superpower</li> <li>Each group researches 2 more real-world applications</li> <li>Groups present their findings to the class</li> </ul>"},{"location":"sims/data-science-superpower/#visual-design","title":"Visual Design","text":"<ul> <li>Blue core - Represents the foundational nature of data science</li> <li>Yellow/gold spokes - Superpowers radiate like energy from the core</li> <li>Green applications - Real-world impact, like growing results</li> <li>Action lines - Comic book aesthetic emphasizes dynamic power</li> </ul>"},{"location":"sims/data-science-workflow/","title":"Six-Step Data Science Workflow","text":""},{"location":"sims/data-science-workflow/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/data-science-workflow/main.html\" height=\"300px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Data Science Workflow MicroSim in Fullscreen</p> <p>Edit this MicroSim in the p5.js Editor</p>"},{"location":"sims/data-science-workflow/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive infographic illustrates the six essential steps of the data science workflow:</p> <ol> <li>Define the Problem - Clearly articulate the question you want to answer</li> <li>Collect Data - Gather relevant data from various sources</li> <li>Clean Data - Process and prepare data for analysis</li> <li>Analyze Data - Apply statistical and machine learning methods</li> <li>Visualize Results - Create charts and graphs to communicate findings</li> <li>Communicate Insights - Share conclusions with stakeholders</li> </ol>"},{"location":"sims/data-science-workflow/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/data-science-workflow/#grade-level-subject-and-topic","title":"Grade Level, Subject and Topic","text":"<p>9th-12th grade introductory data science. Understanding the data science process.</p>"},{"location":"sims/data-science-workflow/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, students will be able to:</p> <ul> <li>Identify the six steps of the data science workflow</li> <li>Explain the purpose of each step in the process</li> <li>Describe how the steps connect to form a complete analysis pipeline</li> </ul>"},{"location":"sims/data-science-workflow/#activities","title":"Activities","text":""},{"location":"sims/data-science-workflow/#step-identification","title":"Step Identification","text":"<p>Have students identify which step they are performing when working on a data project.</p>"},{"location":"sims/data-science-workflow/#real-world-examples","title":"Real-World Examples","text":"<p>Discuss real-world data science projects and map them to the six-step workflow.</p>"},{"location":"sims/data-science-workflow/#process-ordering","title":"Process Ordering","text":"<p>Provide students with scrambled workflow steps and have them arrange them in the correct order.</p>"},{"location":"sims/data-science-workflow/#references","title":"References","text":"<ol> <li>What is Data Science? - Wikipedia - Overview of data science as a field</li> <li>The Data Science Process - KDnuggets - Detailed explanation of data science methodology</li> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/ds-prog-lang/","title":"Data Science Programming Language Popularity","text":""},{"location":"sims/ds-prog-lang/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/ds-prog-lang/main.html\" height=\"590px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p>"},{"location":"sims/ds-prog-lang/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive Chart.js visualization shows the popularity trends of major programming languages used in data science from 2010 to 2025. The chart was created using Claude Sonnet 4 using the following prompt:</p> <p>Prompt</p> <p>Please generate a line plot that shows the popularity of different programming languages for use in data science over the last 15 years.  Make sure to include both Python and R.  Document the source of the dataset.  Make time be the horizontal axis and start at 2010.</p> <p>Claude Sonnet 4 Transcript</p> <p>Claude Public Artifact</p>"},{"location":"sims/ds-prog-lang/#claude-transcript","title":"Claude Transcript","text":"<p>I'll create a line plot showing the popularity of programming languages in data science from 2010 to 2025. Since this requires current data about programming language trends, let me search for reliable sources first.Now let me search for more specific historical data and datasets about programming language trends:Let me fetch some specific data from the Stack Overflow surveys to get actual numbers:Let me search for more historical data about programming language trends:Now I have enough information to create a comprehensive visualization. Let me create a line plot showing programming language popularity trends from 2010-2025, using the data I've gathered from multiple sources.## Programming Language Popularity in Data Science (2010-2025)</p> <p>I've created a comprehensive line plot showing the popularity trends of major programming languages used in data science from 2010 to 2025. The visualization clearly shows time on the horizontal axis starting from 2010, as requested.</p> <p>Key Findings:</p> <p>Python's Dominance: Python has seen explosive growth, reaching approximately 90% usage among data science professionals by 2025, with a 7 percentage point increase from 2024 to 2025 alone. Python achieved a 25.35% share in the TIOBE index by May 2025, the highest rating for any language since Java in 2001.</p> <p>R's Steady Position: R's usage has declined to around 38% in 2025, but remains dominant in research, with more than 60% of papers using regression analysis in 2024-2025 utilizing R.</p> <p>SQL's Continued Relevance: SQL maintains its critical role, utilized by about 53% of data science professionals for data management and querying tasks.</p>"},{"location":"sims/ds-prog-lang/#data-sources","title":"Data Sources","text":"<p>The chart is based on comprehensive data from multiple authoritative sources:</p> <ul> <li>TIOBE Programming Community Index: Monthly rankings based on search engine results from 25 major search engines including Google, Bing, Yahoo, and Wikipedia</li> <li>Stack Overflow Developer Surveys: Annual surveys with 49,000+ to 90,000+ developers from 177 countries, providing insights into actual usage patterns</li> <li>Industry Analysis: Reports from DataCamp, upGrad, and other data science education platforms tracking industry trends</li> <li>Academic Sources: Research from Stanford University and Nature Data Science publications</li> </ul> <p>The visualization demonstrates Python's rise from a niche language to the dominant force in data science, driven by its versatility in AI, machine learning, and automation, while other languages like R and SQL maintain their specialized roles in the data science ecosystem.</p>"},{"location":"sims/ds-prog-lang/#data-source-links","title":"Data Source Links","text":"<ol> <li> <p>Top Programming Language Trends in Data Science: 2025 Insights - June 12, 2025 - upGrad - Analysis of Python, R, and SQL trends in data science, comparing use cases and future programming shifts</p> </li> <li> <p>The Popularity of Data Science Languages: Python and R (April 2023 Update) - April 11, 2023 - Medium - Examination of Python and R rankings in TIOBE, PYPL, and Stack Overflow indices</p> </li> <li> <p>The Popularity of Data Science Languages: Python and R (October 2023 Update) - November 2, 2023 - Medium - Analysis of Python and R trends with Stack Overflow survey insights</p> </li> <li> <p>Popularity Ranking of Programming Languages - January 24, 2025 - Towards Data Science - Comprehensive analysis of programming language trends using Stack Overflow data</p> </li> <li> <p>The Popularity of Data Science Languages: Python and R (March 2024 Update) - November 29, 2024 - Medium - Latest trends showing Python's continued dominance and R's specialized role</p> </li> <li> <p>Top 12 Programming Languages for Data Scientists in 2025 - July 25, 2024 - DataCamp - Comprehensive guide to programming languages for data science with pros and cons analysis</p> </li> <li> <p>Python vs. R vs. SQL: 2025 Data Science Programming Language Trends - April 26, 2025 - Analytics Insight - Comparison of the three dominant data science languages and their use cases</p> </li> <li> <p>The Most Popular Programming Languages in 2025 - February 18, 2025 - Career in STEM - Analysis of programming language popularity with market share percentages</p> </li> <li> <p>Don't Miss Out: 12 Data Science Programming Languages 2025! - July 10, 2025 - upGrad - Overview of essential data science programming languages and their applications</p> </li> <li> <p>Stack Overflow Developer Survey 2025 - 2025 - Stack Overflow - Annual survey of 49,000+ developers from 177 countries on technology usage and trends</p> </li> <li> <p>2025 Stack Overflow Developer Survey Technology Section - 2025 - Stack Overflow - Detailed breakdown of programming language usage and popularity trends</p> </li> <li> <p>TIOBE Programming Index News May 2025 - May 13, 2025 - TechRepublic - Analysis of Python reaching highest TIOBE rating in programming language history</p> </li> <li> <p>Python popularity climbs to highest ever \u2013 Tiobe - May 9, 2025 - InfoWorld - Report on Python achieving 25.35% share in TIOBE index</p> </li> <li> <p>TIOBE Index for August 2025 - August 2025 - TechRepublic - Latest programming language rankings showing Python's continued dominance</p> </li> <li> <p>TIOBE Index - Ongoing - TIOBE Software - Monthly programming language popularity index based on search engine results from 25 major platforms</p> </li> </ol> <p>Note that these links were accessed in the past and the links may longer work.</p>"},{"location":"sims/ds-prog-lang/#link-verification","title":"Link Verification","text":"<p>Prompt</p> <p>Verify that these links all work.</p> <ol> <li>Top Programming Language Trends in Data Science: 2025 Insights - June 12, 2025 - upGrad - \u2705 WORKING - Analysis of Python, R, and SQL trends in data science, comparing use cases and future programming shifts</li> <li>The Popularity of Data Science Languages: Python and R (April 2023 Update) - April 11, 2023 - Medium - \u274c NOT ACCESSIBLE - Examination of Python and R rankings in TIOBE, PYPL, and Stack Overflow indices</li> <li>The Popularity of Data Science Languages: Python and R (October 2023 Update) - November 2, 2023 - Medium - \u2705 WORKING - Analysis of Python and R trends with Stack Overflow survey insights</li> <li>Popularity Ranking of Programming Languages - January 24, 2025 - Towards Data Science - \u2705 WORKING - Comprehensive analysis of programming language trends using Stack Overflow data</li> <li>The Popularity of Data Science Languages: Python and R (March 2024 Update) - November 29, 2024 - Medium - \u2705 WORKING - Latest trends showing Python's continued dominance and R's specialized role</li> <li>Top 12 Programming Languages for Data Scientists in 2025 - July 25, 2024 - DataCamp - \u274c NOT ACCESSIBLE - Comprehensive guide to programming languages for data science with pros and cons analysis</li> <li>Python vs. R vs. SQL: 2025 Data Science Programming Language Trends - April 26, 2025 - Analytics Insight - \u2705 WORKING - Comparison of the three dominant data science languages and their use cases</li> <li>The Most Popular Programming Languages in 2025 - February 18, 2025 - Career in STEM - Referenced but not directly verified</li> <li>Don't Miss Out: 12 Data Science Programming Languages 2025! - July 10, 2025 - upGrad - Referenced but not directly verified</li> <li>Stack Overflow Developer Survey 2025 - 2025 - Stack Overflow - \u2705 WORKING - Annual survey of 49,000+ developers from 177 countries on technology usage and trends</li> <li>2025 Stack Overflow Developer Survey Technology Section - 2025 - Stack Overflow - \u2705 WORKING - Detailed breakdown of programming language usage and popularity trends</li> <li>TIOBE Programming Index News May 2025 - May 13, 2025 - TechRepublic - Referenced in search results</li> <li>Python popularity climbs to highest ever -- Tiobe - May 9, 2025 - InfoWorld - Referenced in search results</li> <li>TIOBE Index for August 2025 - August 2025 - TechRepublic - Referenced in search results</li> <li>TIOBE Index - Ongoing - TIOBE Software - Monthly programming language popularity index based on search engine results from 25 major platforms</li> </ol> <p>Summary: Out of the primary sources, 5 out of 7 verified links are working (71% success rate). The non-accessible links appear to be Medium articles that may have access restrictions or have been moved. The core data sources from Stack Overflow, major tech publications, and educational platforms are functioning properly and provide reliable data for the visualization.</p>"},{"location":"sims/four-types-of-data/","title":"Four Types of Data Science Data","text":""},{"location":"sims/four-types-of-data/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/four-types-of-data/main.html\" height=\"600px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p>"},{"location":"sims/four-types-of-data/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive infographic illustrates the four main types of data used in data science:</p> <ol> <li>Image Data - Photographs, medical scans, satellite imagery</li> <li>Sequence Data - Time series, text, audio, DNA sequences</li> <li>Tabular Data - Spreadsheets, databases, structured records</li> <li>Graph Data - Networks, relationships, social connections</li> </ol>"},{"location":"sims/four-types-of-data/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/four-types-of-data/#grade-level-subject-and-topic","title":"Grade Level, Subject and Topic","text":"<p>9th-12th grade introductory data science. Understanding data types.</p>"},{"location":"sims/four-types-of-data/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, students will be able to:</p> <ul> <li>Identify the four main types of data used in data science</li> <li>Give examples of each data type from real-world applications</li> <li>Explain which machine learning approaches work best with each data type</li> </ul>"},{"location":"sims/four-types-of-data/#activities","title":"Activities","text":""},{"location":"sims/four-types-of-data/#data-type-classification","title":"Data Type Classification","text":"<p>Present students with various datasets and have them classify which of the four data types they represent.</p>"},{"location":"sims/four-types-of-data/#real-world-examples","title":"Real-World Examples","text":"<p>Research and present examples of how each data type is used in industry (medical imaging, natural language processing, database analytics, social network analysis).</p>"},{"location":"sims/four-types-of-data/#references","title":"References","text":"<ol> <li>Types of Data in Data Science - Wikipedia - Overview of data types in computing</li> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>An interactive visualization tool for exploring the AI Based Data Science with Python learning graph.</p> <p>Run Graph Viewer MicroSim</p>"},{"location":"sims/graph-viewer/#features","title":"Features","text":""},{"location":"sims/graph-viewer/#search-functionality","title":"Search Functionality","text":"<ul> <li>Type-ahead search with dropdown results</li> <li>Shows category information for each concept</li> <li>Click a result to focus and highlight the node in the graph</li> </ul>"},{"location":"sims/graph-viewer/#category-filtering","title":"Category Filtering","text":"<ul> <li>Color-coded categories in the sidebar legend</li> <li>Checkboxes to show/hide specific taxonomy groups</li> <li>\"Check All\" and \"Uncheck All\" buttons for quick filtering</li> <li>Collapsible sidebar for expanded graph viewing</li> </ul>"},{"location":"sims/graph-viewer/#real-time-statistics","title":"Real-time Statistics","text":"<ul> <li>Visible Nodes: Count of currently displayed concepts</li> <li>Visible Edges: Count of currently displayed dependencies</li> <li>Orphan Nodes: Count of concepts with no connections (when filters applied)</li> </ul>"},{"location":"sims/graph-viewer/#interactive-visualization","title":"Interactive Visualization","text":"<ul> <li>Powered by vis.js network visualization</li> <li>Color-coded nodes by taxonomy category</li> <li>Directed arrows showing concept dependencies</li> <li>Zoomable and draggable interface</li> <li>Physics-based layout with stabilization</li> </ul>"},{"location":"sims/graph-viewer/#categories","title":"Categories","text":"Category Color Description Foundational Concepts Red Core definitions and introduction Python Programming Gold Python language fundamentals Python Libraries Green NumPy, Pandas, and other libraries Data Manipulation Blue Data cleaning and transformation Statistical Analysis Purple Statistical methods and analysis Machine Learning Pink ML algorithms and techniques Data Visualization Gray Plotting and visualization tools Natural Language Processing Brown NLP concepts and methods Generative AI Teal GenAI and advanced topics Projects Cyan Hands-on project concepts Soft Skills Light Green Communication and teamwork Learning Goals Gold Star Course objectives"},{"location":"sims/graph-viewer/#usage-tips","title":"Usage Tips","text":"<ol> <li>Explore by Category: Use the checkboxes to focus on specific topic areas</li> <li>Search for Concepts: Start typing to find specific concepts quickly</li> <li>Navigate the Graph: Drag to pan, scroll to zoom, click nodes to select</li> <li>Toggle Sidebar: Click the hamburger menu to collapse the sidebar for more graph space</li> </ol>"},{"location":"sims/independent-dependent-variables/","title":"Independent vs Dependent Variables","text":"<p>Run the Independent vs Dependent Variables MicroSim Fullscreen</p>"},{"location":"sims/independent-dependent-variables/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive scatter plot visualization helps students understand the fundamental concept of independent and dependent variables by exploring the relationship between study time and test scores.</p>"},{"location":"sims/independent-dependent-variables/#key-concepts","title":"Key Concepts","text":"<ul> <li>Independent Variable (X-axis): Hours Studied - This is what you control or change</li> <li>Dependent Variable (Y-axis): Test Score - This is what you measure or observe</li> </ul>"},{"location":"sims/independent-dependent-variables/#interactive-features","title":"Interactive Features","text":"Control Description Add Student button Adds a random student data point based on current relationship strength Generate Class of 30 button Creates a realistic dataset of 30 students Clear All button Removes all data points Relationship Strength slider Controls how predictable scores are from study time (0-100%) Show Trend Line toggle Display/hide the regression line Show Prediction toggle Show predicted score when hovering over the plot Click on plot Add a student at that exact study time/score location"},{"location":"sims/independent-dependent-variables/#statistics-panel","title":"Statistics Panel","text":"<p>The right panel displays:</p> <ul> <li>Number of students in the dataset</li> <li>Average study time (hours)</li> <li>Average test score (points)</li> <li>Correlation strength with emoji indicator (\ud83d\udcaa strong, \ud83d\udc4d moderate, \ud83e\udd37 weak)</li> </ul>"},{"location":"sims/independent-dependent-variables/#visual-elements","title":"Visual Elements","text":"<ul> <li>\ud83c\udf93 Regular student data points</li> <li>\ud83c\udf1f Outlier students (those who significantly beat or missed expectations)</li> <li>Blue trend line showing the best-fit relationship</li> <li>Light blue confidence band around the trend line</li> <li>Orange dashed prediction lines when hovering</li> </ul>"},{"location":"sims/independent-dependent-variables/#iframe-embedding","title":"Iframe Embedding","text":"<p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/independent-dependent-variables/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/independent-dependent-variables/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/independent-dependent-variables/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this activity, students will be able to:</p> <ol> <li>Identify which variable is independent and which is dependent in a scenario</li> <li>Explain that the independent variable is what you control, and the dependent variable is what you measure</li> <li>Observe how correlation strength affects the scatter of data points</li> <li>Predict approximate outcomes based on the trend line</li> <li>Recognize that relationships show tendencies, not guarantees (there's always variation)</li> </ol>"},{"location":"sims/independent-dependent-variables/#guided-exploration-15-20-minutes","title":"Guided Exploration (15-20 minutes)","text":""},{"location":"sims/independent-dependent-variables/#part-1-understanding-the-relationship-5-min","title":"Part 1: Understanding the Relationship (5 min)","text":"<ol> <li>Start with the default settings (70% relationship strength, 10 students)</li> <li>Ask students: \"Which variable does the researcher control?\"</li> <li>Ask: \"What happens to test scores as study time increases?\"</li> <li>Click \"Generate Class of 30\" to see a more complete picture</li> </ol>"},{"location":"sims/independent-dependent-variables/#part-2-exploring-correlation-strength-5-min","title":"Part 2: Exploring Correlation Strength (5 min)","text":"<ol> <li>Set relationship strength to 100% - observe tight clustering around the line</li> <li>Set relationship strength to 50% - observe more scatter</li> <li>Set relationship strength to 10% - observe near-random scatter</li> <li>Discuss: \"What does a 'strong relationship' mean in real life?\"</li> </ol>"},{"location":"sims/independent-dependent-variables/#part-3-making-predictions-5-min","title":"Part 3: Making Predictions (5 min)","text":"<ol> <li>Enable \"Show Prediction\" and hover over the plot</li> <li>Ask: \"If someone studies 6 hours, what score might they expect?\"</li> <li>Click on the plot to add students at specific locations</li> <li>Discuss outliers: \"Why might someone study a lot but still score low?\"</li> </ol>"},{"location":"sims/independent-dependent-variables/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What are some other examples of independent and dependent variables?</li> <li>Water amount (independent) \u2192 Plant growth (dependent)</li> <li>Exercise time (independent) \u2192 Fitness level (dependent)</li> <li> <p>Temperature (independent) \u2192 Ice cream sales (dependent)</p> </li> <li> <p>Why do we call one variable \"dependent\"?</p> </li> <li> <p>Can we ever be 100% certain about a prediction? Why or why not?</p> </li> <li> <p>What does the correlation emoji (\ud83d\udcaa, \ud83d\udc4d, \ud83e\udd37) tell us about the data?</p> </li> </ol>"},{"location":"sims/independent-dependent-variables/#extension-activities","title":"Extension Activities","text":"<ul> <li>Have students design their own experiment with independent/dependent variables</li> <li>Discuss the difference between correlation and causation</li> <li>Explore what happens when you add extreme outliers manually</li> </ul>"},{"location":"sims/independent-dependent-variables/#technical-details","title":"Technical Details","text":"<ul> <li>Library: p5.js 1.11.10</li> <li>Canvas Size: 700\u00d7500px (responsive width)</li> <li>Draw Height: 400px</li> <li>Control Height: 100px</li> <li>Bloom's Taxonomy Level: Apply (L3)</li> <li>Recommended Duration: 15-20 minutes</li> </ul>"},{"location":"sims/independent-dependent-variables/#image-note","title":"Image Note","text":"<p>Remember to create a screenshot of this MicroSim and save it as <code>independent-dependent-variables.png</code> in this directory for social media preview images.</p>"},{"location":"sims/learning-graph/","title":"View Learning Graph","text":"<p>Run Learning Graph MicroSim</p>"},{"location":"sims/least-squares/","title":"Least Squares MicroSim","text":""},{"location":"sims/least-squares/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/least-squares/main.html\" height=\"565px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the Least Squares MicroSim in Fullscreen</p> <p>Edit the Least Squares MicroSim in the p5.js Editor</p>"},{"location":"sims/least-squares/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive simulation helps students understand the least-squares regression algorithm by visualizing how changing the slope and intercept of a line affects its fit to a set of data points. Students can see the squared residuals (errors) displayed as colored squares, making the concept of \"minimizing squared error\" tangible and intuitive.</p>"},{"location":"sims/least-squares/#lesson-plan-for-high-school-algebra","title":"Lesson Plan for High School Algebra","text":"<p>Linear Functions: Understanding Slope and Intercept</p> <p>Duration: 50 minutes</p> <p>Grade Level: 9-10</p> <p>Subject: Algebra 1</p>"},{"location":"sims/least-squares/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, students will be able to:</p> <ul> <li>Define slope as a rate of change</li> <li>Explain the meaning of y-intercept in a linear function</li> <li>Identify how changes in slope and intercept affect the graph of a line</li> <li>Use an interactive visualization to explore linear functions</li> <li>Solve real-world problems involving slope and intercept</li> </ul>"},{"location":"sims/least-squares/#materials-needed","title":"Materials Needed","text":"<ul> <li>Interactive Slope-Intercept Visualization (p5.js application)</li> <li>Student devices (computers, tablets, or smartphones)</li> <li>Guided worksheet (printed or digital)</li> <li>Whiteboard/projector</li> </ul>"},{"location":"sims/least-squares/#prerequisite-knowledge","title":"Prerequisite Knowledge","text":"<ul> <li>Basic understanding of coordinate plane</li> <li>Ability to plot points on a graph</li> <li>Familiarity with the equation y = mx + b</li> </ul>"},{"location":"sims/least-squares/#lesson-outline","title":"Lesson Outline","text":""},{"location":"sims/least-squares/#1-introduction-5-minutes","title":"1. Introduction (5 minutes)","text":"<ul> <li>Begin with a real-world scenario: \"If you earn $15 per hour at your job, how would you calculate your total earnings?\"</li> <li>Discuss how the relationship between hours worked and money earned forms a linear relationship</li> <li>Introduce the lesson focus: understanding how slope and intercept affect linear functions</li> </ul>"},{"location":"sims/least-squares/#2-review-of-key-concepts-10-minutes","title":"2. Review of Key Concepts (10 minutes)","text":"<ul> <li>Review the slope-intercept form of a line: y = mx + b</li> <li>Define slope (m) as the rate of change (rise/run)</li> <li>Define y-intercept (b) as the point where the line crosses the y-axis (0,b)</li> <li>Demonstrate examples on the board with different values for m and b</li> </ul>"},{"location":"sims/least-squares/#3-interactive-exploration-15-minutes","title":"3. Interactive Exploration (15 minutes)","text":"<ul> <li>Introduce the Slope-Intercept Visualization tool</li> <li>Demonstrate how to use the sliders to change slope and intercept values</li> <li>Explain the visual elements:<ul> <li>Green points represent actual data points</li> <li>Purple points show where the line would predict those values</li> <li>Colored squares show the \"error\" or difference between actual and predicted points</li> </ul> </li> <li>Guided exploration:<ol> <li>What happens when the slope increases? Decreases? Becomes negative?</li> <li>What happens when the y-intercept changes?</li> <li>Can you find values that minimize the differences between actual and predicted points?</li> </ol> </li> </ul>"},{"location":"sims/least-squares/#4-pair-work-10-minutes","title":"4. Pair Work (10 minutes)","text":"<ul> <li>Students work in pairs using the visualization tool</li> <li>Challenge: Find the linear function that best fits the green data points</li> <li>Each pair should record their \"best fit\" values for slope and intercept</li> <li>Discuss strategy: How can you tell when you've found a good fit?</li> </ul>"},{"location":"sims/least-squares/#5-connection-to-real-world-applications-5-minutes","title":"5. Connection to Real-World Applications (5 minutes)","text":"<p>Discuss how the slope-intercept model applies to:</p> <ul> <li>Economics: price vs. quantity relationships</li> <li>Physics: distance vs. time in constant velocity</li> <li>Business: fixed costs (y-intercept) and variable costs (slope)</li> </ul> <p>Show how the colored squares relate to \"error\" in predictions</p>"},{"location":"sims/least-squares/#6-closure-and-assessment-5-minutes","title":"6. Closure and Assessment (5 minutes)","text":"<ul> <li>Quick check for understanding:<ul> <li>\"If a line has a slope of 2 and a y-intercept of -3, what is its equation?\"</li> <li>\"If a line has a negative slope, what does that tell us about the relationship?\"</li> </ul> </li> <li>Exit ticket: Students write one insight they gained from using the visualization</li> </ul>"},{"location":"sims/least-squares/#extension-activities","title":"Extension Activities","text":"<ul> <li>Challenge students to create their own set of points and find the best-fitting line</li> <li>Introduce the concept of \"least squares regression\" as a mathematical way to find the best fit</li> <li>Connect to data science concepts: predictions, error measurements, and model accuracy</li> </ul>"},{"location":"sims/least-squares/#differentiation","title":"Differentiation","text":"<ul> <li>Support: Provide a step-by-step guide for using the visualization tool</li> <li>Extension: Ask advanced students to modify the code to add new features or data points</li> </ul>"},{"location":"sims/least-squares/#assessment","title":"Assessment","text":"<ul> <li>Formative: Observation during interactive exploration and pair work</li> <li>Summative: Exit ticket responses and follow-up homework assignment</li> </ul>"},{"location":"sims/least-squares/#homework","title":"Homework","text":"<ul> <li>Complete practice problems involving writing equations in slope-intercept form</li> <li>Find a real-world example where a linear relationship exists and identify what the slope and intercept represent in that context</li> </ul>"},{"location":"sims/least-squares/#follow-up-lesson-ideas","title":"Follow-Up Lesson Ideas","text":"<ul> <li>Comparing linear vs. non-linear relationships</li> <li>Introduction to systems of linear equations</li> <li>Linear regression with larger datasets</li> </ul>"},{"location":"sims/least-squares/#lesson-plan-focusing-on-prediction-of-future-events","title":"Lesson Plan focusing on Prediction of Future Events","text":""},{"location":"sims/least-squares/#learning-objectives_1","title":"Learning Objectives","text":"<p>By the end of this lesson, students will be able to: - Define slope as a rate of change - Explain the meaning of y-intercept in a linear function - Identify how changes in slope and intercept affect the graph of a line - Use an interactive visualization to explore linear functions - Use a linear model to make predictions for new x-values - Evaluate the reliability of predictions using a linear model - Solve real-world problems involving slope and intercept</p>"},{"location":"sims/least-squares/#materials-needed_1","title":"Materials Needed","text":"<ul> <li>Interactive Slope-Intercept Visualization (p5.js application)</li> <li>Student devices (computers, tablets, or smartphones)</li> <li>Guided worksheet (printed or digital)</li> <li>Whiteboard/projector</li> </ul>"},{"location":"sims/least-squares/#prerequisite-knowledge_1","title":"Prerequisite Knowledge","text":"<ul> <li>Basic understanding of coordinate plane</li> <li>Ability to plot points on a graph</li> <li>Familiarity with the equation y = mx + b</li> </ul>"},{"location":"sims/least-squares/#lesson-outline_1","title":"Lesson Outline","text":""},{"location":"sims/least-squares/#1-introduction-5-minutes_1","title":"1. Introduction (5 minutes)","text":"<ul> <li>Begin with a real-world scenario: \"If you earn $15 per hour at your job, how would you calculate your total earnings?\"</li> <li>Discuss how the relationship between hours worked and money earned forms a linear relationship</li> <li>Introduce the lesson focus: understanding how slope and intercept affect linear functions</li> </ul>"},{"location":"sims/least-squares/#2-review-of-key-concepts-10-minutes_1","title":"2. Review of Key Concepts (10 minutes)","text":"<ul> <li>Review the slope-intercept form of a line: y = mx + b</li> <li>Define slope (m) as the rate of change (rise/run)</li> <li>Define y-intercept (b) as the point where the line crosses the y-axis (0,b)</li> <li>Demonstrate examples on the board with different values for m and b</li> </ul>"},{"location":"sims/least-squares/#3-interactive-exploration-15-minutes_1","title":"3. Interactive Exploration (15 minutes)","text":"<ul> <li>Introduce the Slope-Intercept Visualization tool</li> <li>Demonstrate how to use the sliders to change slope and intercept values</li> <li>Explain the visual elements:</li> <li>Green points represent actual data points</li> <li>Purple points show where the line would predict those values</li> <li>Colored squares show the \"error\" or difference between actual and predicted points</li> <li>Guided exploration:</li> <li>What happens when the slope increases? Decreases? Becomes negative?</li> <li>What happens when the y-intercept changes?</li> <li>Can you find values that minimize the differences between actual and predicted points?</li> </ul>"},{"location":"sims/least-squares/#4-pair-work-10-minutes_1","title":"4. Pair Work (10 minutes)","text":"<ul> <li>Students work in pairs using the visualization tool</li> <li>Challenge: Find the linear function that best fits the green data points</li> <li>Each pair should record their \"best fit\" values for slope and intercept</li> <li>Discuss strategy: How can you tell when you've found a good fit?</li> </ul>"},{"location":"sims/least-squares/#5-prediction-and-real-world-applications-10-minutes","title":"5. Prediction and Real-World Applications (10 minutes)","text":"<ul> <li>Discuss how the slope-intercept model applies to:</li> <li>Economics: price vs. quantity relationships</li> <li>Physics: distance vs. time in constant velocity</li> <li>Business: fixed costs (y-intercept) and variable costs (slope)</li> <li>Show how the colored squares relate to \"error\" in predictions</li> <li>Prediction activity:</li> <li>Given our current \"best fit\" line with slope m and intercept b, what would be the predicted y-value for:<ol> <li>x = 250 (a value within our current data range)</li> <li>x = 600 (a value outside our current data range)</li> </ol> </li> <li>Discuss the concept of interpolation vs. extrapolation</li> <li>Question: \"How confident are we in these predictions and why?\"</li> <li>Question: \"What factors might affect the accuracy of our predictions?\"</li> </ul>"},{"location":"sims/least-squares/#6-closure-and-assessment-5-minutes_1","title":"6. Closure and Assessment (5 minutes)","text":"<ul> <li>Quick check for understanding: </li> <li>\"If a line has a slope of 2 and a y-intercept of -3, what is its equation?\"</li> <li>\"If a line has a negative slope, what does that tell us about the relationship?\"</li> <li>\"Using the equation y = 0.5x + 25, predict the y-value when x = 120\"</li> <li>\"How would you use our linear model to predict a new value not shown on the graph?\"</li> <li>Exit ticket: Students write one insight they gained about using linear models for prediction</li> </ul>"},{"location":"sims/least-squares/#extension-activities_1","title":"Extension Activities","text":"<ul> <li>Challenge students to create their own set of points and find the best-fitting line</li> <li>Introduce the concept of \"least squares regression\" as a mathematical way to find the best fit</li> <li>Connect to data science concepts: predictions, error measurements, and model accuracy</li> </ul>"},{"location":"sims/least-squares/#differentiation_1","title":"Differentiation","text":"<ul> <li>Support: Provide a step-by-step guide for using the visualization tool</li> <li>Extension: Ask advanced students to modify the code to add new features or data points</li> </ul>"},{"location":"sims/least-squares/#assessment_1","title":"Assessment","text":"<ul> <li>Formative: Observation during interactive exploration and pair work</li> <li>Summative: Exit ticket responses and follow-up homework assignment</li> </ul>"},{"location":"sims/least-squares/#homework_1","title":"Homework","text":"<ul> <li>Complete practice problems involving writing equations in slope-intercept form</li> <li>Find a real-world example where a linear relationship exists and identify what the slope and intercept represent in that context</li> <li>Prediction challenge: Given the linear model y = 1.5x + 10:</li> <li>Predict values for x = 50, x = 100, and x = 150</li> <li>If you measured y = 85, what would be the corresponding x value?</li> <li>Create a real-world scenario where this model might be useful, and explain what the slope and intercept represent</li> <li>Explain a situation where this model might break down or become unreliable for predictions</li> </ul>"},{"location":"sims/least-squares/#follow-up-lesson-ideas_1","title":"Follow-Up Lesson Ideas","text":"<ul> <li>Comparing linear vs. non-linear relationships</li> <li>Introduction to systems of linear equations</li> <li>Linear regression with larger datasets</li> </ul>"},{"location":"sims/least-squares/#references","title":"References","text":"<ol> <li>Least Squares Regression - Wikipedia - Comprehensive overview of least squares method and its mathematical foundations</li> <li>Khan Academy: Least Squares Approximation - Khan Academy - Video tutorials explaining regression line calculation</li> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/measurement-scales-pyramid/","title":"Measurement Scales Pyramid","text":"<p>Run the Measurement Scales Pyramid Fullscreen</p>"},{"location":"sims/measurement-scales-pyramid/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive infographic visualizes the four measurement scales as a hierarchy where each level adds mathematical capabilities. The pyramid shape reinforces that:</p> <ul> <li>More variables fit into the lower categories (wider base)</li> <li>More operations are possible at higher levels (narrower top = more exclusive)</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#how-to-use","title":"How to Use","text":"<ul> <li>Hover over any layer to see 5 example variables of that measurement type</li> <li>Click a layer to see which statistical tests are appropriate for that scale</li> <li>Click again to deselect and return to hover mode</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#the-four-measurement-scales","title":"The Four Measurement Scales","text":""},{"location":"sims/measurement-scales-pyramid/#1-nominal-base","title":"1. Nominal (Base)","text":"<ul> <li>Operations: Equality only (= \u2260)</li> <li>Example: Jersey numbers, eye color, blood type</li> <li>Statistics: Mode, chi-square tests, frequency tables</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#2-ordinal","title":"2. Ordinal","text":"<ul> <li>Operations: Equality + ordering (= \u2260 &lt; &gt;)</li> <li>Example: Race positions, Likert scales, pain ratings</li> <li>Statistics: Median, Spearman correlation, Mann-Whitney U</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#3-interval","title":"3. Interval","text":"<ul> <li>Operations: Equality + ordering + equal intervals (= \u2260 &lt; &gt; + \u2212)</li> <li>Example: Temperature in Fahrenheit, IQ scores, calendar dates</li> <li>Statistics: Mean, t-tests, Pearson correlation, regression</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#4-ratio-top","title":"4. Ratio (Top)","text":"<ul> <li>Operations: Full arithmetic (= \u2260 &lt; &gt; + \u2212 \u00d7 \u00f7)</li> <li>Example: Height, weight, age, income</li> <li>Statistics: All statistical tests, geometric mean, coefficient of variation</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#why-this-matters","title":"Why This Matters","text":"<p>Choosing the wrong statistical test for your measurement scale can lead to meaningless results. For example:</p> <ul> <li>Computing the mean of jersey numbers is meaningless (nominal data)</li> <li>You can't say \"80\u00b0F is twice as hot as 40\u00b0F\" (interval, not ratio)</li> <li>Race positions 1st, 2nd, 3rd don't tell you the time gaps (ordinal)</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#learning-objectives","title":"Learning Objectives","text":"<p>After exploring this visualization, students should be able to:</p> <ol> <li>Remember (L1): Name the four measurement scales in order</li> <li>Understand (L1): Explain what operations are valid for each scale</li> <li>Apply (L2): Classify any variable into the correct measurement scale</li> <li>Analyze (L3): Select appropriate statistical tests based on measurement scale</li> </ol>"},{"location":"sims/measurement-scales-pyramid/#related-resources","title":"Related Resources","text":"<ul> <li>Chapter 1: Introduction to Data Science</li> <li>Variable Types Decision Tree</li> <li>Four Types of Data</li> </ul>"},{"location":"sims/measurement-scales-pyramid/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>You can include this MicroSim on your website using the following iframe:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/measurement-scales-pyramid/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/microsim-lists/","title":"List of Data Science MicroSims Lists","text":"<p>What would be the most useful MicroSims in a Data Science course?</p> <p>We can ask our generative AI tools what concepts are the most difficult to understand in Data Science that could be explained by a MicroSim.</p> <p>Here are the top 10 microsims for reviewing basic statistics: - 10 Stats Review MicroSims</p> <p>Here are the top 100 data science microsims: - 100 Data Science MicroSims</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/","title":"10 Basic Statistics Review MicroSims","text":"<p>Here\u2019s a scaffolded set of Ten Basic Statistics MicroSims designed for students entering your data science course without a strong statistics background. I\u2019ve ordered them from simplest to more complex, drawing from the MicroSim styles already in your library.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#1-exploring-data-points","title":"1. Exploring Data Points","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description","title":"Description","text":"<p>Students click to add or remove points on a 2D scatter plot, instantly seeing the effect on the dataset.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals","title":"Learning Goals","text":"<ul> <li>Recognize individual data points as observations.</li> <li>Understand how points are represented in x\u2013y coordinates.</li> <li>See how changes in points affect the shape of a dataset.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls","title":"Input Controls","text":"<ol> <li>Add Point (click on canvas)</li> <li>Remove Point (click existing point)</li> <li>Clear All Points (button)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#2-histogram-builder","title":"2. Histogram Builder","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_1","title":"Description","text":"<p>Students adjust bin sizes to see how histograms change, revealing over-smoothing or excessive detail.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_1","title":"Learning Goals","text":"<ul> <li>Understand bins and frequency counts.</li> <li>Relate bin size to detail retention in data.</li> <li>Connect histograms to raw data.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_1","title":"Input Controls","text":"<ol> <li>Bin Size Slider</li> <li>Dataset Selector (normal, uniform, skewed)</li> <li>Toggle Grid Lines (checkbox)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#3-mean-and-median-explorer","title":"3. Mean and Median Explorer","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_2","title":"Description","text":"<p>Drag points along a number line to see how the mean and median respond differently to changes.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_2","title":"Learning Goals","text":"<ul> <li>Differentiate mean vs. median.</li> <li>Understand how outliers affect each measure.</li> <li>See why median is more robust.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_2","title":"Input Controls","text":"<ol> <li>Drag Points (mouse)</li> <li>Add Outlier (button)</li> <li>Reset Points (button)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#4-distribution-shape-explorer","title":"4. Distribution Shape Explorer","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_3","title":"Description","text":"<p>Morph between uniform, normal, skewed, and bimodal distributions and see parameter changes in real time.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_3","title":"Learning Goals","text":"<ul> <li>Identify common distribution shapes.</li> <li>Understand skewness and kurtosis.</li> <li>Connect distribution properties to data characteristics.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_3","title":"Input Controls","text":"<ol> <li>Distribution Type Selector</li> <li>Skewness Slider</li> <li>Kurtosis Slider</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#5-correlation-playground","title":"5. Correlation Playground","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_4","title":"Description","text":"<p>Students drag clusters of points to change correlation and watch the correlation coefficient update.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_4","title":"Learning Goals","text":"<ul> <li>Visualize correlation strength and direction.</li> <li>Understand positive, negative, and zero correlation.</li> <li>See the impact of noise on correlation.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_4","title":"Input Controls","text":"<ol> <li>Drag Cluster (mouse)</li> <li>Add Noise (slider)</li> <li>Show Best Fit Line (toggle)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#6-least-squares-line-fitter","title":"6. Least Squares Line Fitter","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_5","title":"Description","text":"<p>Adjust slope and intercept to minimize squared residuals with visual feedback.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_5","title":"Learning Goals","text":"<ul> <li>Understand slope, intercept, and residuals.</li> <li>Experience manual fitting before automation.</li> <li>Build intuition for regression.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_5","title":"Input Controls","text":"<ol> <li>Slope Slider</li> <li>Intercept Slider</li> <li>Toggle Residual Squares (checkbox)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#7-r2-intuition-builder","title":"7. R\u00b2 Intuition Builder","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_6","title":"Description","text":"<p>Manipulate data spread around a regression line and see how R\u00b2 changes.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_6","title":"Learning Goals","text":"<ul> <li>Understand what R\u00b2 measures.</li> <li>Relate R\u00b2 to model fit.</li> <li>Recognize limits of R\u00b2 as a single metric.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_6","title":"Input Controls","text":"<ol> <li>Noise Level Slider</li> <li>Number of Points Slider</li> <li>Reset Dataset (button)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#8-sampling-bias-demonstrator","title":"8. Sampling Bias Demonstrator","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_7","title":"Description","text":"<p>Draw samples from representative or biased datasets to see effects on mean/median estimates.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_7","title":"Learning Goals","text":"<ul> <li>Recognize sampling bias.</li> <li>Connect bias to flawed conclusions.</li> <li>Practice identifying representative samples.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_7","title":"Input Controls","text":"<ol> <li>Sampling Method Selector (random, biased)</li> <li>Sample Size Slider</li> <li>Reset Data (button)</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#9-hypothesis-testing-visualizer","title":"9. Hypothesis Testing Visualizer","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_8","title":"Description","text":"<p>Adjust sample statistics to see how p-values change under different population means.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_8","title":"Learning Goals","text":"<ul> <li>Understand null and alternative hypotheses.</li> <li>Interpret p-values visually.</li> <li>See the effect of sample size on significance.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_8","title":"Input Controls","text":"<ol> <li>Population Mean Slider</li> <li>Sample Mean Slider</li> <li>Sample Size Slider</li> </ol>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#10-confidence-interval-explorer","title":"10. Confidence Interval Explorer","text":""},{"location":"sims/microsim-lists/10-stats-review-microsims/#description_9","title":"Description","text":"<p>Show multiple sample means with confidence intervals to explore coverage probability.</p>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#learning-goals_9","title":"Learning Goals","text":"<ul> <li>Understand confidence intervals conceptually.</li> <li>Connect sample size to interval width.</li> <li>Interpret intervals in the context of repeated sampling.</li> </ul>"},{"location":"sims/microsim-lists/10-stats-review-microsims/#input-controls_9","title":"Input Controls","text":"<ol> <li>Confidence Level Slider</li> <li>Sample Size Slider</li> <li>Number of Samples Slider</li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/","title":"List of 100 Top MicroSims","text":"<p>This is a list of the 100 most important MicroSims for a Data Science course according to GPT-5. Each MicroSim is described in a level-2 header with the description, learning goals and input controls in level 3 headers.  We will have agents generate these MicroSims one at a time.</p> <p>Note that if students don't have a strong statistics background, they should review them.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#1-exploring-data-points","title":"1. Exploring Data Points","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description","title":"Description","text":"<p>Students click to add or remove points on a 2D scatter plot, instantly seeing the effect on the overall distribution.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals","title":"Learning Goals","text":"<ul> <li> <p>Recognize individual observations in a dataset</p> </li> <li> <p>Understand x--y coordinate representation</p> </li> <li> <p>See how adding/removing points changes data shape</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls","title":"Input Controls","text":"<ol> <li> <p>Add Point (click on canvas)</p> </li> <li> <p>Remove Point (click existing point)</p> </li> <li> <p>Clear All Points (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#2-histogram-builder","title":"2. Histogram Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_1","title":"Description","text":"<p>Students adjust bin sizes to see how histograms change, revealing over-smoothing and under-smoothing effects.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_1","title":"Learning Goals","text":"<ul> <li> <p>Understand bins and frequencies</p> </li> <li> <p>Relate bin size to data detail retention</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_1","title":"Input Controls","text":"<ol> <li> <p>Bin Size Slider</p> </li> <li> <p>Dataset Selector (dropdown: normal, uniform, skewed)</p> </li> <li> <p>Toggle Grid Lines (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#3-mean-and-median-explorer","title":"3. Mean and Median Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_2","title":"Description","text":"<p>Drag points along a number line to see how mean and median shift differently.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_2","title":"Learning Goals","text":"<ul> <li> <p>Differentiate mean vs. median</p> </li> <li> <p>Observe robustness of median to outliers</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_2","title":"Input Controls","text":"<ol> <li> <p>Drag Points (mouse)</p> </li> <li> <p>Add Outlier (button)</p> </li> <li> <p>Reset Points (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#4-correlation-playground","title":"4. Correlation Playground","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_3","title":"Description","text":"<p>Students drag clusters of points to adjust correlation, watching the correlation coefficient update in real-time.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_3","title":"Learning Goals","text":"<ul> <li> <p>Visualize correlation strength and direction</p> </li> <li> <p>Understand positive, negative, and zero correlation</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_3","title":"Input Controls","text":"<ol> <li> <p>Drag Cluster (mouse)</p> </li> <li> <p>Add Noise (slider)</p> </li> <li> <p>Show Best Fit Line (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#5-least-squares-line-fitter","title":"5. Least Squares Line Fitter","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_4","title":"Description","text":"<p>Adjust slope and intercept manually to minimize sum of squared errors, with real-time residual visualization.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_4","title":"Learning Goals","text":"<ul> <li> <p>Understand slope, intercept, and residuals</p> </li> <li> <p>Experience trial-and-error fitting</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_4","title":"Input Controls","text":"<ol> <li> <p>Slope Slider</p> </li> <li> <p>Intercept Slider</p> </li> <li> <p>Toggle Residual Squares (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#6-r2-intuition-builder","title":"6. R\u00b2 Intuition Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_5","title":"Description","text":"<p>Manipulate data spread around a fitted line to see how R\u00b2 changes.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_5","title":"Learning Goals","text":"<ul> <li> <p>Understand coefficient of determination</p> </li> <li> <p>Relate R\u00b2 to model fit quality</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_5","title":"Input Controls","text":"<ol> <li> <p>Noise Level Slider</p> </li> <li> <p>Number of Points Slider</p> </li> <li> <p>Reset Dataset (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#7-train-test-split-visualizer","title":"7. Train-Test Split Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_6","title":"Description","text":"<p>Randomly split a dataset and see how train/test points differ in model performance.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_6","title":"Learning Goals","text":"<ul> <li> <p>Understand importance of splitting data</p> </li> <li> <p>See overfitting risk when test set is too small</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_6","title":"Input Controls","text":"<ol> <li> <p>Train/Test Ratio Slider</p> </li> <li> <p>Resample Dataset (button)</p> </li> <li> <p>Model Complexity Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#8-cross-validation-simulator","title":"8. Cross-Validation Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_7","title":"Description","text":"<p>Animate k-fold cross-validation, showing shifting train/test subsets and aggregated scores.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_7","title":"Learning Goals","text":"<ul> <li> <p>Understand cross-validation mechanics</p> </li> <li> <p>See benefits over single train-test split</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_7","title":"Input Controls","text":"<ol> <li> <p>Number of Folds Slider</p> </li> <li> <p>Dataset Size Slider</p> </li> <li> <p>Play/Pause Animation (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#9-overfitting-vs-underfitting-explorer","title":"9. Overfitting vs. Underfitting Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_8","title":"Description","text":"<p>Adjust polynomial degree to see bias--variance trade-off on train vs. test errors.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_8","title":"Learning Goals","text":"<ul> <li> <p>Recognize overfitting and underfitting patterns</p> </li> <li> <p>Connect complexity to generalization</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_8","title":"Input Controls","text":"<ol> <li> <p>Polynomial Degree Slider</p> </li> <li> <p>Noise Level Slider</p> </li> <li> <p>Toggle Error Curves (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#10-2-multiple-regression-plane","title":"10. $2 Multiple Regression Plane","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_9","title":"Description","text":"<p>Manipulate two independent variables in 3D space to see a regression plane fit to data.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_9","title":"Learning Goals","text":"<ul> <li> <p>Visualize multivariate linear regression</p> </li> <li> <p>See plane adjustment with variable changes</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_9","title":"Input Controls","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#1-rotate-view-mouse-drag","title":"1.  Rotate View (mouse drag)","text":"<ol> <li> <p>Noise Level Slider</p> </li> <li> <p>Add/Remove Points (click)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#11-residuals-heatmap-viewer","title":"11. Residuals Heatmap Viewer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_10","title":"Description","text":"<p>Color-code residuals on a scatter plot to identify patterns and non-linearity.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_10","title":"Learning Goals","text":"<ul> <li> <p>Understand residual analysis</p> </li> <li> <p>Detect systematic errors in model predictions</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_10","title":"Input Controls","text":"<ol> <li> <p>Model Complexity Slider</p> </li> <li> <p>Toggle Residual Colors</p> </li> <li> <p>Noise Level Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#12-distribution-shape-explorer","title":"12. Distribution Shape Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_11","title":"Description","text":"<p>Morph between uniform, normal, skewed, and bimodal distributions.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_11","title":"Learning Goals","text":"<ul> <li> <p>Identify common data distributions</p> </li> <li> <p>Understand skewness and kurtosis visually</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_11","title":"Input Controls","text":"<ol> <li> <p>Distribution Type Selector</p> </li> <li> <p>Skewness Slider</p> </li> <li> <p>Kurtosis Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#13-box-plot-anatomy","title":"13. Box Plot Anatomy","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_12","title":"Description","text":"<p>Interactively adjust dataset values to see quartiles, whiskers, and outliers update in real time.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_12","title":"Learning Goals","text":"<ul> <li> <p>Interpret box plot components</p> </li> <li> <p>Relate box plot features to dataset properties</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_12","title":"Input Controls","text":"<ol> <li> <p>Drag Data Points</p> </li> <li> <p>Add Outlier (button)</p> </li> <li> <p>Reset Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#14-central-limit-theorem-animator","title":"14. Central Limit Theorem Animator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_13","title":"Description","text":"<p>Sample repeatedly from various population distributions and watch sampling distribution approach normality.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_13","title":"Learning Goals","text":"<ul> <li> <p>Visualize the CLT in action</p> </li> <li> <p>Understand why normality emerges</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_13","title":"Input Controls","text":"<ol> <li> <p>Population Distribution Selector</p> </li> <li> <p>Sample Size Slider</p> </li> <li> <p>Number of Samples Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#15-sampling-bias-demonstrator","title":"15. Sampling Bias Demonstrator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_14","title":"Description","text":"<p>Draw samples from skewed or representative datasets to see effect on mean/median estimates.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_14","title":"Learning Goals","text":"<ul> <li> <p>Recognize sampling bias</p> </li> <li> <p>Relate bias to poor generalization</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_14","title":"Input Controls","text":"<ol> <li> <p>Sampling Method Selector (random, biased)</p> </li> <li> <p>Sample Size Slider</p> </li> <li> <p>Reset Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#16-hypothesis-testing-visualizer","title":"16. Hypothesis Testing Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_15","title":"Description","text":"<p>Adjust population mean and see how p-values change for given sample statistics.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_15","title":"Learning Goals","text":"<ul> <li> <p>Understand null/alternative hypotheses</p> </li> <li> <p>Interpret p-values visually</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_15","title":"Input Controls","text":"<ol> <li> <p>Population Mean Slider</p> </li> <li> <p>Sample Mean Slider</p> </li> <li> <p>Sample Size Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#17-confidence-interval-explorer","title":"17. Confidence Interval Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_16","title":"Description","text":"<p>Show multiple sample means with confidence intervals and see coverage percentage.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_16","title":"Learning Goals","text":"<ul> <li> <p>Understand confidence interval interpretation</p> </li> <li> <p>See how sample size affects interval width</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_16","title":"Input Controls","text":"<ol> <li> <p>Confidence Level Slider</p> </li> <li> <p>Sample Size Slider</p> </li> <li> <p>Number of Samples Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#18-t-test-simulator","title":"18. t-Test Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_17","title":"Description","text":"<p>Compare means of two groups with adjustable overlap and see t-statistic and p-value.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_17","title":"Learning Goals","text":"<ul> <li> <p>Perform and interpret t-tests</p> </li> <li> <p>Relate group separation to statistical significance</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_17","title":"Input Controls","text":"<ol> <li> <p>Mean Difference Slider</p> </li> <li> <p>Sample Size Slider</p> </li> <li> <p>Variance Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#19-correlation-vs-causation-scenario-builder","title":"19. Correlation vs. Causation Scenario Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_18","title":"Description","text":"<p>Toggle between linked and independent variables with visual storytelling elements.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_18","title":"Learning Goals","text":"<ul> <li> <p>Distinguish correlation from causation</p> </li> <li> <p>Recognize spurious correlations</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_18","title":"Input Controls","text":"<ol> <li> <p>Relationship Type Selector</p> </li> <li> <p>Add Confounder Variable (button)</p> </li> <li> <p>Noise Level Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#20-data-cleaning-sandbox","title":"20. Data Cleaning Sandbox","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_19","title":"Description","text":"<p>Interactively identify and fix missing values, duplicates, and inconsistencies in a small dataset.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_19","title":"Learning Goals","text":"<ul> <li> <p>Practice data cleaning operations</p> </li> <li> <p>Recognize data quality issues</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_19","title":"Input Controls","text":"<ol> <li> <p>Highlight Missing Values (checkbox)</p> </li> <li> <p>Fill Missing Values Method Selector</p> </li> <li> <p>Remove Duplicates (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#21-missing-data-imputation-lab","title":"21. Missing Data Imputation Lab","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_20","title":"Description","text":"<p>Students choose different strategies to fill in missing values and compare how summaries change.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_20","title":"Learning Goals","text":"<ul> <li> <p>Explore mean, median, mode, and model-based imputation</p> </li> <li> <p>See effects of imputation on dataset statistics</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_20","title":"Input Controls","text":"<ol> <li> <p>Imputation Method Selector</p> </li> <li> <p>Preview Changes (toggle)</p> </li> <li> <p>Apply Changes (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#22-one-hot-encoding-demonstrator","title":"22. One-Hot Encoding Demonstrator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_21","title":"Description","text":"<p>Convert categorical variables into binary columns and see the dataset shape change.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_21","title":"Learning Goals","text":"<ul> <li> <p>Understand one-hot encoding</p> </li> <li> <p>Recognize dataset expansion with categorical variables</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_21","title":"Input Controls","text":"<ol> <li> <p>Category Count Slider</p> </li> <li> <p>Toggle Encoding (checkbox)</p> </li> <li> <p>Reset Categories (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#23-feature-scaling-visualizer","title":"23. Feature Scaling Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_22","title":"Description","text":"<p>Scale features using min-max, standardization, or robust scaling, and compare results.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_22","title":"Learning Goals","text":"<ul> <li> <p>Understand scaling methods</p> </li> <li> <p>Recognize scaling's impact on model training</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_22","title":"Input Controls","text":"<ol> <li> <p>Scaling Method Selector</p> </li> <li> <p>Dataset Selector</p> </li> <li> <p>Apply Scaling (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#24-scatter-plot-matrix-explorer","title":"24. Scatter Plot Matrix Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_23","title":"Description","text":"<p>Select variables to display in an interactive scatter plot matrix.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_23","title":"Learning Goals","text":"<ul> <li> <p>Visualize pairwise relationships</p> </li> <li> <p>Identify potential multicollinearity</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_23","title":"Input Controls","text":"<ol> <li> <p>Variable Selector (multi-select)</p> </li> <li> <p>Highlight Correlated Pairs (toggle)</p> </li> <li> <p>Reset Matrix (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#25-multicollinearity-detector","title":"25. Multicollinearity Detector","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_24","title":"Description","text":"<p>Add or remove features and see the correlation heatmap update in real time.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_24","title":"Learning Goals","text":"<ul> <li> <p>Recognize multicollinearity</p> </li> <li> <p>Learn its impact on regression models</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_24","title":"Input Controls","text":"<ol> <li> <p>Add Feature (dropdown)</p> </li> <li> <p>Remove Feature (click)</p> </li> <li> <p>Threshold Slider for correlation warning</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#26-gradient-descent-animation","title":"26. Gradient Descent Animation","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_25","title":"Description","text":"<p>Visualize gradient descent steps on a 3D loss surface.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_25","title":"Learning Goals","text":"<ul> <li> <p>Understand optimization paths</p> </li> <li> <p>See effects of learning rate changes</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_25","title":"Input Controls","text":"<ol> <li> <p>Learning Rate Slider</p> </li> <li> <p>Start Position Selector</p> </li> <li> <p>Play/Pause Steps (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#27-loss-function-comparator","title":"27. Loss Function Comparator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_26","title":"Description","text":"<p>Compare MSE, MAE, and Huber loss on the same dataset.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_26","title":"Learning Goals","text":"<ul> <li> <p>Understand different loss functions</p> </li> <li> <p>Recognize how they respond to outliers</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_26","title":"Input Controls","text":"<ol> <li> <p>Loss Function Selector</p> </li> <li> <p>Add Outlier (button)</p> </li> <li> <p>Reset Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#28-logistic-regression-probability-curve","title":"28. Logistic Regression Probability Curve","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_27","title":"Description","text":"<p>Adjust slope and intercept to see how the logistic curve shifts and steepens.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_27","title":"Learning Goals","text":"<ul> <li> <p>Understand logistic regression shape</p> </li> <li> <p>Relate slope to classification threshold sharpness</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_27","title":"Input Controls","text":"<ol> <li> <p>Slope Slider</p> </li> <li> <p>Intercept Slider</p> </li> <li> <p>Show Decision Boundary (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#29-confusion-matrix-builder","title":"29. Confusion Matrix Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_28","title":"Description","text":"<p>Manually adjust predictions to see confusion matrix cells update and metrics recalculate.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_28","title":"Learning Goals","text":"<ul> <li> <p>Interpret precision, recall, F1-score</p> </li> <li> <p>See trade-offs in prediction thresholds</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_28","title":"Input Controls","text":"<ol> <li> <p>Threshold Slider</p> </li> <li> <p>Toggle Misclassification Highlight (checkbox)</p> </li> <li> <p>Reset Predictions (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#30-roc-curve-interactive-plotter","title":"30. ROC Curve Interactive Plotter","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_29","title":"Description","text":"<p>Drag threshold point along the curve to see corresponding confusion matrix metrics.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_29","title":"Learning Goals","text":"<ul> <li> <p>Understand ROC curves</p> </li> <li> <p>Relate AUC to model performance</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_29","title":"Input Controls","text":"<ol> <li> <p>Move Threshold Point (mouse drag)</p> </li> <li> <p>Toggle AUC Display (checkbox)</p> </li> <li> <p>Dataset Selector</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#31-precision-recall-trade-off-tool","title":"31. Precision-Recall Trade-off Tool","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_30","title":"Description","text":"<p>Visualize precision and recall lines as threshold changes, highlighting the F1-score peak.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_30","title":"Learning Goals","text":"<ul> <li> <p>Recognize trade-offs between precision and recall</p> </li> <li> <p>Identify optimal balance point</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_30","title":"Input Controls","text":"<ol> <li> <p>Threshold Slider</p> </li> <li> <p>Show F1 Peak (toggle)</p> </li> <li> <p>Reset Chart (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#32-decision-tree-split-explorer","title":"32. Decision Tree Split Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_31","title":"Description","text":"<p>Select split features and thresholds to see how data partitions change.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_31","title":"Learning Goals","text":"<ul> <li> <p>Understand feature-based splitting</p> </li> <li> <p>Recognize overfitting in deep trees</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_31","title":"Input Controls","text":"<ol> <li> <p>Feature Selector</p> </li> <li> <p>Threshold Slider</p> </li> <li> <p>Add Split (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#33-random-forest-voting-visualizer","title":"33. Random Forest Voting Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_32","title":"Description","text":"<p>Show predictions from individual trees and how majority vote determines the final prediction.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_32","title":"Learning Goals","text":"<ul> <li> <p>Understand ensemble voting</p> </li> <li> <p>See stability from multiple models</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_32","title":"Input Controls","text":"<ol> <li> <p>Number of Trees Slider</p> </li> <li> <p>Tree Depth Slider</p> </li> <li> <p>Noise Level Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#34-bagging-vs-boosting-simulator","title":"34. Bagging vs. Boosting Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_33","title":"Description","text":"<p>Switch between bagging and boosting to compare error reduction over iterations.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_33","title":"Learning Goals","text":"<ul> <li> <p>Contrast two ensemble methods</p> </li> <li> <p>Understand impact on bias and variance</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_33","title":"Input Controls","text":"<ol> <li> <p>Method Selector</p> </li> <li> <p>Number of Estimators Slider</p> </li> <li> <p>Learning Rate Slider (for boosting)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#35-k-means-clustering-playground","title":"35. k-Means Clustering Playground","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_34","title":"Description","text":"<p>Move cluster centers and see point assignments change instantly.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_34","title":"Learning Goals","text":"<ul> <li> <p>Understand k-means mechanics</p> </li> <li> <p>Recognize sensitivity to initialization</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_34","title":"Input Controls","text":"<ol> <li> <p>Number of Clusters Slider</p> </li> <li> <p>Drag Cluster Centers</p> </li> <li> <p>Reset Clusters (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#36-elbow-method-visualizer","title":"36. Elbow Method Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_35","title":"Description","text":"<p>Generate k-means cost curve to find optimal k.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_35","title":"Learning Goals","text":"<ul> <li> <p>Apply elbow method for cluster selection</p> </li> <li> <p>Interpret inertia curve</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_35","title":"Input Controls","text":"<ol> <li> <p>Max k Slider</p> </li> <li> <p>Dataset Selector</p> </li> <li> <p>Recalculate Curve (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#37-hierarchical-clustering-dendrogram","title":"37. Hierarchical Clustering Dendrogram","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_36","title":"Description","text":"<p>Cut dendrogram at different heights to form clusters.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_36","title":"Learning Goals","text":"<ul> <li> <p>Interpret dendrograms</p> </li> <li> <p>Relate cut height to cluster count</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_36","title":"Input Controls","text":"<ol> <li> <p>Cut Height Slider</p> </li> <li> <p>Dataset Selector</p> </li> <li> <p>Toggle Leaf Labels (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#38-pca-variance-explorer","title":"38. PCA Variance Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_37","title":"Description","text":"<p>Adjust number of principal components and see variance explained update.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_37","title":"Learning Goals","text":"<ul> <li> <p>Understand dimensionality reduction</p> </li> <li> <p>Relate components to variance retention</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_37","title":"Input Controls","text":"<ol> <li> <p>Number of Components Slider</p> </li> <li> <p>Dataset Selector</p> </li> <li> <p>Show Projection (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#39-pca-projection-visualizer","title":"39. PCA Projection Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_38","title":"Description","text":"<p>Project high-dimensional data into 2D and explore structure.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_38","title":"Learning Goals","text":"<ul> <li> <p>Visualize principal component projections</p> </li> <li> <p>Detect patterns in reduced space</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_38","title":"Input Controls","text":"<ol> <li> <p>Rotate Projection (mouse drag)</p> </li> <li> <p>Highlight Class Labels (toggle)</p> </li> <li> <p>Reset View (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#40-feature-importance-bar-chart","title":"40. Feature Importance Bar Chart","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_39","title":"Description","text":"<p>Interactively remove features and see model accuracy update.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_39","title":"Learning Goals","text":"<ul> <li> <p>Rank feature contributions</p> </li> <li> <p>Recognize redundancy in predictors</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_39","title":"Input Controls","text":"<ol> <li> <p>Remove Feature (click bar)</p> </li> <li> <p>Recalculate Accuracy (button)</p> </li> <li> <p>Reset Features (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#41-time-series-trend-explorer","title":"41. Time Series Trend Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_40","title":"Description","text":"<p>Students add or remove long-term upward or downward trends to see their effect on time series plots.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_40","title":"Learning Goals","text":"<ul> <li> <p>Recognize trends in time series data</p> </li> <li> <p>Separate trend from noise visually</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_40","title":"Input Controls","text":"<ol> <li> <p>Trend Slope Slider</p> </li> <li> <p>Noise Level Slider</p> </li> <li> <p>Reset Series (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#42-seasonality-animator","title":"42. Seasonality Animator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_41","title":"Description","text":"<p>Add seasonal patterns to time series and adjust amplitude/frequency.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_41","title":"Learning Goals","text":"<ul> <li> <p>Understand seasonality components</p> </li> <li> <p>Differentiate seasonal effects from trends</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_41","title":"Input Controls","text":"<ol> <li> <p>Amplitude Slider</p> </li> <li> <p>Frequency Slider</p> </li> <li> <p>Toggle Seasonal Component (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#43-autocorrelation-plot-builder","title":"43. Autocorrelation Plot Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_42","title":"Description","text":"<p>Interactively generate autocorrelation plots for different time series patterns.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_42","title":"Learning Goals","text":"<ul> <li> <p>Recognize autocorrelation signatures</p> </li> <li> <p>Link patterns to time lags</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_42","title":"Input Controls","text":"<ol> <li> <p>Pattern Selector (trend, seasonal, white noise)</p> </li> <li> <p>Series Length Slider</p> </li> <li> <p>Recalculate Plot (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#44-moving-average-filter","title":"44. Moving Average Filter","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_43","title":"Description","text":"<p>Smooth noisy time series using different window sizes.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_43","title":"Learning Goals","text":"<ul> <li> <p>Apply moving average smoothing</p> </li> <li> <p>Understand trade-off between smoothing and responsiveness</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_43","title":"Input Controls","text":"<ol> <li> <p>Window Size Slider</p> </li> <li> <p>Toggle Original Series (checkbox)</p> </li> <li> <p>Reset Filter (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#45-exponential-smoothing-explorer","title":"45. Exponential Smoothing Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_44","title":"Description","text":"<p>Adjust smoothing factor to see effect on responsiveness to new data.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_44","title":"Learning Goals","text":"<ul> <li> <p>Understand exponential smoothing</p> </li> <li> <p>Compare to simple moving average</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_44","title":"Input Controls","text":"<ol> <li> <p>Smoothing Factor Slider (0--1)</p> </li> <li> <p>Toggle Forecast Values (checkbox)</p> </li> <li> <p>Reset Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#46-arima-model-simulator","title":"46. ARIMA Model Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_45","title":"Description","text":"<p>Experiment with AR, I, and MA parameters to fit simple time series.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_45","title":"Learning Goals","text":"<ul> <li> <p>Recognize ARIMA components</p> </li> <li> <p>See parameter effects on forecast shape</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_45","title":"Input Controls","text":"<ol> <li> <p>AR Order Slider</p> </li> <li> <p>I Order Slider</p> </li> <li> <p>MA Order Slider</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#47-train-test-split-for-time-series","title":"47. Train-Test Split for Time Series","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_46","title":"Description","text":"<p>Split data chronologically and compare forecasting performance.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_46","title":"Learning Goals","text":"<ul> <li> <p>Understand why random splits don't work in time series</p> </li> <li> <p>Practice chronological evaluation</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_46","title":"Input Controls","text":"<ol> <li> <p>Split Point Slider</p> </li> <li> <p>Model Selector</p> </li> <li> <p>Show Forecast Horizon (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#48-outlier-impact-on-time-series","title":"48. Outlier Impact on Time Series","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_47","title":"Description","text":"<p>Insert outliers into time series and see how forecasts change.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_47","title":"Learning Goals","text":"<ul> <li> <p>Recognize sensitivity to anomalies</p> </li> <li> <p>Understand need for preprocessing</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_47","title":"Input Controls","text":"<ol> <li> <p>Insert Outlier (click point)</p> </li> <li> <p>Outlier Magnitude Slider</p> </li> <li> <p>Remove Outliers (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#49-tf-idf-text-weighting-tool","title":"49. TF-IDF Text Weighting Tool","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_48","title":"Description","text":"<p>Type or paste text and see term frequencies and TF-IDF scores update live.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_48","title":"Learning Goals","text":"<ul> <li> <p>Understand term frequency weighting</p> </li> <li> <p>Recognize the role of inverse document frequency</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_48","title":"Input Controls","text":"<ol> <li> <p>Text Input Box</p> </li> <li> <p>Toggle Stopword Removal (checkbox)</p> </li> <li> <p>Recalculate Scores (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#50-tokenization-visualizer","title":"50. Tokenization Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_49","title":"Description","text":"<p>See text split into tokens using different tokenization rules.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_49","title":"Learning Goals","text":"<ul> <li> <p>Understand tokenization</p> </li> <li> <p>Compare word vs. subword methods</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_49","title":"Input Controls","text":"<ol> <li> <p>Text Input Box</p> </li> <li> <p>Tokenizer Type Selector</p> </li> <li> <p>Show Token IDs (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#51-word-embedding-explorer","title":"51. Word Embedding Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_50","title":"Description","text":"<p>Plot word embeddings in 2D space and explore semantic similarity.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_50","title":"Learning Goals","text":"<ul> <li> <p>Understand word vector representations</p> </li> <li> <p>See clusters of related words</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_50","title":"Input Controls","text":"<ol> <li> <p>Select Word to Highlight (dropdown)</p> </li> <li> <p>Show Similar Words (toggle)</p> </li> <li> <p>Reset Embeddings (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#52-sentiment-classification-threshold-tool","title":"52. Sentiment Classification Threshold Tool","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_51","title":"Description","text":"<p>Adjust sentiment score threshold to see how classifications change.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_51","title":"Learning Goals","text":"<ul> <li> <p>Understand sentiment score distributions</p> </li> <li> <p>See trade-offs in precision and recall for sentiment tasks</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_51","title":"Input Controls","text":"<ol> <li> <p>Threshold Slider</p> </li> <li> <p>Show Confusion Matrix (checkbox)</p> </li> <li> <p>Dataset Selector</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#53-bag-of-words-vs-embeddings","title":"53. Bag-of-Words vs. Embeddings","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_52","title":"Description","text":"<p>Switch between BoW and embedding-based representations to compare classification accuracy.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_52","title":"Learning Goals","text":"<ul> <li> <p>Contrast sparse vs. dense text features</p> </li> <li> <p>Recognize embedding advantages</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_52","title":"Input Controls","text":"<ol> <li> <p>Representation Selector</p> </li> <li> <p>Dataset Selector</p> </li> <li> <p>Recalculate Accuracy (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#54-neural-network-layer-visualizer","title":"54. Neural Network Layer Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_53","title":"Description","text":"<p>Show how inputs propagate through fully connected layers with activation functions.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_53","title":"Learning Goals","text":"<ul> <li> <p>Understand forward propagation</p> </li> <li> <p>Visualize activation transformations</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_53","title":"Input Controls","text":"<ol> <li> <p>Number of Layers Slider</p> </li> <li> <p>Activation Function Selector</p> </li> <li> <p>Reset Network (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#55-activation-function-explorer","title":"55. Activation Function Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_54","title":"Description","text":"<p>Compare sigmoid, ReLU, and tanh shapes and outputs for input ranges.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_54","title":"Learning Goals","text":"<ul> <li> <p>Recognize activation function behaviors</p> </li> <li> <p>See saturation and dead neuron effects</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_54","title":"Input Controls","text":"<ol> <li> <p>Function Selector</p> </li> <li> <p>Input Range Slider</p> </li> <li> <p>Toggle Derivative Curve (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#56-weight-initialization-impact","title":"56. Weight Initialization Impact","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_55","title":"Description","text":"<p>Initialize neural network weights differently and observe training convergence.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_55","title":"Learning Goals","text":"<ul> <li> <p>Understand initialization strategies</p> </li> <li> <p>See effect on loss curve and accuracy</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_55","title":"Input Controls","text":"<ol> <li> <p>Initialization Method Selector</p> </li> <li> <p>Learning Rate Slider</p> </li> <li> <p>Reset Training (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#57-learning-rate-finder","title":"57. Learning Rate Finder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_56","title":"Description","text":"<p>Gradually increase learning rate to see where loss diverges or minimizes fastest.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_56","title":"Learning Goals","text":"<ul> <li> <p>Tune learning rate</p> </li> <li> <p>Recognize underfitting and instability from wrong rates</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_56","title":"Input Controls","text":"<ol> <li> <p>Start Rate Slider</p> </li> <li> <p>End Rate Slider</p> </li> <li> <p>Run LR Finder (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#58-convolution-filter-visualizer","title":"58. Convolution Filter Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_57","title":"Description","text":"<p>Apply filters to images and see resulting feature maps.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_57","title":"Learning Goals","text":"<ul> <li> <p>Understand convolution in CNNs</p> </li> <li> <p>Recognize edge and texture detection</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_57","title":"Input Controls","text":"<ol> <li> <p>Filter Type Selector</p> </li> <li> <p>Kernel Size Slider</p> </li> <li> <p>Toggle Original Image (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#59-pooling-layer-explorer","title":"59. Pooling Layer Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_58","title":"Description","text":"<p>Compare max pooling and average pooling effects on feature maps.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_58","title":"Learning Goals","text":"<ul> <li> <p>Understand pooling</p> </li> <li> <p>See dimensionality reduction effects</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_58","title":"Input Controls","text":"<ol> <li> <p>Pooling Type Selector</p> </li> <li> <p>Pool Size Slider</p> </li> <li> <p>Toggle Feature Map Overlay (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#60-overfitting-in-deep-networks","title":"60. Overfitting in Deep Networks","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_59","title":"Description","text":"<p>Increase network capacity and watch training vs. validation loss diverge.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_59","title":"Learning Goals","text":"<ul> <li> <p>Recognize overfitting in neural nets</p> </li> <li> <p>See regularization benefits</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_59","title":"Input Controls","text":"<ol> <li> <p>Number of Neurons Slider</p> </li> <li> <p>Dropout Rate Slider</p> </li> <li> <p>Toggle Validation Curve (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#61-l1-vs-l2-regularization-visualizer","title":"61. L1 vs. L2 Regularization Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_60","title":"Description","text":"<p>Toggle between L1 and L2 regularization and see coefficient shrinkage effects.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_60","title":"Learning Goals","text":"<ul> <li> <p>Understand Lasso vs. Ridge regression</p> </li> <li> <p>See how regularization affects weights</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_60","title":"Input Controls","text":"<ol> <li> <p>Regularization Type Selector</p> </li> <li> <p>Penalty Strength Slider</p> </li> <li> <p>Reset Coefficients (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#62-dropout-effect-simulator","title":"62. Dropout Effect Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_61","title":"Description","text":"<p>Adjust dropout rates and watch neuron activations disappear during training.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_61","title":"Learning Goals","text":"<ul> <li> <p>Understand dropout regularization</p> </li> <li> <p>Recognize its role in preventing overfitting</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_61","title":"Input Controls","text":"<ol> <li> <p>Dropout Rate Slider</p> </li> <li> <p>Toggle Training/Inference View (checkbox)</p> </li> <li> <p>Reset Network (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#63-early-stopping-demonstrator","title":"63. Early Stopping Demonstrator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_62","title":"Description","text":"<p>Visualize training and validation loss to determine optimal stop point.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_62","title":"Learning Goals","text":"<ul> <li> <p>Understand early stopping criteria</p> </li> <li> <p>Avoid overtraining a model</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_62","title":"Input Controls","text":"<ol> <li> <p>Patience Slider</p> </li> <li> <p>Max Epochs Slider</p> </li> <li> <p>Toggle Loss Curves (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#64-shap-value-explorer","title":"64. SHAP Value Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_63","title":"Description","text":"<p>Show feature contributions to individual predictions using SHAP values.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_63","title":"Learning Goals","text":"<ul> <li> <p>Interpret model predictions</p> </li> <li> <p>Recognize key contributing features</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_63","title":"Input Controls","text":"<ol> <li> <p>Select Data Point (dropdown)</p> </li> <li> <p>Show Positive/Negative Contributions (toggle)</p> </li> <li> <p>Reset View (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#65-partial-dependence-plot-tool","title":"65. Partial Dependence Plot Tool","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_64","title":"Description","text":"<p>Adjust a single feature and see average prediction change while holding others constant.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_64","title":"Learning Goals","text":"<ul> <li> <p>Interpret partial dependence</p> </li> <li> <p>Detect feature impact trends</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_64","title":"Input Controls","text":"<ol> <li> <p>Feature Selector</p> </li> <li> <p>Value Slider</p> </li> <li> <p>Toggle Confidence Interval (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#66-counterfactual-example-generator","title":"66. Counterfactual Example Generator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_65","title":"Description","text":"<p>Change features to flip a prediction outcome.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_65","title":"Learning Goals","text":"<ul> <li> <p>Understand counterfactual reasoning</p> </li> <li> <p>Identify decision boundaries</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_65","title":"Input Controls","text":"<ol> <li> <p>Feature Sliders</p> </li> <li> <p>Toggle Prediction Probability (checkbox)</p> </li> <li> <p>Reset Features (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#67-bias-detection-dashboard","title":"67. Bias Detection Dashboard","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_66","title":"Description","text":"<p>Compare model accuracy across demographic subgroups.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_66","title":"Learning Goals","text":"<ul> <li> <p>Detect model bias</p> </li> <li> <p>Understand fairness metrics</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_66","title":"Input Controls","text":"<ol> <li> <p>Group Selector</p> </li> <li> <p>Metric Selector</p> </li> <li> <p>Show Disparity Alert (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#68-fairness-metric-comparator","title":"68. Fairness Metric Comparator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_67","title":"Description","text":"<p>Compare demographic parity, equalized odds, and other fairness metrics.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_67","title":"Learning Goals","text":"<ul> <li> <p>Interpret multiple fairness definitions</p> </li> <li> <p>Recognize trade-offs between them</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_67","title":"Input Controls","text":"<ol> <li> <p>Metric Selector</p> </li> <li> <p>Group Selector</p> </li> <li> <p>Highlight Best Metric (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#69-adversarial-example-creator","title":"69. Adversarial Example Creator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_68","title":"Description","text":"<p>Add small perturbations to input data and see if predictions change.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_68","title":"Learning Goals","text":"<ul> <li> <p>Understand adversarial vulnerability</p> </li> <li> <p>Recognize security risks in ML</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_68","title":"Input Controls","text":"<ol> <li> <p>Perturbation Magnitude Slider</p> </li> <li> <p>Noise Pattern Selector</p> </li> <li> <p>Reset Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#70-model-drift-monitor","title":"70. Model Drift Monitor","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_69","title":"Description","text":"<p>Compare live data predictions to historical model performance.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_69","title":"Learning Goals","text":"<ul> <li> <p>Detect concept and data drift</p> </li> <li> <p>Understand retraining triggers</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_69","title":"Input Controls","text":"<ol> <li> <p>Time Window Selector</p> </li> <li> <p>Drift Metric Selector</p> </li> <li> <p>Refresh Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#71-hyperparameter-search-playground","title":"71. Hyperparameter Search Playground","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_70","title":"Description","text":"<p>Run grid/random searches and compare performance heatmaps.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_70","title":"Learning Goals","text":"<ul> <li> <p>Understand hyperparameter optimization</p> </li> <li> <p>Interpret search results</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_70","title":"Input Controls","text":"<ol> <li> <p>Search Type Selector</p> </li> <li> <p>Parameter Range Sliders</p> </li> <li> <p>Run Search (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#72-model-stacking-visualizer","title":"72. Model Stacking Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_71","title":"Description","text":"<p>Show predictions from multiple base models and meta-learner output.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_71","title":"Learning Goals","text":"<ul> <li> <p>Understand stacking ensembles</p> </li> <li> <p>See diversity benefits</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_71","title":"Input Controls","text":"<ol> <li> <p>Base Model Selector</p> </li> <li> <p>Meta-Learner Selector</p> </li> <li> <p>Toggle Base Predictions (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#73-pipeline-builder","title":"73. Pipeline Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_72","title":"Description","text":"<p>Chain preprocessing and modeling steps interactively.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_72","title":"Learning Goals","text":"<ul> <li> <p>Understand ML pipelines</p> </li> <li> <p>Ensure reproducible workflows</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_72","title":"Input Controls","text":"<ol> <li> <p>Add Step (dropdown)</p> </li> <li> <p>Remove Step (click)</p> </li> <li> <p>Run Pipeline (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#74-model-export-and-import-simulator","title":"74. Model Export and Import Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_73","title":"Description","text":"<p>Save and reload trained models to demonstrate persistence.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_73","title":"Learning Goals","text":"<ul> <li> <p>Understand model serialization</p> </li> <li> <p>Practice deployment readiness</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_73","title":"Input Controls","text":"<ol> <li> <p>Save Model (button)</p> </li> <li> <p>Load Model (button)</p> </li> <li> <p>Reset Session (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#75-api-endpoint-tester","title":"75. API Endpoint Tester","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_74","title":"Description","text":"<p>Send requests to a mock ML API and view JSON responses.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_74","title":"Learning Goals","text":"<ul> <li> <p>Understand model serving endpoints</p> </li> <li> <p>Practice request formatting</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_74","title":"Input Controls","text":"<ol> <li> <p>Input Data Field</p> </li> <li> <p>Send Request (button)</p> </li> <li> <p>View Raw Response (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#76-batch-vs-real-time-prediction-tool","title":"76. Batch vs. Real-Time Prediction Tool","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_75","title":"Description","text":"<p>Switch between batch file processing and live API predictions.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_75","title":"Learning Goals","text":"<ul> <li> <p>Understand latency differences</p> </li> <li> <p>Recognize trade-offs in deployment modes</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_75","title":"Input Controls","text":"<ol> <li> <p>Mode Selector</p> </li> <li> <p>Upload Dataset (file input)</p> </li> <li> <p>Simulate Real-Time (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#77-model-version-comparator","title":"77. Model Version Comparator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_76","title":"Description","text":"<p>Load two model versions and compare accuracy and latency.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_76","title":"Learning Goals","text":"<ul> <li> <p>Track performance over versions</p> </li> <li> <p>Make informed upgrade decisions</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_76","title":"Input Controls","text":"<ol> <li> <p>Version Selector A</p> </li> <li> <p>Version Selector B</p> </li> <li> <p>Compare Now (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#78-ab-testing-simulator","title":"78. A/B Testing Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_77","title":"Description","text":"<p>Split traffic between two models and track conversions.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_77","title":"Learning Goals","text":"<ul> <li> <p>Understand online experimentation</p> </li> <li> <p>Interpret statistical significance</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_77","title":"Input Controls","text":"<ol> <li> <p>Traffic Split Slider</p> </li> <li> <p>Run Experiment (button)</p> </li> <li> <p>View p-Value (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#79-cost-of-prediction-calculator","title":"79. Cost of Prediction Calculator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_78","title":"Description","text":"<p>Estimate compute cost for different model sizes and usage levels.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_78","title":"Learning Goals","text":"<ul> <li> <p>Relate model complexity to cost</p> </li> <li> <p>Make cost-aware deployment decisions</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_78","title":"Input Controls","text":"<ol> <li> <p>Model Size Slider</p> </li> <li> <p>Requests per Minute Slider</p> </li> <li> <p>Region Selector</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#80-energy-efficiency-meter","title":"80. Energy Efficiency Meter","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_79","title":"Description","text":"<p>Track power consumption estimates during model inference.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_79","title":"Learning Goals","text":"<ul> <li> <p>Recognize environmental impact of ML</p> </li> <li> <p>Optimize for efficiency</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_79","title":"Input Controls","text":"<ol> <li> <p>Model Type Selector</p> </li> <li> <p>Batch Size Slider</p> </li> <li> <p>Toggle Energy Display (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#81-model-interpretability-dashboard","title":"81. Model Interpretability Dashboard","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_80","title":"Description","text":"<p>Combine SHAP, partial dependence, and counterfactuals in one view for a selected prediction.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_80","title":"Learning Goals","text":"<ul> <li> <p>Integrate multiple interpretability methods</p> </li> <li> <p>Develop storytelling skills for predictions</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_80","title":"Input Controls","text":"<ol> <li> <p>Data Point Selector</p> </li> <li> <p>Interpretation Method Toggle</p> </li> <li> <p>Export Dashboard (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#82-lime-explainer-tool","title":"82. LIME Explainer Tool","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_81","title":"Description","text":"<p>Generate local linear approximations for individual predictions.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_81","title":"Learning Goals","text":"<ul> <li> <p>Understand local interpretability</p> </li> <li> <p>Compare to global feature importance</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_81","title":"Input Controls","text":"<ol> <li> <p>Data Point Selector</p> </li> <li> <p>Number of Samples Slider</p> </li> <li> <p>Toggle Highlighted Features (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#83-what-if-analysis-playground","title":"83. What-If Analysis Playground","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_82","title":"Description","text":"<p>Change feature values and watch prediction changes in real time.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_82","title":"Learning Goals","text":"<ul> <li> <p>Explore \"what-if\" scenarios</p> </li> <li> <p>Understand sensitivity of predictions</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_82","title":"Input Controls","text":"<ol> <li> <p>Feature Sliders</p> </li> <li> <p>Reset to Original (button)</p> </li> <li> <p>Show Probability Curve (toggle)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#84-bias-mitigation-simulator","title":"84. Bias Mitigation Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_83","title":"Description","text":"<p>Apply pre-processing or in-processing bias mitigation and measure impact.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_83","title":"Learning Goals","text":"<ul> <li> <p>Evaluate fairness interventions</p> </li> <li> <p>Compare accuracy before and after</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_83","title":"Input Controls","text":"<ol> <li> <p>Mitigation Method Selector</p> </li> <li> <p>Target Group Selector</p> </li> <li> <p>Recalculate Metrics (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#85-model-robustness-tester","title":"85. Model Robustness Tester","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_84","title":"Description","text":"<p>Add noise, missing values, or feature shifts to test model stability.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_84","title":"Learning Goals","text":"<ul> <li> <p>Assess robustness under real-world conditions</p> </li> <li> <p>Identify fragile models</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_84","title":"Input Controls","text":"<ol> <li> <p>Noise Level Slider</p> </li> <li> <p>Missing Value Percentage Slider</p> </li> <li> <p>Feature Shift Toggle</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#86-ensemble-diversity-visualizer","title":"86. Ensemble Diversity Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_85","title":"Description","text":"<p>Plot decision boundaries of ensemble members to show diversity benefits.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_85","title":"Learning Goals","text":"<ul> <li> <p>Understand why diversity improves ensembles</p> </li> <li> <p>Detect overcorrelated base models</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_85","title":"Input Controls","text":"<ol> <li> <p>Number of Models Slider</p> </li> <li> <p>Model Type Selector</p> </li> <li> <p>Toggle Overlay Boundaries (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#87-transfer-learning-feature-explorer","title":"87. Transfer Learning Feature Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_86","title":"Description","text":"<p>Load pre-trained model features and visualize them for a custom dataset.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_86","title":"Learning Goals","text":"<ul> <li> <p>Understand feature reuse</p> </li> <li> <p>See adaptation benefits</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_86","title":"Input Controls","text":"<ol> <li> <p>Pre-Trained Model Selector</p> </li> <li> <p>Layer Output Selector</p> </li> <li> <p>Toggle Feature Map Display</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#88-fine-tuning-tracker","title":"88. Fine-Tuning Tracker","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_87","title":"Description","text":"<p>Compare base and fine-tuned model accuracy/loss curves.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_87","title":"Learning Goals","text":"<ul> <li> <p>Understand fine-tuning process</p> </li> <li> <p>Evaluate improvements over baseline</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_87","title":"Input Controls","text":"<ol> <li> <p>Learning Rate Slider</p> </li> <li> <p>Epoch Count Slider</p> </li> <li> <p>Toggle Base Model Curve (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#89-multi-task-learning-visualizer","title":"89. Multi-Task Learning Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_88","title":"Description","text":"<p>Train on two tasks simultaneously and track performance for each.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_88","title":"Learning Goals","text":"<ul> <li> <p>Understand shared representations</p> </li> <li> <p>Recognize trade-offs in multi-task setups</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_88","title":"Input Controls","text":"<ol> <li> <p>Task Weight Sliders</p> </li> <li> <p>Epoch Count Slider</p> </li> <li> <p>Toggle Shared Layers (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#90-attention-mechanism-explorer","title":"90. Attention Mechanism Explorer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_89","title":"Description","text":"<p>Visualize attention weights for sequence-to-sequence models.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_89","title":"Learning Goals","text":"<ul> <li> <p>Understand how models focus on parts of input</p> </li> <li> <p>Interpret attention heatmaps</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_89","title":"Input Controls","text":"<ol> <li> <p>Input Sequence Field</p> </li> <li> <p>Highlight Attention Matrix (checkbox)</p> </li> <li> <p>Reset Example (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#91-transformer-architecture-flow","title":"91. Transformer Architecture Flow","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_90","title":"Description","text":"<p>Step through encoder and decoder layers with visual activations.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_90","title":"Learning Goals","text":"<ul> <li> <p>See data flow in transformer models</p> </li> <li> <p>Recognize role of each sublayer</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_90","title":"Input Controls","text":"<ol> <li> <p>Layer Stepper (next/prev)</p> </li> <li> <p>Toggle Positional Encoding View</p> </li> <li> <p>Reset Sequence (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#92-hyperparameter-sensitivity-map","title":"92. Hyperparameter Sensitivity Map","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_91","title":"Description","text":"<p>Generate heatmaps showing accuracy changes across parameter ranges.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_91","title":"Learning Goals","text":"<ul> <li> <p>Identify sensitive parameters</p> </li> <li> <p>Focus tuning efforts effectively</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_91","title":"Input Controls","text":"<ol> <li> <p>Parameter Range Sliders</p> </li> <li> <p>Run Grid Search (button)</p> </li> <li> <p>Toggle Best Point Marker (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#93-model-compression-simulator","title":"93. Model Compression Simulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_92","title":"Description","text":"<p>Prune weights and quantize parameters, tracking accuracy drop.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_92","title":"Learning Goals","text":"<ul> <li> <p>Understand trade-offs between size and performance</p> </li> <li> <p>Recognize deployment benefits of smaller models</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_92","title":"Input Controls","text":"<ol> <li> <p>Pruning Percentage Slider</p> </li> <li> <p>Quantization Level Selector</p> </li> <li> <p>Apply Compression (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#94-edge-deployment-emulator","title":"94. Edge Deployment Emulator","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_93","title":"Description","text":"<p>Simulate running a model on constrained hardware.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_93","title":"Learning Goals","text":"<ul> <li> <p>Understand latency and memory constraints</p> </li> <li> <p>Optimize for edge environments</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_93","title":"Input Controls","text":"<ol> <li> <p>Hardware Profile Selector</p> </li> <li> <p>Batch Size Slider</p> </li> <li> <p>Toggle Latency Display (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#95-streaming-data-dashboard","title":"95. Streaming Data Dashboard","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_94","title":"Description","text":"<p>Stream incoming data and update predictions in real time.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_94","title":"Learning Goals","text":"<ul> <li> <p>Handle continuous inputs</p> </li> <li> <p>Recognize challenges in online learning</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_94","title":"Input Controls","text":"<ol> <li> <p>Stream Speed Slider</p> </li> <li> <p>Pause/Resume Stream (button)</p> </li> <li> <p>Reset Dashboard (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#96-online-learning-visualizer","title":"96. Online Learning Visualizer","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_95","title":"Description","text":"<p>Update model incrementally with new data and track evolving accuracy.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_95","title":"Learning Goals","text":"<ul> <li> <p>Understand incremental training</p> </li> <li> <p>Monitor performance drift</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_95","title":"Input Controls","text":"<ol> <li> <p>Learning Rate Slider</p> </li> <li> <p>Batch Size Slider</p> </li> <li> <p>Toggle History Chart (checkbox)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#97-capstone-project-data-selector","title":"97. Capstone Project Data Selector","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_96","title":"Description","text":"<p>Choose dataset for final project from curated sources and preview statistics.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_96","title":"Learning Goals","text":"<ul> <li> <p>Practice dataset selection skills</p> </li> <li> <p>Evaluate dataset suitability</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_96","title":"Input Controls","text":"<ol> <li> <p>Dataset Selector</p> </li> <li> <p>Preview Stats (button)</p> </li> <li> <p>Download Data (button)</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#98-model-comparison-dashboard","title":"98. Model Comparison Dashboard","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_97","title":"Description","text":"<p>Compare multiple models across accuracy, latency, and fairness metrics.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_97","title":"Learning Goals","text":"<ul> <li> <p>Perform multi-metric evaluation</p> </li> <li> <p>Select best model for deployment</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_97","title":"Input Controls","text":"<ol> <li> <p>Model Selector (multi-select)</p> </li> <li> <p>Metric Selector</p> </li> <li> <p>Toggle Best Model Highlight</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#99-end-to-end-workflow-builder","title":"99. End-to-End Workflow Builder","text":""},{"location":"sims/microsim-lists/100-data-science-microsims/#description_98","title":"Description","text":"<p>Drag-and-drop stages to build a full ML pipeline from data to deployment.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_98","title":"Learning Goals","text":"<ul> <li> <p>Integrate all learned concepts</p> </li> <li> <p>Visualize project workflow</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_98","title":"Input Controls","text":"<ol> <li> <p>Stage Palette (drag items)</p> </li> <li> <p>Connect Stages (mouse drag)</p> </li> <li> <p>Run Workflow (button)</p> </li> <li> <p>Ethical Impact Assessment Tool</p> </li> </ol>"},{"location":"sims/microsim-lists/100-data-science-microsims/#description_99","title":"Description","text":"<p>Rate model across transparency, fairness, privacy, and societal impact dimensions.</p>"},{"location":"sims/microsim-lists/100-data-science-microsims/#learning-goals_99","title":"Learning Goals","text":"<ul> <li> <p>Incorporate ethics into data science projects</p> </li> <li> <p>Balance technical and social factors</p> </li> </ul>"},{"location":"sims/microsim-lists/100-data-science-microsims/#input-controls_99","title":"Input Controls","text":"<ol> <li> <p>Impact Category Sliders</p> </li> <li> <p>Generate Report (button)</p> </li> <li> <p>Reset Assessment (button)</p> </li> </ol>"},{"location":"sims/normal-dice-distribution/","title":"Normal Dice Distribution","text":""},{"location":"sims/normal-dice-distribution/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/normal-dice-distribution/main.html\" height=\"470px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p>"},{"location":"sims/normal-dice-distribution/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive simulation demonstrates the probability distribution of rolling a single six-sided die. As more rolls accumulate, students can observe how the distribution approaches a uniform distribution with equal probability (1/6) for each outcome.</p>"},{"location":"sims/normal-dice-distribution/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand uniform probability distribution</li> <li>Observe how empirical results approach theoretical probability with more trials</li> <li>Compare discrete uniform distribution to other distributions</li> </ul>"},{"location":"sims/normal-dice-distribution/#references","title":"References","text":"<ol> <li>Probability Distributions - Wikipedia - Overview of probability distributions</li> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/pair-dice-sum-distribution/","title":"Two-Dice Sum Distribution","text":""},{"location":"sims/pair-dice-sum-distribution/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/pair-dice-sum-distribution/main.html\" height=\"500px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p>"},{"location":"sims/pair-dice-sum-distribution/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive simulation demonstrates the probability distribution of rolling two six-sided dice and summing the results. Students can observe how:</p> <ul> <li>The sum of 7 is the most likely outcome (6/36 = 16.7% probability)</li> <li>The distribution forms a triangular shape</li> <li>Sums near the extremes (2 and 12) are least likely (1/36 = 2.8% probability)</li> </ul>"},{"location":"sims/pair-dice-sum-distribution/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand discrete probability distributions</li> <li>Calculate probabilities for compound events</li> <li>Observe the law of large numbers in action</li> <li>Connect combinatorics to probability</li> </ul>"},{"location":"sims/pair-dice-sum-distribution/#references","title":"References","text":"<ol> <li>Dice Probability - Wikipedia - Probability theory for dice</li> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build this interactive simulation</li> </ol>"},{"location":"sims/template/","title":"MicroSim Template","text":""},{"location":"sims/template/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/template/main.html\" height=\"500px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p>"},{"location":"sims/template/#about-this-microsim","title":"About This MicroSim","text":"<p>This is a template MicroSim that demonstrates the standard structure and formatting for creating new interactive simulations. Use this as a starting point when developing new MicroSims for the data science course.</p>"},{"location":"sims/template/#how-to-use-this-template","title":"How to Use This Template","text":"<ol> <li>Copy the entire <code>template</code> directory to create a new MicroSim</li> <li>Rename the directory to your MicroSim name (use kebab-case)</li> <li>Update <code>metadata.json</code> with your MicroSim's information</li> <li>Replace <code>main.html</code> and the JavaScript file with your simulation code</li> <li>Update this <code>index.md</code> file with your documentation</li> </ol>"},{"location":"sims/template/#sample-prompt-for-creating-a-microsim","title":"Sample Prompt for Creating a MicroSim","text":"<p>Prompt</p> <p>Create a single file p5.js sketch. Draw a green circle on a 600x400 canvas with a radius of 200.</p>"},{"location":"sims/template/#references","title":"References","text":"<ol> <li>p5.js Reference - p5.js Documentation - JavaScript library used to build interactive simulations</li> <li>Processing Wiki on Positioning Your Canvas - Guide for canvas positioning</li> </ol>"},{"location":"sims/ten-stats/","title":"Ten Basic Statistics MicroSims","text":""},{"location":"sims/ten-stats/#embed-this-microsim","title":"Embed This MicroSim","text":"<p>Copy this iframe to embed this MicroSim in your website:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/data-science-course/sims/ten-stats/main.html\" height=\"700px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim in Fullscreen</p>"},{"location":"sims/ten-stats/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive vis-network visualization shows the learning progression through ten fundamental statistics MicroSims. The graph illustrates:</p> <ul> <li>Concept dependencies - Which concepts must be learned before others</li> <li>Learning pathways - Multiple valid routes through the material</li> <li>Topic groupings - Color-coded by subject area (fundamentals, distributions, relationships, regression, evaluation, sampling, inference)</li> </ul> <p>Click on any node to see its prerequisites and what concepts it leads to.</p>"},{"location":"sims/ten-stats/#the-ten-microsims","title":"The Ten MicroSims","text":"<ol> <li>Exploring Data Points - Build intuition about observations and coordinates</li> <li>Histogram Builder - Understand frequency distributions and bin sizes</li> <li>Mean &amp; Median Explorer - Compare measures of central tendency</li> <li>Distribution Shape Explorer - Learn about skewness and kurtosis</li> <li>Correlation Playground - Explore relationships between variables</li> <li>Least Squares Line Fitter - Minimize squared residuals</li> <li>R\u00b2 Intuition Builder - Understand model fit quality</li> <li>Sampling Bias Demonstrator - Compare biased vs. random sampling</li> <li>Hypothesis Testing Visualizer - Observe p-values under null hypothesis</li> <li>Confidence Interval Explorer - Visualize interval coverage</li> </ol>"},{"location":"sims/ten-stats/#references","title":"References","text":"<ol> <li>vis-network Documentation - vis.js - JavaScript library used for the network visualization</li> <li>Statistics Education Resources - American Statistical Association - Teaching statistics effectively</li> </ol>"},{"location":"sims/variable-type-sorter/","title":"Variable Type Sorter","text":"<p>An interactive quiz game that tests your ability to classify variables into their correct types: Continuous, Discrete, Ordinal, or Nominal.</p> <p>Run the Variable Type Sorter MicroSim in Full Screen Edit the Variable Type Sorter MicroSim using the p5.js Editor</p>"},{"location":"sims/variable-type-sorter/#how-to-play","title":"How to Play","text":"<ol> <li>Read the variable description shown in the box</li> <li>Click one of the four buttons to classify the variable:<ul> <li>Continuous (blue): Numerical data that can be any value including decimals</li> <li>Discrete (purple): Numerical data that can only be whole numbers</li> <li>Ordinal (green): Categorical data with a meaningful order</li> <li>Nominal (orange): Categorical data with no inherent order</li> </ul> </li> <li>Get 10 correct answers to complete the Side Quest!</li> </ol>"},{"location":"sims/variable-type-sorter/#game-features","title":"Game Features","text":"<ul> <li>Score Tracking: Your score is displayed in the upper right corner</li> <li>Celebration Animation: Colorful particles burst when you answer correctly</li> <li>Helpful Hints: If you answer incorrectly, you'll see a hint to help you understand</li> <li>Side Quest Complete: Answer all 10 questions correctly to earn +100 Data Scientist Health Points!</li> </ul>"},{"location":"sims/variable-type-sorter/#learning-objectives","title":"Learning Objectives","text":"<p>After playing this game, students should be able to:</p> <ol> <li>Distinguish between numerical and categorical data</li> <li>Identify continuous vs discrete numerical variables</li> <li>Recognize ordinal vs nominal categorical variables</li> <li>Apply the decision tree logic for classifying any variable</li> </ol>"},{"location":"sims/variable-type-sorter/#the-four-variable-types","title":"The Four Variable Types","text":"Type Category Description Examples Continuous Numerical Can take any value including decimals Height, Weight, Temperature Discrete Numerical Can only be whole numbers Number of children, Cars owned Ordinal Categorical Has a meaningful order or ranking Grade levels, Satisfaction ratings Nominal Categorical No inherent order between categories Eye color, Blood type, Country"},{"location":"sims/variable-type-sorter/#related-resources","title":"Related Resources","text":"<ul> <li>Variable Types Decision Tree - Visual flowchart for classifying variables</li> <li>Introduction to Data Science - Chapter covering data types</li> </ul>"},{"location":"sims/variable-types-decision-tree/","title":"Variable Types Decision Tree","text":"<p>This interactive decision tree helps students classify any variable they encounter into its proper type: Continuous, Discrete, Ordinal, or Nominal.</p> <p>Run the Variable Types Decision Tree in Full Screen</p>"},{"location":"sims/variable-types-decision-tree/#how-to-use-this-diagram","title":"How to Use This Diagram","text":"<ol> <li>Start by examining the variable you want to classify</li> <li>Ask yourself the key questions as you follow the flowchart</li> <li>Hover over any node to see additional examples and explanations</li> <li>Click \"Test Yourself\" to practice classifying random variables</li> </ol>"},{"location":"sims/variable-types-decision-tree/#the-two-main-categories","title":"The Two Main Categories","text":""},{"location":"sims/variable-types-decision-tree/#numerical-data-can-do-math-with-it","title":"Numerical Data (Can do math with it)","text":"<p>Values that represent quantities or measurements where mathematical operations make sense.</p> <ul> <li>Continuous: Can take any value within a range, including decimals<ul> <li>Examples: Height (5.7 ft), Temperature (98.6F), Time (3.5 hours)</li> </ul> </li> <li>Discrete: Can only take specific, countable values (usually whole numbers)<ul> <li>Examples: Number of siblings (2), Cars owned (3), Students in class (25)</li> </ul> </li> </ul>"},{"location":"sims/variable-types-decision-tree/#categorical-data-cannot-do-math-with-it","title":"Categorical Data (Cannot do math with it)","text":"<p>Values that represent groups or categories where math operations are meaningless.</p> <ul> <li>Ordinal: Categories with a meaningful order or ranking<ul> <li>Examples: Grade levels (Freshman &lt; Senior), Satisfaction ratings (Poor &lt; Excellent)</li> </ul> </li> <li>Nominal: Categories with no inherent order<ul> <li>Examples: Eye color (blue, brown, green), Blood type (A, B, AB, O)</li> </ul> </li> </ul>"},{"location":"sims/variable-types-decision-tree/#key-decision-questions","title":"Key Decision Questions","text":"Question YES means... NO means... Can you do math with it? Numerical data Categorical data Can it be any value (including decimals)? Continuous Discrete Is there a meaningful order? Ordinal Nominal"},{"location":"sims/variable-types-decision-tree/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ol> <li>ZIP codes are NOT numerical - Even though they contain digits, you can't meaningfully add or average them</li> <li>Age in years can be continuous - While often recorded as whole numbers, age is actually continuous (you can be 25.5 years old)</li> <li>Rating scales can be ordinal OR discrete - A 1-10 pain scale is ordinal (the order matters), but if you're counting how many times something happened, it's discrete</li> </ol>"},{"location":"sims/variable-types-decision-tree/#related-concepts","title":"Related Concepts","text":"<ul> <li>Data Types Overview</li> <li>Descriptive Statistics</li> </ul>"}]}