{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Data Science with Python and AI","text":"<p>Welcome to our website for the course that uses AI to help students learn data science.  This course is designed to help advanced high-school and freshman college students quickly learn the core concepts of data science with extensive use of Micro-Simulations and AI.</p> <p>Please contact me on LinkedIn if you have any questions.</p> <ul> <li>Dan McCreary</li> </ul>"},{"location":"about/","title":"About This Book","text":"<p>In today\u2019s data-rich world, the ability to extract meaningful insights from information isn\u2019t just an advantage\u2014it\u2019s essential. This book, Introduction to Data Science with Python and AI, aims to empower advanced high-school and early college students with the tools, concepts, and critical thinking skills needed to become fluent in the language of data. Through interactive modules\u2014MicroSims\u2014you\u2019ll build a deep understanding of linear regression and progressively more sophisticated predictive models using Python, NumPy, and PyTorch. Our focus remains on balancing interpretability and predictive performance, helping you discover how simple models can often offer powerful\u2014and explainable\u2014solutions.</p>"},{"location":"about/#why-this-book-matters","title":"Why This Book Matters","text":""},{"location":"about/#1-data-science-skills-are-in-high-demand","title":"1. Data Science Skills Are in High Demand","text":"<p>The U.S. Bureau of Labor Statistics projects that jobs for data scientists will grow by 36% from 2021 to 2031\u2014far exceeding average job growth across all occupations (Harvard Extension School). IBM research also forecasts approximately 2.7 million job openings in data science and data engineering in the near future (Medium).</p>"},{"location":"about/#2-businesses-thrive-with-data-expertise","title":"2. Businesses Thrive with Data Expertise","text":"<p>An empirical study spanning 2.9 million employees across 7,408 investment firms (2008\u20132021) found that firms with more data specialists performed better, particularly when local universities introduced data science programs\u2014a pattern suggestive of a causal link (F.N. London).</p>"},{"location":"about/#3-cross-sector-demand-for-data-literacy","title":"3. Cross-Sector Demand for Data Literacy","text":"<p>Beyond specialized roles, basic data literacy is quickly becoming a core competency across all fields. A global study by Forrester Consulting (sponsored by Tableau) found that 82% of decision-makers expect basic data literacy from employees in every department, with projections that by 2025 nearly 70% of employees will heavily rely on data (TIME).</p>"},{"location":"about/#4-ai-increases-demand-for-complementary-human-skills","title":"4. AI Increases Demand for Complementary Human Skills","text":"<p>Research analyzing 12 million job postings from 2018\u20132023 in the U.S. shows that AI is raising the demand\u2014and wage premiums\u2014for complementary human skills such as digital literacy, teamwork, and resilience, especially in AI-impacted roles like data science (arXiv). This means that learning to work effectively with data and AI tools will remain valuable\u2014and increasingly so.</p>"},{"location":"about/#5-data-driven-innovation-boosts-subject-matter-experts","title":"5. Data-Driven Innovation Boosts Subject Matter Experts","text":"<p>A case study of 85 Subject Matter Expertss in the U.K. highlighted how data science techniques like customer forecasting and predictive maintenance drive productivity, innovation, and job creation\u2014though these benefits hinge on developing both skills and infrastructure (Reddit, arXiv).</p> <p>Through this book, you won\u2019t just learn to code\u2014you\u2019ll gain an essential toolkit for navigating, interrogating, and communicating with data in any field you choose. By starting with clear, hands-on simulations and building up to more advanced techniques, we ensure that every concept you encounter is grounded in understanding, not just syntax.</p> <p>Here\u2019s an additional section you can insert right after the \u201cWhy This Book Matters\u201d part of your document. It highlights why using AI-generated interactive simulations (MicroSims) offers a superior learning experience compared to traditional static textbooks.</p>"},{"location":"about/#why-our-ai-powered-interactive-simulations-outperform-static-textbooks","title":"Why Our AI-Powered Interactive Simulations Outperform Static Textbooks","text":"<p>Traditional textbooks, while valuable as references, often present information in a linear, static, and passive format. This approach leaves students reading about concepts without the opportunity to experiment, test, and immediately see the impact of changing parameters. Our methodology fundamentally changes that dynamic by integrating AI-generated interactive simulations (MicroSims) into the core of the learning experience.</p> <p>Key Advantages:</p> <ol> <li> <p>Active Learning Over Passive Reading    Students don\u2019t just read about a concept\u2014they manipulate variables, observe results, and form hypotheses in real time. This active engagement significantly improves retention compared to passive reading.</p> </li> <li> <p>Personalized Exploration    AI adapts simulations to a student\u2019s pace and curiosity. If a learner struggles with a concept, the system can generate additional guided examples or simplify the scenario. Advanced learners can explore \u201cwhat-if\u201d situations beyond the basic curriculum.</p> </li> <li> <p>Immediate Feedback Loop    Instead of waiting until a homework assignment is graded, students get instant feedback within the simulation. This encourages experimentation and builds confidence.</p> </li> <li> <p>Bridging Theory and Practice    Concepts such as statistical distributions, regression coefficients, and the bias-variance tradeoff become tangible. By adjusting sliders or toggling options, abstract equations transform into visual, intuitive insights.</p> </li> <li> <p>Adaptive Content Updates    AI-generated content allows the textbook to evolve with the field. When new methods or datasets emerge, we can rapidly integrate them into simulations\u2014keeping learning materials relevant without waiting for a new print edition.</p> </li> <li> <p>Data-Driven Instruction    Student interactions within MicroSims can be logged (with privacy safeguards) to help educators identify common misunderstandings and adapt teaching strategies accordingly.</p> </li> <li> <p>Democratizing Access    Because simulations run in the browser, they require no expensive software or specialized hardware\u2014making cutting-edge, interactive learning accessible to a global audience.</p> </li> </ol> <p>The Result: Instead of memorizing definitions and formulas in isolation, learners develop conceptual mastery through guided experimentation, preparing them not only to understand data science today but also to apply it creatively to tomorrow\u2019s problems.</p> <p>Here\u2019s the additional section you can place toward the end of the About This Book page, right before the References section.</p>"},{"location":"about/#open-licensing-for-educator-adaptation","title":"Open Licensing for Educator Adaptation","text":"<p>This textbook is published under the Creative Commons Attribution\u2013NonCommercial\u2013ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED) license. This means you are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format.</li> <li>Adapt \u2014 remix, transform, and build upon the material.</li> </ul> <p>As long as you follow these conditions:</p> <ol> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> </ol>"},{"location":"about/#why-this-matters-for-instructors","title":"Why This Matters for Instructors","text":"<p>The open license ensures that educators can quickly adapt this textbook to match their school\u2019s curriculum, pacing, and student needs without seeking additional permissions.</p> <p>Instructors can add new examples relevant to their region, translate content into different languages, or integrate local datasets and case studies\u2014while still contributing back to the global teaching community.</p> <p>For the full legal text of the license, see: https://creativecommons.org/licenses/by-nc-sa/4.0/</p>"},{"location":"about/#references","title":"References","text":"<ol> <li> <p>https://en.wikipedia.org/wiki/Data_science \u2013 today \u2013 Wikipedia \u2013 Provides foundational context and history of data science, framing the evolution of the field (Wikipedia).</p> </li> <li> <p>https://extension.harvard.edu/blog/why-study-data-science/ \u2013 Jul\u202f21\u202f2023 \u2013 Harvard Extension School blog \u2013 Highlights job growth (36\u202f%) and career opportunities in data science (Harvard Extension School).</p> </li> <li> <p>https://medium.com/analysts-corner/bridging-the-data-science-skills-gap-c90b4d994bff \u2013 last 9 months \u2013 Medium article \u2013 Cites IBM prediction of 2.7 million roles in data science and engineering (Medium).</p> </li> <li> <p>https://www.fnlondon.com/articles/firms-say-they-like-arts-graduates-but-its-the-data-geeks-who-perform\u2011e2d6e034 \u2013 May\u202f5\u202f2025 \u2013 Financial News London \u2013 Reports study showing better investment firm performance tied to hiring more data specialists (F.N. London).</p> </li> <li> <p>https://time.com/6290684/data-literacy-us-national-security/ \u2013 Jun\u202f29\u202f2023 \u2013 TIME \u2013 Summarizes Forrester/Tab\u200bleau finding that 82\u202f% of decision\u2011makers expect data literacy, with 70\u202f% of employees using data heavily by 2025 (TIME).</p> </li> <li> <p>https://arxiv.org/abs/2412.19754 \u2013 Dec\u202f27\u202f2024 \u2013 arXiv preprint \u2013 Shows AI increases demand for complementary skills like digital literacy and resilience, especially in data science roles (arXiv).</p> </li> <li> <p>https://arxiv.org/abs/2305.15454 \u2013 May\u202f24\u202f2023 \u2013 arXiv preprint \u2013 Case studies of 85 UK SMEs showing data science supports productivity, innovation, customer insight, but requires investment in skills and infrastructure (arXiv).</p> </li> <li> <p>https://en.wikipedia.org/wiki/Data_literacy \u2013 6 months ago \u2013 Wikipedia \u2013 Defines \u201cdata literacy\u201d as a fundamental capability involving reading, interpreting, evaluating, and communicating data effectively (Wikipedia).</p> </li> <li> <p>https://en.wikipedia.org/wiki/Analytical_skill \u2013 last month \u2013 Wikipedia \u2013 Describes analytical skill set (including data analysis, critical thinking) as critical across professions (Wikipedia).</p> </li> <li> <p>https://sciencedirect.com/science/article/pii/S0263237322000810 \u2013 2022 \u2013 ScienceDirect \u2013 Miller and Hughes study on market demand for data science skills across six industries (ScienceDirect).</p> </li> </ol> <p>Let me know if you'd like help drafting other sections (e.g. Preface, Tutorials, Chapter intros).</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory</li> <li>Test the build</li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> </ol>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn if you have any questions.</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description for Introduction to Data Science with Python","text":"<p>Course Duration: 10 weeks Target Audience: Advanced high school students and college freshmen Prerequisites: Basic algebra and introductory programming experience recommended</p>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This introductory course provides students with foundational knowledge and practical skills in data science using Python.  Through hands-on experience with interactive simulations MicroSims and real-world datasets,  students will develop competency in data analysis, visualization, and predictive modeling.  The course emphasizes the critical balance between model explainability and predictive accuracy,  guiding students to identify the simplest effective solutions to data-driven problems.</p>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <p>Remember (Knowledge) - Recall fundamental data science terminology and concepts - Identify key Python libraries for data science (NumPy, pandas, matplotlib, PyTorch) - Recognize different types of data and measurement scales - List the steps in the data science workflow</p> <p>Understand (Comprehension) - Explain the relationship between independent and dependent variables - Describe how linear regression models make predictions - Interpret basic statistical measures and visualizations - Summarize the trade-offs between model complexity and interpretability</p> <p>Apply (Application) - Implement basic data cleaning and preprocessing techniques - Create visualizations using Python libraries - Build simple linear regression models - Execute standard data science workflows on new datasets</p> <p>Analyze (Analysis) - Examine datasets to identify patterns and relationships - Compare different modeling approaches for the same problem - Distinguish between correlation and causation in data relationships - Evaluate model performance using appropriate metrics</p> <p>Evaluate (Evaluation) - Assess the quality and reliability of data sources - Critique model assumptions and limitations - Judge the appropriateness of different models for specific problems - Validate model performance and identify potential overfitting</p> <p>Create (Synthesis) - Design experiments to test hypotheses using data - Construct predictive models for real-world scenarios - Develop data-driven solutions to complex problems - Generate original insights from exploratory data analysis</p>"},{"location":"course-description/#sample-weekly-schedule","title":"Sample Weekly Schedule","text":""},{"location":"course-description/#week-1-foundations-of-data-science","title":"Week 1: Foundations of Data Science","text":"<ul> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> </ul>"},{"location":"course-description/#week-2-data-exploration-and-visualization","title":"Week 2: Data Exploration and Visualization","text":"<ul> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> </ul>"},{"location":"course-description/#week-3-statistical-foundations","title":"Week 3: Statistical Foundations","text":"<ul> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> </ul>"},{"location":"course-description/#week-4-simple-linear-regression","title":"Week 4: Simple Linear Regression","text":"<ul> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> </ul>"},{"location":"course-description/#week-5-model-evaluation-and-validation","title":"Week 5: Model Evaluation and Validation","text":"<ul> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> </ul>"},{"location":"course-description/#week-6-multiple-linear-regression","title":"Week 6: Multiple Linear Regression","text":"<ul> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> </ul>"},{"location":"course-description/#week-7-introduction-to-numpy-and-advanced-computation","title":"Week 7: Introduction to NumPy and Advanced Computation","text":"<ul> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> </ul>"},{"location":"course-description/#week-8-non-linear-models-and-regularization","title":"Week 8: Non-linear Models and Regularization","text":"<ul> <li>Polynomial regression and feature transformation</li> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> </ul>"},{"location":"course-description/#week-9-introduction-to-machine-learning-with-pytorch","title":"Week 9: Introduction to Machine Learning with PyTorch","text":"<ul> <li>Neural networks and deep learning concepts</li> <li>Building simple networks with PyTorch</li> <li>MicroSim: Neural network playground</li> <li>Comparing traditional and deep learning approaches</li> </ul>"},{"location":"course-description/#week-10-capstone-project-and-model-deployment","title":"Week 10: Capstone Project and Model Deployment","text":"<ul> <li>End-to-end data science project</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> </ul>"},{"location":"course-description/#assessment-methods","title":"Assessment Methods","text":"<p>Formative Assessment (60%) - Weekly MicroSim exercises and reflections (30%) - Homework assignments applying concepts to new datasets (20%) - Peer review activities and collaborative problem-solving (10%)</p> <p>Summative Assessment (40%) - Midterm project: Complete data analysis report (15%) - Final capstone project: Original predictive modeling solution (20%) - Final examination covering theoretical concepts (5%)</p>"},{"location":"course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Computer with Python 3.8+ installed</li> <li>Access to interactive online textbook with MicroSims</li> <li>Jupyter Notebook environment</li> <li>Required Python packages: pandas, NumPy, matplotlib, scikit-learn, PyTorch</li> </ul>"},{"location":"course-description/#key-learning-principles","title":"Key Learning Principles","text":"<p>Interactive Learning: Each week features hands-on MicroSims that allow students to manipulate parameters and observe results in real-time, reinforcing theoretical concepts through experiential learning.</p> <p>Scaffolded Complexity: The course progresses systematically from simple linear relationships to complex neural networks, ensuring students build confidence before tackling advanced topics.</p> <p>Explainable AI Focus: Throughout the course, emphasis is placed on understanding and interpreting models rather than simply achieving high accuracy, preparing students for ethical and transparent data science practice.</p> <p>Real-world Applications: All examples and projects use authentic datasets and scenarios, helping students connect academic learning to practical problem-solving.</p>"},{"location":"course-description/#course-philosophy","title":"Course Philosophy","text":"<p>This course is built on the principle that effective data science requires both technical competence and critical thinking. Students will learn not just how to build predictive models, but when to use them, how to interpret their results, and how to communicate findings to diverse audiences. The integration of interactive simulations ensures that abstract mathematical concepts become concrete and intuitive, while the progression from simple to complex models helps students appreciate the value of parsimony in modeling.</p> <p>By the end of this course, students will have developed both the technical skills and analytical mindset necessary for success in advanced data science coursework or entry-level positions in data-driven fields.</p>"},{"location":"faqs/","title":"Introduction to Data Science FAQs","text":""},{"location":"faqs/#what-is-data-science-and-why-is-it-important","title":"What is data science, and why is it important?","text":"<p>Data science is an interdisciplinary field that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data. It is important because it enables organizations to make informed decisions, predict trends, and solve complex problems by analyzing large datasets.</p>"},{"location":"faqs/#why-is-python-used-in-data-science","title":"Why is Python used in data science?","text":"<p>Python is widely used in data science due to its simplicity and versatility. It offers a vast ecosystem of libraries and frameworks like NumPy, Pandas, Matplotlib, and scikit-learn, which facilitate data manipulation, analysis, visualization, and machine learning tasks.</p>"},{"location":"faqs/#what-are-jupyter-notebooks","title":"What are Jupyter Notebooks?","text":"<p>Jupyter Notebooks are interactive web-based computing environments that allow you to create and share documents containing live code, equations, visualizations, and narrative text. They are particularly popular in data science, research, and education for their ability to combine code execution with rich documentation.</p> <p>Key Features: - Interactive Code Execution: Run code cells individually and see results immediately - Multiple Language Support: Primarily Python, but also supports R, Julia, Scala, and 40+ other languages - Rich Output Display: Show plots, tables, HTML, images, and interactive widgets inline - Markdown Support: Write formatted text, equations (LaTeX), and documentation alongside code - Easy Sharing: Export to HTML, PDF, or share via GitHub, making collaboration simple</p> <p>Why Use Jupyter Notebooks for Data Science? - Exploratory Data Analysis: Perfect for iterative data exploration and visualization - Prototyping: Quickly test ideas and algorithms before building production code - Documentation: Combine code with explanations, making analysis reproducible and understandable - Education: Ideal for learning and teaching data science concepts with immediate feedback - Presentation: Create compelling data stories with code, visualizations, and narrative</p> <p>Common Use Cases: - Data cleaning and preprocessing - Statistical analysis and modeling - Machine learning experimentation - Creating reproducible research - Building interactive dashboards - Educational tutorials and assignments</p> <p>In this course, you'll use Jupyter Notebooks extensively for hands-on exercises, MicroSims, and project work, allowing you to experiment with data science concepts in an interactive environment.</p>"},{"location":"faqs/#what-are-the-key-python-libraries-for-data-analysis","title":"What are the key Python libraries for data analysis?","text":"<p>The key Python libraries for data analysis include:</p> <ul> <li>NumPy: For numerical computing and array operations.</li> <li>Pandas: For data manipulation and analysis using data structures like DataFrames.</li> <li>Plotly: Modern interactive data visualization within a browser</li> <li>Matplotlib: For creating static, animated, and interactive visualizations.</li> <li>Seaborn: For statistical data visualization built on top of Matplotlib.</li> <li>scikit-learn: For machine learning algorithms and predictive data analysis.</li> </ul>"},{"location":"faqs/#how-do-you-import-a-csv-file-into-a-pandas-dataframe","title":"How do you import a CSV file into a Pandas DataFrame?","text":"<p>You can import a CSV file using the <code>read_csv()</code> function from Pandas:</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv('file_name.csv')\n</code></pre> <p>Replace <code>'file_name.csv'</code> with the path to your CSV file.</p>"},{"location":"faqs/#what-is-a-dataframe-in-pandas","title":"What is a DataFrame in Pandas?","text":"<p>A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). It is similar to a spreadsheet or SQL table and is the primary data structure used in Pandas for data manipulation.</p>"},{"location":"faqs/#how-do-you-handle-missing-data-in-a-dataset","title":"How do you handle missing data in a dataset?","text":"<p>Missing data can be handled by:</p> <ul> <li>Removing missing values: Using <code>dropna()</code> to remove rows or columns with missing values.</li> <li>Imputing missing values: Using <code>fillna()</code> to replace missing values with a specific value, mean, median, or mode.</li> <li>Interpolate missing values: Using <code>interpolate()</code> to estimate missing values based on other data points.</li> </ul>"},{"location":"faqs/#what-is-the-difference-between-numpy-arrays-and-python-lists","title":"What is the difference between NumPy arrays and Python lists?","text":"<p>NumPy arrays are fixed-size, homogeneous collections of elements (all of the same data type) optimized for numerical computations. Python lists are dynamic, heterogeneous collections that can contain elements of different data types. NumPy arrays offer better performance for mathematical operations.</p>"},{"location":"faqs/#how-do-you-select-a-subset-of-data-from-a-dataframe","title":"How do you select a subset of data from a DataFrame?","text":"<p>You can select subsets using two methods:</p> <ol> <li>Label-based indexing</li> <li> <p>Integer-based indexing</p> </li> <li> <p>Label-based indexing with <code>.loc</code>:</p> </li> </ol> <pre><code>df_subset = df.loc[row_labels, column_labels]\n</code></pre> <ul> <li>Integer-based indexing with <code>.iloc</code>:</li> </ul> <pre><code>df_subset = df.iloc[row_indices, column_indices]\n</code></pre>"},{"location":"faqs/#what-is-data-visualization-and-why-is-it-important","title":"What is data visualization, and why is it important?","text":"<p>Data visualization is the graphical representation of data to communicate information clearly and efficiently. It is important because it helps identify patterns, trends, and outliers in data, making complex data more accessible and understandable.</p>"},{"location":"faqs/#how-do-you-create-a-simple-line-plot-using-matplotlib","title":"How do you create a simple line plot using Matplotlib?","text":"<pre><code>import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny = [10, 20, 25, 30]\n\nplt.plot(x, y)\nplt.xlabel('X-axis Label')\nplt.ylabel('Y-axis Label')\nplt.title('Simple Line Plot')\nplt.show()\n</code></pre>"},{"location":"faqs/#what-is-the-purpose-of-the-groupby-function-in-pandas","title":"What is the purpose of the <code>groupby()</code> function in Pandas?","text":"<p>The <code>groupby()</code> function is used to split data into groups based on some criteria, perform operations on each group independently, and then combine the results. It is useful for aggregation, transformation, and filtration of data.</p>"},{"location":"faqs/#how-do-you-merge-two-dataframes-in-pandas","title":"How do you merge two DataFrames in Pandas?","text":"<p>You can merge two DataFrames using the <code>merge()</code> function:</p> <pre><code>merged_df = pd.merge(df1, df2, on='common_column')\n</code></pre> <p>Replace <code>'common_column'</code> with the column name that is common to both DataFrames.</p>"},{"location":"faqs/#what-is-the-difference-between-merge-and-concat-in-pandas","title":"What is the difference between <code>merge()</code> and <code>concat()</code> in Pandas?","text":"<ul> <li><code>merge()</code>: Combines two DataFrames based on the values of common columns (similar to SQL joins).</li> <li><code>concat()</code>: Concatenates DataFrames either vertically or horizontally, stacking them along an axis.</li> </ul>"},{"location":"faqs/#how-do-you-calculate-basic-statistical-measures-like-mean-and-median-in-pandas","title":"How do you calculate basic statistical measures like mean and median in Pandas?","text":"<p>You can use built-in functions:</p> <ul> <li>Mean:</li> </ul> <pre><code>mean_value = df['column_name'].mean()\n</code></pre> <ul> <li>Median:</li> </ul> <pre><code>median_value = df['column_name'].median()\n</code></pre>"},{"location":"faqs/#what-is-the-purpose-of-the-apply-function-in-pandas","title":"What is the purpose of the <code>apply()</code> function in Pandas?","text":"<p>The <code>apply()</code> function allows you to apply a function along an axis of the DataFrame (either rows or columns). It is useful for performing complex operations on DataFrame elements.</p>"},{"location":"faqs/#how-do-you-create-a-pivot-table-in-pandas","title":"How do you create a pivot table in Pandas?","text":"<p>You can create a pivot table using the <code>pivot_table()</code> function:</p> <pre><code>pivot = pd.pivot_table(df, values='value_column', index='index_column', columns='columns_column', aggfunc='mean')\n</code></pre>"},{"location":"faqs/#what-is-the-difference-between-supervised-and-unsupervised-learning","title":"What is the difference between supervised and unsupervised learning?","text":"<ul> <li>Supervised Learning: Involves training a model on labeled data, where the target outcome is known. Examples include regression and classification.</li> <li>Unsupervised Learning: Involves finding patterns in unlabeled data without predefined outcomes. Examples include clustering and dimensionality reduction.</li> </ul>"},{"location":"faqs/#how-do-you-perform-linear-regression-using-scikit-learn","title":"How do you perform linear regression using scikit-learn?","text":"<pre><code>from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"faqs/#what-is-overfitting-in-machine-learning","title":"What is overfitting in machine learning?","text":"<p>Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor generalization to new, unseen data. It performs well on training data but poorly on test data.</p>"},{"location":"faqs/#how-can-you-prevent-overfitting","title":"How can you prevent overfitting?","text":"<p>Overfitting can be prevented by:</p> <ul> <li>Cross-validation: Splitting data into training and validation sets.</li> <li>Regularization: Adding penalties for complex models (e.g., Lasso, Ridge).</li> <li>Simplifying the model: Reducing the number of features.</li> <li>Early stopping: Halting training when performance on a validation set starts to degrade.</li> </ul>"},{"location":"faqs/#what-is-cross-validation","title":"What is cross-validation?","text":"<p>Cross-validation is a technique for assessing how a model will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets, and validating it on the remaining ones.</p>"},{"location":"faqs/#how-do-you-evaluate-the-performance-of-a-regression-model","title":"How do you evaluate the performance of a regression model?","text":"<p>Common metrics include:</p> <ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-squared (Coefficient of Determination)</li> </ul>"},{"location":"faqs/#what-is-the-purpose-of-feature-scaling","title":"What is the purpose of feature scaling?","text":"<p>Feature scaling standardizes the range of independent variables, improving the performance and convergence speed of some machine learning algorithms that are sensitive to the scale of data, such as gradient descent optimization.</p>"},{"location":"faqs/#how-do-you-perform-feature-scaling-in-python","title":"How do you perform feature scaling in Python?","text":"<p>Using scikit-learn's <code>StandardScaler</code>:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n</code></pre>"},{"location":"faqs/#what-is-one-hot-encoding","title":"What is one-hot encoding?","text":"<p>One-hot encoding is a process of converting categorical variables into a binary (0 or 1) representation. Each category becomes a new column, and a value of 1 indicates the presence of that category.</p>"},{"location":"faqs/#how-do-you-perform-one-hot-encoding-in-pandas","title":"How do you perform one-hot encoding in Pandas?","text":"<p>Using the <code>get_dummies()</code> function:</p> <pre><code>encoded_df = pd.get_dummies(df, columns=['categorical_column'])\n</code></pre>"},{"location":"faqs/#what-is-a-confusion-matrix","title":"What is a confusion matrix?","text":"<p>A confusion matrix is a table used to evaluate the performance of a classification model. It displays the true positives, true negatives, false positives, and false negatives, providing insight into the types of errors made by the model.</p>"},{"location":"faqs/#how-do-you-calculate-accuracy-precision-and-recall-from-a-confusion-matrix","title":"How do you calculate accuracy, precision, and recall from a confusion matrix?","text":"<ul> <li>Accuracy: (TP + TN) / (TP + TN + FP + FN)</li> <li>Precision: TP / (TP + FP)</li> <li>Recall: TP / (TP + FN)</li> </ul> <p>where:</p> <ul> <li>TP=True Positives</li> <li>TN=True Negatives</li> <li>FP=False Positives</li> <li>FN=False Negatives.</li> </ul>"},{"location":"faqs/#what-is-the-purpose-of-the-train_test_split-function","title":"What is the purpose of the <code>train_test_split</code> function?","text":"<p>The <code>train_test_split</code> function splits a dataset into training and testing sets, allowing you to train the model on one subset and evaluate its performance on another to prevent overfitting.</p>"},{"location":"faqs/#how-do-you-split-data-into-training-and-testing-sets-in-scikit-learn","title":"How do you split data into training and testing sets in scikit-learn?","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n</code></pre>"},{"location":"faqs/#what-is-the-difference-between-classification-and-regression","title":"What is the difference between classification and regression?","text":"<ul> <li>Classification: Predicts categorical labels or classes.</li> <li>Regression: Predicts continuous numerical values.</li> </ul>"},{"location":"faqs/#how-do-you-handle-categorical-variables-in-machine-learning-models","title":"How do you handle categorical variables in machine learning models?","text":"<p>Categorical variables can be handled by:</p> <ul> <li>Label Encoding: Assigning a unique integer to each category.</li> <li>One-Hot Encoding: Creating binary columns for each category.</li> </ul>"},{"location":"faqs/#what-is-k-means-clustering","title":"What is k-means clustering?","text":"<p>K-means clustering is an unsupervised learning algorithm that partitions data into k clusters, where each data point belongs to the cluster with the nearest mean. It aims to minimize the within-cluster sum of squares.</p>"},{"location":"faqs/#how-do-you-determine-the-optimal-number-of-clusters-in-k-means","title":"How do you determine the optimal number of clusters in k-means?","text":"<p>Common methods include:</p> <ul> <li>Elbow Method: Plotting the explained variance as a function of the number of clusters and looking for an \"elbow\" point.</li> <li>Silhouette Score: Measuring how similar a data point is to its own cluster compared to other clusters.</li> </ul>"},{"location":"faqs/#what-is-principal-component-analysis-pca","title":"What is principal component analysis (PCA)?","text":"<p>PCA is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information by identifying the principal components (directions of maximum variance).</p>"},{"location":"faqs/#how-do-you-perform-pca-in-scikit-learn","title":"How do you perform PCA in scikit-learn?","text":"<pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data)\n</code></pre>"},{"location":"faqs/#what-is-the-bias-variance-tradeoff","title":"What is the bias-variance tradeoff?","text":"<p>The bias-variance tradeoff is the balance between a model's ability to generalize to new data (low variance) and its accuracy on training data (low bias). High bias can lead to underfitting, while high variance can lead to overfitting.</p>"},{"location":"faqs/#what-is-regularization-in-machine-learning","title":"What is regularization in machine learning?","text":"<p>Regularization involves adding a penalty term to the loss function to prevent overfitting by discouraging complex models. Common regularization techniques include Lasso (L1) and Ridge (L2) regression.</p>"},{"location":"faqs/#how-do-you-implement-ridge-regression-in-scikit-learn","title":"How do you implement Ridge regression in scikit-learn?","text":"<pre><code>from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faqs/#what-is-logistic-regression","title":"What is logistic regression?","text":"<p>Logistic regression is a classification algorithm used to predict binary outcomes (0 or 1) by modeling the probability of a certain class using a logistic function.</p>"},{"location":"faqs/#how-do-you-evaluate-the-performance-of-a-classification-model","title":"How do you evaluate the performance of a classification model?","text":"<p>Common metrics include:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1 Score</li> <li>ROC AUC Score</li> </ul>"},{"location":"faqs/#what-is-the-receiver-operating-characteristic-roc-curve","title":"What is the Receiver Operating Characteristic (ROC) curve?","text":"<p>The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It illustrates the diagnostic ability of a binary classifier.</p>"},{"location":"faqs/#how-do-you-handle-imbalanced-datasets","title":"How do you handle imbalanced datasets?","text":"<p>Techniques include:</p> <ul> <li>Resampling Methods: Oversampling the minority class or undersampling the majority class.</li> <li>Synthetic Data Generation: Using methods like SMOTE to generate synthetic examples.</li> <li>Using Appropriate Evaluation Metrics: Focusing on precision, recall, or F1 score instead of accuracy.</li> </ul>"},{"location":"faqs/#what-is-time-series-analysis","title":"What is time series analysis?","text":"<p>Time series analysis involves analyzing data points collected or recorded at specific time intervals to identify trends, cycles, and seasonal variations for forecasting and other purposes.</p>"},{"location":"faqs/#how-do-you-deal-with-date-and-time-data-in-pandas","title":"How do you deal with date and time data in Pandas?","text":"<p>Pandas provides the <code>to_datetime()</code> function to convert strings to datetime objects, and you can use datetime properties and methods to manipulate date and time data.</p> <pre><code>df['date_column'] = pd.to_datetime(df['date_column'])\n</code></pre>"},{"location":"faqs/#what-is-autocorrelation-in-time-series-data","title":"What is autocorrelation in time series data?","text":"<p>Autocorrelation is the correlation of a signal with a delayed copy of itself. In time series data, it measures the relationship between a variable's current value and its past values.</p>"},{"location":"faqs/#how-do-you-perform-forecasting-using-arima-models","title":"How do you perform forecasting using ARIMA models?","text":"<p>Using the <code>statsmodels</code> library:</p> <pre><code>from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(time_series_data, order=(p, d, q))\nmodel_fit = model.fit()\nforecast = model_fit.forecast(steps=10)\n</code></pre>"},{"location":"faqs/#what-is-natural-language-processing-nlp","title":"What is natural language processing (NLP)?","text":"<p>NLP is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language.</p>"},{"location":"faqs/#how-do-you-perform-text-preprocessing-in-nlp","title":"How do you perform text preprocessing in NLP?","text":"<p>Common steps include:</p> <ul> <li>Tokenization: Splitting text into words or sentences.</li> <li>Stop Word Removal: Removing common words that add little meaning.</li> <li>Stemming/Lemmatization: Reducing words to their base or root form.</li> <li>Encoding: Converting text to numerical representation using methods like TF-IDF or word embeddings.</li> </ul>"},{"location":"faqs/#what-is-tf-idf","title":"What is TF-IDF?","text":"<p>Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects how important a word is to a document in a collection. It balances the frequency of a word in a document with how common the word is across all documents.</p>"},{"location":"faqs/#how-do-you-handle-large-datasets-that-dont-fit-into-memory","title":"How do you handle large datasets that don't fit into memory?","text":"<p>Techniques include:</p> <ul> <li>Data Sampling: Working with a subset of the data.</li> <li>Batch Processing: Processing data in chunks.</li> <li>Distributed Computing: Using tools like Apache Spark.</li> <li>Out-of-core Learning: Using algorithms that can learn from data incrementally.</li> </ul>"},{"location":"faqs/#what-is-a-pipeline-in-scikit-learn","title":"What is a pipeline in scikit-learn?","text":"<p>A pipeline is a sequence of data processing steps assembled into a single object. It ensures that all steps are applied consistently during training and testing, simplifying the workflow.</p> <pre><code>from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\n</code></pre>"},{"location":"faqs/#how-do-you-save-and-load-trained-models-in-scikit-learn","title":"How do you save and load trained models in scikit-learn?","text":"<p>Using the <code>joblib</code> library:</p> <pre><code>import joblib\n\n# Save model\njoblib.dump(model, 'model.pkl')\n\n# Load model\nmodel = joblib.load('model.pkl')\n</code></pre>"},{"location":"faqs/#what-is-gradient-descent","title":"What is gradient descent?","text":"<p>Gradient descent is an optimization algorithm used to minimize the cost function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.</p>"},{"location":"faqs/#what-is-the-difference-between-batch-stochastic-and-mini-batch-gradient-descent","title":"What is the difference between batch, stochastic, and mini-batch gradient descent?","text":"<ul> <li>Batch Gradient Descent: Uses the entire dataset to compute gradients.</li> <li>Stochastic Gradient Descent (SGD): Uses one sample at a time.</li> <li>Mini-Batch Gradient Descent: Uses a small subset (batch) of the data.</li> </ul>"},{"location":"faqs/#how-do-you-handle-multicollinearity-in-regression-analysis","title":"How do you handle multicollinearity in regression analysis?","text":"<p>Techniques include:</p> <ul> <li>Removing correlated features.</li> <li>Principal Component Analysis (PCA) to reduce dimensionality.</li> <li>Regularization methods like Ridge regression.</li> </ul>"},{"location":"faqs/#what-is-the-central-limit-theorem","title":"What is the Central Limit Theorem?","text":"<p>The Central Limit Theorem states that the sampling distribution of the sample means approaches a normal distribution as the sample size becomes large, regardless of the population's distribution.</p>"},{"location":"faqs/#what-is-hypothesis-testing","title":"What is hypothesis testing?","text":"<p>Hypothesis testing is a statistical method used to decide whether there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis based on sample data.</p>"},{"location":"faqs/#what-is-p-value","title":"What is p-value?","text":"<p>A p-value is the probability of observing results at least as extreme as those measured when the null hypothesis is true. A low p-value indicates that the observed data is unlikely under the null hypothesis.</p>"},{"location":"faqs/#how-do-you-perform-a-t-test-in-python","title":"How do you perform a t-test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>from scipy import stats\n\nt_statistic, p_value = stats.ttest_ind(sample1, sample2)\n</code></pre>"},{"location":"faqs/#what-is-the-difference-between-type-i-and-type-ii-errors","title":"What is the difference between Type I and Type II errors?","text":"<ul> <li>Type I Error: Rejecting a true null hypothesis (false positive).</li> <li>Type II Error: Failing to reject a false null hypothesis (false negative).</li> </ul>"},{"location":"faqs/#what-is-an-anova-test","title":"What is an ANOVA test?","text":"<p>Analysis of Variance (ANOVA) is a statistical method used to compare means across three or more groups to see if at least one mean is different from the others.</p>"},{"location":"faqs/#how-do-you-perform-an-anova-test-in-python","title":"How do you perform an ANOVA test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>from scipy import stats\n\nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n</code></pre>"},{"location":"faqs/#what-is-bootstrapping-in-statistics","title":"What is bootstrapping in statistics?","text":"<p>Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It allows estimation of the sampling distribution of almost any statistic.</p>"},{"location":"faqs/#what-is-the-law-of-large-numbers","title":"What is the law of large numbers?","text":"<p>The law of large numbers states that as the number of trials increases, the sample mean will converge to the expected value (population mean).</p>"},{"location":"faqs/#what-is-a-probability-distribution","title":"What is a probability distribution?","text":"<p>A probability distribution describes how the values of a random variable are distributed. It defines the probabilities of different outcomes.</p>"},{"location":"faqs/#what-are-common-probability-distributions-used-in-data-science","title":"What are common probability distributions used in data science?","text":"<ul> <li>Normal Distribution</li> <li>Binomial Distribution</li> <li>Poisson Distribution</li> <li>Exponential Distribution</li> </ul>"},{"location":"faqs/#how-do-you-generate-random-numbers-following-a-normal-distribution-in-numpy","title":"How do you generate random numbers following a normal distribution in NumPy?","text":"<pre><code>import numpy as np\n\nrandom_numbers = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n</code></pre>"},{"location":"faqs/#what-is-the-curse-of-dimensionality","title":"What is the curse of dimensionality?","text":"<p>The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces, often leading to challenges like overfitting and increased computational cost.</p>"},{"location":"faqs/#how-do-you-reduce-dimensionality","title":"How do you reduce dimensionality?","text":"<p>Techniques include:</p> <ul> <li>Feature Selection: Choosing a subset of relevant features.</li> <li>Feature Extraction: Transforming data into a lower-dimensional space (e.g., PCA).</li> </ul>"},{"location":"faqs/#what-is-the-difference-between-bagging-and-boosting","title":"What is the difference between bagging and boosting?","text":"<ul> <li>Bagging: Combines multiple models (usually of the same type) trained on different subsets of the data to reduce variance.</li> <li>Boosting: Sequentially trains models, where each new model focuses on correcting errors made by previous ones, reducing bias.</li> </ul>"},{"location":"faqs/#what-is-a-decision-tree","title":"What is a decision tree?","text":"<p>A decision tree is a flowchart-like structure used for classification and regression that splits data into branches based on feature values to make predictions.</p>"},{"location":"faqs/#how-do-you-prevent-a-decision-tree-from-overfitting","title":"How do you prevent a decision tree from overfitting?","text":"<p>By:</p> <ul> <li>Pruning: Removing branches that have little power in predicting target variables.</li> <li>Setting a maximum depth: Limiting the depth of the tree.</li> <li>Setting a minimum number of samples per leaf.</li> </ul>"},{"location":"faqs/#what-is-random-forest","title":"What is random forest?","text":"<p>Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.</p>"},{"location":"faqs/#how-do-you-implement-a-random-forest-classifier-in-scikit-learn","title":"How do you implement a random forest classifier in scikit-learn?","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faqs/#what-is-ensemble-learning","title":"What is ensemble learning?","text":"<p>Ensemble learning combines predictions from multiple machine learning algorithms to produce a more accurate prediction than any individual model.</p>"},{"location":"faqs/#what-is-a-neural-network","title":"What is a neural network?","text":"<p>A neural network is a computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons) that can learn complex patterns through training.</p>"},{"location":"faqs/#what-is-the-difference-between-deep-learning-and-machine-learning","title":"What is the difference between deep learning and machine learning?","text":"<ul> <li>Machine Learning: Involves algorithms that parse data, learn from it, and make decisions.</li> <li>Deep Learning: A subset of machine learning using neural networks with multiple layers to model complex patterns.</li> </ul>"},{"location":"faqs/#how-do-you-handle-class-imbalance-in-classification-problems","title":"How do you handle class imbalance in classification problems?","text":"<p>By:</p> <ul> <li>Resampling the dataset: Oversampling the minority class or undersampling the majority class.</li> <li>Using appropriate evaluation metrics: Such as ROC AUC, precision-recall curve.</li> <li>Using algorithms that handle imbalance: Like XGBoost.</li> </ul>"},{"location":"faqs/#what-is-the-purpose-of-the-map-function-in-pandas","title":"What is the purpose of the <code>map()</code> function in Pandas?","text":"<p>The <code>map()</code> function is used to map values of a Series according to an input mapping or function, useful for substituting values or applying a function element-wise.</p>"},{"location":"faqs/#how-do-you-sort-a-dataframe-in-pandas","title":"How do you sort a DataFrame in Pandas?","text":"<p>Using the <code>sort_values()</code> function:</p> <pre><code>sorted_df = df.sort_values(by='column_name', ascending=True)\n</code></pre>"},{"location":"faqs/#what-is-the-difference-between-apply-and-applymap-in-pandas","title":"What is the difference between <code>apply()</code> and <code>applymap()</code> in Pandas?","text":"<ul> <li><code>apply()</code>: Applies a function along an axis of the DataFrame (rows or columns).</li> <li><code>applymap()</code>: Applies a function element-wise to the entire DataFrame.</li> </ul>"},{"location":"faqs/#how-do-you-remove-duplicates-from-a-dataframe","title":"How do you remove duplicates from a DataFrame?","text":"<p>Using the <code>drop_duplicates()</code> function:</p> <pre><code>df_unique = df.drop_duplicates()\n</code></pre>"},{"location":"faqs/#what-is-an-outlier-and-how-do-you-detect-them","title":"What is an outlier, and how do you detect them?","text":"<p>An outlier is a data point significantly different from others. Detection methods include:</p> <ul> <li>Statistical methods: Using Z-scores or IQR.</li> <li>Visualization: Box plots or scatter plots.</li> </ul>"},{"location":"faqs/#how-do-you-handle-outliers-in-data","title":"How do you handle outliers in data?","text":"<p>By:</p> <ul> <li>Removing them: If they are errors.</li> <li>Transforming data: Using log or square root transformations.</li> <li>Using robust algorithms: That are less sensitive to outliers.</li> </ul>"},{"location":"faqs/#what-is-data-normalization","title":"What is data normalization?","text":"<p>Data normalization scales numerical data into a specific range, typically [0,1], ensuring that each feature contributes equally to the analysis.</p>"},{"location":"faqs/#how-do-you-perform-data-normalization-in-scikit-learn","title":"How do you perform data normalization in scikit-learn?","text":"<p>Using <code>MinMaxScaler</code>:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(data)\n</code></pre>"},{"location":"faqs/#what-is-a-heatmap-and-when-do-you-use-it","title":"What is a heatmap, and when do you use it?","text":"<p>A heatmap is a graphical representation of data where individual values are represented as colors. It is used to visualize correlation matrices or to display patterns in data.</p>"},{"location":"faqs/#how-do-you-create-a-heatmap-in-seaborn","title":"How do you create a heatmap in Seaborn?","text":"<pre><code>import seaborn as sns\n\nsns.heatmap(data, annot=True)\n</code></pre>"},{"location":"faqs/#what-is-a-pairplot-in-seaborn","title":"What is a pairplot in Seaborn?","text":"<p>A pairplot creates a grid of Axes such that each variable in the data is shared across the y-axes across a single row and the x-axes across a single column, visualizing pairwise relationships.</p> <pre><code>sns.pairplot(df)\n</code></pre>"},{"location":"faqs/#how-do-you-interpret-a-correlation-coefficient","title":"How do you interpret a correlation coefficient?","text":"<p>A correlation coefficient measures the strength and direction of a linear relationship between two variables. Values range from -1 to 1:</p> <ul> <li>-1: Perfect negative correlation.</li> <li>0: No correlation.</li> <li>1: Perfect positive correlation.</li> </ul>"},{"location":"faqs/#what-is-the-purpose-of-the-describe-function-in-pandas","title":"What is the purpose of the <code>describe()</code> function in Pandas?","text":"<p>The <code>describe()</code> function generates descriptive statistics, including count, mean, standard deviation, min, max, and quartile values for numerical columns.</p>"},{"location":"faqs/#how-do-you-handle-datetime-features-for-machine-learning-models","title":"How do you handle date/time features for machine learning models?","text":"<p>By extracting meaningful components:</p> <ul> <li>Year, Month, Day</li> <li>Weekday</li> <li>Hour, Minute, Second</li> <li>Time since a specific date</li> </ul>"},{"location":"faqs/#what-is-the-difference-between-loc-and-iloc-in-pandas","title":"What is the difference between <code>.loc</code> and <code>.iloc</code> in Pandas?","text":"<ul> <li><code>.loc</code>: Label-based indexing to select data by row and column labels.</li> <li><code>.iloc</code>: Integer-based indexing to select data by row and column positions.</li> </ul>"},{"location":"faqs/#how-do-you-rename-columns-in-a-dataframe","title":"How do you rename columns in a DataFrame?","text":"<p>Using the <code>rename()</code> function:</p> <pre><code>df.rename(columns={'old_name': 'new_name'}, inplace=True)\n</code></pre>"},{"location":"faqs/#what-is-the-purpose-of-the-astype-function-in-pandas","title":"What is the purpose of the <code>astype()</code> function in Pandas?","text":"<p>The <code>astype()</code> function is used to change the data type of a Series or DataFrame columns.</p> <pre><code>df['column_name'] = df['column_name'].astype('float')\n</code></pre>"},{"location":"faqs/#how-do-you-detect-missing-values-in-a-dataframe","title":"How do you detect missing values in a DataFrame?","text":"<p>Using <code>isnull()</code> or <code>isna()</code> functions:</p> <pre><code>missing_values = df.isnull().sum()\n</code></pre>"},{"location":"faqs/#what-is-an-ensemble-method-in-machine-learning","title":"What is an ensemble method in machine learning?","text":"<p>An ensemble method combines predictions from multiple machine learning models to improve performance over a single model. Examples include Random Forest, Gradient Boosting.</p>"},{"location":"faqs/#how-do-you-implement-gradient-boosting-in-scikit-learn","title":"How do you implement Gradient Boosting in scikit-learn?","text":"<pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faqs/#what-is-xgboost","title":"What is XGBoost?","text":"<p>XGBoost (Extreme Gradient Boosting) is an optimized gradient boosting library designed to be highly efficient, flexible, and portable, widely used for its performance and speed.</p>"},{"location":"faqs/#how-do-you-install-and-use-xgboost-in-python","title":"How do you install and use XGBoost in Python?","text":"<p>Install using pip:</p> <pre><code>bash\npip install xgboost\n</code></pre> <p>Use in code:</p> <pre><code>import xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"faqs/#what-is-cross-entropy-loss","title":"What is cross-entropy loss?","text":"<p>Cross-entropy loss measures the performance of a classification model whose output is a probability between 0 and 1. It increases as the predicted probability diverges from the actual label.</p>"},{"location":"faqs/#how-do-you-calculate-the-learning-rate-in-gradient-descent","title":"How do you calculate the learning rate in gradient descent?","text":"<p>The learning rate is a hyperparameter that you set manually. It determines the step size at each iteration while moving toward a minimum of a loss function.</p>"},{"location":"faqs/#what-is-the-difference-between-epochs-batches-and-iterations","title":"What is the difference between epochs, batches, and iterations?","text":"<ul> <li>Epoch: One complete pass through the entire training dataset.</li> <li>Batch: A subset of the training data used in one iteration.</li> <li>Iteration: One update of the model's parameters.</li> </ul>"},{"location":"faqs/#how-do-you-perform-hyperparameter-tuning","title":"How do you perform hyperparameter tuning?","text":"<p>By:</p> <ul> <li>Grid Search: Exhaustively searching through a specified subset of hyperparameters.</li> <li>Random Search: Randomly sampling hyperparameter combinations.</li> <li>Bayesian Optimization: Using probabilistic models to select hyperparameters.</li> </ul>"},{"location":"faqs/#what-is-the-purpose-of-the-pipeline-module-in-scikit-learn","title":"What is the purpose of the <code>pipeline</code> module in scikit-learn?","text":"<p>It allows you to chain preprocessing steps and estimators in a sequential manner, ensuring consistent application of transformations during training and testing.</p>"},{"location":"faqs/#how-do-you-evaluate-a-clustering-algorithm","title":"How do you evaluate a clustering algorithm?","text":"<p>Using metrics like:</p> <ul> <li>Silhouette Score</li> <li>Calinski-Harabasz Index</li> <li>Davies-Bouldin Index</li> </ul>"},{"location":"faqs/#what-is-a-dummy-variable-trap","title":"What is a dummy variable trap?","text":"<p>The dummy variable trap occurs when multicollinearity is introduced in a regression model due to the inclusion of dummy variables that are linearly dependent. It can be avoided by dropping one dummy variable.</p>"},{"location":"faqs/#how-do-you-create-a-correlation-matrix-in-pandas","title":"How do you create a correlation matrix in Pandas?","text":"<pre><code>corr_matrix = df.corr()\n</code></pre>"},{"location":"faqs/#what-is-an-roc-curve-and-how-do-you-plot-it","title":"What is an ROC curve, and how do you plot it?","text":"<p>An ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate at various threshold settings. You can plot it using scikit-learn:</p> <pre><code>from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nplt.plot(fpr, tpr)\n</code></pre>"},{"location":"faqs/#what-is-a-type-i-error","title":"What is a type I error?","text":"<p>A type I error occurs when the null hypothesis is true, but we incorrectly reject it (false positive).</p>"},{"location":"faqs/#what-is-a-type-ii-error","title":"What is a type II error?","text":"<p>A type II error occurs when the null hypothesis is false, but we fail to reject it (false negative).</p>"},{"location":"faqs/#how-do-you-calculate-the-p-value-in-a-hypothesis-test","title":"How do you calculate the p-value in a hypothesis test?","text":"<p>Using statistical tests from libraries like <code>scipy.stats</code>, which return the p-value as part of the output.</p>"},{"location":"faqs/#what-is-the-difference-between-parametric-and-non-parametric-tests","title":"What is the difference between parametric and non-parametric tests?","text":"<ul> <li>Parametric Tests: Assume underlying statistical distributions (e.g., t-test).</li> <li>Non-Parametric Tests: Do not assume any specific distribution (e.g., Mann-Whitney U test).</li> </ul>"},{"location":"faqs/#how-do-you-perform-a-chi-squared-test-in-python","title":"How do you perform a chi-squared test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>from scipy.stats import chi2_contingency\n\nchi2, p, dof, expected = chi2_contingency(observed_values)\n</code></pre>"},{"location":"faqs/#what-is-the-purpose-of-the-seaborn-library","title":"What is the purpose of the <code>seaborn</code> library?","text":"<p>Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics.</p>"},{"location":"faqs/#how-do-you-create-a-box-plot-in-seaborn","title":"How do you create a box plot in Seaborn?","text":"<pre><code>import seaborn as sns\n\nsns.boxplot(x='categorical_column', y='numerical_column', data=df)\n</code></pre>"},{"location":"faqs/#what-is-a-violin-plot","title":"What is a violin plot?","text":"<p>A violin plot combines a box plot and a kernel density plot to provide a richer depiction of the data distribution.</p>"},{"location":"faqs/#how-do-you-handle-multivariate-time-series-data","title":"How do you handle multivariate time series data?","text":"<p>By:</p> <ul> <li>Using models designed for multivariate data: Like VAR (Vector Autoregression).</li> <li>Feature engineering: Creating lag features for each variable.</li> </ul>"},{"location":"faqs/#what-is-an-ab-test","title":"What is an A/B test?","text":"<p>An A/B test is an experiment comparing two variants (A and B) to determine which one performs better regarding a specific metric.</p>"},{"location":"faqs/#how-do-you-analyze-ab-test-results","title":"How do you analyze A/B test results?","text":"<p>By:</p> <ul> <li>Calculating the difference in metrics between groups.</li> <li>Performing statistical tests: Like t-tests or chi-squared tests.</li> <li>Checking for statistical significance: Using p-values and confidence intervals.</li> </ul>"},{"location":"faqs/#what-is-the-bonferroni-correction","title":"What is the Bonferroni correction?","text":"<p>A method to adjust p-values when multiple comparisons are made to reduce the chances of obtaining false-positive results (Type I errors).</p>"},{"location":"faqs/#what-is-survivorship-bias","title":"What is survivorship bias?","text":"<p>Survivorship bias occurs when analyses are conducted only on surviving subjects, leading to skewed results due to the overlooking of those that did not survive or were not included.</p>"},{"location":"faqs/#what-is-data-leakage","title":"What is data leakage?","text":"<p>Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates that won't generalize to new data.</p>"},{"location":"faqs/#how-do-you-prevent-data-leakage","title":"How do you prevent data leakage?","text":"<p>By:</p> <ul> <li>Separating training and testing data properly.</li> <li>Performing data preprocessing steps within cross-validation.</li> <li>Avoiding using future data in model training.</li> </ul>"},{"location":"faqs/#what-is-a-hash-table","title":"What is a hash table?","text":"<p>A hash table is a data structure that implements an associative array, mapping keys to values using a hash function to compute an index into an array of buckets.</p>"},{"location":"faqs/#what-is-memoization","title":"What is memoization?","text":"<p>Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.</p>"},{"location":"faqs/#how-do-you-reverse-a-linked-list","title":"How do you reverse a linked list?","text":"<p>By iterating through the list and reversing the pointers of each node to point to the previous node.</p>"},{"location":"faqs/#what-is-the-time-complexity-of-quicksort","title":"What is the time complexity of quicksort?","text":"<p>The average time complexity is O(n log n), while the worst-case is O(n^2).</p>"},{"location":"faqs/#what-is-a-generator-in-python","title":"What is a generator in Python?","text":"<p>A generator is a special type of function that returns an iterator object which can iterate over a sequence of values. It uses the <code>yield</code> keyword.</p> <pre><code>def my_generator():\n    yield value\n</code></pre>"},{"location":"faqs/#how-do-you-handle-exceptions-in-python","title":"How do you handle exceptions in Python?","text":"<p>Using try-except blocks:</p> <pre><code>try:\n    # Code that may raise an exception\nexcept ExceptionType as e:\n    # Code to handle the exception\n</code></pre>"},{"location":"faqs/#what-is-a-decorator-in-python","title":"What is a decorator in Python?","text":"<p>A decorator is a function that modifies the behavior of another function or method. It allows for the addition of functionality to existing code in a modular way.</p> <pre><code>def decorator_function(func):\n    def wrapper():\n        # Code before function call\n        func()\n        # Code after function call\n    return wrapper\n</code></pre>"},{"location":"faqs/#how-do-you-read-and-write-json-files-in-python","title":"How do you read and write JSON files in Python?","text":"<p>Using the <code>json</code> module:</p> <pre><code>import json\n\n# Read JSON\nwith open('file.json', 'r') as f:\n    data = json.load(f)\n\n# Write JSON\nwith open('file.json', 'w') as f:\n    json.dump(data, f)\n</code></pre>"},{"location":"faqs/#what-is-multithreading-and-how-do-you-implement-it-in-python","title":"What is multithreading, and how do you implement it in Python?","text":"<p>Multithreading allows concurrent execution of threads (lightweight processes) to improve performance. In Python, you can use the <code>threading</code> module:</p> <pre><code>import threading\n\ndef function_to_run():\n    pass\n\nthread = threading.Thread(target=function_to_run)\nthread.start()\n</code></pre>"},{"location":"faqs/#what-is-the-global-interpreter-lock-gil-in-python","title":"What is the Global Interpreter Lock (GIL) in Python?","text":"<p>The GIL is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. It simplifies memory management but can limit performance in CPU-bound multi-threaded programs.</p>"},{"location":"faqs/#how-do-you-handle-file-operations-in-python","title":"How do you handle file operations in Python?","text":"<p>Using <code>open()</code>:</p> <pre><code>with open('file.txt', 'r') as file:\n    content = file.read()\n</code></pre>"},{"location":"faqs/#what-are-lambda-functions-in-python","title":"What are lambda functions in Python?","text":"<p>Lambda functions are anonymous functions defined using the <code>lambda</code> keyword, useful for short, simple functions.</p> <pre><code>add = lambda x, y: x + y\n</code></pre>"},{"location":"faqs/#how-do-you-work-with-databases-in-python","title":"How do you work with databases in Python?","text":"<p>By using database connectors and ORMs like:</p> <ul> <li>SQLite: Using <code>sqlite3</code> module.</li> <li>MySQL: Using <code>mysql-connector-python</code>.</li> <li>ORMs: Using libraries like SQLAlchemy.</li> </ul>"},{"location":"faqs/#what-is-the-purpose-of-virtual-environments-in-python","title":"What is the purpose of virtual environments in Python?","text":"<p>Virtual environments allow you to create isolated Python environments with specific packages and dependencies, preventing conflicts between projects.</p> <pre><code>bash\npython -m venv myenv\n</code></pre>"},{"location":"faqs/#how-do-you-install-packages-in-python","title":"How do you install packages in Python?","text":"<p>Using <code>pip</code>:</p> <pre><code>bash\npip install package_name\n</code></pre>"},{"location":"faqs/#what-are-the-common-data-types-in-python","title":"What are the common data types in Python?","text":"<ul> <li>Numeric Types: int, float, complex</li> <li>Sequence Types: list, tuple, range</li> <li>Text Type: str</li> <li>Mapping Type: dict</li> <li>Set Types: set, frozenset</li> <li>Boolean Type: bool</li> </ul>"},{"location":"faqs/#how-do-you-create-a-class-in-python","title":"How do you create a class in Python?","text":"<pre><code>class MyClass:\n    def __init__(self, attribute):\n        self.attribute = attribute\n</code></pre>"},{"location":"faqs/#what-is-inheritance-in-python","title":"What is inheritance in Python?","text":"<p>Inheritance allows a class (child) to inherit attributes and methods from another class (parent), promoting code reusability.</p> <pre><code>class ChildClass(ParentClass):\n    pass\n</code></pre>"},{"location":"faqs/#what-is-polymorphism-in-python","title":"What is polymorphism in Python?","text":"<p>Polymorphism allows methods to have the same name but behave differently in different classes. It enables methods to be used interchangeably.</p> <p>o1</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#glossary-of-terms-for-introduction-to-data-science-with-python","title":"Glossary of Terms for Introduction to Data Science with Python","text":""},{"location":"glossary/#ac","title":"A\u2013C","text":""},{"location":"glossary/#accuracy","title":"Accuracy","text":"<p>A metric used to evaluate classification models, representing the proportion of correct predictions over the total number of predictions. Example: In a spam email classifier, if the model correctly identifies 90 out of 100 emails, the accuracy is 90%.</p>"},{"location":"glossary/#algorithm","title":"Algorithm","text":"<p>A step-by-step procedure or set of rules designed to perform a specific task or solve a problem. Example: Implementing the k-means clustering algorithm to group similar data points in an unsupervised learning task.</p>"},{"location":"glossary/#anaconda","title":"Anaconda","text":"<p>A free and open-source distribution of Python and R programming languages for scientific computing and data science. Example: Using Anaconda to manage Python packages and environments for data analysis projects in the course.</p>"},{"location":"glossary/#analytics","title":"Analytics","text":"<p>The process of examining datasets to draw conclusions about the information they contain, often using specialized software and statistical techniques. Example: Performing customer behavior analytics using Pandas and Matplotlib to improve marketing strategies.</p>"},{"location":"glossary/#anomaly-detection","title":"Anomaly Detection","text":"<p>The process of identifying unusual patterns or observations in data that do not conform to expected behavior. Example: Detecting fraudulent credit card transactions using an isolation forest model.</p>"},{"location":"glossary/#api-application-programming-interface","title":"API (Application Programming Interface)","text":"<p>A set of functions and protocols that allows software applications to communicate with each other. Example: Using the OpenWeather API to import real-time temperature data into a Python program.</p>"},{"location":"glossary/#application-layer","title":"Application Layer","text":"<p>The highest level in a computing system that interacts directly with user-facing applications and services. Example: A Python script running in Jupyter Notebook is executed at the application layer.</p>"},{"location":"glossary/#area-under-the-curve-auc","title":"Area Under the Curve (AUC)","text":"<p>A performance metric for classification models that measures the ability to distinguish between classes, calculated from the ROC curve. Example: A model with an AUC of 0.95 is better at distinguishing positive from negative cases than one with 0.70.</p>"},{"location":"glossary/#array","title":"Array","text":"<p>A data structure that stores a collection of items, typically of the same type, at contiguous memory locations. Example: Using NumPy arrays for fast vectorized mathematical computations.</p>"},{"location":"glossary/#artificial-intelligence-ai","title":"Artificial Intelligence (AI)","text":"<p>The field of computer science focused on creating systems capable of performing tasks that normally require human intelligence. Example: Building a neural network to recognize handwritten digits.</p>"},{"location":"glossary/#association-rule-mining","title":"Association Rule Mining","text":"<p>A method for finding relationships between variables in large datasets. Example: Identifying that customers who buy bread often also purchase butter.</p>"},{"location":"glossary/#attribute","title":"Attribute","text":"<p>A property or characteristic of an object, often represented as a column in a dataset. Example: In a dataset of houses, \"number of bedrooms\" is an attribute.</p>"},{"location":"glossary/#autocorrelation","title":"Autocorrelation","text":"<p>A measure of how related a variable is with a lagged version of itself over time. Example: Detecting repeating seasonal patterns in monthly sales data.</p>"},{"location":"glossary/#bagging-bootstrap-aggregating","title":"Bagging (Bootstrap Aggregating)","text":"<p>A machine learning ensemble technique that combines predictions from multiple models trained on different subsets of the data. Example: Using bagging with decision trees to create a random forest classifier.</p>"},{"location":"glossary/#bar-chart","title":"Bar Chart","text":"<p>A visualization that uses rectangular bars to represent data values for different categories. Example: Displaying average exam scores for different majors in a bar chart.</p>"},{"location":"glossary/#baseline-model","title":"Baseline Model","text":"<p>A simple model used as a reference point for comparing the performance of more complex models. Example: Using the mean value of the target variable as predictions to compare against a regression model.</p>"},{"location":"glossary/#batch-processing","title":"Batch Processing","text":"<p>A method of processing large volumes of data in groups instead of in real time. Example: Running a nightly script to clean and aggregate all transaction data from the previous day.</p>"},{"location":"glossary/#bayes-theorem","title":"Bayes\u2019 Theorem","text":"<p>A formula used to update probabilities based on new evidence. Example: Updating the probability of having a disease after a positive test result.</p>"},{"location":"glossary/#bias-in-machine-learning","title":"Bias (in Machine Learning)","text":"<p>The error introduced when a model makes assumptions that oversimplify the problem, potentially leading to underfitting. Example: Using a linear model for a highly non-linear dataset can introduce high bias.</p>"},{"location":"glossary/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The balance between underfitting (high bias) and overfitting (high variance) when building predictive models. Example: Choosing a model complexity that minimizes both bias and variance.</p>"},{"location":"glossary/#big-data","title":"Big Data","text":"<p>Extremely large datasets that require specialized storage and processing methods. Example: Analyzing petabytes of social media data for sentiment analysis.</p>"},{"location":"glossary/#bin","title":"Bin","text":"<p>A grouping of continuous numerical values into intervals for analysis or visualization. Example: Grouping ages into bins like 0\u201318, 19\u201335, and 36\u201360 for analysis.</p>"},{"location":"glossary/#binary-classification","title":"Binary Classification","text":"<p>A classification problem with only two possible outcomes. Example: Predicting whether an email is spam or not spam.</p>"},{"location":"glossary/#binary-variable","title":"Binary Variable","text":"<p>A variable with only two possible values, often represented as 0 and 1. Example: \"Passed Exam\" variable coded as 1 for pass and 0 for fail.</p>"},{"location":"glossary/#box-plot","title":"Box Plot","text":"<p>A graphical summary of data distribution showing median, quartiles, and potential outliers. Example: Using a box plot to compare exam scores across different classes.</p>"},{"location":"glossary/#categorical-data","title":"Categorical Data","text":"<p>Data that represents discrete groups or categories. Example: \"Car color\" with values red, blue, or green.</p>"},{"location":"glossary/#causal-relationship","title":"Causal Relationship","text":"<p>A relationship where a change in one variable directly produces a change in another. Example: Increasing advertising budget directly increasing sales.</p>"},{"location":"glossary/#central-limit-theorem-clt","title":"Central Limit Theorem (CLT)","text":"<p>A statistical theory stating that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases. Example: Using the CLT to justify using normal-based confidence intervals.</p>"},{"location":"glossary/#class-in-classification","title":"Class (in Classification)","text":"<p>A label assigned to a group of similar data points in a classification task. Example: \"Cat\" and \"Dog\" are two classes in an animal image classifier.</p>"},{"location":"glossary/#classification","title":"Classification","text":"<p>The process of predicting a categorical label for new observations based on training data. Example: Using logistic regression to classify emails as spam or not spam.</p>"},{"location":"glossary/#classification-report","title":"Classification Report","text":"<p>A summary of a classification model\u2019s performance, including precision, recall, and F1-score. Example: Generating a classification report in scikit-learn after model training.</p>"},{"location":"glossary/#clustering","title":"Clustering","text":"<p>An unsupervised learning method that groups data points into clusters based on similarity. Example: Segmenting customers into groups based on purchasing habits.</p>"},{"location":"glossary/#coefficient","title":"Coefficient","text":"<p>A numerical value representing the strength and direction of the relationship between a predictor and the response variable in a regression model. Example: In a linear regression, a coefficient of 2.5 for \"hours studied\" means each extra hour increases predicted score by 2.5 points.</p>"},{"location":"glossary/#collinearity","title":"Collinearity","text":"<p>A situation where two or more predictor variables are highly correlated, which can affect model stability. Example: Including both height in inches and height in centimeters in a regression model creates collinearity.</p>"},{"location":"glossary/#column","title":"Column","text":"<p>A vertical arrangement of data values in a table, representing a single attribute. Example: In a Pandas DataFrame, each column holds one variable\u2019s data.</p>"},{"location":"glossary/#confusion-matrix","title":"Confusion Matrix","text":"<p>A table used to evaluate classification performance by showing counts of true positives, false positives, true negatives, and false negatives. Example: A confusion matrix showing that a model misclassified 5 out of 100 test emails.</p>"},{"location":"glossary/#correlation","title":"Correlation","text":"<p>A statistical measure that describes the strength and direction of a linear relationship between two variables. Example: A correlation coefficient of 0.9 between height and weight indicates a strong positive relationship.</p>"},{"location":"glossary/#correlation-coefficient","title":"Correlation Coefficient","text":"<p>A numerical value between -1 and 1 that indicates the degree of linear association between two variables. Example: A value of -0.75 shows a strong negative correlation.</p>"},{"location":"glossary/#cross-validation","title":"Cross-Validation","text":"<p>A technique for assessing how well a model generalizes by dividing data into multiple training and testing subsets. Example: Using 5-fold cross-validation to evaluate a regression model\u2019s accuracy.</p>"},{"location":"glossary/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>A simple file format for storing tabular data where each line represents a row and values are separated by commas. Example: Importing a CSV file of sales data into Pandas for analysis.</p>"},{"location":"glossary/#dh","title":"D\u2013H","text":""},{"location":"glossary/#data-cleaning","title":"Data Cleaning","text":"<p>The process of detecting and correcting inaccurate, incomplete, or irrelevant data in a dataset. Example: Removing duplicate rows and correcting misspelled city names in a dataset.</p>"},{"location":"glossary/#data-exploration","title":"Data Exploration","text":"<p>The initial phase of data analysis where patterns, trends, and relationships are identified through visualization and statistics. Example: Using Pandas and Matplotlib to examine sales trends by month.</p>"},{"location":"glossary/#data-frame","title":"Data Frame","text":"<p>A two-dimensional labeled data structure in Pandas that can hold different data types in columns. Example: Loading a CSV file into a Pandas DataFrame for manipulation.</p>"},{"location":"glossary/#data-mining","title":"Data Mining","text":"<p>The process of discovering patterns, correlations, and insights from large datasets using statistical and computational techniques. Example: Finding product purchase patterns from transaction logs.</p>"},{"location":"glossary/#data-normalization","title":"Data Normalization","text":"<p>Adjusting values in a dataset to a common scale without distorting differences in ranges. Example: Scaling features between 0 and 1 before training a machine learning model.</p>"},{"location":"glossary/#data-preprocessing","title":"Data Preprocessing","text":"<p>Transforming raw data into a clean and usable format for analysis or modeling. Example: Encoding categorical variables as integers before running a regression.</p>"},{"location":"glossary/#data-science","title":"Data Science","text":"<p>An interdisciplinary field that combines statistics, programming, and domain knowledge to extract insights from data. Example: Using Python to analyze and visualize health survey data.</p>"},{"location":"glossary/#data-type","title":"Data Type","text":"<p>A classification specifying the kind of value a variable can hold, such as integer, float, or string. Example: In Python, <code>3</code> is an integer, and <code>3.14</code> is a float.</p>"},{"location":"glossary/#dataset","title":"Dataset","text":"<p>A structured collection of data, typically organized in rows and columns. Example: The Iris dataset is commonly used for classification demonstrations.</p>"},{"location":"glossary/#decision-tree","title":"Decision Tree","text":"<p>A model that makes decisions by splitting data into branches based on feature values. Example: Predicting loan approval using a decision tree classifier.</p>"},{"location":"glossary/#deep-learning","title":"Deep Learning","text":"<p>A subfield of machine learning that uses multi-layered neural networks to model complex patterns. Example: Training a convolutional neural network to recognize handwritten digits.</p>"},{"location":"glossary/#dependent-variable","title":"Dependent Variable","text":"<p>The outcome variable being predicted or explained in a model. Example: Predicting \"house price\" based on features like size and location.</p>"},{"location":"glossary/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Statistical methods for summarizing and describing data features. Example: Calculating mean, median, and standard deviation for exam scores.</p>"},{"location":"glossary/#dimension-reduction","title":"Dimension Reduction","text":"<p>Techniques for reducing the number of variables while preserving relevant information. Example: Using Principal Component Analysis (PCA) to reduce feature count before clustering.</p>"},{"location":"glossary/#discrete-data","title":"Discrete Data","text":"<p>Data that can only take on specific, distinct values. Example: Number of students in a classroom.</p>"},{"location":"glossary/#distribution","title":"Distribution","text":"<p>The way values of a variable are spread or arranged. Example: A bell-shaped distribution for students\u2019 test scores.</p>"},{"location":"glossary/#dummy-variable","title":"Dummy Variable","text":"<p>A binary variable created to represent categorical data for modeling purposes. Example: Encoding \"Gender\" as 0 for male and 1 for female.</p>"},{"location":"glossary/#eda-exploratory-data-analysis","title":"EDA (Exploratory Data Analysis)","text":"<p>The process of visually and statistically summarizing a dataset to uncover patterns and insights. Example: Plotting histograms and scatter plots to explore data before modeling.</p>"},{"location":"glossary/#epoch","title":"Epoch","text":"<p>One complete pass through the entire training dataset in machine learning. Example: Training a neural network for 50 epochs to achieve better accuracy.</p>"},{"location":"glossary/#evaluation-metric","title":"Evaluation Metric","text":"<p>A standard of measurement used to assess model performance. Example: Mean Squared Error (MSE) for regression models.</p>"},{"location":"glossary/#evidence-based-decision","title":"Evidence-Based Decision","text":"<p>A decision guided by analysis and data rather than intuition alone. Example: Allocating marketing budget based on data-driven campaign performance.</p>"},{"location":"glossary/#explanatory-variable","title":"Explanatory Variable","text":"<p>A variable used to predict or explain the dependent variable. Example: In predicting crop yield, rainfall amount is an explanatory variable.</p>"},{"location":"glossary/#exponential-smoothing","title":"Exponential Smoothing","text":"<p>A forecasting method that applies decreasing weights to older observations. Example: Forecasting monthly sales using exponential smoothing.</p>"},{"location":"glossary/#feature","title":"Feature","text":"<p>An individual measurable property of the data used in modeling. Example: \"Age\" and \"Income\" as features in predicting spending habits.</p>"},{"location":"glossary/#feature-engineering","title":"Feature Engineering","text":"<p>The process of creating new features or transforming existing ones to improve model performance. Example: Creating a \"BMI\" feature from height and weight columns.</p>"},{"location":"glossary/#feature-scaling","title":"Feature Scaling","text":"<p>Adjusting feature values to the same scale to ensure fair model training. Example: Standardizing all features to have zero mean and unit variance.</p>"},{"location":"glossary/#feature-selection","title":"Feature Selection","text":"<p>Choosing the most relevant features to improve model efficiency and accuracy. Example: Removing features with low correlation to the target variable.</p>"},{"location":"glossary/#f1-score","title":"F1-Score","text":"<p>A metric that combines precision and recall into a single value. Example: An F1-score of 0.85 indicates a balanced model performance.</p>"},{"location":"glossary/#false-negative","title":"False Negative","text":"<p>A prediction that incorrectly classifies a positive case as negative. Example: A medical test failing to detect a disease when it is present.</p>"},{"location":"glossary/#false-positive","title":"False Positive","text":"<p>A prediction that incorrectly classifies a negative case as positive. Example: A spam filter marking a legitimate email as spam.</p>"},{"location":"glossary/#feature-matrix","title":"Feature Matrix","text":"<p>A two-dimensional array containing feature values for all observations in a dataset. Example: A NumPy array where each row is a sample and each column is a feature.</p>"},{"location":"glossary/#forecasting","title":"Forecasting","text":"<p>Predicting future values based on historical data. Example: Using time series models to forecast next month\u2019s sales.</p>"},{"location":"glossary/#function-python","title":"Function (Python)","text":"<p>A reusable block of code that performs a specific task. Example: Defining a function to calculate the mean of a list.</p>"},{"location":"glossary/#gaussian-distribution","title":"Gaussian Distribution","text":"<p>A symmetric, bell-shaped probability distribution characterized by mean and standard deviation. Example: Heights of adult men following a Gaussian distribution.</p>"},{"location":"glossary/#generalization","title":"Generalization","text":"<p>The ability of a model to perform well on new, unseen data. Example: A model that predicts accurately on both training and test datasets generalizes well.</p>"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"<p>An optimization algorithm used to minimize a function by iteratively moving toward the lowest point. Example: Training linear regression by adjusting weights with gradient descent.</p>"},{"location":"glossary/#ground-truth","title":"Ground Truth","text":"<p>The actual, verified values used to evaluate predictions. Example: Comparing model predictions to actual customer purchase data.</p>"},{"location":"glossary/#groupby-pandas","title":"GroupBy (Pandas)","text":"<p>A Pandas method for splitting data into groups for aggregation or transformation. Example: Grouping sales data by region and summing total revenue.</p>"},{"location":"glossary/#heatmap","title":"Heatmap","text":"<p>A graphical representation of data values using color shading. Example: A correlation heatmap showing relationships between features.</p>"},{"location":"glossary/#heuristic","title":"Heuristic","text":"<p>A rule-of-thumb strategy for solving problems quickly, though not always optimally. Example: Using a decision tree depth limit of 5 as a heuristic for speed.</p>"},{"location":"glossary/#histogram","title":"Histogram","text":"<p>A plot that groups numerical data into intervals (bins) and shows their frequency. Example: Plotting the distribution of exam scores in bins of 10 points.</p>"},{"location":"glossary/#hyperparameter","title":"Hyperparameter","text":"<p>A parameter set before training that controls model behavior. Example: Setting the learning rate in gradient descent.</p>"},{"location":"glossary/#im","title":"I\u2013M","text":""},{"location":"glossary/#imbalanced-dataset","title":"Imbalanced Dataset","text":"<p>A dataset where some classes have significantly more samples than others. Example: A fraud detection dataset with 99% non-fraud and 1% fraud cases.</p>"},{"location":"glossary/#imputation","title":"Imputation","text":"<p>The process of replacing missing data with substituted values. Example: Filling missing ages with the mean age of the dataset.</p>"},{"location":"glossary/#independent-variable","title":"Independent Variable","text":"<p>A variable used as an input to predict another variable. Example: Study time as an independent variable in predicting test scores.</p>"},{"location":"glossary/#index-pandas","title":"Index (Pandas)","text":"<p>Labels that identify rows in a Pandas DataFrame or Series. Example: Setting the \"date\" column as the index for a time series.</p>"},{"location":"glossary/#inferential-statistics","title":"Inferential Statistics","text":"<p>Statistical methods for making predictions or inferences about a population from a sample. Example: Using a sample of voters to predict an election outcome.</p>"},{"location":"glossary/#information-gain","title":"Information Gain","text":"<p>A measure of how much a feature improves classification by reducing uncertainty. Example: A decision tree choosing a feature with the highest information gain for the first split.</p>"},{"location":"glossary/#interpolation","title":"Interpolation","text":"<p>Estimating unknown values within the range of known data points. Example: Estimating missing temperature readings between two known days.</p>"},{"location":"glossary/#iteration","title":"Iteration","text":"<p>A single pass through a loop or process in programming. Example: Each pass through a <code>for</code> loop is an iteration.</p>"},{"location":"glossary/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>An interactive environment for writing and running Python code, combining code, visualizations, and text. Example: Running data analysis and visualizations in a Jupyter Notebook.</p>"},{"location":"glossary/#json-javascript-object-notation","title":"JSON (JavaScript Object Notation)","text":"<p>A lightweight data format for storing and exchanging structured data. Example: Importing JSON-formatted API data into Pandas.</p>"},{"location":"glossary/#k-fold-cross-validation","title":"k-Fold Cross-Validation","text":"<p>A method of splitting data into k subsets for training and testing to evaluate model performance. Example: Using 10-fold cross-validation to assess a regression model.</p>"},{"location":"glossary/#k-means-clustering","title":"k-Means Clustering","text":"<p>An unsupervised learning algorithm that groups data into k clusters based on similarity. Example: Segmenting customers into 4 clusters using purchase history.</p>"},{"location":"glossary/#kernel-in-computing","title":"Kernel (in Computing)","text":"<p>The central component of an operating system or computational environment. Example: Restarting the Python kernel in Jupyter Notebook to clear variables.</p>"},{"location":"glossary/#keyword-argument-python","title":"Keyword Argument (Python)","text":"<p>An argument passed to a function by explicitly naming the parameter. Example: Calling <code>plot(x, y, color=\"blue\")</code> uses a keyword argument.</p>"},{"location":"glossary/#kolmogorov-smirnov-test","title":"Kolmogorov-Smirnov Test","text":"<p>A statistical test to compare a sample distribution with a reference probability distribution. Example: Checking if a dataset follows a normal distribution.</p>"},{"location":"glossary/#label-in-machine-learning","title":"Label (in Machine Learning)","text":"<p>The output or target variable in supervised learning. Example: \"Species\" in the Iris dataset is the label.</p>"},{"location":"glossary/#label-encoding","title":"Label Encoding","text":"<p>Converting categorical values into numeric codes. Example: Encoding \"low\", \"medium\", \"high\" as 0, 1, 2.</p>"},{"location":"glossary/#lasso-regression","title":"Lasso Regression","text":"<p>A linear regression method with L1 regularization that can reduce coefficients to zero. Example: Using Lasso to perform feature selection in a high-dimensional dataset.</p>"},{"location":"glossary/#learning-rate","title":"Learning Rate","text":"<p>A hyperparameter controlling how much to adjust model weights during training. Example: Setting a learning rate of 0.01 for gradient descent.</p>"},{"location":"glossary/#least-squares-method","title":"Least Squares Method","text":"<p>A method for estimating regression coefficients by minimizing the sum of squared residuals. Example: Fitting a line to data points using least squares.</p>"},{"location":"glossary/#legend-in-visualization","title":"Legend (in Visualization)","text":"<p>An element in a chart that explains the meaning of colors, symbols, or line styles. Example: A legend identifying which line corresponds to each city in a temperature chart.</p>"},{"location":"glossary/#library-python","title":"Library (Python)","text":"<p>A collection of modules that provide specific functionality. Example: NumPy is a library for numerical computing.</p>"},{"location":"glossary/#linear-regression","title":"Linear Regression","text":"<p>A method for modeling the relationship between a dependent variable and one or more independent variables using a linear equation. Example: Predicting house prices from square footage.</p>"},{"location":"glossary/#logistic-regression","title":"Logistic Regression","text":"<p>A statistical model used for binary classification that predicts probabilities using a logistic function. Example: Predicting whether a student passes an exam based on study hours.</p>"},{"location":"glossary/#loss-function","title":"Loss Function","text":"<p>A function that measures the error between predicted and actual values. Example: Mean Squared Error is a loss function for regression.</p>"},{"location":"glossary/#machine-learning","title":"Machine Learning","text":"<p>A subset of AI where models improve their performance by learning from data. Example: Training a decision tree to classify loan applications.</p>"},{"location":"glossary/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"<p>A regression metric measuring the average absolute difference between predicted and actual values. Example: An MAE of 5 means predictions are off by 5 units on average.</p>"},{"location":"glossary/#map-function-python","title":"Map Function (Python)","text":"<p>A function that applies another function to each item in an iterable. Example: Using <code>map(str.upper, words)</code> to convert all words to uppercase.</p>"},{"location":"glossary/#matplotlib","title":"Matplotlib","text":"<p>A Python library for creating static, animated, and interactive visualizations. Example: Plotting a line chart of sales over time using Matplotlib.</p>"},{"location":"glossary/#matrix","title":"Matrix","text":"<p>A two-dimensional array of numbers arranged in rows and columns. Example: A 3\u00d73 NumPy array representing a matrix.</p>"},{"location":"glossary/#mean","title":"Mean","text":"<p>The average value of a dataset, found by summing all values and dividing by the count. Example: The mean of 2, 4, and 6 is 4.</p>"},{"location":"glossary/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>A regression metric that measures the average squared difference between predicted and actual values. Example: An MSE of 9 means the average squared error is 9 units.</p>"},{"location":"glossary/#median","title":"Median","text":"<p>The middle value when data is ordered from smallest to largest. Example: The median of 1, 3, 5 is 3.</p>"},{"location":"glossary/#microsimulation","title":"MicroSimulation","text":"<p>An interactive simulation that runs in your browser that helps you learn a data science concept.</p> <p>Example: You can visualize the least squares method of liner regression by changing the slope and intercept of a line through a series of points.</p>"},{"location":"glossary/#min-max-scaling","title":"Min-Max Scaling","text":"<p>A feature scaling technique that rescales values to a given range, usually [0, 1]. Example: Converting exam scores from a 0\u2013100 scale to 0\u20131.</p>"},{"location":"glossary/#missing-data","title":"Missing Data","text":"<p>Values not recorded in a dataset. Example: A blank entry for \"age\" in a survey response.</p>"},{"location":"glossary/#mode","title":"Mode","text":"<p>The most frequently occurring value in a dataset. Example: In 2, 3, 3, 4, 5, the mode is 3.</p>"},{"location":"glossary/#model","title":"Model","text":"<p>A mathematical representation of a process or system built from data to make predictions or decisions. Example: A linear regression equation predicting house prices.</p>"},{"location":"glossary/#model-complexity","title":"Model Complexity","text":"<p>The level of detail and number of parameters in a model. Example: A deep neural network is more complex than a simple linear regression.</p>"},{"location":"glossary/#model-evaluation","title":"Model Evaluation","text":"<p>The process of assessing a model\u2019s performance using metrics and validation techniques. Example: Evaluating a classification model with precision and recall.</p>"},{"location":"glossary/#model-overfitting","title":"Model Overfitting","text":"<p>When a model learns noise in the training data, reducing performance on new data. Example: A decision tree that memorizes training examples but fails on test data.</p>"},{"location":"glossary/#model-underfitting","title":"Model Underfitting","text":"<p>When a model is too simple to capture the underlying structure of the data. Example: Using a straight line to fit highly curved data.</p>"},{"location":"glossary/#module-python","title":"Module (Python)","text":"<p>A file containing Python definitions and functions. Example: Importing the <code>math</code> module to use mathematical functions.</p>"},{"location":"glossary/#multicollinearity","title":"Multicollinearity","text":"<p>A situation where predictor variables in a regression model are highly correlated with each other. Example: Including both temperature in Celsius and Fahrenheit in the same model.</p>"},{"location":"glossary/#multiple-linear-regression","title":"Multiple Linear Regression","text":"<p>A linear regression model with two or more independent variables. Example: Predicting house price using square footage, number of bedrooms, and location.</p>"},{"location":"glossary/#ns","title":"N\u2013S","text":""},{"location":"glossary/#naive-bayes","title":"Naive Bayes","text":"<p>A probabilistic classification algorithm based on Bayes\u2019 theorem with the assumption of feature independence. Example: Classifying text documents into categories using Naive Bayes.</p>"},{"location":"glossary/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>A field of AI focused on enabling computers to understand and process human language. Example: Using NLP to analyze sentiment in customer reviews.</p>"},{"location":"glossary/#neural-network","title":"Neural Network","text":"<p>A computational model inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers. Example: A neural network classifying handwritten digits.</p>"},{"location":"glossary/#noise-data","title":"Noise (Data)","text":"<p>Random variations or irrelevant information in data that obscure patterns. Example: Sensor measurement errors in temperature readings.</p>"},{"location":"glossary/#nominal-data","title":"Nominal Data","text":"<p>Categorical data without an inherent order. Example: Car brands such as Toyota, Ford, and Honda.</p>"},{"location":"glossary/#normal-distribution","title":"Normal Distribution","text":"<p>A symmetric, bell-shaped probability distribution characterized by mean and standard deviation. Example: Adult human heights often approximate a normal distribution.</p>"},{"location":"glossary/#normalization","title":"Normalization","text":"<p>The process of scaling numerical data to a standard range or format. Example: Scaling all features to a 0\u20131 range before training.</p>"},{"location":"glossary/#null-hypothesis","title":"Null Hypothesis","text":"<p>A default statistical assumption that there is no effect or relationship between variables. Example: Testing whether two teaching methods result in the same average test score.</p>"},{"location":"glossary/#numpy","title":"NumPy","text":"<p>A Python library for numerical computing, providing array objects and mathematical functions. Example: Performing matrix multiplication using NumPy arrays.</p>"},{"location":"glossary/#observation","title":"Observation","text":"<p>A single record or row in a dataset. Example: One student's exam scores in a class dataset.</p>"},{"location":"glossary/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>A method of converting categorical variables into binary indicator variables. Example: Encoding \"red\", \"blue\", \"green\" into three separate binary columns.</p>"},{"location":"glossary/#open-source-software","title":"Open Source Software","text":"<p>Software with source code made freely available for modification and distribution. Example: Pandas is open source.</p>"},{"location":"glossary/#optimization","title":"Optimization","text":"<p>The process of adjusting model parameters to minimize or maximize an objective function. Example: Using gradient descent to minimize prediction error.</p>"},{"location":"glossary/#outlier","title":"Outlier","text":"<p>A data point significantly different from other observations. Example: A monthly sales value 10 times higher than all others.</p>"},{"location":"glossary/#overfitting","title":"Overfitting","text":"<p>When a model fits training data too closely, capturing noise rather than general patterns. Example: A polynomial regression curve that perfectly passes through all training points but fails on test data.</p>"},{"location":"glossary/#overfitting-prevention","title":"Overfitting Prevention","text":"<p>Techniques used to reduce overfitting, such as regularization, pruning, and cross-validation. Example: Using Lasso regression to reduce unnecessary model complexity.</p>"},{"location":"glossary/#p-value","title":"p-Value","text":"<p>A statistical measure of the probability of obtaining results at least as extreme as the observed results, assuming the null hypothesis is true. Example: A p-value of 0.03 suggests statistical significance at the 5% level.</p>"},{"location":"glossary/#pandas","title":"Pandas","text":"<p>A Python library for data manipulation and analysis with DataFrame and Series objects. Example: Loading and cleaning a CSV dataset with Pandas.</p>"},{"location":"glossary/#parameter-model","title":"Parameter (Model)","text":"<p>A value learned from data that defines how the model makes predictions. Example: The slope in a linear regression equation.</p>"},{"location":"glossary/#parameter-tuning","title":"Parameter Tuning","text":"<p>Adjusting model hyperparameters to optimize performance. Example: Testing multiple learning rates to find the best one.</p>"},{"location":"glossary/#parsing","title":"Parsing","text":"<p>Breaking down data or text into structured components for processing. Example: Parsing JSON data from an API response.</p>"},{"location":"glossary/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":"<p>A statistic measuring the linear relationship between two variables. Example: A Pearson coefficient of 0.8 indicates a strong positive relationship.</p>"},{"location":"glossary/#performance-metric","title":"Performance Metric","text":"<p>A quantitative measure used to evaluate the effectiveness of a model. Example: Accuracy, precision, and recall are performance metrics for classification.</p>"},{"location":"glossary/#pipeline-machine-learning","title":"Pipeline (Machine Learning)","text":"<p>A sequence of data processing and modeling steps applied consistently to training and test data. Example: A pipeline for scaling features and then applying logistic regression.</p>"},{"location":"glossary/#pivot-table","title":"Pivot Table","text":"<p>A table that summarizes data by grouping and aggregating values. Example: Summarizing sales by region and month using a Pandas pivot table.</p>"},{"location":"glossary/#polynomial-regression","title":"Polynomial Regression","text":"<p>A regression method where the relationship between variables is modeled as an nth-degree polynomial. Example: Fitting a quadratic curve to non-linear data.</p>"},{"location":"glossary/#population","title":"Population","text":"<p>The entire set of items or individuals of interest in a study. Example: All registered voters in a country.</p>"},{"location":"glossary/#precision","title":"Precision","text":"<p>The proportion of positive predictions that are actually correct. Example: A precision of 0.9 means 90% of predicted positives were correct.</p>"},{"location":"glossary/#predictive-modeling","title":"Predictive Modeling","text":"<p>The process of creating models to forecast outcomes based on historical data. Example: Using past sales data to predict next month\u2019s revenue.</p>"},{"location":"glossary/#predictor-variable","title":"Predictor Variable","text":"<p>An independent variable used to forecast the dependent variable. Example: Advertising spend as a predictor of sales.</p>"},{"location":"glossary/#preprocessing","title":"Preprocessing","text":"<p>Preparing raw data for analysis or modeling through cleaning, transformation, and encoding. Example: Removing missing values and scaling features.</p>"},{"location":"glossary/#pre-trained-model","title":"Pre-Trained Model","text":"<p>A model that has been previously trained on a large dataset and can be fine-tuned for a new task. Example: Using a pre-trained image classifier for a smaller dataset.</p>"},{"location":"glossary/#probability","title":"Probability","text":"<p>A measure of the likelihood that an event will occur, ranging from 0 to 1. Example: A probability of 0.75 means a 75% chance of occurrence.</p>"},{"location":"glossary/#probability-distribution","title":"Probability Distribution","text":"<p>A function that describes the likelihood of all possible outcomes. Example: A uniform distribution gives equal probability to all values.</p>"},{"location":"glossary/#pytorch","title":"PyTorch","text":"<p>An open-source machine learning library for deep learning and tensor computation. Example: Building a neural network in PyTorch for image recognition.</p>"},{"location":"glossary/#quantile","title":"Quantile","text":"<p>A cut point dividing data into equal-sized intervals. Example: The median is the 0.5 quantile.</p>"},{"location":"glossary/#quartile","title":"Quartile","text":"<p>One of three points that divide data into four equal groups. Example: The first quartile is the 25th percentile.</p>"},{"location":"glossary/#query-database","title":"Query (Database)","text":"<p>A request for specific information from a database. Example: Using SQL to retrieve rows where sales exceed \\$500.</p>"},{"location":"glossary/#r2-coefficient-of-determination","title":"R\u00b2 (Coefficient of Determination)","text":"<p>A metric indicating the proportion of variance in the dependent variable explained by the model. Example: An R\u00b2 of 0.85 means 85% of the variation is explained by the model.</p>"},{"location":"glossary/#random-forest","title":"Random Forest","text":"<p>An ensemble learning method combining multiple decision trees for better performance. Example: Classifying emails using a random forest model.</p>"},{"location":"glossary/#recall","title":"Recall","text":"<p>The proportion of actual positives correctly identified by the model. Example: A recall of 0.8 means the model identified 80% of actual positives.</p>"},{"location":"glossary/#recursive-feature-elimination-rfe","title":"Recursive Feature Elimination (RFE)","text":"<p>A method for selecting features by recursively removing the least important ones. Example: Using RFE to reduce feature count before training.</p>"},{"location":"glossary/#regression","title":"Regression","text":"<p>A statistical method for modeling relationships between variables to predict numerical outcomes. Example: Predicting car prices from mileage and age.</p>"},{"location":"glossary/#regularization","title":"Regularization","text":"<p>A technique for reducing model complexity to prevent overfitting by adding a penalty term to the loss function. Example: L2 regularization in Ridge regression.</p>"},{"location":"glossary/#residual","title":"Residual","text":"<p>The difference between the observed value and the value predicted by a model. Example: If actual sales are 200 and predicted sales are 180, the residual is 20.</p>"},{"location":"glossary/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>A regression metric measuring the square root of the average squared prediction error. Example: An RMSE of 3 means predictions are off by 3 units on average.</p>"},{"location":"glossary/#sample","title":"Sample","text":"<p>A subset of a population used for analysis. Example: Surveying 500 people from a city of 100,000 residents.</p>"},{"location":"glossary/#sampling","title":"Sampling","text":"<p>Selecting a subset of data from a larger dataset or population. Example: Randomly selecting 100 customers for a feedback survey.</p>"},{"location":"glossary/#sampling-bias","title":"Sampling Bias","text":"<p>A bias introduced when the sample is not representative of the population. Example: Surveying only morning shoppers to estimate daily spending.</p>"},{"location":"glossary/#scatter-plot","title":"Scatter Plot","text":"<p>A graph showing data points on an x-y axis to visualize relationships between variables. Example: Plotting height against weight to explore correlation.</p>"},{"location":"glossary/#scikit-learn","title":"Scikit-learn","text":"<p>A Python library for machine learning, providing tools for modeling, preprocessing, and evaluation. Example: Building a decision tree classifier with Scikit-learn.</p>"},{"location":"glossary/#scripting","title":"Scripting","text":"<p>Writing and executing code to automate tasks. Example: Writing a Python script to download and clean data files.</p>"},{"location":"glossary/#series-pandas","title":"Series (Pandas)","text":"<p>A one-dimensional labeled array in Pandas. Example: A Series of student grades with names as labels.</p>"},{"location":"glossary/#standard-deviation","title":"Standard Deviation","text":"<p>A measure of the dispersion of values around the mean. Example: A standard deviation of 2 means most values fall within \u00b12 units of the mean.</p>"},{"location":"glossary/#standardization","title":"Standardization","text":"<p>Scaling features so they have zero mean and unit variance. Example: Standardizing exam scores before analysis.</p>"},{"location":"glossary/#tz","title":"T\u2013Z","text":""},{"location":"glossary/#target-variable","title":"Target Variable","text":"<p>The variable a model aims to predict. Example: Predicting \"exam score\" as the target variable using study hours and sleep.</p>"},{"location":"glossary/#t-statistic","title":"t-Statistic","text":"<p>A value used in hypothesis testing to determine if sample means differ significantly. Example: Using a t-statistic to compare average weights of two groups.</p>"},{"location":"glossary/#tensor","title":"Tensor","text":"<p>A multi-dimensional array used in deep learning frameworks like PyTorch. Example: Representing a batch of images as a 4D tensor.</p>"},{"location":"glossary/#test-data","title":"Test Data","text":"<p>A dataset used to evaluate model performance after training. Example: Testing a spam filter on previously unseen emails.</p>"},{"location":"glossary/#test-train-split","title":"Test-Train Split","text":"<p>The process of dividing data into separate training and testing subsets. Example: Using 80% of data for training and 20% for testing.</p>"},{"location":"glossary/#text-mining","title":"Text Mining","text":"<p>Extracting useful information from text data through statistical and computational methods. Example: Identifying frequent keywords in news articles.</p>"},{"location":"glossary/#time-series","title":"Time Series","text":"<p>A sequence of data points collected over time intervals. Example: Daily stock prices over a year.</p>"},{"location":"glossary/#tokenization","title":"Tokenization","text":"<p>Breaking text into smaller units such as words or phrases. Example: Splitting \"Data Science is fun\" into [\"Data\", \"Science\", \"is\", \"fun\"].</p>"},{"location":"glossary/#training-data","title":"Training Data","text":"<p>The dataset used to fit or train a model. Example: Using past sales data to train a revenue prediction model.</p>"},{"location":"glossary/#training-epoch","title":"Training Epoch","text":"<p>One complete pass of the training dataset through the model during learning. Example: Training a neural network for 10 epochs.</p>"},{"location":"glossary/#transformation","title":"Transformation","text":"<p>Applying a mathematical function to change data format or scale. Example: Applying a logarithmic transformation to skewed data.</p>"},{"location":"glossary/#tree-based-models","title":"Tree-Based Models","text":"<p>Models that use decision trees or ensembles of trees for prediction. Example: Random forest and gradient boosting are tree-based models.</p>"},{"location":"glossary/#trend-time-series","title":"Trend (Time Series)","text":"<p>A long-term movement or pattern in time series data. Example: Increasing monthly sales over several years.</p>"},{"location":"glossary/#true-negative","title":"True Negative","text":"<p>A case where a model correctly predicts a negative outcome. Example: A medical test correctly identifying a healthy patient.</p>"},{"location":"glossary/#true-positive","title":"True Positive","text":"<p>A case where a model correctly predicts a positive outcome. Example: A cancer detection test correctly identifying a patient with cancer.</p>"},{"location":"glossary/#t-test","title":"t-Test","text":"<p>A statistical test comparing the means of two groups. Example: Testing whether two different diets result in different average weight loss.</p>"},{"location":"glossary/#underfitting","title":"Underfitting","text":"<p>When a model is too simple to capture the underlying data patterns. Example: Using a straight line to predict a highly curved dataset.</p>"},{"location":"glossary/#univariate-analysis","title":"Univariate Analysis","text":"<p>Analysis of a single variable to summarize and find patterns. Example: Plotting a histogram of exam scores.</p>"},{"location":"glossary/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>A machine learning approach where models find structure in data without labeled outcomes. Example: Using k-means clustering to group customers.</p>"},{"location":"glossary/#validation-data","title":"Validation Data","text":"<p>A dataset used during training to tune model hyperparameters. Example: Using validation data to choose the number of decision tree splits.</p>"},{"location":"glossary/#value-counts-pandas","title":"Value Counts (Pandas)","text":"<p>A method to count occurrences of each unique value in a Pandas Series. Example: Counting how many times each grade appears in a class.</p>"},{"location":"glossary/#variable","title":"Variable","text":"<p>A storage location identified by a name that holds a value. Example: In Python, <code>age = 20</code> assigns 20 to the variable <code>age</code>.</p>"},{"location":"glossary/#variance","title":"Variance","text":"<p>A measure of the spread of data points around the mean. Example: A variance of 4 means average squared deviations are 4 units\u00b2.</p>"},{"location":"glossary/#vectorization","title":"Vectorization","text":"<p>Performing operations on entire arrays rather than using loops. Example: Adding two NumPy arrays without explicit loops.</p>"},{"location":"glossary/#visualization","title":"Visualization","text":"<p>The graphical representation of data to reveal patterns and insights. Example: Using Matplotlib to plot a scatter diagram of height vs. weight.</p>"},{"location":"glossary/#visual-studio-code","title":"Visual Studio Code","text":"<p>A free integrated development environment that is used widely by programmers and data science science professionals.</p> <p>Visual Studio Code (VS-Code) also has many extensions for editing different languages and data types.</p> <p>Other examples of AI-based IDEs include Cursor and Windsurf.  The Cline extension is also useful.</p> <p>Example: We use VS-Code to edit, run, debug and test our Python code.</p>"},{"location":"glossary/#weighted-average","title":"Weighted Average","text":"<p>An average where each value contributes according to its assigned weight. Example: Calculating a final grade with different weights for assignments and exams.</p>"},{"location":"glossary/#weight-model","title":"Weight (Model)","text":"<p>A parameter that determines the influence of a feature in a model\u2019s prediction. Example: A weight of 0.5 for \"hours studied\" in a regression model.</p>"},{"location":"glossary/#white-noise-time-series","title":"White Noise (Time Series)","text":"<p>A random sequence of values with no predictable pattern. Example: Fluctuations in daily stock prices without a trend.</p>"},{"location":"glossary/#wilcoxon-test","title":"Wilcoxon Test","text":"<p>A non-parametric test for comparing paired samples. Example: Comparing pre- and post-training test scores without assuming normality.</p>"},{"location":"glossary/#yaml-yaml-aint-markup-language","title":"YAML (YAML Ain\u2019t Markup Language)","text":"<p>A human-readable data serialization format. Example: Storing configuration settings in a YAML file.</p>"},{"location":"glossary/#z-score","title":"z-Score","text":"<p>A standardized score indicating how many standard deviations a value is from the mean. Example: A z-score of 2 means the value is two standard deviations above the mean.</p>"},{"location":"glossary/#zero-inflated-data","title":"Zero-Inflated Data","text":"<p>Data with an excess number of zero values compared to a standard distribution. Example: A dataset of monthly purchases where many customers have zero purchases.</p>"},{"location":"glossary/#zero-shot-learning","title":"Zero-Shot Learning","text":"<p>A machine learning method where a model makes predictions for classes it was not explicitly trained on. Example: A text classifier identifying a new topic without prior examples.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"chapters/","title":"Course Chapters - Table of Contents","text":"<p>This course provides a comprehensive introduction to data science using Python, progressing from foundational concepts to advanced machine learning techniques. Each chapter includes interactive MicroSims to reinforce learning through hands-on experience.</p>"},{"location":"chapters/#course-structure-overview","title":"Course Structure Overview","text":"<p>Duration: 10 weeks Target Audience: Advanced high school students and college freshmen Prerequisites: Basic algebra and introductory programming experience</p>"},{"location":"chapters/#chapter-progression","title":"Chapter Progression","text":""},{"location":"chapters/#foundation-phase-weeks-1-3","title":"Foundation Phase (Weeks 1-3)","text":""},{"location":"chapters/#chapter-0-setup","title":"Chapter 0: Setup","text":"<ul> <li>Python environment and Jupyter notebooks setup</li> <li>Conda virtual environment configuration</li> <li>Required package installation</li> <li>Development environment best practices</li> </ul>"},{"location":"chapters/#chapter-1-foundations-of-data-science","title":"Chapter 1: Foundations of Data Science","text":"<ul> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> <li>Understanding the data science workflow</li> <li>Data science roles and career paths</li> <li>Ethics and best practices in data science</li> </ul>"},{"location":"chapters/#chapter-2-data-exploration-and-visualization","title":"Chapter 2: Data Exploration and Visualization","text":"<ul> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> <li>Data profiling and quality assessment</li> <li>Handling missing values and outliers</li> <li>Exploratory data analysis techniques</li> </ul>"},{"location":"chapters/#chapter-3-data-visualization-techniques","title":"Chapter 3: Data Visualization Techniques","text":"<ul> <li>Principles of effective data visualization</li> <li>Matplotlib fundamentals and customization</li> <li>Plotly for interactive visualizations</li> <li>Statistical plots and distributions</li> <li>Time series visualization</li> <li>Multi-dimensional data representation</li> <li>MicroSim: Visualization parameter explorer</li> </ul>"},{"location":"chapters/#statistical-foundation-phase-weeks-4-5","title":"Statistical Foundation Phase (Weeks 4-5)","text":""},{"location":"chapters/#chapter-4-statistical-foundations","title":"Chapter 4: Statistical Foundations","text":"<ul> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> <li>Central limit theorem and sampling distributions</li> <li>Hypothesis testing fundamentals</li> <li>Correlation vs. causation</li> </ul>"},{"location":"chapters/#chapter-5-simple-linear-regression","title":"Chapter 5: Simple Linear Regression","text":"<ul> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> <li>Assumptions of linear regression</li> <li>Residual analysis and diagnostics</li> <li>Making predictions with linear models</li> </ul>"},{"location":"chapters/#model-development-phase-weeks-6-8","title":"Model Development Phase (Weeks 6-8)","text":""},{"location":"chapters/#chapter-6-model-evaluation-and-validation","title":"Chapter 6: Model Evaluation and Validation","text":"<ul> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> <li>Bias-variance trade-off</li> <li>Model selection criteria</li> <li>Performance metrics for different problem types</li> </ul>"},{"location":"chapters/#chapter-7-multiple-linear-regression","title":"Chapter 7: Multiple Linear Regression","text":"<ul> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> <li>Interaction effects and polynomial terms</li> <li>Multicollinearity detection and treatment</li> <li>Model interpretation in multiple dimensions</li> </ul>"},{"location":"chapters/#chapter-8-introduction-to-numpy-and-advanced-computation","title":"Chapter 8: Introduction to NumPy and Advanced Computation","text":"<ul> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> <li>Broadcasting and array manipulation</li> <li>Mathematical functions and statistics</li> <li>Integration with pandas and matplotlib</li> </ul>"},{"location":"chapters/#advanced-modeling-phase-weeks-9-10","title":"Advanced Modeling Phase (Weeks 9-10)","text":""},{"location":"chapters/#chapter-9-non-linear-models-and-feature-engineering","title":"Chapter 9: Non-linear Models and Feature Engineering","text":"<ul> <li>Polynomial regression and feature transformation</li> <li>Understanding non-linear relationships</li> <li>MicroSim: Polynomial degree explorer</li> <li>Feature engineering techniques</li> <li>Basis functions and kernel methods</li> <li>Model complexity and interpretation trade-offs</li> <li>When to use non-linear approaches</li> </ul>"},{"location":"chapters/#chapter-10-regularization-techniques","title":"Chapter 10: Regularization Techniques","text":"<ul> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> <li>Cross-validation for hyperparameter tuning</li> <li>Elastic Net and other regularization methods</li> <li>Feature selection through regularization</li> <li>Preventing overfitting in complex models</li> </ul>"},{"location":"chapters/#machine-learning-phase-advanced-topics","title":"Machine Learning Phase (Advanced Topics)","text":""},{"location":"chapters/#chapter-11-introduction-to-machine-learning","title":"Chapter 11: Introduction to Machine Learning","text":"<ul> <li>Supervised vs. unsupervised learning</li> <li>Classification and regression problems</li> <li>Decision trees and ensemble methods</li> <li>MicroSim: Algorithm comparison explorer</li> <li>Feature importance and selection</li> <li>Model interpretability techniques</li> <li>Introduction to scikit-learn</li> </ul>"},{"location":"chapters/#chapter-12-neural-networks-and-deep-learning","title":"Chapter 12: Neural Networks and Deep Learning","text":"<ul> <li>Neural networks and deep learning concepts</li> <li>Perceptrons and multi-layer networks</li> <li>Activation functions and backpropagation</li> <li>MicroSim: Neural network playground</li> <li>Training neural networks</li> <li>Common architectures and applications</li> <li>When to use neural networks vs. traditional methods</li> </ul>"},{"location":"chapters/#chapter-13-introduction-to-machine-learning-with-pytorch","title":"Chapter 13: Introduction to Machine Learning with PyTorch","text":"<ul> <li>Building simple networks with PyTorch</li> <li>Tensors and automatic differentiation</li> <li>Creating and training models</li> <li>MicroSim: PyTorch model builder</li> <li>Comparing traditional and deep learning approaches</li> <li>GPU acceleration and optimization</li> <li>Model saving and deployment</li> </ul>"},{"location":"chapters/#chapter-14-advanced-model-evaluation","title":"Chapter 14: Advanced Model Evaluation","text":"<ul> <li>Comprehensive performance metrics</li> <li>ROC curves and AUC analysis</li> <li>Confusion matrices and classification reports</li> <li>MicroSim: Metric comparison explorer</li> <li>Statistical significance testing</li> <li>Model comparison techniques</li> <li>Reporting and communicating results</li> </ul>"},{"location":"chapters/#chapter-15-capstone-project-and-model-deployment","title":"Chapter 15: Capstone Project and Model Deployment","text":"<ul> <li>End-to-end data science project planning</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> <li>Model deployment strategies</li> <li>Documentation and reproducibility</li> <li>Presenting data science findings</li> </ul>"},{"location":"chapters/#special-topics","title":"Special Topics","text":""},{"location":"chapters/#matplotlib-vs-plotly-comparison","title":"Matplotlib vs Plotly Comparison","text":"<p>Detailed comparison of visualization libraries for AI-generated plots and animations, including pros and cons for different use cases.</p>"},{"location":"chapters/#learning-methodology","title":"Learning Methodology","text":"<p>Each chapter incorporates: - Interactive MicroSims for hands-on parameter exploration - Real-world datasets and practical applications - Progressive complexity building from simple to advanced concepts - Explainable AI focus emphasizing model interpretability - Code examples with complete implementations</p>"},{"location":"chapters/#course-philosophy","title":"Course Philosophy","text":"<p>This course emphasizes the balance between model explainability and predictive accuracy, guiding students to identify the simplest effective solutions to data-driven problems. The integration of interactive simulations ensures abstract mathematical concepts become concrete and intuitive.</p>"},{"location":"chapters/matplotlib-vs-plotly/","title":"Comparison of Matplotlib and Plotly for GenAI-Created Plots and Animations","text":""},{"location":"chapters/matplotlib-vs-plotly/#prompt","title":"Prompt","text":"<pre><code>Compare the Python libraries matplotlib with plotly.\nWhat are the pros and cons for GenAI created plots and animations?\n\nWhen choosing between **Matplotlib** and **Plotly** for creating\nplots and animations with generative AI, here are their pros and cons?\n</code></pre>"},{"location":"chapters/matplotlib-vs-plotly/#chatgpt-response","title":"ChatGPT Response","text":""},{"location":"chapters/matplotlib-vs-plotly/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is a versatile and widely-used Python library for creating static, animated, and interactive visualizations.</p>"},{"location":"chapters/matplotlib-vs-plotly/#pros","title":"Pros","text":"<ol> <li>Simplicity and Customization:</li> <li>Great for generating static, publication-quality plots with fine-grained control.</li> <li> <p>Straightforward API for generating standard visualizations like line plots, bar charts, and scatter plots.</p> </li> <li> <p>Broad Ecosystem Support:</p> </li> <li>Integrates well with NumPy, pandas, and other scientific libraries.</li> <li> <p>Supported by many GenAI frameworks for generating data visualizations.</p> </li> <li> <p>Animations:</p> </li> <li>Provides tools for creating animations (e.g., <code>FuncAnimation</code>).</li> <li> <p>Ideal for embedding animations in applications or saving them as GIFs or videos.</p> </li> <li> <p>Customization:</p> </li> <li>Allows deep customization of every aspect of the plot.</li> <li> <p>Perfect for generating AI-tailored visualizations that require specific formats or aesthetics.</p> </li> <li> <p>Lightweight and Efficient:</p> </li> <li>Better for lightweight tasks where interactivity is not required.</li> </ol>"},{"location":"chapters/matplotlib-vs-plotly/#cons","title":"Cons","text":"<ol> <li>Steep Learning Curve:</li> <li>Customizing plots can become verbose and complicated, especially for advanced layouts.</li> <li> <p>Syntax can feel tedious for beginners or for generating multiple visualizations programmatically.</p> </li> <li> <p>Limited Interactivity:</p> </li> <li>Interactivity is minimal compared to modern libraries like Plotly.</li> <li> <p>Interactive plots require third-party tools (e.g., <code>mpld3</code>) and are often less intuitive.</p> </li> <li> <p>Outdated Animations:</p> </li> <li>Animations are not as smooth or interactive as those created by Plotly.</li> </ol>"},{"location":"chapters/matplotlib-vs-plotly/#plotly","title":"Plotly","text":"<p>Plotly is a modern library for creating highly interactive and dynamic visualizations.</p>"},{"location":"chapters/matplotlib-vs-plotly/#pros_1","title":"Pros","text":"<ol> <li>Interactivity:</li> <li>All visualizations are interactive by default (e.g., zooming, panning, tooltips).</li> <li> <p>Excellent for dashboards, presentations, and GenAI-generated insights.</p> </li> <li> <p>Ease of Use:</p> </li> <li>Simple syntax for creating complex plots, suitable for AI-generated visualizations.</li> <li> <p>Works seamlessly with data from pandas, NumPy, and other libraries.</p> </li> <li> <p>Browser-Based Rendering:</p> </li> <li>Plots render in web browsers using JavaScript, making them highly portable and shareable.</li> <li> <p>Supports embedding in Jupyter Notebooks, web apps, and standalone HTML files.</p> </li> <li> <p>Built-in Animations:</p> </li> <li>Easy to create smooth, interactive animations with native support.</li> <li> <p>Ideal for demonstrating dynamic AI models or processes.</p> </li> <li> <p>Wide Range of Charts:</p> </li> <li>Includes advanced visualizations like 3D plots, heatmaps, choropleth maps, and Sankey diagrams.</li> <li>Perfect for showing complex GenAI outputs in an intuitive way.</li> </ol>"},{"location":"chapters/matplotlib-vs-plotly/#cons_1","title":"Cons","text":"<ol> <li>Heavier and Slower:</li> <li>More resource-intensive due to JavaScript-based interactivity.</li> <li> <p>Can slow down with large datasets or complex visualizations.</p> </li> <li> <p>Limited Fine-Grained Control:</p> </li> <li>While customizable, it lacks the deep control over plot details that Matplotlib offers.</li> <li> <p>Certain niche visualizations might be harder to implement.</p> </li> <li> <p>Dependency on JavaScript:</p> </li> <li> <p>Requires JavaScript runtime for rendering, which might not be ideal for all environments.</p> </li> <li> <p>Animation Complexity:</p> </li> <li>While animations are smoother, creating complex animations programmatically can be less intuitive than with Matplotlib.</li> </ol>"},{"location":"chapters/matplotlib-vs-plotly/#which-to-choose-for-genai","title":"Which to Choose for GenAI?","text":"<ol> <li>Static Plots:</li> <li>Use Matplotlib for quick, customizable, high-quality static plots.</li> <li> <p>Example: Generating high-resolution images for reports or papers.</p> </li> <li> <p>Interactive and Animated Plots:</p> </li> <li>Use Plotly for dynamic and interactive plots.</li> <li> <p>Example: Interactive GenAI dashboards or showcasing dynamic AI model predictions.</p> </li> <li> <p>Collaborative and Web-Based Use:</p> </li> <li> <p>Use Plotly if your plots need to be shared on the web or included in interactive applications.</p> </li> <li> <p>Complex Animations:</p> </li> <li>Use Matplotlib for complex, programmatically controlled animations.</li> <li>Use Plotly for simple, interactive animations with smooth transitions.</li> </ol>"},{"location":"chapters/matplotlib-vs-plotly/#conclusion","title":"Conclusion","text":"<p>Both libraries are valuable for different contexts: - Matplotlib shines in fine-grained control and efficient static or programmatic visualizations. - Plotly is the go-to for interactivity, browser-based plots, and user-friendly animations.</p> <p>For GenAI, combining both libraries strategically (e.g., static plots with Matplotlib and interactive plots with Plotly) can provide the best of both worlds.</p>"},{"location":"chapters/00-setup/","title":"Setting up Python, Visual Studio Code and Jupyter Notebooks","text":"<p>In this course will will use, Python, Visual Studio Code and Jupyter Notebooks to do our classroom work. We selected these tools because they are the most popular development environments in data science today.  You are free to use other tools in this course, however you may need to use generative AI to translate our code into other systems. In addition, we strongly recommend each student create their own personal GitHub repository to store their work.  Because GitHub is backed up the students only need to check their code into GitHub using either a command line or Source Control section of Visual Studio Code.</p> <p>We will be using the following tools for our desktop setup:</p> <ol> <li>Visual Studio Code - integrated development environment</li> <li>Conda - virtual environment</li> <li>Pip - Python installer package</li> </ol>"},{"location":"chapters/00-setup/#conda","title":"Conda","text":"<p>Create a \"Data Science\" conda environment.</p> <pre><code>conda create -n \"ds\" python=3\nconda activate ds\n</code></pre>"},{"location":"chapters/00-setup/#installing-jupyter-notebooks","title":"Installing Jupyter Notebooks","text":"<p>Jupyter Notebooks are essential for this course as they provide an interactive environment for data science work. There are several ways to install and use Jupyter Notebooks:</p>"},{"location":"chapters/00-setup/#method-1-install-via-conda-recommended","title":"Method 1: Install via Conda (Recommended)","text":"<p>After activating your conda environment, install Jupyter:</p> <pre><code>conda activate ds\nconda install jupyter\n</code></pre>"},{"location":"chapters/00-setup/#method-2-install-via-pip","title":"Method 2: Install via Pip","text":"<p>Alternatively, you can install using pip:</p> <pre><code>pip install jupyter\n</code></pre>"},{"location":"chapters/00-setup/#method-3-install-jupyterlab-enhanced-interface","title":"Method 3: Install JupyterLab (Enhanced Interface)","text":"<p>For a more modern interface, you can install JupyterLab:</p> <pre><code>pip install jupyterlab\n</code></pre>"},{"location":"chapters/00-setup/#running-jupyter-notebooks","title":"Running Jupyter Notebooks","text":""},{"location":"chapters/00-setup/#starting-jupyter-notebook-server","title":"Starting Jupyter Notebook Server","text":"<p>To start the Jupyter Notebook server:</p> <pre><code># Activate your environment first\nconda activate ds\n\n# Start Jupyter Notebook\njupyter notebook\n</code></pre> <p>This will: 1. Start a local server (usually at <code>http://localhost:8888</code>) 2. Automatically open your web browser to the Jupyter interface 3. Display your file system where you can create new notebooks or open existing ones</p>"},{"location":"chapters/00-setup/#starting-jupyterlab","title":"Starting JupyterLab","text":"<p>If you installed JupyterLab:</p> <pre><code>jupyter lab\n</code></pre>"},{"location":"chapters/00-setup/#using-jupyter-notebooks-in-visual-studio-code","title":"Using Jupyter Notebooks in Visual Studio Code","text":"<p>Visual Studio Code provides excellent built-in support for Jupyter Notebooks, offering a seamless development experience.</p>"},{"location":"chapters/00-setup/#installing-the-jupyter-extension","title":"Installing the Jupyter Extension","text":"<ol> <li>Open Visual Studio Code</li> <li>Go to the Extensions view (<code>Ctrl+Shift+X</code> or <code>Cmd+Shift+X</code>)</li> <li>Search for \"Jupyter\" </li> <li>Install the official \"Jupyter\" extension by Microsoft</li> </ol>"},{"location":"chapters/00-setup/#working-with-notebooks-in-vs-code","title":"Working with Notebooks in VS Code","text":"<p>Creating a New Notebook: 1. Open VS Code 2. Create a new file with <code>.ipynb</code> extension (e.g., <code>my_analysis.ipynb</code>) 3. VS Code will automatically recognize it as a Jupyter notebook</p> <p>Selecting Python Interpreter: 1. Open a notebook file 2. Click on \"Select Kernel\" in the top-right corner 3. Choose your conda environment (<code>ds</code>) as the Python interpreter</p> <p>Running Code Cells: - Run Single Cell: Click the play button next to a cell or press <code>Shift+Enter</code> - Run All Cells: Use <code>Ctrl+Shift+P</code> (or <code>Cmd+Shift+P</code>) and search \"Jupyter: Run All Cells\" - Add New Cell: Click the <code>+ Code</code> or <code>+ Markdown</code> buttons between cells</p>"},{"location":"chapters/00-setup/#advantages-of-using-jupyter-in-vs-code","title":"Advantages of Using Jupyter in VS Code","text":"<ul> <li>Integrated Development: Code editing, debugging, and notebook execution in one interface</li> <li>Git Integration: Easy version control for notebook files</li> <li>IntelliSense: Code completion and syntax highlighting</li> <li>Variable Explorer: View variables and data structures in real-time</li> <li>Debugging Support: Set breakpoints and debug notebook code</li> <li>Extensions: Access to VS Code's rich extension ecosystem</li> </ul>"},{"location":"chapters/00-setup/#installing-essential-data-science-packages","title":"Installing Essential Data Science Packages","text":"<p>Install the core packages you'll need for this course:</p> <pre><code># Activate your environment\nconda activate ds\n\n# Install essential data science packages\npip install pandas numpy matplotlib seaborn scikit-learn jupyter\n\n# Optional: Install additional useful packages\npip install plotly ipywidgets\n</code></pre>"},{"location":"chapters/00-setup/#transcript","title":"Transcript","text":"<pre><code>Retrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 23.5.2\n  latest version: 24.9.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=24.9.2\n\n\n## Package Plan ##\n\n  environment location: /Users/danmccreary/miniconda3/envs/ds\n\n  added / updated specs:\n    - python=3\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2024.9.24  |       hca03da5_0         131 KB\n    expat-2.6.3                |       h313beb8_0         154 KB\n    libmpdec-4.0.0             |       h80987f9_0          69 KB\n    openssl-3.0.15             |       h80987f9_0         4.3 MB\n    pip-24.2                   |  py313hca03da5_0         2.4 MB\n    python-3.13.0              |h4862095_100_cp313        13.7 MB\n    python_abi-3.13            |          0_cp313           7 KB\n    setuptools-75.1.0          |  py313hca03da5_0         1.9 MB\n    tzdata-2024b               |       h04d1e81_0         115 KB\n    wheel-0.44.0               |  py313hca03da5_0         139 KB\n    ------------------------------------------------------------\n                                           Total:        23.0 MB\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              pkgs/main/osx-arm64::bzip2-1.0.8-h80987f9_6 \n  ca-certificates    pkgs/main/osx-arm64::ca-certificates-2024.9.24-hca03da5_0 \n  expat              pkgs/main/osx-arm64::expat-2.6.3-h313beb8_0 \n  libcxx             pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 \n  libffi             pkgs/main/osx-arm64::libffi-3.4.4-hca03da5_1 \n  libmpdec           pkgs/main/osx-arm64::libmpdec-4.0.0-h80987f9_0 \n  ncurses            pkgs/main/osx-arm64::ncurses-6.4-h313beb8_0 \n  openssl            pkgs/main/osx-arm64::openssl-3.0.15-h80987f9_0 \n  pip                pkgs/main/osx-arm64::pip-24.2-py313hca03da5_0 \n  python             pkgs/main/osx-arm64::python-3.13.0-h4862095_100_cp313 \n  python_abi         pkgs/main/osx-arm64::python_abi-3.13-0_cp313 \n  readline           pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 \n  setuptools         pkgs/main/osx-arm64::setuptools-75.1.0-py313hca03da5_0 \n  sqlite             pkgs/main/osx-arm64::sqlite-3.45.3-h80987f9_0 \n  tk                 pkgs/main/osx-arm64::tk-8.6.14-h6ba3021_0 \n  tzdata             pkgs/main/noarch::tzdata-2024b-h04d1e81_0 \n  wheel              pkgs/main/osx-arm64::wheel-0.44.0-py313hca03da5_0 \n  xz                 pkgs/main/osx-arm64::xz-5.4.6-h80987f9_1 \n  zlib               pkgs/main/osx-arm64::zlib-1.2.13-h18a0788_1 \n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\n\nPreparing transaction: done                                                                          \nVerifying transaction: done                                                                          \nExecuting transaction: done                                                                        \n#                                                                                                    \n# To activate this environment, use                                                                  \n#                                                                                                    \n#     $ conda activate ds                                                                            \n#                                                                                                    \n# To deactivate an active environment, use                                                           \n#\n#     $ conda deactivate\n</code></pre>"},{"location":"chapters/00-setup/#pip-install-transcript","title":"Pip Install Transcript","text":"<p>(ds) src/line-plot $ pip install matplotlib</p> <pre><code>Collecting matplotlib\n  Downloading matplotlib-3.9.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\nCollecting contourpy&gt;=1.0.1 (from matplotlib)\n  Downloading contourpy-1.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.4 kB)\nCollecting cycler&gt;=0.10 (from matplotlib)\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools&gt;=4.22.0 (from matplotlib)\n  Downloading fonttools-4.55.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (164 kB)\nCollecting kiwisolver&gt;=1.3.1 (from matplotlib)\n  Downloading kiwisolver-1.4.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\nCollecting numpy&gt;=1.23 (from matplotlib)\n  Downloading numpy-2.1.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\nCollecting packaging&gt;=20.0 (from matplotlib)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pillow&gt;=8 (from matplotlib)\n  Downloading pillow-11.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.1 kB)\nCollecting pyparsing&gt;=2.3.1 (from matplotlib)\n  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\nCollecting python-dateutil&gt;=2.7 (from matplotlib)\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting six&gt;=1.5 (from python-dateutil&gt;=2.7-&gt;matplotlib)\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nDownloading matplotlib-3.9.2-cp313-cp313-macosx_11_0_arm64.whl (7.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.8/7.8 MB 35.2 MB/s eta 0:00:00\nDownloading contourpy-1.3.1-cp313-cp313-macosx_11_0_arm64.whl (255 kB)\nUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\nDownloading fonttools-4.55.0-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.8/2.8 MB 19.8 MB/s eta 0:00:00\nDownloading kiwisolver-1.4.7-cp313-cp313-macosx_11_0_arm64.whl (63 kB)\nDownloading numpy-2.1.3-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.1/5.1 MB 33.8 MB/s eta 0:00:00\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\nDownloading pillow-11.0.0-cp313-cp313-macosx_11_0_arm64.whl (3.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.0/3.0 MB 28.6 MB/s eta 0:00:00\nDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\nUsing cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, python-dateutil, contourpy, matplotlib\nSuccessfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.0 kiwisolver-1.4.7 matplotlib-3.9.2 numpy-2.1.3 packaging-24.2 pillow-11.0.0 pyparsing-3.2.0 python-dateutil-2.9.0.post0 six-1.16.0\n(ds) src/line-plot $ python line-plot.py   \n2024-11-15 07:14:21.701 python[53332:2841291] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-11-15 07:14:21.701 python[53332:2841291] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n</code></pre>"},{"location":"chapters/01-foundations/","title":"Foundations of Data Science","text":"<ol> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> <li>Understanding the data science workflow</li> <li>Data science roles and career paths</li> <li>Ethics and best practices in data science</li> </ol>"},{"location":"chapters/02-data-exploration/","title":"Data Exploration and Visualization","text":"<ol> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> <li>Data profiling and quality assessment</li> <li>Handling missing values and outliers</li> <li>Exploratory data analysis techniques</li> </ol>"},{"location":"chapters/03-data-visualization/","title":"Data Visualization Techniques","text":"<ol> <li>Principles of effective data visualization</li> <li>Matplotlib fundamentals and customization</li> <li>Plotly for interactive visualizations</li> <li>Statistical plots and distributions</li> <li>Time series visualization</li> <li>Multi-dimensional data representation</li> <li>MicroSim: Visualization parameter explorer</li> </ol>"},{"location":"chapters/04-statistics/","title":"Statistical Foundations","text":"<ol> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> <li>Central limit theorem and sampling distributions</li> <li>Hypothesis testing fundamentals</li> <li>Correlation vs. causation</li> </ol>"},{"location":"chapters/05-linear-regression/","title":"Simple Linear Regression","text":"<ol> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> <li>Assumptions of linear regression</li> <li>Residual analysis and diagnostics</li> <li>Making predictions with linear models</li> </ol>"},{"location":"chapters/06-models/","title":"Model Evaluation and Validation","text":"<ol> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> <li>Bias-variance trade-off</li> <li>Model selection criteria</li> <li>Performance metrics for different problem types</li> </ol>"},{"location":"chapters/07-multiple-linear-regression/","title":"Multiple Linear Regression","text":"<ol> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> <li>Interaction effects and polynomial terms</li> <li>Multicollinearity detection and treatment</li> <li>Model interpretation in multiple dimensions</li> </ol>"},{"location":"chapters/08-numpy/","title":"Introduction to NumPy and Advanced Computation","text":"<ol> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> <li>Broadcasting and array manipulation</li> <li>Mathematical functions and statistics</li> <li>Integration with pandas and matplotlib</li> </ol>"},{"location":"chapters/09-nonlinear-models/","title":"Non-linear Models and Feature Engineering","text":"<ol> <li>Polynomial regression and feature transformation</li> <li>Understanding non-linear relationships</li> <li>MicroSim: Polynomial degree explorer</li> <li>Feature engineering techniques</li> <li>Basis functions and kernel methods</li> <li>Model complexity and interpretation trade-offs</li> <li>When to use non-linear approaches</li> </ol>"},{"location":"chapters/10-regularization/","title":"Regularization Techniques","text":"<ol> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> <li>Cross-validation for hyperparameter tuning</li> <li>Elastic Net and other regularization methods</li> <li>Feature selection through regularization</li> <li>Preventing overfitting in complex models</li> </ol>"},{"location":"chapters/11-machine-learning/","title":"Introduction to Machine Learning","text":"<ol> <li>Supervised vs. unsupervised learning</li> <li>Classification and regression problems</li> <li>Decision trees and ensemble methods</li> <li>MicroSim: Algorithm comparison explorer</li> <li>Feature importance and selection</li> <li>Model interpretability techniques</li> <li>Introduction to scikit-learn</li> </ol>"},{"location":"chapters/12-neural-networks/","title":"Neural Networks and Deep Learning","text":"<ol> <li>Neural networks and deep learning concepts</li> <li>Perceptrons and multi-layer networks</li> <li>Activation functions and backpropagation</li> <li>MicroSim: Neural network playground</li> <li>Training neural networks</li> <li>Common architectures and applications</li> <li>When to use neural networks vs. traditional methods</li> </ol>"},{"location":"chapters/13-pytorch/","title":"Introduction to Machine Learning with PyTorch","text":"<ol> <li>Building simple networks with PyTorch</li> <li>Tensors and automatic differentiation</li> <li>Creating and training models</li> <li>MicroSim: PyTorch model builder</li> <li>Comparing traditional and deep learning approaches</li> <li>GPU acceleration and optimization</li> <li>Model saving and deployment</li> </ol>"},{"location":"chapters/14-model-evaluation/","title":"Advanced Model Evaluation","text":"<ol> <li>Comprehensive performance metrics</li> <li>ROC curves and AUC analysis</li> <li>Confusion matrices and classification reports</li> <li>MicroSim: Metric comparison explorer</li> <li>Statistical significance testing</li> <li>Model comparison techniques</li> <li>Reporting and communicating results</li> </ol>"},{"location":"chapters/15-projects/","title":"Capstone Project and Model Deployment","text":"<ol> <li>End-to-end data science project planning</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> <li>Model deployment strategies</li> <li>Documentation and reproducibility</li> <li>Presenting data science findings</li> </ol>"},{"location":"prompts/concept-dependencies/","title":"Concept Dependencies","text":""},{"location":"prompts/concept-dependencies/#prompt","title":"Prompt","text":"<pre><code>That was perfect.  Thank you very much.\nThe next step is to create a dependency graph\nof all the 150 concepts on the list. \nFor each concept, think about other concepts\nthat this concept depends upon for understanding.\n\nFor each of the 150 concepts return a single row in CSV format.\n\nColumn 1 is the ConceptID (the integer 1 to 150)\nColumn 2 is the ConceptLabel\nColumn 3 is the Dependencies in the form of pipe-delimited list of ConceptIDs\n\nMake sure that Foundation Concepts (prerequisites) like knowledge of Python don't have any dependencies.\nMake sure that every concept except Foundation Concepts have at least one dependency.\n</code></pre>"},{"location":"prompts/concept-enumeration/","title":"Concept Enumeration","text":""},{"location":"prompts/concept-enumeration/#prompt","title":"Prompt","text":"<p>```linenums=\"0\" I would like to develop a new course called  \"Introduction to Data Science with Python using AI\". </p> <p>I would like you to generate a list of all the core concepts that we would cover in this course and the dependency relationships between these concepts so we could order them and create an learning graph. This course would assume that students have basic Python programming experience.  This course would be a class that covers the following topics: </p> <ol> <li>What is data science? </li> <li>Summary of business needs for employees with data science skills. </li> <li>Review of Python concepts </li> <li>How generative AI can help you learn Python </li> <li>Basic Python libraries for handling data such as NumPy </li> <li>Python statistical libraries </li> <li>Python libraries for machine learning </li> <li>Python libraries for working with graph data </li> <li>Python libraries for visualization with a focus on Plotly </li> <li>Python libraries for NLP </li> <li>Python libraries for vector stores </li> <li>Python libraries for GenAI including LangChain and LlamaIndex </li> </ol> <p>Please create a numbered list of the 150 most important topics  that would be taught in this course. Begin with the prerequisite concepts and then proceed to defining key terms. Then include the core concepts used in Python data analysis. Finally, close with some potential final project concepts  that students could create. Think carefully about the ordering  of the concepts from simple to more abstract. For each concept,  return a conceptID followed by short name which would be the label for the concept in a learning graph (concept dependency). Feel free to use short names and abbreviations.</p> <p>ChatGPT</p> <p>Thought for 1m 34s</p> <ol> <li>DS Definition - Definition of Data Science</li> <li>DS Importance - Importance of Data Science</li> <li>DS vs DA - Data Science vs. Data Analysis</li> <li>DS Workflow - Data Science Workflow</li> <li>DS Roles - Roles in Data Science</li> <li>DS DataCollection - Data Collection Methods</li> <li>DS DataCleaning - Data Cleaning Processes</li> <li>DS DataExploration - Exploratory Data Analysis</li> <li>DS FeatureEngineering - Feature Engineering</li> <li>DS Modeling - Modeling and Algorithms</li> <li>DS Evaluation - Model Evaluation Techniques</li> <li>DS Deployment - Model Deployment Strategies</li> <li>Business DS Needs - Business Needs for Data Science Skills</li> <li>Business DS Impact - Impact on Decision Making</li> <li>Business DS Examples - Business Applications of Data Science</li> <li>Python DataTypes - Data Types in Python</li> <li>Python DataStructures - Data Structures (List, Dict, Set)</li> <li>Python Control - Control Structures (Loops, Conditionals)</li> <li>Python Functions - Functions and Modules</li> <li>Python OOP - Object-Oriented Programming</li> <li>Python FileHandling - File Handling</li> <li>Python Exceptions - Exception Handling</li> <li>Python Debugging - Debugging Techniques</li> <li>Python LibrariesIntro - Introduction to Python Libraries</li> <li>AI Intro - Introduction to Generative AI</li> <li>AI LearnPython - AI Tools for Learning Python</li> <li>AI CodeAssistants - Using AI Assistants for Coding</li> <li>AI LearningResources - AI-Generated Learning Resources</li> <li>AI Ethics - Ethical Considerations of AI</li> <li>NumPy Intro - Introduction to NumPy</li> <li>NumPy Arrays - NumPy Arrays and Operations</li> <li>NumPy Broadcasting - Broadcasting in NumPy</li> <li>NumPy AdvancedIndexing - Advanced Indexing</li> <li>NumPy MathFunctions - Mathematical Functions</li> <li>Pandas Intro - Introduction to Pandas</li> <li>Pandas DataFrames - DataFrames and Series</li> <li>Pandas IO - Reading and Writing Data</li> <li>Pandas Selection - Data Selection and Filtering</li> <li>Pandas Cleaning - Data Cleaning in Pandas</li> <li>Pandas MissingData - Handling Missing Data</li> <li>Pandas Merging - Merging and Joining DataFrames</li> <li>Pandas Grouping - Data Aggregation and Grouping</li> <li>Pandas Pivot - Pivot Tables</li> <li>Pandas TimeSeries - Time Series Data</li> <li>Stats Intro - Introduction to Statistics</li> <li>Stats Descriptive - Descriptive Statistics</li> <li>Stats Inferential - Inferential Statistics</li> <li>Stats Distributions - Probability Distributions</li> <li>SciPy Intro - SciPy for Statistical Analysis</li> <li>Statsmodels Intro - Statsmodels Library</li> <li>Stats Hypothesis - Hypothesis Testing</li> <li>Stats Regression - Regression Analysis</li> <li>ML Intro - Introduction to Machine Learning</li> <li>ML Supervised - Supervised Learning</li> <li>ML Unsupervised - Unsupervised Learning</li> <li>Sklearn Intro - scikit-learn Library</li> <li>ML Preprocessing - Data Preprocessing</li> <li>ML FeatureSelection - Feature Selection Techniques</li> <li>ML CrossValidation - Cross-Validation Methods</li> <li>ML Classification - Classification Algorithms</li> <li>ML Regression - Regression Algorithms</li> <li>ML Clustering - Clustering Algorithms</li> <li>ML Evaluation - Model Evaluation and Validation</li> <li>NetworkX Intro - NetworkX Library</li> <li>NetworkX Create - Creating Graphs</li> <li>NetworkX Algorithms - Graph Algorithms</li> <li>NetworkX Visualize - Graph Visualization</li> <li>Viz Importance - Importance of Data Visualization</li> <li>Matplotlib Intro - Introduction to Matplotlib</li> <li>Seaborn Intro - Introduction to Seaborn</li> <li>Plotly Intro - Introduction to Plotly</li> <li>Plotly Basic - Basic Plots with Plotly</li> <li>Plotly Interactive - Interactive Visualizations</li> <li>Plotly Dash - Plotly Dash for Web Apps</li> <li>NLP Intro - Introduction to NLP</li> <li>NLP TextPreprocessing - Text Preprocessing Techniques</li> <li>NLP Tokenization - Tokenization Methods</li> <li>NLP WordEmbeddings - Word Embeddings</li> <li>NLTK Intro - NLTK Library</li> <li>Spacy Intro - spaCy Library</li> <li>NLP Sentiment - Sentiment Analysis</li> <li>NLP TextClassify - Text Classification</li> <li>VectorStores Intro - Introduction to Vector Stores</li> <li>Faiss Intro - FAISS Library</li> <li>Annoy Intro - Annoy Library</li> <li>VectorStores Apps - Vector Store Applications</li> <li>GenAI Intro - Introduction to Generative AI</li> <li>LangChain Intro - LangChain Library</li> <li>LangChain Pipelines - Creating Pipelines</li> <li>LangChain COT - Chain-of-thought Prompting</li> <li>LlamaIndex Intro - LlamaIndex Library</li> <li>LlamaIndex Indexing - Indexing Data</li> <li>LlamaIndex Querying - Querying Data</li> <li>Project Ideas - Potential Final Projects Overview</li> <li>Project DataAnalysis - Data Analysis Project</li> <li>Project MLModel - Machine Learning Model Project</li> <li>Project NLP - NLP Application Project</li> <li>Project Visualization - Data Visualization Dashboard</li> <li>Project GraphAnalysis - Graph Analysis Project</li> <li>Project AIChatbot - AI-Powered Chatbot Project</li> <li>Project Planning - Planning a Data Science Project</li> <li>Project Dataset - Choosing Datasets</li> <li>Project Teams - Team Collaboration</li> <li>Project Presentation - Presenting Findings</li> <li>Project Documentation - Project Documentation</li> <li>Project Deployment - Deploying Projects</li> <li>ML DeepLearning - Introduction to Deep Learning</li> <li>TensorFlow Intro - TensorFlow Library</li> <li>PyTorch Intro - PyTorch Library</li> <li>DL NeuralNetworks - Neural Network Fundamentals</li> <li>DL CNNs - Convolutional Neural Networks</li> <li>DL RNNs - Recurrent Neural Networks</li> <li>DL TransferLearning - Transfer Learning</li> <li>DL NLP Transformers - Transformers Architecture</li> <li>DL NLP BERT - BERT Model</li> <li>TimeSeries Intro - Time Series Analysis</li> <li>TimeSeries Decomposition - Time Series Decomposition</li> <li>TimeSeries ARIMA - ARIMA Models</li> <li>TimeSeries Forecasting - Forecasting Techniques</li> <li>Recommender Intro - Recommender Systems</li> <li>Recommender CollabFilter - Collaborative Filtering</li> <li>Recommender ContentBased - Content-Based Filtering</li> <li>Recommender Hybrid - Hybrid Recommenders</li> <li>DataEng Intro - Introduction to Data Engineering</li> <li>DataEng ETL - ETL Processes</li> <li>DataEng DataWarehouses - Data Warehouses</li> <li>DataEng DataLakes - Data Lakes</li> <li>ML Reinforcement - Reinforcement Learning Basics</li> <li>ML GANs - Generative Adversarial Networks</li> <li>ML AutoML - Automated Machine Learning</li> <li>ML Explainability - Model Explainability</li> <li>BigData Intro - Introduction to Big Data</li> <li>PySpark Intro - Introduction to PySpark</li> <li>Cloud Intro - Cloud Computing Basics</li> <li>AWS Intro - AWS Overview</li> <li>Docker Intro - Introduction to Docker</li> <li>Ethics AI - Ethical Considerations in AI</li> <li>Privacy Data - Data Privacy and Security</li> <li>Bias Fairness - Bias and Fairness in ML</li> <li>Git Intro - Introduction to Git</li> <li>GitHub Intro - Using GitHub</li> <li>Viz BestPractices - Visualization Best Practices</li> <li>Dask Intro - Dask for Parallel Computing</li> <li>CaseStudies Intro - Case Studies Overview</li> <li>Communication Skills - Communication Skills</li> <li>CriticalThinking Skills - Critical Thinking</li> <li>Leadership Skills - Leadership in Data Science</li> <li>Final Review - Course Summary and Review</li> <li>Next Steps - Next Steps in Learning</li> <li>Course Evaluation - Course Feedback and Evaluation</li> </ol> <p>o1</p>"},{"location":"prompts/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>Certainly! Based on the course content you've provided, here is a 10-category taxonomy of the types of concepts in this course:</p> <ol> <li>Foundation Concepts (Prerequisites)</li> <li>Key Terms</li> <li>Python Programming Concepts</li> <li>Python Libraries</li> <li>Data Manipulation and Analysis</li> <li>Statistical Analysis</li> <li>Machine Learning Concepts</li> <li>Data Visualization</li> <li>Natural Language Processing (NLP)</li> <li>Generative AI and Advanced Topics</li> </ol>"},{"location":"prompts/concept-taxonomy/#detailed-taxonomy","title":"Detailed Taxonomy:","text":"<ol> <li> <p>Foundation Concepts (Prerequisites)     Fundamental skills and knowledge required before diving into data science topics, including basic programming concepts and understanding of data types.</p> </li> <li> <p>Key Terms     Essential terminology and definitions used in data science, machine learning, AI, and related fields to build a strong conceptual foundation.</p> </li> <li> <p>Python Programming Concepts     Core Python programming skills necessary for data science, such as control structures, functions, object-oriented programming, file handling, and debugging.</p> </li> <li> <p>Python Libraries     Introduction to important Python libraries used in data science, including NumPy, Pandas, SciPy, Statsmodels, scikit-learn, NetworkX, and more.</p> </li> <li> <p>Data Manipulation and Analysis     Techniques for collecting, cleaning, exploring, and transforming data to prepare it for analysis, including data handling with Pandas and NumPy.</p> </li> <li> <p>Statistical Analysis     Concepts and methods in statistics necessary for analyzing data and making inferences, including descriptive and inferential statistics, probability distributions, and hypothesis testing.</p> </li> <li> <p>Machine Learning Concepts     Understanding of machine learning algorithms, model building, evaluation techniques, and deployment strategies using libraries like scikit-learn.</p> </li> <li> <p>Data Visualization     Tools and techniques for visualizing data to effectively communicate insights, focusing on libraries like Matplotlib, Seaborn, and Plotly.</p> </li> <li> <p>Natural Language Processing (NLP)     Concepts and tools for processing and analyzing textual data, including text preprocessing, tokenization, sentiment analysis, and using libraries like NLTK and spaCy.</p> </li> <li> <p>Generative AI and Advanced Topics     Advanced topics including generative AI, vector stores, deep learning frameworks, and the use of libraries like LangChain and LlamaIndex for building AI applications.</p> </li> </ol> <p>This taxonomy covers the breadth of topics in your course, organizing them into coherent categories that reflect their roles in the learning pathway. Each category groups related concepts together, making it easier to structure the course content and understand the dependencies between topics.</p> <p>o1</p>"},{"location":"prompts/course-description/","title":"Course Description for Introduction to Data Science with Python","text":"<p>Prompt</p> <p>Please generate a detailed course description for a 10-week class on the subject of introduction to data science with Python.</p> <pre><code>The course description should be written in Markdown.  Follow Bloom's 2001 taxonomy.\n\nThe course description follows an  online textbook about data science using the Python programming language.  The audience is advanced high-school students or freshmen in college.  The focus of the textbook is a series of interactive simulations (MicroSims) that allow the students to interact with data.  We start with simple linear regression and then build increasingly complicated predictive models with Python libraries like NumPy and PyTorch.  The focus is to find the simplest model that combines expandability with prediction fidelity.\n</code></pre>"},{"location":"prompts/course-description/#response-from-claude-sonnet-4","title":"Response from Claude Sonnet-4","text":"<p>Course Duration: 10 weeks Target Audience: Advanced high school students and college freshmen Prerequisites: Basic algebra and introductory programming experience recommended</p>"},{"location":"prompts/course-description/#course-overview","title":"Course Overview","text":"<p>This introductory course provides students with foundational knowledge and practical skills in data science using Python. Through hands-on experience with interactive simulations (MicroSims) and real-world datasets, students will develop competency in data analysis, visualization, and predictive modeling. The course emphasizes the critical balance between model explainability and predictive accuracy, guiding students to identify the simplest effective solutions to data-driven problems.</p>"},{"location":"prompts/course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <p>Remember (Knowledge) - Recall fundamental data science terminology and concepts - Identify key Python libraries for data science (NumPy, pandas, matplotlib, PyTorch) - Recognize different types of data and measurement scales - List the steps in the data science workflow</p> <p>Understand (Comprehension) - Explain the relationship between independent and dependent variables - Describe how linear regression models make predictions - Interpret basic statistical measures and visualizations - Summarize the trade-offs between model complexity and interpretability</p> <p>Apply (Application) - Implement basic data cleaning and preprocessing techniques - Create visualizations using Python libraries - Build simple linear regression models - Execute standard data science workflows on new datasets</p> <p>Analyze (Analysis) - Examine datasets to identify patterns and relationships - Compare different modeling approaches for the same problem - Distinguish between correlation and causation in data relationships - Evaluate model performance using appropriate metrics</p> <p>Evaluate (Evaluation) - Assess the quality and reliability of data sources - Critique model assumptions and limitations - Judge the appropriateness of different models for specific problems - Validate model performance and identify potential overfitting</p> <p>Create (Synthesis) - Design experiments to test hypotheses using data - Construct predictive models for real-world scenarios - Develop data-driven solutions to complex problems - Generate original insights from exploratory data analysis</p>"},{"location":"prompts/course-description/#weekly-schedule","title":"Weekly Schedule","text":""},{"location":"prompts/course-description/#week-1-foundations-of-data-science","title":"Week 1: Foundations of Data Science","text":"<ul> <li>Introduction to data science and its applications</li> <li>Setting up Python environment and Jupyter notebooks</li> <li>First MicroSim: Exploring sample datasets</li> <li>Basic data types and structures in Python</li> </ul>"},{"location":"prompts/course-description/#week-2-data-exploration-and-visualization","title":"Week 2: Data Exploration and Visualization","text":"<ul> <li>Loading and examining datasets with pandas</li> <li>Creating basic plots with matplotlib</li> <li>MicroSim: Interactive data visualization</li> <li>Identifying patterns in data through visual exploration</li> </ul>"},{"location":"prompts/course-description/#week-3-statistical-foundations","title":"Week 3: Statistical Foundations","text":"<ul> <li>Descriptive statistics and summary measures</li> <li>Understanding distributions and variability</li> <li>MicroSim: Statistical parameter exploration</li> <li>Introduction to probability concepts</li> </ul>"},{"location":"prompts/course-description/#week-4-simple-linear-regression","title":"Week 4: Simple Linear Regression","text":"<ul> <li>Mathematical foundations of linear regression</li> <li>Implementing regression from scratch</li> <li>MicroSim: Interactive regression line fitting</li> <li>Interpreting coefficients and model output</li> </ul>"},{"location":"prompts/course-description/#week-5-model-evaluation-and-validation","title":"Week 5: Model Evaluation and Validation","text":"<ul> <li>Measuring model performance (R\u00b2, MSE, MAE)</li> <li>Training and testing data splits</li> <li>MicroSim: Cross-validation simulation</li> <li>Understanding overfitting and underfitting</li> </ul>"},{"location":"prompts/course-description/#week-6-multiple-linear-regression","title":"Week 6: Multiple Linear Regression","text":"<ul> <li>Extending to multiple predictor variables</li> <li>Feature selection and engineering</li> <li>MicroSim: Multi-dimensional regression explorer</li> <li>Handling categorical variables</li> </ul>"},{"location":"prompts/course-description/#week-7-introduction-to-numpy-and-advanced-computation","title":"Week 7: Introduction to NumPy and Advanced Computation","text":"<ul> <li>NumPy arrays and vectorized operations</li> <li>Matrix operations for regression</li> <li>MicroSim: Linear algebra visualization</li> <li>Computational efficiency in data science</li> </ul>"},{"location":"prompts/course-description/#week-8-non-linear-models-and-regularization","title":"Week 8: Non-linear Models and Regularization","text":"<ul> <li>Polynomial regression and feature transformation</li> <li>Ridge and Lasso regularization</li> <li>MicroSim: Bias-variance trade-off explorer</li> <li>Model selection strategies</li> </ul>"},{"location":"prompts/course-description/#week-9-introduction-to-machine-learning-with-pytorch","title":"Week 9: Introduction to Machine Learning with PyTorch","text":"<ul> <li>Neural networks and deep learning concepts</li> <li>Building simple networks with PyTorch</li> <li>MicroSim: Neural network playground</li> <li>Comparing traditional and deep learning approaches</li> </ul>"},{"location":"prompts/course-description/#week-10-capstone-project-and-model-deployment","title":"Week 10: Capstone Project and Model Deployment","text":"<ul> <li>End-to-end data science project</li> <li>Model interpretation and communication</li> <li>MicroSim: Model comparison dashboard</li> <li>Best practices and ethical considerations</li> </ul>"},{"location":"prompts/course-description/#assessment-methods","title":"Assessment Methods","text":"<p>Formative Assessment (60%) - Weekly MicroSim exercises and reflections (30%) - Homework assignments applying concepts to new datasets (20%) - Peer review activities and collaborative problem-solving (10%)</p> <p>Summative Assessment (40%) - Midterm project: Complete data analysis report (15%) - Final capstone project: Original predictive modeling solution (20%) - Final examination covering theoretical concepts (5%)</p>"},{"location":"prompts/course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Computer with Python 3.8+ installed</li> <li>Access to interactive online textbook with MicroSims</li> <li>Jupyter Notebook environment</li> <li>Required Python packages: pandas, NumPy, matplotlib, scikit-learn, PyTorch</li> </ul>"},{"location":"prompts/course-description/#key-learning-principles","title":"Key Learning Principles","text":"<p>Interactive Learning: Each week features hands-on MicroSims that allow students to manipulate parameters and observe results in real-time, reinforcing theoretical concepts through experiential learning.</p> <p>Scaffolded Complexity: The course progresses systematically from simple linear relationships to complex neural networks, ensuring students build confidence before tackling advanced topics.</p> <p>Explainable AI Focus: Throughout the course, emphasis is placed on understanding and interpreting models rather than simply achieving high accuracy, preparing students for ethical and transparent data science practice.</p> <p>Real-world Applications: All examples and projects use authentic datasets and scenarios, helping students connect academic learning to practical problem-solving.</p>"},{"location":"prompts/course-description/#course-philosophy","title":"Course Philosophy","text":"<p>This course is built on the principle that effective data science requires both technical competence and critical thinking. Students will learn not just how to build predictive models, but when to use them, how to interpret their results, and how to communicate findings to diverse audiences. The integration of interactive simulations ensures that abstract mathematical concepts become concrete and intuitive, while the progression from simple to complex models helps students appreciate the value of parsimony in modeling.</p> <p>By the end of this course, students will have developed both the technical skills and analytical mindset necessary for success in advanced data science coursework or entry-level positions in data-driven fields.</p>"},{"location":"prompts/faqs/","title":"FAQs","text":""},{"location":"prompts/faqs/#prompt","title":"Prompt","text":"<pre><code>I am preparing to teach a new college-level course called \"Introduction to Data Science with Python\".  The prerequisites to this course are Beginning Python,  Algebra and simple statistics.  What are the 100 most common questions that will come up in this course?\n\nPlease return the questions and their answer as a in Markdown format. \nPlace the question text in a level-4 header (####) and the response in the body text.\n</code></pre>"},{"location":"prompts/faqs/#chatgpt-o1-preview-response","title":"ChatGPT o1-preview Response","text":""},{"location":"prompts/faqs/#what-is-data-science-and-why-is-it-important","title":"What is data science, and why is it important?","text":"<p>Data science is an interdisciplinary field that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data. It is important because it enables organizations to make informed decisions, predict trends, and solve complex problems by analyzing large datasets.</p>"},{"location":"prompts/faqs/#how-is-python-used-in-data-science","title":"How is Python used in data science?","text":"<p>Python is widely used in data science due to its simplicity and versatility. It offers a vast ecosystem of libraries and frameworks like NumPy, Pandas, Matplotlib, and scikit-learn, which facilitate data manipulation, analysis, visualization, and machine learning tasks.</p>"},{"location":"prompts/faqs/#what-are-the-key-python-libraries-for-data-analysis","title":"What are the key Python libraries for data analysis?","text":"<p>The key Python libraries for data analysis include:</p> <ul> <li>NumPy: For numerical computing and array operations.</li> <li>Pandas: For data manipulation and analysis using data structures like DataFrames.</li> <li>Matplotlib: For creating static, animated, and interactive visualizations.</li> <li>Seaborn: For statistical data visualization built on top of Matplotlib.</li> <li>scikit-learn: For machine learning algorithms and predictive data analysis.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-import-a-csv-file-into-a-pandas-dataframe","title":"How do you import a CSV file into a Pandas DataFrame?","text":"<p>You can import a CSV file using the <code>read_csv()</code> function from Pandas:</p> <pre><code>Copy code\n\n`import pandas as pd\n\ndf = pd.read_csv('file_name.csv')\n`\n</code></pre> <p>Replace <code>'file_name.csv'</code> with the path to your CSV file.</p>"},{"location":"prompts/faqs/#what-is-a-dataframe-in-pandas","title":"What is a DataFrame in Pandas?","text":"<p>A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). It is similar to a spreadsheet or SQL table and is the primary data structure used in Pandas for data manipulation.</p>"},{"location":"prompts/faqs/#how-do-you-handle-missing-data-in-a-dataset","title":"How do you handle missing data in a dataset?","text":"<p>Missing data can be handled by:</p> <ul> <li>Removing missing values: Using <code>dropna()</code> to remove rows or columns with missing values.</li> <li>Imputing missing values: Using <code>fillna()</code> to replace missing values with a specific value, mean, median, or mode.</li> <li>Interpolate missing values: Using <code>interpolate()</code> to estimate missing values based on other data points.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-difference-between-numpy-arrays-and-python-lists","title":"What is the difference between NumPy arrays and Python lists?","text":"<p>NumPy arrays are fixed-size, homogeneous collections of elements (all of the same data type) optimized for numerical computations. Python lists are dynamic, heterogeneous collections that can contain elements of different data types. NumPy arrays offer better performance for mathematical operations.</p>"},{"location":"prompts/faqs/#how-do-you-select-a-subset-of-data-from-a-dataframe","title":"How do you select a subset of data from a DataFrame?","text":"<p>You can select subsets using:</p> <ul> <li> <p>Label-based indexing with <code>.loc</code>:</p> <pre><code>python\nCopy code\n\n`df_subset = df.loc[row_labels, column_labels]\n`\n</code></pre> </li> <li> <p>Integer-based indexing with <code>.iloc</code>:</p> <pre><code>python\nCopy code\n\n`df_subset = df.iloc[row_indices, column_indices]\n`\n</code></pre> </li> </ul>"},{"location":"prompts/faqs/#what-is-data-visualization-and-why-is-it-important","title":"What is data visualization, and why is it important?","text":"<p>Data visualization is the graphical representation of data to communicate information clearly and efficiently. It is important because it helps identify patterns, trends, and outliers in data, making complex data more accessible and understandable.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-simple-line-plot-using-matplotlib","title":"How do you create a simple line plot using Matplotlib?","text":"<pre><code>python\nCopy code\n\n`import matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4]\ny = [10, 20, 25, 30]\n\nplt.plot(x, y)\nplt.xlabel('X-axis Label')\nplt.ylabel('Y-axis Label')\nplt.title('Simple Line Plot')\nplt.show()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-groupby-function-in-pandas","title":"What is the purpose of the <code>groupby()</code> function in Pandas?","text":"<p>The <code>groupby()</code> function is used to split data into groups based on some criteria, perform operations on each group independently, and then combine the results. It is useful for aggregation, transformation, and filtration of data.</p>"},{"location":"prompts/faqs/#how-do-you-merge-two-dataframes-in-pandas","title":"How do you merge two DataFrames in Pandas?","text":"<p>You can merge two DataFrames using the <code>merge()</code> function:</p> <pre><code>python\nCopy code\n\n`merged_df = pd.merge(df1, df2, on='common_column')\n`\n</code></pre> <p>Replace <code>'common_column'</code> with the column name that is common to both DataFrames.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-merge-and-concat-in-pandas","title":"What is the difference between <code>merge()</code> and <code>concat()</code> in Pandas?","text":"<ul> <li><code>merge()</code>: Combines two DataFrames based on the values of common columns (similar to SQL joins).</li> <li><code>concat()</code>: Concatenates DataFrames either vertically or horizontally, stacking them along an axis.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-calculate-basic-statistical-measures-like-mean-and-median-in-pandas","title":"How do you calculate basic statistical measures like mean and median in Pandas?","text":"<p>You can use built-in functions:</p> <ul> <li> <p>Mean:</p> <pre><code>python\nCopy code\n\n`mean_value = df['column_name'].mean()\n`\n</code></pre> </li> <li> <p>Median:</p> <pre><code>python\nCopy code\n\n`median_value = df['column_name'].median()\n`\n</code></pre> </li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-apply-function-in-pandas","title":"What is the purpose of the <code>apply()</code> function in Pandas?","text":"<p>The <code>apply()</code> function allows you to apply a function along an axis of the DataFrame (either rows or columns). It is useful for performing complex operations on DataFrame elements.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-pivot-table-in-pandas","title":"How do you create a pivot table in Pandas?","text":"<p>You can create a pivot table using the <code>pivot_table()</code> function:</p> <pre><code>python\nCopy code\n\n`pivot = pd.pivot_table(df, values='value_column', index='index_column', columns='columns_column', aggfunc='mean')\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-supervised-and-unsupervised-learning","title":"What is the difference between supervised and unsupervised learning?","text":"<ul> <li>Supervised Learning: Involves training a model on labeled data, where the target outcome is known. Examples include regression and classification.</li> <li>Unsupervised Learning: Involves finding patterns in unlabeled data without predefined outcomes. Examples include clustering and dimensionality reduction.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-perform-linear-regression-using-scikit-learn","title":"How do you perform linear regression using scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-overfitting-in-machine-learning","title":"What is overfitting in machine learning?","text":"<p>Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor generalization to new, unseen data. It performs well on training data but poorly on test data.</p>"},{"location":"prompts/faqs/#how-can-you-prevent-overfitting","title":"How can you prevent overfitting?","text":"<p>Overfitting can be prevented by:</p> <ul> <li>Cross-validation: Splitting data into training and validation sets.</li> <li>Regularization: Adding penalties for complex models (e.g., Lasso, Ridge).</li> <li>Simplifying the model: Reducing the number of features.</li> <li>Early stopping: Halting training when performance on a validation set starts to degrade.</li> </ul>"},{"location":"prompts/faqs/#what-is-cross-validation","title":"What is cross-validation?","text":"<p>Cross-validation is a technique for assessing how a model will generalize to an independent dataset. It involves partitioning the data into subsets, training the model on some subsets, and validating it on the remaining ones.</p>"},{"location":"prompts/faqs/#how-do-you-evaluate-the-performance-of-a-regression-model","title":"How do you evaluate the performance of a regression model?","text":"<p>Common metrics include:</p> <ul> <li>Mean Absolute Error (MAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-squared (Coefficient of Determination)</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-feature-scaling","title":"What is the purpose of feature scaling?","text":"<p>Feature scaling standardizes the range of independent variables, improving the performance and convergence speed of some machine learning algorithms that are sensitive to the scale of data, such as gradient descent optimization.</p>"},{"location":"prompts/faqs/#how-do-you-perform-feature-scaling-in-python","title":"How do you perform feature scaling in Python?","text":"<p>Using scikit-learn's <code>StandardScaler</code>:</p> <pre><code>python\nCopy code\n\n`from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-one-hot-encoding","title":"What is one-hot encoding?","text":"<p>One-hot encoding is a process of converting categorical variables into a binary (0 or 1) representation. Each category becomes a new column, and a value of 1 indicates the presence of that category.</p>"},{"location":"prompts/faqs/#how-do-you-perform-one-hot-encoding-in-pandas","title":"How do you perform one-hot encoding in Pandas?","text":"<p>Using the <code>get_dummies()</code> function:</p> <pre><code>python\nCopy code\n\n`encoded_df = pd.get_dummies(df, columns=['categorical_column'])\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-confusion-matrix","title":"What is a confusion matrix?","text":"<p>A confusion matrix is a table used to evaluate the performance of a classification model. It displays the true positives, true negatives, false positives, and false negatives, providing insight into the types of errors made by the model.</p>"},{"location":"prompts/faqs/#how-do-you-calculate-accuracy-precision-and-recall-from-a-confusion-matrix","title":"How do you calculate accuracy, precision, and recall from a confusion matrix?","text":"<ul> <li>Accuracy: (TP + TN) / (TP + TN + FP + FN)</li> <li>Precision: TP / (TP + FP)</li> <li>Recall: TP / (TP + FN)</li> </ul> <p>Where TP=True Positives, TN=True Negatives, FP=False Positives, FN=False Negatives.</p>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-train_test_split-function","title":"What is the purpose of the <code>train_test_split</code> function?","text":"<p>The <code>train_test_split</code> function splits a dataset into training and testing sets, allowing you to train the model on one subset and evaluate its performance on another to prevent overfitting.</p>"},{"location":"prompts/faqs/#how-do-you-split-data-into-training-and-testing-sets-in-scikit-learn","title":"How do you split data into training and testing sets in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-classification-and-regression","title":"What is the difference between classification and regression?","text":"<ul> <li>Classification: Predicts categorical labels or classes.</li> <li>Regression: Predicts continuous numerical values.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-categorical-variables-in-machine-learning-models","title":"How do you handle categorical variables in machine learning models?","text":"<p>Categorical variables can be handled by:</p> <ul> <li>Label Encoding: Assigning a unique integer to each category.</li> <li>One-Hot Encoding: Creating binary columns for each category.</li> </ul>"},{"location":"prompts/faqs/#what-is-k-means-clustering","title":"What is k-means clustering?","text":"<p>K-means clustering is an unsupervised learning algorithm that partitions data into k clusters, where each data point belongs to the cluster with the nearest mean. It aims to minimize the within-cluster sum of squares.</p>"},{"location":"prompts/faqs/#how-do-you-determine-the-optimal-number-of-clusters-in-k-means","title":"How do you determine the optimal number of clusters in k-means?","text":"<p>Common methods include:</p> <ul> <li>Elbow Method: Plotting the explained variance as a function of the number of clusters and looking for an \"elbow\" point.</li> <li>Silhouette Score: Measuring how similar a data point is to its own cluster compared to other clusters.</li> </ul>"},{"location":"prompts/faqs/#what-is-principal-component-analysis-pca","title":"What is principal component analysis (PCA)?","text":"<p>PCA is a dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information by identifying the principal components (directions of maximum variance).</p>"},{"location":"prompts/faqs/#how-do-you-perform-pca-in-scikit-learn","title":"How do you perform PCA in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-bias-variance-tradeoff","title":"What is the bias-variance tradeoff?","text":"<p>The bias-variance tradeoff is the balance between a model's ability to generalize to new data (low variance) and its accuracy on training data (low bias). High bias can lead to underfitting, while high variance can lead to overfitting.</p>"},{"location":"prompts/faqs/#what-is-regularization-in-machine-learning","title":"What is regularization in machine learning?","text":"<p>Regularization involves adding a penalty term to the loss function to prevent overfitting by discouraging complex models. Common regularization techniques include Lasso (L1) and Ridge (L2) regression.</p>"},{"location":"prompts/faqs/#how-do-you-implement-ridge-regression-in-scikit-learn","title":"How do you implement Ridge regression in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-logistic-regression","title":"What is logistic regression?","text":"<p>Logistic regression is a classification algorithm used to predict binary outcomes (0 or 1) by modeling the probability of a certain class using a logistic function.</p>"},{"location":"prompts/faqs/#how-do-you-evaluate-the-performance-of-a-classification-model","title":"How do you evaluate the performance of a classification model?","text":"<p>Common metrics include:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1 Score</li> <li>ROC AUC Score</li> </ul>"},{"location":"prompts/faqs/#what-is-the-receiver-operating-characteristic-roc-curve","title":"What is the Receiver Operating Characteristic (ROC) curve?","text":"<p>The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It illustrates the diagnostic ability of a binary classifier.</p>"},{"location":"prompts/faqs/#how-do-you-handle-imbalanced-datasets","title":"How do you handle imbalanced datasets?","text":"<p>Techniques include:</p> <ul> <li>Resampling Methods: Oversampling the minority class or undersampling the majority class.</li> <li>Synthetic Data Generation: Using methods like SMOTE to generate synthetic examples.</li> <li>Using Appropriate Evaluation Metrics: Focusing on precision, recall, or F1 score instead of accuracy.</li> </ul>"},{"location":"prompts/faqs/#what-is-time-series-analysis","title":"What is time series analysis?","text":"<p>Time series analysis involves analyzing data points collected or recorded at specific time intervals to identify trends, cycles, and seasonal variations for forecasting and other purposes.</p>"},{"location":"prompts/faqs/#how-do-you-deal-with-date-and-time-data-in-pandas","title":"How do you deal with date and time data in Pandas?","text":"<p>Pandas provides the <code>to_datetime()</code> function to convert strings to datetime objects, and you can use datetime properties and methods to manipulate date and time data.</p> <pre><code>python\nCopy code\n\n`df['date_column'] = pd.to_datetime(df['date_column'])\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-autocorrelation-in-time-series-data","title":"What is autocorrelation in time series data?","text":"<p>Autocorrelation is the correlation of a signal with a delayed copy of itself. In time series data, it measures the relationship between a variable's current value and its past values.</p>"},{"location":"prompts/faqs/#how-do-you-perform-forecasting-using-arima-models","title":"How do you perform forecasting using ARIMA models?","text":"<p>Using the <code>statsmodels</code> library:</p> <pre><code>python\nCopy code\n\n`from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(time_series_data, order=(p, d, q))\nmodel_fit = model.fit()\nforecast = model_fit.forecast(steps=10)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-natural-language-processing-nlp","title":"What is natural language processing (NLP)?","text":"<p>NLP is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language.</p>"},{"location":"prompts/faqs/#how-do-you-perform-text-preprocessing-in-nlp","title":"How do you perform text preprocessing in NLP?","text":"<p>Common steps include:</p> <ul> <li>Tokenization: Splitting text into words or sentences.</li> <li>Stop Word Removal: Removing common words that add little meaning.</li> <li>Stemming/Lemmatization: Reducing words to their base or root form.</li> <li>Encoding: Converting text to numerical representation using methods like TF-IDF or word embeddings.</li> </ul>"},{"location":"prompts/faqs/#what-is-tf-idf","title":"What is TF-IDF?","text":"<p>Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects how important a word is to a document in a collection. It balances the frequency of a word in a document with how common the word is across all documents.</p>"},{"location":"prompts/faqs/#how-do-you-handle-large-datasets-that-dont-fit-into-memory","title":"How do you handle large datasets that don't fit into memory?","text":"<p>Techniques include:</p> <ul> <li>Data Sampling: Working with a subset of the data.</li> <li>Batch Processing: Processing data in chunks.</li> <li>Distributed Computing: Using tools like Apache Spark.</li> <li>Out-of-core Learning: Using algorithms that can learn from data incrementally.</li> </ul>"},{"location":"prompts/faqs/#what-is-a-pipeline-in-scikit-learn","title":"What is a pipeline in scikit-learn?","text":"<p>A pipeline is a sequence of data processing steps assembled into a single object. It ensures that all steps are applied consistently during training and testing, simplifying the workflow.</p> <pre><code>python\nCopy code\n\n`from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LinearRegression())\n])\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-save-and-load-trained-models-in-scikit-learn","title":"How do you save and load trained models in scikit-learn?","text":"<p>Using the <code>joblib</code> library:</p> <pre><code>python\nCopy code\n\n`import joblib\n\n# Save model\njoblib.dump(model, 'model.pkl')\n\n# Load model\nmodel = joblib.load('model.pkl')\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-gradient-descent","title":"What is gradient descent?","text":"<p>Gradient descent is an optimization algorithm used to minimize the cost function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-batch-stochastic-and-mini-batch-gradient-descent","title":"What is the difference between batch, stochastic, and mini-batch gradient descent?","text":"<ul> <li>Batch Gradient Descent: Uses the entire dataset to compute gradients.</li> <li>Stochastic Gradient Descent (SGD): Uses one sample at a time.</li> <li>Mini-Batch Gradient Descent: Uses a small subset (batch) of the data.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-multicollinearity-in-regression-analysis","title":"How do you handle multicollinearity in regression analysis?","text":"<p>Techniques include:</p> <ul> <li>Removing correlated features.</li> <li>Principal Component Analysis (PCA) to reduce dimensionality.</li> <li>Regularization methods like Ridge regression.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-central-limit-theorem","title":"What is the Central Limit Theorem?","text":"<p>The Central Limit Theorem states that the sampling distribution of the sample means approaches a normal distribution as the sample size becomes large, regardless of the population's distribution.</p>"},{"location":"prompts/faqs/#what-is-hypothesis-testing","title":"What is hypothesis testing?","text":"<p>Hypothesis testing is a statistical method used to decide whether there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis based on sample data.</p>"},{"location":"prompts/faqs/#what-is-p-value","title":"What is p-value?","text":"<p>A p-value is the probability of observing results at least as extreme as those measured when the null hypothesis is true. A low p-value indicates that the observed data is unlikely under the null hypothesis.</p>"},{"location":"prompts/faqs/#how-do-you-perform-a-t-test-in-python","title":"How do you perform a t-test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>python\nCopy code\n\n`from scipy import stats\n\nt_statistic, p_value = stats.ttest_ind(sample1, sample2)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-type-i-and-type-ii-errors","title":"What is the difference between Type I and Type II errors?","text":"<ul> <li>Type I Error: Rejecting a true null hypothesis (false positive).</li> <li>Type II Error: Failing to reject a false null hypothesis (false negative).</li> </ul>"},{"location":"prompts/faqs/#what-is-an-anova-test","title":"What is an ANOVA test?","text":"<p>Analysis of Variance (ANOVA) is a statistical method used to compare means across three or more groups to see if at least one mean is different from the others.</p>"},{"location":"prompts/faqs/#how-do-you-perform-an-anova-test-in-python","title":"How do you perform an ANOVA test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>python\nCopy code\n\n`from scipy import stats\n\nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-bootstrapping-in-statistics","title":"What is bootstrapping in statistics?","text":"<p>Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It allows estimation of the sampling distribution of almost any statistic.</p>"},{"location":"prompts/faqs/#what-is-the-law-of-large-numbers","title":"What is the law of large numbers?","text":"<p>The law of large numbers states that as the number of trials increases, the sample mean will converge to the expected value (population mean).</p>"},{"location":"prompts/faqs/#what-is-a-probability-distribution","title":"What is a probability distribution?","text":"<p>A probability distribution describes how the values of a random variable are distributed. It defines the probabilities of different outcomes.</p>"},{"location":"prompts/faqs/#what-are-common-probability-distributions-used-in-data-science","title":"What are common probability distributions used in data science?","text":"<ul> <li>Normal Distribution</li> <li>Binomial Distribution</li> <li>Poisson Distribution</li> <li>Exponential Distribution</li> </ul>"},{"location":"prompts/faqs/#how-do-you-generate-random-numbers-following-a-normal-distribution-in-numpy","title":"How do you generate random numbers following a normal distribution in NumPy?","text":"<pre><code>python\nCopy code\n\n`import numpy as np\n\nrandom_numbers = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-curse-of-dimensionality","title":"What is the curse of dimensionality?","text":"<p>The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces, often leading to challenges like overfitting and increased computational cost.</p>"},{"location":"prompts/faqs/#how-do-you-reduce-dimensionality","title":"How do you reduce dimensionality?","text":"<p>Techniques include:</p> <ul> <li>Feature Selection: Choosing a subset of relevant features.</li> <li>Feature Extraction: Transforming data into a lower-dimensional space (e.g., PCA).</li> </ul>"},{"location":"prompts/faqs/#what-is-the-difference-between-bagging-and-boosting","title":"What is the difference between bagging and boosting?","text":"<ul> <li>Bagging: Combines multiple models (usually of the same type) trained on different subsets of the data to reduce variance.</li> <li>Boosting: Sequentially trains models, where each new model focuses on correcting errors made by previous ones, reducing bias.</li> </ul>"},{"location":"prompts/faqs/#what-is-a-decision-tree","title":"What is a decision tree?","text":"<p>A decision tree is a flowchart-like structure used for classification and regression that splits data into branches based on feature values to make predictions.</p>"},{"location":"prompts/faqs/#how-do-you-prevent-a-decision-tree-from-overfitting","title":"How do you prevent a decision tree from overfitting?","text":"<p>By:</p> <ul> <li>Pruning: Removing branches that have little power in predicting target variables.</li> <li>Setting a maximum depth: Limiting the depth of the tree.</li> <li>Setting a minimum number of samples per leaf.</li> </ul>"},{"location":"prompts/faqs/#what-is-random-forest","title":"What is random forest?","text":"<p>Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.</p>"},{"location":"prompts/faqs/#how-do-you-implement-a-random-forest-classifier-in-scikit-learn","title":"How do you implement a random forest classifier in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-ensemble-learning","title":"What is ensemble learning?","text":"<p>Ensemble learning combines predictions from multiple machine learning algorithms to produce a more accurate prediction than any individual model.</p>"},{"location":"prompts/faqs/#what-is-a-neural-network","title":"What is a neural network?","text":"<p>A neural network is a computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons) that can learn complex patterns through training.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-deep-learning-and-machine-learning","title":"What is the difference between deep learning and machine learning?","text":"<ul> <li>Machine Learning: Involves algorithms that parse data, learn from it, and make decisions.</li> <li>Deep Learning: A subset of machine learning using neural networks with multiple layers to model complex patterns.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-class-imbalance-in-classification-problems","title":"How do you handle class imbalance in classification problems?","text":"<p>By:</p> <ul> <li>Resampling the dataset: Oversampling the minority class or undersampling the majority class.</li> <li>Using appropriate evaluation metrics: Such as ROC AUC, precision-recall curve.</li> <li>Using algorithms that handle imbalance: Like XGBoost.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-map-function-in-pandas","title":"What is the purpose of the <code>map()</code> function in Pandas?","text":"<p>The <code>map()</code> function is used to map values of a Series according to an input mapping or function, useful for substituting values or applying a function element-wise.</p>"},{"location":"prompts/faqs/#how-do-you-sort-a-dataframe-in-pandas","title":"How do you sort a DataFrame in Pandas?","text":"<p>Using the <code>sort_values()</code> function:</p> <pre><code>python\nCopy code\n\n`sorted_df = df.sort_values(by='column_name', ascending=True)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-difference-between-apply-and-applymap-in-pandas","title":"What is the difference between <code>apply()</code> and <code>applymap()</code> in Pandas?","text":"<ul> <li><code>apply()</code>: Applies a function along an axis of the DataFrame (rows or columns).</li> <li><code>applymap()</code>: Applies a function element-wise to the entire DataFrame.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-remove-duplicates-from-a-dataframe","title":"How do you remove duplicates from a DataFrame?","text":"<p>Using the <code>drop_duplicates()</code> function:</p> <pre><code>python\nCopy code\n\n`df_unique = df.drop_duplicates()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-an-outlier-and-how-do-you-detect-them","title":"What is an outlier, and how do you detect them?","text":"<p>An outlier is a data point significantly different from others. Detection methods include:</p> <ul> <li>Statistical methods: Using Z-scores or IQR.</li> <li>Visualization: Box plots or scatter plots.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-handle-outliers-in-data","title":"How do you handle outliers in data?","text":"<p>By:</p> <ul> <li>Removing them: If they are errors.</li> <li>Transforming data: Using log or square root transformations.</li> <li>Using robust algorithms: That are less sensitive to outliers.</li> </ul>"},{"location":"prompts/faqs/#what-is-data-normalization","title":"What is data normalization?","text":"<p>Data normalization scales numerical data into a specific range, typically [0,1], ensuring that each feature contributes equally to the analysis.</p>"},{"location":"prompts/faqs/#how-do-you-perform-data-normalization-in-scikit-learn","title":"How do you perform data normalization in scikit-learn?","text":"<p>Using <code>MinMaxScaler</code>:</p> <pre><code>python\nCopy code\n\n`from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(data)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-heatmap-and-when-do-you-use-it","title":"What is a heatmap, and when do you use it?","text":"<p>A heatmap is a graphical representation of data where individual values are represented as colors. It is used to visualize correlation matrices or to display patterns in data.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-heatmap-in-seaborn","title":"How do you create a heatmap in Seaborn?","text":"<pre><code>python\nCopy code\n\n`import seaborn as sns\n\nsns.heatmap(data, annot=True)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-pairplot-in-seaborn","title":"What is a pairplot in Seaborn?","text":"<p>A pairplot creates a grid of Axes such that each variable in the data is shared across the y-axes across a single row and the x-axes across a single column, visualizing pairwise relationships.</p> <pre><code>python\nCopy code\n\n`sns.pairplot(df)\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-interpret-a-correlation-coefficient","title":"How do you interpret a correlation coefficient?","text":"<p>A correlation coefficient measures the strength and direction of a linear relationship between two variables. Values range from -1 to 1:</p> <ul> <li>-1: Perfect negative correlation.</li> <li>0: No correlation.</li> <li>1: Perfect positive correlation.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-describe-function-in-pandas","title":"What is the purpose of the <code>describe()</code> function in Pandas?","text":"<p>The <code>describe()</code> function generates descriptive statistics, including count, mean, standard deviation, min, max, and quartile values for numerical columns.</p>"},{"location":"prompts/faqs/#how-do-you-handle-datetime-features-for-machine-learning-models","title":"How do you handle date/time features for machine learning models?","text":"<p>By extracting meaningful components:</p> <ul> <li>Year, Month, Day</li> <li>Weekday</li> <li>Hour, Minute, Second</li> <li>Time since a specific date</li> </ul>"},{"location":"prompts/faqs/#what-is-the-difference-between-loc-and-iloc-in-pandas","title":"What is the difference between <code>.loc</code> and <code>.iloc</code> in Pandas?","text":"<ul> <li><code>.loc</code>: Label-based indexing to select data by row and column labels.</li> <li><code>.iloc</code>: Integer-based indexing to select data by row and column positions.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-rename-columns-in-a-dataframe","title":"How do you rename columns in a DataFrame?","text":"<p>Using the <code>rename()</code> function:</p> <pre><code>python\nCopy code\n\n`df.rename(columns={'old_name': 'new_name'}, inplace=True)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-astype-function-in-pandas","title":"What is the purpose of the <code>astype()</code> function in Pandas?","text":"<p>The <code>astype()</code> function is used to change the data type of a Series or DataFrame columns.</p> <pre><code>python\nCopy code\n\n`df['column_name'] = df['column_name'].astype('float')\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-detect-missing-values-in-a-dataframe","title":"How do you detect missing values in a DataFrame?","text":"<p>Using <code>isnull()</code> or <code>isna()</code> functions:</p> <pre><code>python\nCopy code\n\n`missing_values = df.isnull().sum()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-an-ensemble-method-in-machine-learning","title":"What is an ensemble method in machine learning?","text":"<p>An ensemble method combines predictions from multiple machine learning models to improve performance over a single model. Examples include Random Forest, Gradient Boosting.</p>"},{"location":"prompts/faqs/#how-do-you-implement-gradient-boosting-in-scikit-learn","title":"How do you implement Gradient Boosting in scikit-learn?","text":"<pre><code>python\nCopy code\n\n`from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-xgboost","title":"What is XGBoost?","text":"<p>XGBoost (Extreme Gradient Boosting) is an optimized gradient boosting library designed to be highly efficient, flexible, and portable, widely used for its performance and speed.</p>"},{"location":"prompts/faqs/#how-do-you-install-and-use-xgboost-in-python","title":"How do you install and use XGBoost in Python?","text":"<p>Install using pip:</p> <pre><code>bash\nCopy code\n\n`pip install xgboost\n`\n</code></pre> <p>Use in code:</p> <pre><code>python\nCopy code\n\n`import xgboost as xgb\n\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-cross-entropy-loss","title":"What is cross-entropy loss?","text":"<p>Cross-entropy loss measures the performance of a classification model whose output is a probability between 0 and 1. It increases as the predicted probability diverges from the actual label.</p>"},{"location":"prompts/faqs/#how-do-you-calculate-the-learning-rate-in-gradient-descent","title":"How do you calculate the learning rate in gradient descent?","text":"<p>The learning rate is a hyperparameter that you set manually. It determines the step size at each iteration while moving toward a minimum of a loss function.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-epochs-batches-and-iterations","title":"What is the difference between epochs, batches, and iterations?","text":"<ul> <li>Epoch: One complete pass through the entire training dataset.</li> <li>Batch: A subset of the training data used in one iteration.</li> <li>Iteration: One update of the model's parameters.</li> </ul>"},{"location":"prompts/faqs/#how-do-you-perform-hyperparameter-tuning","title":"How do you perform hyperparameter tuning?","text":"<p>By:</p> <ul> <li>Grid Search: Exhaustively searching through a specified subset of hyperparameters.</li> <li>Random Search: Randomly sampling hyperparameter combinations.</li> <li>Bayesian Optimization: Using probabilistic models to select hyperparameters.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-pipeline-module-in-scikit-learn","title":"What is the purpose of the <code>pipeline</code> module in scikit-learn?","text":"<p>It allows you to chain preprocessing steps and estimators in a sequential manner, ensuring consistent application of transformations during training and testing.</p>"},{"location":"prompts/faqs/#how-do-you-evaluate-a-clustering-algorithm","title":"How do you evaluate a clustering algorithm?","text":"<p>Using metrics like:</p> <ul> <li>Silhouette Score</li> <li>Calinski-Harabasz Index</li> <li>Davies-Bouldin Index</li> </ul>"},{"location":"prompts/faqs/#what-is-a-dummy-variable-trap","title":"What is a dummy variable trap?","text":"<p>The dummy variable trap occurs when multicollinearity is introduced in a regression model due to the inclusion of dummy variables that are linearly dependent. It can be avoided by dropping one dummy variable.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-correlation-matrix-in-pandas","title":"How do you create a correlation matrix in Pandas?","text":"<pre><code>python\nCopy code\n\n`corr_matrix = df.corr()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-an-roc-curve-and-how-do-you-plot-it","title":"What is an ROC curve, and how do you plot it?","text":"<p>An ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate at various threshold settings. You can plot it using scikit-learn:</p> <pre><code>python\nCopy code\n\n`from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nplt.plot(fpr, tpr)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-type-i-error","title":"What is a type I error?","text":"<p>A type I error occurs when the null hypothesis is true, but we incorrectly reject it (false positive).</p>"},{"location":"prompts/faqs/#what-is-a-type-ii-error","title":"What is a type II error?","text":"<p>A type II error occurs when the null hypothesis is false, but we fail to reject it (false negative).</p>"},{"location":"prompts/faqs/#how-do-you-calculate-the-p-value-in-a-hypothesis-test","title":"How do you calculate the p-value in a hypothesis test?","text":"<p>Using statistical tests from libraries like <code>scipy.stats</code>, which return the p-value as part of the output.</p>"},{"location":"prompts/faqs/#what-is-the-difference-between-parametric-and-non-parametric-tests","title":"What is the difference between parametric and non-parametric tests?","text":"<ul> <li>Parametric Tests: Assume underlying statistical distributions (e.g., t-test).</li> <li>Non-Parametric Tests: Do not assume any specific distribution (e.g., Mann-Whitney U test).</li> </ul>"},{"location":"prompts/faqs/#how-do-you-perform-a-chi-squared-test-in-python","title":"How do you perform a chi-squared test in Python?","text":"<p>Using <code>scipy.stats</code>:</p> <pre><code>python\nCopy code\n\n`from scipy.stats import chi2_contingency\n\nchi2, p, dof, expected = chi2_contingency(observed_values)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-purpose-of-the-seaborn-library","title":"What is the purpose of the <code>seaborn</code> library?","text":"<p>Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics.</p>"},{"location":"prompts/faqs/#how-do-you-create-a-box-plot-in-seaborn","title":"How do you create a box plot in Seaborn?","text":"<pre><code>python\nCopy code\n\n`import seaborn as sns\n\nsns.boxplot(x='categorical_column', y='numerical_column', data=df)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-violin-plot","title":"What is a violin plot?","text":"<p>A violin plot combines a box plot and a kernel density plot to provide a richer depiction of the data distribution.</p>"},{"location":"prompts/faqs/#how-do-you-handle-multivariate-time-series-data","title":"How do you handle multivariate time series data?","text":"<p>By:</p> <ul> <li>Using models designed for multivariate data: Like VAR (Vector Autoregression).</li> <li>Feature engineering: Creating lag features for each variable.</li> </ul>"},{"location":"prompts/faqs/#what-is-an-ab-test","title":"What is an A/B test?","text":"<p>An A/B test is an experiment comparing two variants (A and B) to determine which one performs better regarding a specific metric.</p>"},{"location":"prompts/faqs/#how-do-you-analyze-ab-test-results","title":"How do you analyze A/B test results?","text":"<p>By:</p> <ul> <li>Calculating the difference in metrics between groups.</li> <li>Performing statistical tests: Like t-tests or chi-squared tests.</li> <li>Checking for statistical significance: Using p-values and confidence intervals.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-bonferroni-correction","title":"What is the Bonferroni correction?","text":"<p>A method to adjust p-values when multiple comparisons are made to reduce the chances of obtaining false-positive results (Type I errors).</p>"},{"location":"prompts/faqs/#what-is-survivorship-bias","title":"What is survivorship bias?","text":"<p>Survivorship bias occurs when analyses are conducted only on surviving subjects, leading to skewed results due to the overlooking of those that did not survive or were not included.</p>"},{"location":"prompts/faqs/#what-is-data-leakage","title":"What is data leakage?","text":"<p>Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates that won't generalize to new data.</p>"},{"location":"prompts/faqs/#how-do-you-prevent-data-leakage","title":"How do you prevent data leakage?","text":"<p>By:</p> <ul> <li>Separating training and testing data properly.</li> <li>Performing data preprocessing steps within cross-validation.</li> <li>Avoiding using future data in model training.</li> </ul>"},{"location":"prompts/faqs/#what-is-a-hash-table","title":"What is a hash table?","text":"<p>A hash table is a data structure that implements an associative array, mapping keys to values using a hash function to compute an index into an array of buckets.</p>"},{"location":"prompts/faqs/#what-is-memoization","title":"What is memoization?","text":"<p>Memoization is an optimization technique used primarily to speed up computer programs by storing the results of expensive function calls and returning the cached result when the same inputs occur again.</p>"},{"location":"prompts/faqs/#how-do-you-reverse-a-linked-list","title":"How do you reverse a linked list?","text":"<p>By iterating through the list and reversing the pointers of each node to point to the previous node.</p>"},{"location":"prompts/faqs/#what-is-the-time-complexity-of-quicksort","title":"What is the time complexity of quicksort?","text":"<p>The average time complexity is O(n log n), while the worst-case is O(n^2).</p>"},{"location":"prompts/faqs/#what-is-a-generator-in-python","title":"What is a generator in Python?","text":"<p>A generator is a special type of function that returns an iterator object which can iterate over a sequence of values. It uses the <code>yield</code> keyword.</p> <pre><code>python\nCopy code\n\n`def my_generator():\n    yield value\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-handle-exceptions-in-python","title":"How do you handle exceptions in Python?","text":"<p>Using try-except blocks:</p> <pre><code>python\nCopy code\n\n`try:\n    # Code that may raise an exception\nexcept ExceptionType as e:\n    # Code to handle the exception\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-a-decorator-in-python","title":"What is a decorator in Python?","text":"<p>A decorator is a function that modifies the behavior of another function or method. It allows for the addition of functionality to existing code in a modular way.</p> <pre><code>python\nCopy code\n\n`def decorator_function(func):\n    def wrapper():\n        # Code before function call\n        func()\n        # Code after function call\n    return wrapper\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-read-and-write-json-files-in-python","title":"How do you read and write JSON files in Python?","text":"<p>Using the <code>json</code> module:</p> <pre><code>python\nCopy code\n\n`import json\n\n# Read JSON\nwith open('file.json', 'r') as f:\n    data = json.load(f)\n\n# Write JSON\nwith open('file.json', 'w') as f:\n    json.dump(data, f)\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-multithreading-and-how-do-you-implement-it-in-python","title":"What is multithreading, and how do you implement it in Python?","text":"<p>Multithreading allows concurrent execution of threads (lightweight processes) to improve performance. In Python, you can use the <code>threading</code> module:</p> <pre><code>python\nCopy code\n\n`import threading\n\ndef function_to_run():\n    pass\n\nthread = threading.Thread(target=function_to_run)\nthread.start()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-the-global-interpreter-lock-gil-in-python","title":"What is the Global Interpreter Lock (GIL) in Python?","text":"<p>The GIL is a mutex that protects access to Python objects, preventing multiple native threads from executing Python bytecodes at once. It simplifies memory management but can limit performance in CPU-bound multi-threaded programs.</p>"},{"location":"prompts/faqs/#how-do-you-handle-file-operations-in-python","title":"How do you handle file operations in Python?","text":"<p>Using <code>open()</code>:</p> <pre><code>python\nCopy code\n\n`with open('file.txt', 'r') as file:\n    content = file.read()\n`\n</code></pre>"},{"location":"prompts/faqs/#what-are-lambda-functions-in-python","title":"What are lambda functions in Python?","text":"<p>Lambda functions are anonymous functions defined using the <code>lambda</code> keyword, useful for short, simple functions.</p> <pre><code>python\nCopy code\n\n`add = lambda x, y: x + y\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-work-with-databases-in-python","title":"How do you work with databases in Python?","text":"<p>By using database connectors and ORMs like:</p> <ul> <li>SQLite: Using <code>sqlite3</code> module.</li> <li>MySQL: Using <code>mysql-connector-python</code>.</li> <li>ORMs: Using libraries like SQLAlchemy.</li> </ul>"},{"location":"prompts/faqs/#what-is-the-purpose-of-virtual-environments-in-python","title":"What is the purpose of virtual environments in Python?","text":"<p>Virtual environments allow you to create isolated Python environments with specific packages and dependencies, preventing conflicts between projects.</p> <pre><code>bash\nCopy code\n\n`python -m venv myenv\n`\n</code></pre>"},{"location":"prompts/faqs/#how-do-you-install-packages-in-python","title":"How do you install packages in Python?","text":"<p>Using <code>pip</code>:</p> <pre><code>bash\nCopy code\n\n`pip install package_name\n`\n</code></pre>"},{"location":"prompts/faqs/#what-are-the-common-data-types-in-python","title":"What are the common data types in Python?","text":"<ul> <li>Numeric Types: int, float, complex</li> <li>Sequence Types: list, tuple, range</li> <li>Text Type: str</li> <li>Mapping Type: dict</li> <li>Set Types: set, frozenset</li> <li>Boolean Type: bool</li> </ul>"},{"location":"prompts/faqs/#how-do-you-create-a-class-in-python","title":"How do you create a class in Python?","text":"<pre><code>python\nCopy code\n\n`class MyClass:\n    def __init__(self, attribute):\n        self.attribute = attribute\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-inheritance-in-python","title":"What is inheritance in Python?","text":"<p>Inheritance allows a class (child) to inherit attributes and methods from another class (parent), promoting code reusability.</p> <pre><code>python\nCopy code\n\n`class ChildClass(ParentClass):\n    pass\n`\n</code></pre>"},{"location":"prompts/faqs/#what-is-polymorphism-in-python","title":"What is polymorphism in Python?","text":"<p>Polymorphism allows methods to have the same name but behave differently in different classes. It enables methods to be used interchangeably.</p> <p>o1</p>"},{"location":"prompts/glossary/","title":"Glossary Generation","text":""},{"location":"prompts/glossary/#prompt","title":"Prompt","text":"<p>Prompt</p> <p>Please generate an glossary of terms for the 250 most frequently used terms used in an introductory course  in introduction to data science with Python. Make sure to include the names of Python libraries used in data science. The target audience for this glossary is college freshmen. Each term should have a term name in a level-4 markdown (####) and the definition placed in the body text. Do not use the term in the definition of the term. The definition should be precise, concise and distinct. If appropriate, create an Example: of how that term is used in the Introduction to Data Science with Python course.  Do not place a newline after the Example: Return the glossary of terms in alphabetical order. A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"prompts/glossary/#accuracy","title":"Accuracy","text":"<p>A metric used to evaluate classification models, representing the proportion of correct predictions over the total number of predictions.</p> <p>Example: In a spam email classifier, if the model correctly identifies 90 out of 100 emails, the accuracy is 90%.</p>"},{"location":"prompts/glossary/#algorithm","title":"Algorithm","text":"<p>A step-by-step procedure or set of rules designed to perform a specific task or solve a problem.</p> <p>Example: Implementing the k-means clustering algorithm to group similar data points in an unsupervised learning task.</p>"},{"location":"prompts/glossary/#anaconda","title":"Anaconda","text":"<p>A free and open-source distribution of Python and R programming languages for scientific computing and data science.</p> <p>Example: Using Anaconda to manage Python packages and environments for data analysis projects in the course.</p>"},{"location":"prompts/glossary/#analytics","title":"Analytics","text":"<p>The examining data sets to draw conclusions about the information they contain, often using specialized software and statistical techniques.</p> <p>Example: Performing customer behavior analytics using Pandas and Matplotlib to improve marketing strategies.</p> <p>```</p>"},{"location":"prompts/glossary/#chatgpt-o1-preview-response","title":"ChatGPT o1-preview Response","text":""},{"location":"prompts/glossary/#accuracy_1","title":"Accuracy","text":"<p>A metric used to evaluate classification models, representing the proportion of correct predictions over the total number of predictions.</p> <p>Example: In a spam email classifier, if the model correctly identifies 90 out of 100 emails, the accuracy is 90%.</p>"},{"location":"prompts/glossary/#algorithm_1","title":"Algorithm","text":"<p>A step-by-step procedure or set of rules designed to perform a specific task or solve a problem.</p> <p>Example: Implementing the k-means clustering algorithm to group similar data points in an unsupervised learning task.</p>"},{"location":"prompts/glossary/#anaconda_1","title":"Anaconda","text":"<p>A free and open-source distribution of Python and R programming languages for scientific computing and data science.</p> <p>Example: Using Anaconda to manage Python packages and environments for data analysis projects in the course.</p>"},{"location":"prompts/glossary/#analytics_1","title":"Analytics","text":"<p>The examining data sets to draw conclusions about the information they contain, often using specialized software and statistical techniques.</p> <p>Example: Performing customer behavior analytics using Pandas and Matplotlib to improve marketing strategies.</p>"},{"location":"prompts/glossary/#api-application-programming-interface","title":"API (Application Programming Interface)","text":"<p>An API is a set of protocols and tools that allow different software applications to communicate with each other.</p> <p>Example: Utilizing the Twitter API to collect real-time tweets for sentiment analysis in Python.</p>"},{"location":"prompts/glossary/#array","title":"Array","text":"<p>An array is a data structure that stores a collection of items at contiguous memory locations, allowing for efficient indexing.</p> <p>Example: Using NumPy arrays to perform vectorized operations for faster numerical computations.</p>"},{"location":"prompts/glossary/#artificial-intelligence-ai","title":"Artificial Intelligence (AI)","text":"<p>AI is the simulation of human intelligence processes by machines, especially computer systems, enabling them to perform tasks that typically require human intelligence.</p> <p>Example: Exploring AI concepts by implementing machine learning models that can recognize images or understand natural language.</p>"},{"location":"prompts/glossary/#attribute","title":"Attribute","text":"<p>An attribute refers to a variable or feature in a dataset that represents a characteristic of the data points.</p> <p>Example: In a dataset of cars, attributes might include horsepower, weight, and fuel efficiency.</p>"},{"location":"prompts/glossary/#auc-area-under-the-curve","title":"AUC (Area Under the Curve)","text":"<p>AUC is a performance metric for classification models, representing the area under the Receiver Operating Characteristic (ROC) curve.</p> <p>Example: Comparing models by evaluating their AUC scores to determine which has better classification performance.</p>"},{"location":"prompts/glossary/#bagging","title":"Bagging","text":"<p>Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that improves model stability and accuracy by combining predictions from multiple models trained on random subsets of the data.</p> <p>Example: Implementing bagging with decision trees to reduce variance and prevent overfitting in the course project.</p>"},{"location":"prompts/glossary/#bar-chart","title":"Bar Chart","text":"<p>A bar chart is a graphical representation of data using rectangular bars to show the frequency or value of different categories.</p> <p>Example: Creating a bar chart with Matplotlib to visualize the count of different species in an ecological dataset.</p>"},{"location":"prompts/glossary/#bias","title":"Bias","text":"<p>Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a much simpler model.</p> <p>Example: Recognizing high bias in a linear model that underfits the data during regression analysis.</p>"},{"location":"prompts/glossary/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>The bias-variance tradeoff is the balance between a model's ability to generalize to new data (variance) and its accuracy on the training data (bias).</p> <p>Example: Adjusting the complexity of a model to find the optimal point where both bias and variance are minimized.</p>"},{"location":"prompts/glossary/#big-data","title":"Big Data","text":"<p>Big Data refers to datasets that are too large or complex for traditional data-processing software to handle efficiently.</p> <p>Example: Discussing how tools like Hadoop or Spark can process big data in the context of data science.</p>"},{"location":"prompts/glossary/#box-plot","title":"Box Plot","text":"<p>A box plot is a graphical representation of data that displays the distribution's quartiles and averages, highlighting the median and outliers.</p> <p>Example: Using Seaborn to create box plots for visualizing the distribution of test scores across different classrooms.</p>"},{"location":"prompts/glossary/#bootstrapping","title":"Bootstrapping","text":"<p>Bootstrapping is a statistical resampling technique that involves repeatedly drawing samples from a dataset with replacement to estimate a population parameter.</p> <p>Example: Applying bootstrapping methods to estimate confidence intervals for a sample mean in a data analysis assignment.</p>"},{"location":"prompts/glossary/#classification","title":"Classification","text":"<p>Classification is a supervised learning task where the goal is to predict discrete labels or categories for given input data.</p> <p>Example: Building a logistic regression model to classify emails as spam or not spam.</p>"},{"location":"prompts/glossary/#clustering","title":"Clustering","text":"<p>Clustering is an unsupervised learning technique that groups similar data points together based on their features.</p> <p>Example: Using k-means clustering to segment customers into different groups based on purchasing behavior.</p>"},{"location":"prompts/glossary/#confusion-matrix","title":"Confusion Matrix","text":"<p>A confusion matrix is a table used to evaluate the performance of a classification model by comparing predicted and actual labels.</p> <p>Example: Analyzing a confusion matrix to calculate precision and recall for a disease diagnosis model.</p>"},{"location":"prompts/glossary/#correlation","title":"Correlation","text":"<p>Correlation measures the statistical relationship between two variables, indicating how one may predict the other.</p> <p>Example: Calculating the correlation coefficient between hours studied and exam scores to determine their relationship.</p>"},{"location":"prompts/glossary/#cross-validation","title":"Cross-Validation","text":"<p>Cross-validation is a technique for assessing how a predictive model will perform on an independent dataset by partitioning the data into complementary subsets for training and validation.</p> <p>Example: Using k-fold cross-validation to evaluate the generalization performance of a machine learning model.</p>"},{"location":"prompts/glossary/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>CSV is a file format that uses commas to separate values, commonly used for storing tabular data.</p> <p>Example: Importing a CSV file into a Pandas DataFrame to begin data analysis.</p>"},{"location":"prompts/glossary/#dataframe","title":"DataFrame","text":"<p>A DataFrame is a two-dimensional labeled data structure in Pandas, similar to a spreadsheet or SQL table.</p> <p>Example: Manipulating data stored in a DataFrame to clean and prepare it for analysis.</p>"},{"location":"prompts/glossary/#data-mining","title":"Data Mining","text":"<p>Data mining is the process of discovering patterns and knowledge from large amounts of data.</p> <p>Example: Extracting useful information from a large customer database to identify purchasing trends.</p>"},{"location":"prompts/glossary/#data-science","title":"Data Science","text":"<p>Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights from structured and unstructured data.</p> <p>Example: Applying data science techniques to analyze social media data for sentiment analysis.</p>"},{"location":"prompts/glossary/#data-visualization","title":"Data Visualization","text":"<p>Data visualization is the graphical representation of data to help people understand complex data easily.</p> <p>Example: Creating interactive dashboards using Matplotlib or Seaborn to present findings.</p>"},{"location":"prompts/glossary/#decision-tree","title":"Decision Tree","text":"<p>A decision tree is a flowchart-like structure used for making decisions or predictions based on input features.</p> <p>Example: Building a decision tree classifier to predict whether a loan application should be approved.</p>"},{"location":"prompts/glossary/#deep-learning","title":"Deep Learning","text":"<p>Deep learning is a subset of machine learning involving neural networks with multiple layers that can learn representations from data.</p> <p>Example: Exploring deep learning concepts by creating a neural network for image recognition tasks.</p>"},{"location":"prompts/glossary/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction involves reducing the number of input variables in a dataset while retaining as much information as possible.</p> <p>Example: Using Principal Component Analysis (PCA) to reduce features before training a model.</p>"},{"location":"prompts/glossary/#distribution","title":"Distribution","text":"<p>A distribution describes how values of a variable are spread or dispersed.</p> <p>Example: Plotting the normal distribution of test scores to analyze class performance.</p>"},{"location":"prompts/glossary/#dummy-variable","title":"Dummy Variable","text":"<p>A dummy variable is a binary variable created to include categorical data in regression models.</p> <p>Example: Converting categorical variables like 'Gender' into dummy variables for a regression analysis.</p>"},{"location":"prompts/glossary/#encoding","title":"Encoding","text":"<p>Encoding transforms data into a different format using a specific scheme.</p> <p>Example: Applying one-hot encoding to convert categorical variables into numerical format for machine learning models.</p>"},{"location":"prompts/glossary/#ensemble-learning","title":"Ensemble Learning","text":"<p>Ensemble learning combines predictions from multiple machine learning models to improve overall performance.</p> <p>Example: Using a random forest, which is an ensemble of decision trees, to enhance prediction accuracy.</p>"},{"location":"prompts/glossary/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<p>EDA is an approach to analyzing data sets to summarize their main characteristics, often using visual methods.</p> <p>Example: Performing EDA to detect anomalies and patterns before building predictive models.</p>"},{"location":"prompts/glossary/#feature-engineering","title":"Feature Engineering","text":"<p>Feature engineering involves creating new input features from existing ones to improve model performance.</p> <p>Example: Combining 'Date of Birth' and 'Current Date' to create a new feature 'Age' for a predictive model.</p>"},{"location":"prompts/glossary/#feature-scaling","title":"Feature Scaling","text":"<p>Feature scaling adjusts the range of features in the data to ensure they contribute equally to the model.</p> <p>Example: Applying standardization to features before using gradient descent algorithms.</p>"},{"location":"prompts/glossary/#feature-selection","title":"Feature Selection","text":"<p>Feature selection is the process of selecting a subset of relevant features for model construction.</p> <p>Example: Using correlation analysis to remove redundant features that do not improve the model.</p>"},{"location":"prompts/glossary/#function","title":"Function","text":"<p>In programming, a function is a block of organized, reusable code that performs a single action.</p> <p>Example: Defining a Python function to calculate the mean of a list of numbers in data analysis.</p>"},{"location":"prompts/glossary/#f1-score","title":"F1 Score","text":"<p>The F1 score is the harmonic mean of precision and recall, used as a measure of a test's accuracy.</p> <p>Example: Evaluating a classification model with imbalanced classes using the F1 score.</p>"},{"location":"prompts/glossary/#gradient-boosting","title":"Gradient Boosting","text":"<p>Gradient Boosting is an ensemble technique that builds models sequentially, each correcting the errors of the previous one.</p> <p>Example: Implementing Gradient Boosting Machines (GBM) to improve prediction accuracy on complex datasets.</p>"},{"location":"prompts/glossary/#gradient-descent","title":"Gradient Descent","text":"<p>Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.</p> <p>Example: Using gradient descent to find the optimal weights in a linear regression model.</p>"},{"location":"prompts/glossary/#grid-search","title":"Grid Search","text":"<p>Grid search is a hyperparameter optimization technique that exhaustively searches through a specified subset of hyperparameters.</p> <p>Example: Applying grid search to find the best combination of parameters for a support vector machine classifier.</p>"},{"location":"prompts/glossary/#histogram","title":"Histogram","text":"<p>A histogram is a graphical representation showing the distribution of numerical data by depicting the number of data points that fall within specified ranges.</p> <p>Example: Creating a histogram to visualize the frequency distribution of ages in a dataset.</p>"},{"location":"prompts/glossary/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Hyperparameter tuning involves adjusting the parameters that govern the training process of a model to improve performance.</p> <p>Example: Tuning the number of trees and depth in a random forest model to achieve better accuracy.</p>"},{"location":"prompts/glossary/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Hypothesis testing is a statistical method used to make decisions about the properties of a population based on sample data.</p> <p>Example: Conducting a t-test to determine if there is a significant difference between two groups' means.</p>"},{"location":"prompts/glossary/#imputation","title":"Imputation","text":"<p>Imputation is the process of replacing missing data with substituted values.</p> <p>Example: Filling missing values in a dataset with the mean or median of the column.</p>"},{"location":"prompts/glossary/#inferential-statistics","title":"Inferential Statistics","text":"<p>Inferential statistics use a random sample of data taken from a population to describe and make inferences about the population.</p> <p>Example: Estimating the average height of all students in a university by sampling a subset.</p>"},{"location":"prompts/glossary/#interpolation","title":"Interpolation","text":"<p>Interpolation is a method of estimating unknown values that fall between known data points.</p> <p>Example: Using interpolation to estimate missing temperature readings in a time series dataset.</p>"},{"location":"prompts/glossary/#json-javascript-object-notation","title":"JSON (JavaScript Object Notation)","text":"<p>JSON is a lightweight data-interchange format that is easy for humans to read and write and for machines to parse and generate.</p> <p>Example: Reading data from a JSON file into a Pandas DataFrame for analysis.</p>"},{"location":"prompts/glossary/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>Jupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text.</p> <p>Example: Using Jupyter Notebook to write Python code and document the data analysis process.</p>"},{"location":"prompts/glossary/#k-means-clustering","title":"k-means Clustering","text":"<p>k-means clustering is an unsupervised learning algorithm that partitions data into k distinct clusters based on feature similarity.</p> <p>Example: Segmenting customers into groups based on purchasing behavior using k-means clustering.</p>"},{"location":"prompts/glossary/#k-nearest-neighbors-knn","title":"K-Nearest Neighbors (KNN)","text":"<p>KNN is a simple, supervised machine learning algorithm that classifies new cases based on the majority class of their k nearest neighbors.</p> <p>Example: Implementing KNN to predict whether a patient has a certain disease based on symptoms.</p>"},{"location":"prompts/glossary/#label-encoding","title":"Label Encoding","text":"<p>Label encoding converts categorical text data into numerical values by assigning a unique integer to each category.</p> <p>Example: Transforming the 'Color' feature into numerical labels before model training.</p>"},{"location":"prompts/glossary/#learning-rate","title":"Learning Rate","text":"<p>The learning rate is a hyperparameter that controls how much we adjust the model weights with respect to the loss gradient.</p> <p>Example: Setting an appropriate learning rate in gradient descent to ensure the model converges.</p>"},{"location":"prompts/glossary/#linear-regression","title":"Linear Regression","text":"<p>Linear regression is a supervised learning algorithm that models the relationship between a dependent variable and one or more independent variables.</p> <p>Example: Predicting house prices based on features like size and location using linear regression.</p>"},{"location":"prompts/glossary/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic regression is a classification algorithm used to predict the probability of a categorical dependent variable.</p> <p>Example: Using logistic regression to determine the likelihood of a customer churning.</p>"},{"location":"prompts/glossary/#machine-learning","title":"Machine Learning","text":"<p>Machine learning is a subset of AI that focuses on building systems that learn from and make decisions based on data.</p> <p>Example: Implementing various machine learning algorithms to solve classification and regression problems in the course.</p>"},{"location":"prompts/glossary/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib is a Python library used for creating static, animated, and interactive visualizations.</p> <p>Example: Plotting data trends using Matplotlib to support data analysis conclusions.</p>"},{"location":"prompts/glossary/#missing-data","title":"Missing Data","text":"<p>Missing data occurs when no value is stored for a variable in an observation, which can impact data analysis.</p> <p>Example: Identifying and handling missing data in a dataset before model training.</p>"},{"location":"prompts/glossary/#model-selection","title":"Model Selection","text":"<p>Model selection involves choosing the best model from a set of candidates based on their predictive performance.</p> <p>Example: Comparing different algorithms like decision trees and logistic regression to select the best model for a classification task.</p>"},{"location":"prompts/glossary/#multicollinearity","title":"Multicollinearity","text":"<p>Multicollinearity occurs when independent variables in a regression model are highly correlated, which can affect the model's stability.</p> <p>Example: Detecting multicollinearity using the Variance Inflation Factor (VIF) and addressing it in the dataset.</p>"},{"location":"prompts/glossary/#natural-language-processing-nlp","title":"Natural Language Processing (NLP)","text":"<p>NLP is a field of AI that gives computers the ability to understand, interpret, and generate human language.</p> <p>Example: Analyzing text data for sentiment analysis using NLP techniques.</p>"},{"location":"prompts/glossary/#neural-network","title":"Neural Network","text":"<p>A neural network is a series of algorithms that mimic the operations of a human brain to recognize patterns and solve complex problems.</p> <p>Example: Building a simple neural network to classify images of handwritten digits.</p>"},{"location":"prompts/glossary/#normal-distribution","title":"Normal Distribution","text":"<p>The normal distribution is a continuous probability distribution characterized by a symmetrical, bell-shaped curve.</p> <p>Example: Assuming normal distribution of residuals in linear regression models.</p>"},{"location":"prompts/glossary/#normalization","title":"Normalization","text":"<p>Normalization scales data to fit within a specific range, often between 0 and 1, to ensure all features contribute equally.</p> <p>Example: Applying Min-Max normalization to features before training a neural network.</p>"},{"location":"prompts/glossary/#numpy","title":"NumPy","text":"<p>NumPy is a Python library used for working with arrays and providing functions for mathematical operations on large, multi-dimensional arrays and matrices.</p> <p>Example: Using NumPy arrays for efficient numerical computations in data science projects.</p>"},{"location":"prompts/glossary/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>One-hot encoding converts categorical variables into a binary matrix representation.</p> <p>Example: Transforming the 'Country' feature into multiple binary columns representing each country.</p>"},{"location":"prompts/glossary/#optimization","title":"Optimization","text":"<p>Optimization involves adjusting the inputs or parameters of a model to minimize or maximize some objective function.</p> <p>Example: Optimizing the weights in a neural network to reduce the loss function during training.</p>"},{"location":"prompts/glossary/#outlier","title":"Outlier","text":"<p>An outlier is a data point that differs significantly from other observations, potentially indicating variability in measurement or experimental errors.</p> <p>Example: Identifying outliers in a dataset using box plots and deciding whether to remove or transform them.</p>"},{"location":"prompts/glossary/#overfitting","title":"Overfitting","text":"<p>Overfitting occurs when a model learns the training data too well, capturing noise and details that negatively impact its performance on new data.</p> <p>Example: Preventing overfitting by using regularization techniques and cross-validation.</p>"},{"location":"prompts/glossary/#pandas","title":"Pandas","text":"<p>Pandas is a Python library providing high-performance, easy-to-use data structures and data analysis tools.</p> <p>Example: Using Pandas DataFrames to manipulate and analyze tabular data in the course.</p>"},{"location":"prompts/glossary/#parameter","title":"Parameter","text":"<p>A parameter is a configuration variable that is internal to the model and estimated from data.</p> <p>Example: The coefficients in a linear regression model are parameters learned during training.</p>"},{"location":"prompts/glossary/#pca-principal-component-analysis","title":"PCA (Principal Component Analysis)","text":"<p>PCA is a dimensionality reduction technique that transforms data into a new coordinate system, reducing the number of variables while retaining most information.</p> <p>Example: Applying PCA to reduce the dimensionality of a dataset before clustering.</p>"},{"location":"prompts/glossary/#pipeline","title":"Pipeline","text":"<p>A pipeline is a sequence of data processing components or steps, where the output of one component is the input to the next.</p> <p>Example: Creating a scikit-learn pipeline to standardize data and train a model in a single workflow.</p>"},{"location":"prompts/glossary/#precision","title":"Precision","text":"<p>Precision is a metric that measures the proportion of true positives among all positive predictions.</p> <p>Example: Calculating precision to evaluate a model where false positives are costly, such as in fraud detection.</p>"},{"location":"prompts/glossary/#predictive-modeling","title":"Predictive Modeling","text":"<p>Predictive modeling uses statistics and data to predict outcomes with data models.</p> <p>Example: Building a predictive model to forecast sales based on historical data.</p>"},{"location":"prompts/glossary/#probability-distribution","title":"Probability Distribution","text":"<p>A probability distribution describes how the values of a random variable are distributed.</p> <p>Example: Using the normal distribution to model the heights of individuals in a population.</p>"},{"location":"prompts/glossary/#python","title":"Python","text":"<p>Python is a high-level, interpreted programming language known for its readability and versatility in data science.</p> <p>Example: Writing Python scripts to automate data cleaning and analysis tasks.</p>"},{"location":"prompts/glossary/#random-forest","title":"Random Forest","text":"<p>Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes for classification tasks.</p> <p>Example: Implementing a random forest classifier to improve accuracy over a single decision tree.</p>"},{"location":"prompts/glossary/#regression","title":"Regression","text":"<p>Regression is a set of statistical processes for estimating the relationships among variables.</p> <p>Example: Performing linear regression to understand how the price of a house varies with its size.</p>"},{"location":"prompts/glossary/#regularization","title":"Regularization","text":"<p>Regularization adds a penalty to the loss function to prevent overfitting by discouraging complex models.</p> <p>Example: Applying Lasso regularization to reduce overfitting in a regression model.</p>"},{"location":"prompts/glossary/#recall","title":"Recall","text":"<p>Recall is a metric that measures the proportion of actual positives correctly identified.</p> <p>Example: Evaluating recall in a medical diagnosis model where missing a positive case is critical.</p>"},{"location":"prompts/glossary/#roc-curve-receiver-operating-characteristic-curve","title":"ROC Curve (Receiver Operating Characteristic Curve)","text":"<p>An ROC curve is a graphical plot illustrating the diagnostic ability of a binary classifier as its discrimination threshold is varied.</p> <p>Example: Plotting the ROC curve to select the optimal threshold for a classification model.</p>"},{"location":"prompts/glossary/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>RMSE is a metric used to measure the difference between values predicted by a model and the actual values.</p> <p>Example: Using RMSE to assess the performance of a regression model predicting housing prices.</p>"},{"location":"prompts/glossary/#sampling","title":"Sampling","text":"<p>Sampling involves selecting a subset of data from a larger dataset to estimate characteristics of the whole population.</p> <p>Example: Drawing a random sample from a large dataset to make computations more manageable.</p>"},{"location":"prompts/glossary/#scikit-learn","title":"Scikit-learn","text":"<p>Scikit-learn is a Python library for machine learning that provides simple and efficient tools for data analysis and modeling.</p> <p>Example: Using scikit-learn to implement machine learning algorithms like SVMs and random forests.</p>"},{"location":"prompts/glossary/#seaborn","title":"Seaborn","text":"<p>Seaborn is a Python data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics.</p> <p>Example: Creating complex visualizations like heatmaps and violin plots using Seaborn.</p>"},{"location":"prompts/glossary/#smote-synthetic-minority-over-sampling-technique","title":"SMOTE (Synthetic Minority Over-sampling Technique)","text":"<p>SMOTE is a technique used to address class imbalance by generating synthetic samples of the minority class.</p> <p>Example: Applying SMOTE to balance the dataset before training a classifier on imbalanced data.</p>"},{"location":"prompts/glossary/#standard-deviation","title":"Standard Deviation","text":"<p>Standard deviation measures the amount of variation or dispersion in a set of values.</p> <p>Example: Calculating the standard deviation to understand the spread of exam scores in a class.</p>"},{"location":"prompts/glossary/#standardscaler","title":"StandardScaler","text":"<p>StandardScaler is a scikit-learn tool that standardizes features by removing the mean and scaling to unit variance.</p> <p>Example: Using StandardScaler to preprocess data before feeding it into a machine learning algorithm.</p>"},{"location":"prompts/glossary/#statistical-significance","title":"Statistical Significance","text":"<p>Statistical significance indicates that the result of a test is unlikely to have occurred by chance alone.</p> <p>Example: Interpreting p-values to determine if the difference between two groups is statistically significant.</p>"},{"location":"prompts/glossary/#supervised-learning","title":"Supervised Learning","text":"<p>Supervised learning is a type of machine learning where models are trained using labeled data.</p> <p>Example: Training a supervised learning model to predict house prices based on historical data.</p>"},{"location":"prompts/glossary/#time-series","title":"Time Series","text":"<p>Time series data is a sequence of data points collected or recorded at time intervals.</p> <p>Example: Analyzing stock prices over time to forecast future market trends.</p>"},{"location":"prompts/glossary/#tokenization","title":"Tokenization","text":"<p>Tokenization is the process of breaking text into smaller units called tokens, often words or phrases.</p> <p>Example: Tokenizing text data for input into a natural language processing model.</p>"},{"location":"prompts/glossary/#training-set","title":"Training Set","text":"<p>A training set is a subset of the dataset used to train machine learning models.</p> <p>Example: Splitting data into training and test sets to build and evaluate a model.</p>"},{"location":"prompts/glossary/#t-test","title":"T-test","text":"<p>A t-test is a statistical test used to compare the means of two groups.</p> <p>Example: Performing a t-test to determine if there is a significant difference in test scores between two classes.</p>"},{"location":"prompts/glossary/#underfitting","title":"Underfitting","text":"<p>Underfitting occurs when a model is too simple and fails to capture the underlying pattern of the data.</p> <p>Example: Addressing underfitting by increasing the complexity of the model or adding more features.</p>"},{"location":"prompts/glossary/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Unsupervised learning involves training models on data without labeled responses, aiming to find hidden patterns.</p> <p>Example: Using unsupervised learning techniques like clustering to segment customers.</p>"},{"location":"prompts/glossary/#validation-set","title":"Validation Set","text":"<p>A validation set is a subset of the dataset used to tune hyperparameters and prevent overfitting during model training.</p> <p>Example: Using a validation set to adjust the learning rate and number of layers in a neural network.</p>"},{"location":"prompts/glossary/#variance","title":"Variance","text":"<p>Variance measures how far a set of numbers is spread out from their average value.</p> <p>Example: Calculating the variance to understand the variability in a dataset.</p>"},{"location":"prompts/glossary/#visualization","title":"Visualization","text":"<p>Visualization refers to the graphical representation of information and data.</p> <p>Example: Creating line charts and scatter plots to visualize trends and relationships in the data.</p>"},{"location":"prompts/glossary/#weight","title":"Weight","text":"<p>In machine learning models, weights are parameters that are learned during training to map input features to outputs.</p> <p>Example: Adjusting weights in a neural network during training to minimize the loss function.</p>"},{"location":"prompts/glossary/#z-score","title":"Z-score","text":"<p>A z-score indicates how many standard deviations an element is from the mean.</p> <p>Example: Calculating z-scores to identify outliers in a dataset.</p>"},{"location":"prompts/glossary/#xgboost","title":"XGBoost","text":"<p>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient and flexible.</p> <p>Example: Implementing XGBoost to improve model performance on a classification task.</p>"},{"location":"prompts/glossary/#confusion-matrix_1","title":"Confusion Matrix","text":"<p>A confusion matrix is a table used to describe the performance of a classification model.</p> <p>Example: Using a confusion matrix to calculate precision, recall, and F1 score for a classifier.</p>"},{"location":"prompts/glossary/#hyperparameter","title":"Hyperparameter","text":"<p>A hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.</p> <p>Example: Setting the number of neighbors in a KNN algorithm as a hyperparameter to tune.</p>"},{"location":"prompts/glossary/#kernel","title":"Kernel","text":"<p>In machine learning, a kernel is a function used in algorithms like SVM to transform data into a higher-dimensional space.</p> <p>Example: Choosing a radial basis function (RBF) kernel for an SVM to handle non-linear data.</p>"},{"location":"prompts/glossary/#lasso-regression","title":"Lasso Regression","text":"<p>Lasso regression is a type of linear regression that uses L1 regularization to reduce overfitting and perform feature selection.</p> <p>Example: Applying lasso regression to identify the most important features in a dataset.</p>"},{"location":"prompts/glossary/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>MAE is a measure of errors between paired observations expressing the same phenomenon.</p> <p>Example: Evaluating a regression model by calculating the MAE between predicted and actual values.</p>"},{"location":"prompts/glossary/#overfitting_1","title":"Overfitting","text":"<p>Overfitting occurs when a model learns the training data too well, capturing noise and details that negatively impact performance on new data.</p> <p>Example: Observing overfitting in a model that performs well on training data but poorly on test data.</p>"},{"location":"prompts/glossary/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":"<p>The Pearson correlation coefficient measures the linear correlation between two variables.</p> <p>Example: Calculating the Pearson coefficient to assess the strength of the relationship between two features.</p>"},{"location":"prompts/glossary/#r-squared-coefficient-of-determination","title":"R-Squared (Coefficient of Determination)","text":"<p>R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by independent variables.</p> <p>Example: Interpreting an R-squared value of 0.85 to mean that 85% of the variance in the dependent variable is predictable.</p>"},{"location":"prompts/glossary/#sampling-bias","title":"Sampling Bias","text":"<p>Sampling bias occurs when some members of a population are systematically more likely to be selected in a sample than others.</p> <p>Example: Ensuring random sampling in data collection to avoid sampling bias.</p>"},{"location":"prompts/glossary/#tokenization_1","title":"Tokenization","text":"<p>Tokenization is the process of splitting text into individual units (tokens), such as words or phrases.</p> <p>Example: Tokenizing customer reviews to prepare text data for sentiment analysis.</p>"},{"location":"prompts/glossary/#univariate-analysis","title":"Univariate Analysis","text":"<p>Univariate analysis examines each variable individually to summarize and find patterns.</p> <p>Example: Performing univariate analysis on the 'Age' feature to understand its distribution.</p>"},{"location":"prompts/glossary/#variance-inflation-factor-vif","title":"Variance Inflation Factor (VIF)","text":"<p>VIF quantifies the severity of multicollinearity in regression analysis.</p> <p>Example: Calculating VIF to detect multicollinearity and decide whether to remove correlated features.</p>"},{"location":"prompts/glossary/#white-noise","title":"White Noise","text":"<p>White noise refers to a time series of random data points that have a constant mean and variance.</p> <p>Example: Checking residuals for white noise to validate the assumptions of a time series model.</p>"},{"location":"prompts/glossary/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>Cross-entropy loss measures the performance of a classification model whose output is a probability between 0 and 1.</p> <p>Example: Using cross-entropy loss as the loss function in a logistic regression model.</p>"},{"location":"prompts/glossary/#epoch","title":"Epoch","text":"<p>An epoch refers to one complete pass through the entire training dataset.</p> <p>Example: Training a neural network for 10 epochs to optimize the weights.</p>"},{"location":"prompts/glossary/#fitting","title":"Fitting","text":"<p>Fitting a model involves adjusting its parameters to best match the data.</p> <p>Example: Fitting a linear regression model to the training data by minimizing the cost function.</p>"},{"location":"prompts/glossary/#hyperplane","title":"Hyperplane","text":"<p>A hyperplane is a flat affine subspace of one dimension less than its ambient space, used in SVMs to separate classes.</p> <p>Example: Understanding how an SVM finds the optimal hyperplane to classify data points.</p>"},{"location":"prompts/glossary/#iteration","title":"Iteration","text":"<p>An iteration refers to one update of the model's parameters during training.</p> <p>Example: Observing loss reduction after each iteration in gradient descent optimization.</p>"},{"location":"prompts/glossary/#learning-curve","title":"Learning Curve","text":"<p>A learning curve plots the model's performance on the training and validation sets over time or as the training set size increases.</p> <p>Example: Analyzing the learning curve to diagnose if a model is overfitting or underfitting.</p>"},{"location":"prompts/glossary/#loss-function","title":"Loss Function","text":"<p>A loss function measures how well a machine learning model performs, guiding the optimization process.</p> <p>Example: Using Mean Squared Error (MSE) as the loss function in a regression model.</p>"},{"location":"prompts/glossary/#mini-batch-gradient-descent","title":"Mini-Batch Gradient Descent","text":"<p>Mini-batch gradient descent is an optimization algorithm that updates the model parameters using small batches of data.</p> <p>Example: Accelerating training by using mini-batches instead of the entire dataset in each iteration.</p>"},{"location":"prompts/glossary/#multivariate-analysis","title":"Multivariate Analysis","text":"<p>Multivariate analysis examines the relationship between multiple variables simultaneously.</p> <p>Example: Performing multivariate regression to understand how multiple features affect the target variable.</p>"},{"location":"prompts/glossary/#natural-language-processing-nlp_1","title":"Natural Language Processing (NLP)","text":"<p>NLP focuses on the interaction between computers and human language.</p> <p>Example: Using NLP techniques to analyze customer feedback and extract key themes.</p>"},{"location":"prompts/glossary/#optimization-algorithm","title":"Optimization Algorithm","text":"<p>An optimization algorithm adjusts the parameters of a model to minimize the loss function.</p> <p>Example: Choosing Adam optimizer for faster convergence in training a neural network.</p>"},{"location":"prompts/glossary/#precision-recall-curve","title":"Precision-Recall Curve","text":"<p>A precision-recall curve plots the trade-off between precision and recall for different threshold settings.</p> <p>Example: Using the precision-recall curve to select the threshold that balances precision and recall.</p>"},{"location":"prompts/glossary/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions and receiving rewards.</p> <p>Example: Discussing reinforcement learning concepts as an advanced topic in the course.</p>"},{"location":"prompts/glossary/#stratified-sampling","title":"Stratified Sampling","text":"<p>Stratified sampling involves dividing the population into subgroups and sampling from each to ensure representation.</p> <p>Example: Using stratified sampling to maintain the class distribution in training and test sets.</p>"},{"location":"prompts/glossary/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<p>SVM is a supervised learning algorithm that finds the hyperplane that best separates classes.</p> <p>Example: Implementing an SVM classifier for a binary classification problem in the course.</p>"},{"location":"prompts/glossary/#synthetic-data","title":"Synthetic Data","text":"<p>Synthetic data is artificially generated data that mimics the properties of real data.</p> <p>Example: Generating synthetic data to augment the dataset and improve model training.</p>"},{"location":"prompts/glossary/#training-loss","title":"Training Loss","text":"<p>Training loss measures the error on the training dataset during model training.</p> <p>Example: Monitoring training loss to assess how well the model is learning from the training data.</p>"},{"location":"prompts/glossary/#type-i-error","title":"Type I Error","text":"<p>A Type I error occurs when the null hypothesis is true but is incorrectly rejected.</p> <p>Example: Understanding Type I errors when interpreting p-values in hypothesis testing.</p>"},{"location":"prompts/glossary/#type-ii-error","title":"Type II Error","text":"<p>A Type II error occurs when the null hypothesis is false but erroneously fails to be rejected.</p> <p>Example: Recognizing the implications of Type II errors in statistical testing.</p>"},{"location":"prompts/glossary/#validation-loss","title":"Validation Loss","text":"<p>Validation loss measures the error on the validation dataset, used to tune model hyperparameters.</p> <p>Example: Observing validation loss to detect overfitting during model training.</p>"},{"location":"prompts/glossary/#weight-initialization","title":"Weight Initialization","text":"<p>Weight initialization is the process of setting the initial values of the weights before training a neural network.</p> <p>Example: Using random initialization methods to start training a deep learning model.</p>"},{"location":"prompts/glossary/#word-embedding","title":"Word Embedding","text":"<p>Word embedding is a representation of text where words with similar meaning have similar vector representations.</p> <p>Example: Implementing word embeddings like Word2Vec in NLP tasks.</p>"},{"location":"prompts/glossary/#xgboost_1","title":"XGBoost","text":"<p>XGBoost is an optimized gradient boosting library designed for performance and speed.</p> <p>Example: Using XGBoost to improve model accuracy in classification problems.</p>"},{"location":"prompts/glossary/#z-score-normalization","title":"Z-score Normalization","text":"<p>Z-score normalization scales data based on mean and standard deviation.</p> <p>Example: Applying z-score normalization to standardize features before training a model.</p> <p>o1</p>"},{"location":"sims/","title":"List of MicroSims for Data Science","text":"<p>Learning Graph</p>"},{"location":"sims/100-microsims/","title":"100 MicroSims for Introduction to Data Science with Python","text":""},{"location":"sims/100-microsims/#1-exploring-data-points","title":"1. Exploring Data Points","text":""},{"location":"sims/100-microsims/#description","title":"Description","text":"<p>Students click to add or remove points on a 2D scatter plot, instantly seeing the effect on the overall distribution.</p>"},{"location":"sims/100-microsims/#learning-goals","title":"Learning Goals","text":"<ul> <li>Recognize individual observations in a dataset</li> <li>Understand x\u2013y coordinate representation</li> <li>See how adding/removing points changes data shape</li> </ul>"},{"location":"sims/100-microsims/#input-controls","title":"Input Controls","text":"<ol> <li>Add Point (click on canvas)  </li> <li>Remove Point (click existing point)  </li> <li>Clear All Points (button)  </li> </ol>"},{"location":"sims/100-microsims/#2-histogram-builder","title":"2. Histogram Builder","text":""},{"location":"sims/100-microsims/#description_1","title":"Description","text":"<p>Students adjust bin sizes to see how histograms change, revealing over-smoothing and under-smoothing effects.</p>"},{"location":"sims/100-microsims/#learning-goals_1","title":"Learning Goals","text":"<ul> <li>Understand bins and frequencies</li> <li>Relate bin size to data detail retention</li> </ul>"},{"location":"sims/100-microsims/#input-controls_1","title":"Input Controls","text":"<ol> <li>Bin Size Slider </li> <li>Dataset Selector (dropdown: normal, uniform, skewed)  </li> <li>Toggle Grid Lines (checkbox)  </li> </ol>"},{"location":"sims/100-microsims/#3-mean-and-median-explorer","title":"3. Mean and Median Explorer","text":""},{"location":"sims/100-microsims/#description_2","title":"Description","text":"<p>Drag points along a number line to see how mean and median shift differently.</p>"},{"location":"sims/100-microsims/#learning-goals_2","title":"Learning Goals","text":"<ul> <li>Differentiate mean vs. median</li> <li>Observe robustness of median to outliers</li> </ul>"},{"location":"sims/100-microsims/#input-controls_2","title":"Input Controls","text":"<ol> <li>Drag Points (mouse)  </li> <li>Add Outlier (button)  </li> <li>Reset Points (button)  </li> </ol>"},{"location":"sims/100-microsims/#4-correlation-playground","title":"4. Correlation Playground","text":""},{"location":"sims/100-microsims/#description_3","title":"Description","text":"<p>Students drag clusters of points to adjust correlation, watching the correlation coefficient update in real-time.</p>"},{"location":"sims/100-microsims/#learning-goals_3","title":"Learning Goals","text":"<ul> <li>Visualize correlation strength and direction</li> <li>Understand positive, negative, and zero correlation</li> </ul>"},{"location":"sims/100-microsims/#input-controls_3","title":"Input Controls","text":"<ol> <li>Drag Cluster (mouse)  </li> <li>Add Noise (slider)  </li> <li>Show Best Fit Line (toggle)  </li> </ol>"},{"location":"sims/100-microsims/#5-least-squares-line-fitter","title":"5. Least Squares Line Fitter","text":""},{"location":"sims/100-microsims/#description_4","title":"Description","text":"<p>Adjust slope and intercept manually to minimize sum of squared errors, with real-time residual visualization.</p>"},{"location":"sims/100-microsims/#learning-goals_4","title":"Learning Goals","text":"<ul> <li>Understand slope, intercept, and residuals</li> <li>Experience trial-and-error fitting</li> </ul>"},{"location":"sims/100-microsims/#input-controls_4","title":"Input Controls","text":"<ol> <li>Slope Slider </li> <li>Intercept Slider </li> <li>Toggle Residual Squares (checkbox)  </li> </ol>"},{"location":"sims/100-microsims/#6-r2-intuition-builder","title":"6. R\u00b2 Intuition Builder","text":""},{"location":"sims/100-microsims/#description_5","title":"Description","text":"<p>Manipulate data spread around a fitted line to see how R\u00b2 changes.</p>"},{"location":"sims/100-microsims/#learning-goals_5","title":"Learning Goals","text":"<ul> <li>Understand coefficient of determination</li> <li>Relate R\u00b2 to model fit quality</li> </ul>"},{"location":"sims/100-microsims/#input-controls_5","title":"Input Controls","text":"<ol> <li>Noise Level Slider </li> <li>Number of Points Slider </li> <li>Reset Dataset (button)  </li> </ol>"},{"location":"sims/100-microsims/#7-train-test-split-visualizer","title":"7. Train-Test Split Visualizer","text":""},{"location":"sims/100-microsims/#description_6","title":"Description","text":"<p>Randomly split a dataset and see how train/test points differ in model performance.</p>"},{"location":"sims/100-microsims/#learning-goals_6","title":"Learning Goals","text":"<ul> <li>Understand importance of splitting data</li> <li>See overfitting risk when test set is too small</li> </ul>"},{"location":"sims/100-microsims/#input-controls_6","title":"Input Controls","text":"<ol> <li>Train/Test Ratio Slider </li> <li>Resample Dataset (button)  </li> <li>Model Complexity Slider </li> </ol>"},{"location":"sims/100-microsims/#8-cross-validation-simulator","title":"8. Cross-Validation Simulator","text":""},{"location":"sims/100-microsims/#description_7","title":"Description","text":"<p>Animate k-fold cross-validation, showing shifting train/test subsets and aggregated scores.</p>"},{"location":"sims/100-microsims/#learning-goals_7","title":"Learning Goals","text":"<ul> <li>Understand cross-validation mechanics</li> <li>See benefits over single train-test split</li> </ul>"},{"location":"sims/100-microsims/#input-controls_7","title":"Input Controls","text":"<ol> <li>Number of Folds Slider </li> <li>Dataset Size Slider </li> <li>Play/Pause Animation (button)  </li> </ol>"},{"location":"sims/100-microsims/#9-overfitting-vs-underfitting-explorer","title":"9. Overfitting vs. Underfitting Explorer","text":""},{"location":"sims/100-microsims/#description_8","title":"Description","text":"<p>Adjust polynomial degree to see bias\u2013variance trade-off on train vs. test errors.</p>"},{"location":"sims/100-microsims/#learning-goals_8","title":"Learning Goals","text":"<ul> <li>Recognize overfitting and underfitting patterns</li> <li>Connect complexity to generalization</li> </ul>"},{"location":"sims/100-microsims/#input-controls_8","title":"Input Controls","text":"<ol> <li>Polynomial Degree Slider </li> <li>Noise Level Slider </li> <li>Toggle Error Curves (checkbox)  </li> </ol>"},{"location":"sims/100-microsims/#10-multiple-regression-plane","title":"10. Multiple Regression Plane","text":""},{"location":"sims/100-microsims/#description_9","title":"Description","text":"<p>Manipulate two independent variables in 3D space to see a regression plane fit to data.</p>"},{"location":"sims/100-microsims/#learning-goals_9","title":"Learning Goals","text":"<ul> <li>Visualize multivariate linear regression</li> <li>See plane adjustment with variable changes</li> </ul>"},{"location":"sims/100-microsims/#input-controls_9","title":"Input Controls","text":"<ol> <li>Rotate View (mouse drag)  </li> <li>Noise Level Slider </li> <li>Add/Remove Points (click)  </li> </ol> <p>...</p>"},{"location":"sims/learning-graph/","title":"View Learning Graph","text":"<p>Run Learning Graph MicroSim</p>"},{"location":"sims/least-squares/","title":"Least Squares","text":"<p>Run the Least Squares MicroSim Edit the Least Squares MicroSim</p>"},{"location":"sims/least-squares/#lesson-plan-for-high-school-algebra","title":"Lesson Plan for High School Algebra","text":"<p>Linear Functions: Understanding Slope and Intercept</p> <p>Duration: 50 minutes</p> <p>Grade Level: 9-10</p> <p>Subject: Algebra 1</p>"},{"location":"sims/least-squares/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, students will be able to:</p> <ul> <li>Define slope as a rate of change</li> <li>Explain the meaning of y-intercept in a linear function</li> <li>Identify how changes in slope and intercept affect the graph of a line</li> <li>Use an interactive visualization to explore linear functions</li> <li>Solve real-world problems involving slope and intercept</li> </ul>"},{"location":"sims/least-squares/#materials-needed","title":"Materials Needed","text":"<ul> <li>Interactive Slope-Intercept Visualization (p5.js application)</li> <li>Student devices (computers, tablets, or smartphones)</li> <li>Guided worksheet (printed or digital)</li> <li>Whiteboard/projector</li> </ul>"},{"location":"sims/least-squares/#prerequisite-knowledge","title":"Prerequisite Knowledge","text":"<ul> <li>Basic understanding of coordinate plane</li> <li>Ability to plot points on a graph</li> <li>Familiarity with the equation y = mx + b</li> </ul>"},{"location":"sims/least-squares/#lesson-outline","title":"Lesson Outline","text":""},{"location":"sims/least-squares/#1-introduction-5-minutes","title":"1. Introduction (5 minutes)","text":"<ul> <li>Begin with a real-world scenario: \"If you earn $15 per hour at your job, how would you calculate your total earnings?\"</li> <li>Discuss how the relationship between hours worked and money earned forms a linear relationship</li> <li>Introduce the lesson focus: understanding how slope and intercept affect linear functions</li> </ul>"},{"location":"sims/least-squares/#2-review-of-key-concepts-10-minutes","title":"2. Review of Key Concepts (10 minutes)","text":"<ul> <li>Review the slope-intercept form of a line: y = mx + b</li> <li>Define slope (m) as the rate of change (rise/run)</li> <li>Define y-intercept (b) as the point where the line crosses the y-axis (0,b)</li> <li>Demonstrate examples on the board with different values for m and b</li> </ul>"},{"location":"sims/least-squares/#3-interactive-exploration-15-minutes","title":"3. Interactive Exploration (15 minutes)","text":"<ul> <li>Introduce the Slope-Intercept Visualization tool</li> <li>Demonstrate how to use the sliders to change slope and intercept values</li> <li>Explain the visual elements:<ul> <li>Green points represent actual data points</li> <li>Purple points show where the line would predict those values</li> <li>Colored squares show the \"error\" or difference between actual and predicted points</li> </ul> </li> <li>Guided exploration:<ol> <li>What happens when the slope increases? Decreases? Becomes negative?</li> <li>What happens when the y-intercept changes?</li> <li>Can you find values that minimize the differences between actual and predicted points?</li> </ol> </li> </ul>"},{"location":"sims/least-squares/#4-pair-work-10-minutes","title":"4. Pair Work (10 minutes)","text":"<ul> <li>Students work in pairs using the visualization tool</li> <li>Challenge: Find the linear function that best fits the green data points</li> <li>Each pair should record their \"best fit\" values for slope and intercept</li> <li>Discuss strategy: How can you tell when you've found a good fit?</li> </ul>"},{"location":"sims/least-squares/#5-connection-to-real-world-applications-5-minutes","title":"5. Connection to Real-World Applications (5 minutes)","text":"<p>Discuss how the slope-intercept model applies to:</p> <ul> <li>Economics: price vs. quantity relationships</li> <li>Physics: distance vs. time in constant velocity</li> <li>Business: fixed costs (y-intercept) and variable costs (slope)</li> </ul> <p>Show how the colored squares relate to \"error\" in predictions</p>"},{"location":"sims/least-squares/#6-closure-and-assessment-5-minutes","title":"6. Closure and Assessment (5 minutes)","text":"<ul> <li>Quick check for understanding:<ul> <li>\"If a line has a slope of 2 and a y-intercept of -3, what is its equation?\"</li> <li>\"If a line has a negative slope, what does that tell us about the relationship?\"</li> </ul> </li> <li>Exit ticket: Students write one insight they gained from using the visualization</li> </ul>"},{"location":"sims/least-squares/#extension-activities","title":"Extension Activities","text":"<ul> <li>Challenge students to create their own set of points and find the best-fitting line</li> <li>Introduce the concept of \"least squares regression\" as a mathematical way to find the best fit</li> <li>Connect to data science concepts: predictions, error measurements, and model accuracy</li> </ul>"},{"location":"sims/least-squares/#differentiation","title":"Differentiation","text":"<ul> <li>Support: Provide a step-by-step guide for using the visualization tool</li> <li>Extension: Ask advanced students to modify the code to add new features or data points</li> </ul>"},{"location":"sims/least-squares/#assessment","title":"Assessment","text":"<ul> <li>Formative: Observation during interactive exploration and pair work</li> <li>Summative: Exit ticket responses and follow-up homework assignment</li> </ul>"},{"location":"sims/least-squares/#homework","title":"Homework","text":"<ul> <li>Complete practice problems involving writing equations in slope-intercept form</li> <li>Find a real-world example where a linear relationship exists and identify what the slope and intercept represent in that context</li> </ul>"},{"location":"sims/least-squares/#follow-up-lesson-ideas","title":"Follow-Up Lesson Ideas","text":"<ul> <li>Comparing linear vs. non-linear relationships</li> <li>Introduction to systems of linear equations</li> <li>Linear regression with larger datasets</li> </ul>"},{"location":"sims/least-squares/#lesson-plan-focusing-on-prediction-of-future-events","title":"Lesson Plan focusing on Prediction of Future Events","text":""},{"location":"sims/least-squares/#learning-objectives_1","title":"Learning Objectives","text":"<p>By the end of this lesson, students will be able to: - Define slope as a rate of change - Explain the meaning of y-intercept in a linear function - Identify how changes in slope and intercept affect the graph of a line - Use an interactive visualization to explore linear functions - Use a linear model to make predictions for new x-values - Evaluate the reliability of predictions using a linear model - Solve real-world problems involving slope and intercept</p>"},{"location":"sims/least-squares/#materials-needed_1","title":"Materials Needed","text":"<ul> <li>Interactive Slope-Intercept Visualization (p5.js application)</li> <li>Student devices (computers, tablets, or smartphones)</li> <li>Guided worksheet (printed or digital)</li> <li>Whiteboard/projector</li> </ul>"},{"location":"sims/least-squares/#prerequisite-knowledge_1","title":"Prerequisite Knowledge","text":"<ul> <li>Basic understanding of coordinate plane</li> <li>Ability to plot points on a graph</li> <li>Familiarity with the equation y = mx + b</li> </ul>"},{"location":"sims/least-squares/#lesson-outline_1","title":"Lesson Outline","text":""},{"location":"sims/least-squares/#1-introduction-5-minutes_1","title":"1. Introduction (5 minutes)","text":"<ul> <li>Begin with a real-world scenario: \"If you earn $15 per hour at your job, how would you calculate your total earnings?\"</li> <li>Discuss how the relationship between hours worked and money earned forms a linear relationship</li> <li>Introduce the lesson focus: understanding how slope and intercept affect linear functions</li> </ul>"},{"location":"sims/least-squares/#2-review-of-key-concepts-10-minutes_1","title":"2. Review of Key Concepts (10 minutes)","text":"<ul> <li>Review the slope-intercept form of a line: y = mx + b</li> <li>Define slope (m) as the rate of change (rise/run)</li> <li>Define y-intercept (b) as the point where the line crosses the y-axis (0,b)</li> <li>Demonstrate examples on the board with different values for m and b</li> </ul>"},{"location":"sims/least-squares/#3-interactive-exploration-15-minutes_1","title":"3. Interactive Exploration (15 minutes)","text":"<ul> <li>Introduce the Slope-Intercept Visualization tool</li> <li>Demonstrate how to use the sliders to change slope and intercept values</li> <li>Explain the visual elements:</li> <li>Green points represent actual data points</li> <li>Purple points show where the line would predict those values</li> <li>Colored squares show the \"error\" or difference between actual and predicted points</li> <li>Guided exploration:</li> <li>What happens when the slope increases? Decreases? Becomes negative?</li> <li>What happens when the y-intercept changes?</li> <li>Can you find values that minimize the differences between actual and predicted points?</li> </ul>"},{"location":"sims/least-squares/#4-pair-work-10-minutes_1","title":"4. Pair Work (10 minutes)","text":"<ul> <li>Students work in pairs using the visualization tool</li> <li>Challenge: Find the linear function that best fits the green data points</li> <li>Each pair should record their \"best fit\" values for slope and intercept</li> <li>Discuss strategy: How can you tell when you've found a good fit?</li> </ul>"},{"location":"sims/least-squares/#5-prediction-and-real-world-applications-10-minutes","title":"5. Prediction and Real-World Applications (10 minutes)","text":"<ul> <li>Discuss how the slope-intercept model applies to:</li> <li>Economics: price vs. quantity relationships</li> <li>Physics: distance vs. time in constant velocity</li> <li>Business: fixed costs (y-intercept) and variable costs (slope)</li> <li>Show how the colored squares relate to \"error\" in predictions</li> <li>Prediction activity:</li> <li>Given our current \"best fit\" line with slope m and intercept b, what would be the predicted y-value for:<ol> <li>x = 250 (a value within our current data range)</li> <li>x = 600 (a value outside our current data range)</li> </ol> </li> <li>Discuss the concept of interpolation vs. extrapolation</li> <li>Question: \"How confident are we in these predictions and why?\"</li> <li>Question: \"What factors might affect the accuracy of our predictions?\"</li> </ul>"},{"location":"sims/least-squares/#6-closure-and-assessment-5-minutes_1","title":"6. Closure and Assessment (5 minutes)","text":"<ul> <li>Quick check for understanding: </li> <li>\"If a line has a slope of 2 and a y-intercept of -3, what is its equation?\"</li> <li>\"If a line has a negative slope, what does that tell us about the relationship?\"</li> <li>\"Using the equation y = 0.5x + 25, predict the y-value when x = 120\"</li> <li>\"How would you use our linear model to predict a new value not shown on the graph?\"</li> <li>Exit ticket: Students write one insight they gained about using linear models for prediction</li> </ul>"},{"location":"sims/least-squares/#extension-activities_1","title":"Extension Activities","text":"<ul> <li>Challenge students to create their own set of points and find the best-fitting line</li> <li>Introduce the concept of \"least squares regression\" as a mathematical way to find the best fit</li> <li>Connect to data science concepts: predictions, error measurements, and model accuracy</li> </ul>"},{"location":"sims/least-squares/#differentiation_1","title":"Differentiation","text":"<ul> <li>Support: Provide a step-by-step guide for using the visualization tool</li> <li>Extension: Ask advanced students to modify the code to add new features or data points</li> </ul>"},{"location":"sims/least-squares/#assessment_1","title":"Assessment","text":"<ul> <li>Formative: Observation during interactive exploration and pair work</li> <li>Summative: Exit ticket responses and follow-up homework assignment</li> </ul>"},{"location":"sims/least-squares/#homework_1","title":"Homework","text":"<ul> <li>Complete practice problems involving writing equations in slope-intercept form</li> <li>Find a real-world example where a linear relationship exists and identify what the slope and intercept represent in that context</li> <li>Prediction challenge: Given the linear model y = 1.5x + 10:</li> <li>Predict values for x = 50, x = 100, and x = 150</li> <li>If you measured y = 85, what would be the corresponding x value?</li> <li>Create a real-world scenario where this model might be useful, and explain what the slope and intercept represent</li> <li>Explain a situation where this model might break down or become unreliable for predictions</li> </ul>"},{"location":"sims/least-squares/#follow-up-lesson-ideas_1","title":"Follow-Up Lesson Ideas","text":"<ul> <li>Comparing linear vs. non-linear relationships</li> <li>Introduction to systems of linear equations</li> <li>Linear regression with larger datasets</li> </ul>"},{"location":"sims/template/","title":"MicroSim Template","text":""},{"location":"sims/template/#sample-iframe","title":"Sample iframe","text":"<p>You can include the following in your HTML to use this MicroSim</p> <pre><code>&lt;iframe src=\"DOC_ROOT/docs/sims/MICROSIM_NAME/main.html\" height=\"400px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Run the MicroSim Edit this MicroSim</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/microsims/sims/spectrum-analyzer/main.html\" height=\"400\"  scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/template/#about-this-microsim","title":"About this MicroSim","text":"<p>This MicroSim teaches us how to specify the drawing of a static filled circle.</p>"},{"location":"sims/template/#sample-prompt","title":"Sample Prompt","text":"<p>Prompt</p> <p>Create a single file p5.js sketch.   Draw a green circle on a 600x400 canvas with a radius of 200.</p>"},{"location":"sims/template/#sample-response","title":"Sample Response","text":"<pre><code>// canvas regions setup\n// let canvasWidth = 750;\nlet drawHeight = 400;\nlet controlHeight = 50;\nlet canvasHeight = drawHeight + controlHeight;\nlet aspectRatio = 1.91; // Open Graph standard\nlet canvasWidth = canvasHeight * aspectRatio;\nlet radius = 200;\n\nfunction setup() {\n  const canvas = createCanvas(canvasWidth, canvasHeight);\n  var mainElement = document.querySelector('main');\n  canvas.parent(mainElement);\n}\n\nfunction draw() {\n  // background of drawing region\n  fill('aliceblue');\n  rect(0, 0, canvasWidth, drawHeight);\n\n  // background of controls\n  fill('whitegray');\n  rect(0, drawHeight, canvasWidth, controlHeight);\n  circle(canvasWidth/2, canvasHeight/2, radius);\n}\n</code></pre>"},{"location":"sims/template/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/template/#grade-level-subject-and-topic","title":"Grade Level, Subject and Topic","text":"<p>9th grade geometry. Circle and radius.</p>"},{"location":"sims/template/#activities","title":"Activities","text":""},{"location":"sims/template/#fill-color","title":"Fill Color","text":"<p>Change the color in the prompt.  You can specify any one of the 140 named colors.</p>"},{"location":"sims/template/#border-weight","title":"Border Weight","text":"<p>Change the prompt to include a black border of width 10.</p>"},{"location":"sims/template/#border-color","title":"Border Color","text":"<p>Change the prompt to make the border purple.</p>"},{"location":"sims/template/#change-the-radius","title":"Change the Radius","text":"<p>Change the prompt to make the circle smaller or larger by changing the radius from 10 to 200.</p>"},{"location":"sims/template/#change-the-location","title":"Change the Location","text":"<p>Change the prompt to move the circle to the left or right.</p>"},{"location":"sims/template/#references","title":"References","text":"<ul> <li>Processing Wiki on Positioning Your Canvas</li> </ul>"}]}