# Model Evaluation and Validation

## Summary

This chapter teaches students how to properly evaluate and validate machine learning models. Students will learn about training and testing data splits, key performance metrics (R-squared, MSE, RMSE, MAE), and residual analysis. The chapter covers the critical concepts of overfitting and underfitting, the bias-variance tradeoff, and various cross-validation techniques. By the end of this chapter, students will be able to assess model quality, compare different models, and select the best model for their data.

## Concepts Covered

This chapter covers the following 25 concepts from the learning graph:

1. Model Performance
2. Training Data
3. Testing Data
4. Train Test Split
5. Validation Data
6. R-Squared
7. Adjusted R-Squared
8. Mean Squared Error
9. Root Mean Squared Error
10. Mean Absolute Error
11. Residual Analysis
12. Residual Plot
13. Overfitting
14. Underfitting
15. Bias
16. Variance
17. Bias-Variance Tradeoff
18. Model Complexity
19. Cross-Validation
20. K-Fold Cross-Validation
21. Leave One Out CV
22. Holdout Method
23. Model Selection
24. Hyperparameters
25. Model Comparison

## Prerequisites

This chapter builds on concepts from:

- [Chapter 6: Statistical Foundations](../06-statistical-foundations/index.md)
- [Chapter 7: Simple Linear Regression](../07-simple-linear-regression/index.md)

---

TODO: Generate Chapter Content
